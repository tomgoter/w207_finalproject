{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpu_fkd_cnn_filter.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Y8FlEtR_MG1u",
        "18cIN-lKMlPM",
        "2xLztoT7lsr5",
        "dQBMAFgtaifI",
        "XoHVGSxZQFE7",
        "5WlfGrDiBYGZ",
        "VxQYX1btaBRH",
        "iSsk092Pw83H",
        "7yfMbGZhXTrL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomgoter/w207_finalproject/blob/master/gpu_fkd_cnn_filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44OHv9I6LtZ8",
        "colab_type": "text"
      },
      "source": [
        "# GPU Notebook for Google Colab\n",
        "## Facial Keypoint Detection\n",
        "### W207 Final Project - Summer 2019\n",
        "#### T. P. Goter\n",
        "\n",
        "This notebook is used to run various convolutional neural networks in Google's Colab environment. This is useful as GPUs are available for free use. These GPUs have been shown to increase runtimes for the networks below by 10-20x. The use of Colab has really enable a thorough evaluation of the neural network hyperparameters and resulted in a better model in the end.\n",
        "\n",
        "#### Problem Abstract\n",
        "\n",
        "The goal of our models are to detect facial keypoint locations on input images that are 96 by 96 pixels. In order to do this detection, the modelers choose to evaluate various Convolutional Neural Networks (CNNs) with various architectures, input transformation and hyperparameter settings. These explorations and evaluations result in a final model that is used to evaluate a fixed set of test data that has not been trained on or investigated whatsoever. Through this process the modelers develop a deeper understanding of machine learning (ML), ML applications, deep learning, data preprocessing, hyperparamter tuning and model optimization.\n",
        "\n",
        "#### Data Source\n",
        "The original training and test data is provided from Kaggle, as this was a competition in 2016. This data is explored in a separate Jupyter Notebook (DataExploration) which can be found in this [GitHub repository](https://github.com/tomgoter/w207_finalproject). Through this data exploration it was determined that many of the provided images only have labels for some of the keypoints. This means there is much data that is not useful for training. This data could be labeled manually, but that would be excessively time consuming and prone to error. Additionally, not all of the images even have all of the keypoints. It was determined that initial models would only use the training examples which have labels for all 15 facial keypoints. Unfortunately this reduced the size of the data set from >7000 to ~2100. Additional discussion of the data cleaning is included in the previously linked to Jupyter Notebook. This notebook was used to clean and pickle the data (kept in a Pandas dataframe object). This pkl file was then copied to the modeler's Google Drive account to enable easy reading with Colaboratory. This file was too large to keep in GitHub.\n",
        "\n",
        "#### Scoring Metric\n",
        "Model quality is determined through the Root Mean Squared Error (RMSE) between predictions and actuals. The  equation for this calculation is given below where the sum is over every prediction (i.e., test examples multiplied by keypoints per example.) This scoring metric will also be used as the loss function (actually just he mean squared error) for the neural network training.\n",
        "\n",
        "$$RMSE = \\sqrt{\\frac{1}{n}\\cdot\\sum\\limits_{i=1}^{n}(y-\\hat{y})^2}$$\n",
        "\n",
        "#### Baseline Models\n",
        "The modeler chose to generate three baseline models from which to gage progress for more advanced models. The first of these baseline models was rudimentary. For any new test image, the mean x and y location for each keypoint was used as the keypoint location prediction. In this case the mean keypoint location was able to be determined through an average over all training data (minus those reserved as development data - ~15%) simply by ignoring the NaNs in the dataset. The resultant RMSE from this simple prediction method was 3.16. Thus, a baseline was established with plenty of room for improvement. For reference, the Public Leaderboard for the [Kaggle Competition](https://www.kaggle.com/c/facial-keypoints-detection/leaderboard) had a high score (or low in terms of RMSE) of 1.53 at the time the competition was completed.\n",
        "\n",
        "In addition to this simplistic model, the modelers also constructed a k-nearest regressor and an r-nearest regressor model with tuned hyperparameters to determine how much better than the baseline model one could get with a simple, but slightly more advanced model than the baseline. The k-nearest regressor model was implemented, and a k-value of *five* was determined to be optimal based on testing on ~15% of the data reserved as a development set. This model merely identifies the five closest images in the training set to the target image. Image proximity is determined through the euclidean distance in pixel values for all 9,216 pixels. The keypoint locations of the target image are then estimated to be the average of the k (five in our case) nearest images. Using this model, the validation set RMSE was determined to be 2.49. Thus, a substantial improvement over the baseline model was achieved with relative ease. The r-nearest regressor model follows the same basic concept as the k-nearest regressor model, but instead of predetermining the number of images from which to average keypoint locations, instead a radial \"distance\" is specified. The target image keypoints are then predicted to be the average of all images found within the pre-specified distance. For the model in question, r was determined to be optimized at an approximate distance of 12. This resulted in a RMSE on the validation set of ~2.68 which is worse than the k-nearest regressor model. Due to this depreciation in accuracy and given the additional training time of this model, it was determined that a reasonable baseline+ model was the k-nearest regressor model.\n",
        "\n",
        "#### Single Layer Neural Nets\n",
        "On the modeling path toward the CNNs, the modelers made a pit stop at the single layer neural net. This was done to get more familiarity with different training options and simply building neural nets in tensor flow (with Keras as abstraction) and optimizing neural nets. Several sensitivities to the number of hidden units, optimizer and activation function were evaluated. All of these sensitivities were interesting, but in general the modelers found that the single layer models were underperforming and overfitting. Investigating the differences in gradient descent optimizers was as interesting outcome of this pit stop, and through these evaluations further emphasis was placed mostly on the stochastic gradient descent (SGD) optimizer with Nesterov (look-ahead) momentum and the ADAM optimizer which adapts training rates for each parameter in the model.\n",
        "\n",
        "#### Double Layer Nerual Nets\n",
        "If single layer neural nets was a pit stop, the modelers foray into double layer perceptrons was more of a rolling-stop past a stop sign. Fruitful results were not obtained; overfitting and underperforming continued. The modelers determined perhaps there is a reason people are using CNNs for image detection after all. Thus, emphasis was next placed on building deeper neural nets. This is discussed in the next section.\n",
        "\n",
        "#### Convolutional Neural Nets\n",
        "\n",
        "Convolutional neural nets typically make use of a deep network architecture with limited breadth. The basic principal for image processing is to start with your image pixel values, and then convolve the image through a smaller kernel matrix such as shown in the image below. Each kernel matrix cell is initialized with a random weight, and these weights are learned by training the model. This kernel slides over the input feature matrix as shown by the image below. As this process continues the features are transformed. This transformation continues in layers. In between convolution layers the feature set size is typically decreased through a pooling layer in which another kernel matrix is used (many times 2x2) to identify either the average feature value (average pooling) or the maximum feature (max pooling) from this small grid. This process results in a greatly reduced number of dimensions (e.g., if using a 2x2 pooling layer, the dimensions are halved). This process of convolving and reducing dimensions basically allows the neural net to slowly transform the starting feature space into higher level items it can use to classify (or in this case regress). After several convolutions and poolings, the remaining dimensions are flattened and passed to one or two fully connected layers. These output layers are then used to perform the regression by narrowing down to 30 outputs. Note that for classification it is typically to see a softmax (version of a sigmoid function) as the output activation function, but for this regression problem a floating point output is desired. As such, a linear (identity) activation function is used for the final output layer.\n",
        "\n",
        "\n",
        "![alt text](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides.gif)\n",
        "\n",
        "The basic principles and conventions behind building a CNN can be summarized by the following bullets.\n",
        "\n",
        "\n",
        "\n",
        "*   Small filters are typically used first (i.e., 2x2 to 5x5 or thereabouts) to gather the local information from the base image.\n",
        "*   As we step deeper into the network, the filter width can be expanded. The theory behind this is that as we go deeper into the network we begin to explore more global features as we have already captured the local features.\n",
        "* In terms of filter depth, start with a lowish number (it seems like 8 to 32 is fairly common), and increase as depth is added\n",
        "* Layers are added until overfitting starts to occur. Overfitting can then be addressed through various methods. One can implement dropout rates or regularization fairly easily at different levels throughout the network to help address this.\n",
        "* There are many existing architectures from which to start from, as discussed below.\n",
        "\n",
        "**AlexNet Inspired Models**\n",
        "\n",
        "The modelers chose to begin with a simple AlexNet-inspired architecture which [has it roots in the early 1980s](https://www.analyticsvidhya.com/blog/2017/08/10-advanced-deep-learning-architectures-data-scientists/).  This architecture, in the grand scheme of different CNN architectures, is quite simple and easy to understand. It was the natural starting point for this regression analysis and is quite quick to train on the Colaboratory GPUs. This architecture is outlined in the figure below. The basic principle of this architecture that was followed for this analysis is to simply alternate convolution and pooling layers. For the models developed herein, three convolution-pooling layers were used. During each convolution layer, kernel depth is increased. This basically means that several independent kernels, each with unique weights to learn, are used during the training. Through this process, as shown by the image below, the feature space is reduced in the original dimensions (let's call them height and width) but increased in the depth dimension. The original feature space is replaced by these learned transformations. At each layer the user can specify kernel size, filter depth, dropout rate, and a variety of other parameters that all help in the model tuning process. During the model building discussed herein sensitivities were run to many of these hyperparameters, and in the end an optimized AlexNet model was developed. This model scored an RMSE of ~1.25 on the validation data. Unfortunately when scored against the actual Kaggle test data, it only achieved a 2.87 RMSE which would be about 53rd on the leaderboard. This result led to development and testing of additional model architectures as discussed below.\n",
        "\n",
        "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/08/08131757/temp6.png)\n",
        "\n",
        "**VGG Net Inspired Models**\n",
        "\n",
        "As an alternative to starting with something that resembles an AlexNet architecture, the modelers also decided to construct a model inspired by the [VGG Net](https://www.analyticsvidhya.com/blog/2017/08/10-advanced-deep-learning-architectures-data-scientists/). As compared to the AlexNet, the VGG Net is much deeper. Its basic idea is to layer many convolutions prior to pooling and applying dimensionality reduction. This results in more parameters to train and longer model training times. A basic representation of a VGG Net is shown in the image below. Overall the results of the VGG Nets were comparable to the AlexNet-inspired models, but the additional runtime (~factor of 8) from the VGG-inspired nets was basically just wasted GPU resources. This path was investigated for a few different sensitivities, but after non-inspiring results, it was abandoned in favor of additional AlexNet studies. For reference, the modelers' best VGG-inspired net resulted in an RMSE of 2.92 on the Kaggle test data.\n",
        "\n",
        "![alt text](https://tech.showmax.com/2017/10/convnet-architectures/image_0-8fa3b810.png)\n",
        "\n",
        "**Specialist Models**\n",
        "\n",
        "The AlexNet results both in terms of efficiency and accuracy led the modelers to continue building more specialized model. Instead of building one model that tries to accurately locate the x- and y-coordinates of all 15 facial keypoints, the decision was made to instead build 15 models. There are two significant benefits and one significant deleterious effect of this decision.  First, the good parts:\n",
        "\n",
        "1. Specialized models will allow the modelers to use all the training data. As previously noted, there were only about 2100 examples with all locations labeled. However some keypoints had > 7000 labels. Thus, the dataset for the combined models was much smaller than it really could be if all data were used. There should be a benefit to using all of the training data for the creation of these specialized models.\n",
        "2. Specialized models were expected to be more accurate, even without additional training data, because the point of the entire network is only to identify a single keypoint. Thus, all of the training and learning done by the model is hyper-focused on the individual output. This also allows the modeler to tune hyperparameters for different keypoints, as necessary.\n",
        "\n",
        "But of course the downside must also be discussed.\n",
        "\n",
        "The specialized models will increase our model training time by at least a factor of 15. In addition to this, if the modelers are to tune each model, the amount of hands-on time is greatly increased. This is not an insignificant challenge to overcome. The modelers tried to balance this downside by first running all 15 models, starting with the combined model weights and hyperparameters. This method by itself resulted in very significant improvements in generalization of the model. The predictions from this model were evaluated against the Kaggle test set and determined to have an RMSE of 2.18 - an improvement of ~0.7 from the combined AlexNet model!\n",
        "\n",
        "The accuracies of the specialized models were then inspected and ranked and focused effort was placed on improving the worst performing models, as shown below. This ranking allowed the modelers to determine which keypoints to prioritize. From the list below, it appears that significant benefit could come just from improving the predictions of the first two keypoints (i.e., mouth_center_bottom_lip and nose_tip).\n",
        "\n",
        "1.   mouth_center_bottom_lip 2.90\n",
        "2.   nose_tip  2.65\n",
        "3.   left_eye_center 2.04\n",
        "4.   right_eye_center           1.94\n",
        "5.   left_eyebrow_outer_end     1.86\n",
        "6.   right_eyebrow_outer_end    1.72\n",
        "7.   right_eyebrow_inner_end    1.64\n",
        "8.   mouth_right_corner         1.54\n",
        "9. left_eyebrow_inner_end     1.51\n",
        "10. mouth_center_top_lip       1.48\n",
        "11. mouth_left_corner          1.42\n",
        "12. right_eye_outer_corner     1.40\n",
        "13. left_eye_outer_corner      1.30\n",
        "14. right_eye_inner_corner     1.10\n",
        "15. left_eye_inner_corner      1.05\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko5fAr9dK0bh",
        "colab_type": "code",
        "outputId": "e4e7ce00-90fe-4898-8c13-5e936fd1b39d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Import tensorflow and check the version - we will be using version 1.14\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# Import needed packages\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow.keras import models, layers, callbacks\n",
        "from tensorflow.keras import optimizers, metrics\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import pprint\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G8PiDIEMewA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean up your session\n",
        "tf.keras.backend.clear_session()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8FlEtR_MG1u",
        "colab_type": "text"
      },
      "source": [
        "## Link to Google Drive Account to get access to our dataset\n",
        "\n",
        "Colaboratory allows users to link directly to their Google Drive. So that makes it relatively easy to access our training data and save models as we run our studies. We separately manage our data between GitHub and Google Drive to stay synchronized and maintain version control."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4iBlsB3MGkV",
        "colab_type": "code",
        "outputId": "1e28b1d7-76cf-44b6-ad48-6c01559494f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Mounting the drive is straightforward but required authentication each time \n",
        "# we reset the session\n",
        "drive.mount('/content/drive')\n",
        "drive_path = '/content/drive/My Drive/FacialKeypointDetection/'\n",
        "! ls /content/drive/My\\ Drive/FacialKeypointDetection/OutputData\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "4l_spec_left_eye_center_d0.2_s0.15_sf20_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_left_eye_center_d0.2_s0.2_sf20_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.05_s0.1_sf8_lrf10_fc1200_fc250_kern2334.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.05_s0.1_sf8_lrf10_fc150_fc250_kern2334.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.0_sf4_lrf1_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.0_sf4_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.1_sf16_lrf10_fc1100_fc250_kern2334.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.1_sf16_lrf10_fc1100_fc250.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.1_sf16_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.1_sf4_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.15_s0.15_sf12_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.15_s0.15_sf16_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.12_sf12_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.15_sf12_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf12_lrf10_fc1100_fc250.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf12_lrf10_fc1200_fc250.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf16_lrf10_fc1200_fc250.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf2_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf10_fc1200_fc2200_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1100_fc2100_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1100_fc2100_kern5432.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1150_fc2150_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1200_fc2200_kern2222.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1200_fc2200_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1200_fc2200_kern5432.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1300_fc2300_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf30_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf6_lrf10_fc1100_fc2100_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf8_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.25_s0.15_sf24_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.05_sf16_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.15_sf20_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.17_sf24_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf6_lrf10_fc1100_fc2100_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf6_lrf10_fc1100_fc2100_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf6_lrf10_fc1100_fc250_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf6_lrf10_fc150_fc250_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf8_lrf10_fc1100_fc250_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf8_lrf10_fc150_fc250_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip.pkl\n",
            "4l_spec_nose_tip_d0.0_s0.1_sf16_lrf10_fc1100_fc250_kern2334.pkl\n",
            "4l_spec_nose_tip_d0.2_s0.15_sf20_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.05_s0.15_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.0_s0.15_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.0_s0.17_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.15_s0.1_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.1_s0.15_sf12_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.1_s0.1_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "cnn_flipped_df2.pkl\n",
            "cnn_flipped_df3.pkl\n",
            "cnn_flipped_df4.pkl\n",
            "cnn_flipped_df5.pkl\n",
            "cnn_flipped_df6.pkl\n",
            "cnn_flipped_df.pkl\n",
            "cnn_lr_df.pkl\n",
            "cnn_stride_df.pkl\n",
            "cnn_vgg_df.pkl\n",
            "cnn_vgg_flipped2_df.pkl\n",
            "cnn_vgg_flipped3_df.pkl\n",
            "cnn_vgg_flipped4_df.pkl\n",
            "cnn_vgg_flipped_df.pkl\n",
            "cnn_vgg_lr_df.pkl\n",
            "single_layer_df.pkl\n",
            "spec_01.pkl\n",
            "spec_left_eyebrow_inner_end.pkl\n",
            "spec_left_eyebrow_outer_end.pkl\n",
            "spec_left_eye_center.pkl\n",
            "spec_left_eye_inner_corner.pkl\n",
            "spec_left_eye_outer_corner.pkl\n",
            "spec_mouth_center_bottom_lip.pkl\n",
            "spec_mouth_center_top_lip.pkl\n",
            "spec_mouth_left_corner.pkl\n",
            "spec_mouth_right_corner.pkl\n",
            "spec_nose_tip.pkl\n",
            "spec_right_eyebrow_inner_end.pkl\n",
            "spec_right_eyebrow_outer_end.pkl\n",
            "spec_right_eye_center.pkl\n",
            "spec_right_eye_inner_corner.pkl\n",
            "spec_right_eye_outer_corner.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18cIN-lKMlPM",
        "colab_type": "text"
      },
      "source": [
        "## Import our data from our drive\n",
        "- Load in the pickle file that was created as part of the EDA in DataExploration.ipynb.\n",
        "- This dataset has the NaNs removed and a few mislabeled images removed as well.\n",
        "- As such there is only limited training and development data to use.\n",
        "- The image data has already been normalized to [0,1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LBGEvacw1ge",
        "colab_type": "code",
        "outputId": "6484127b-1df0-4a81-9e9a-234dc7584d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "! ls /content/drive/My\\ Drive/FacialKeypointDetection/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_nostache_nonan.pkl\t      df_nostache_w_flip.pkl  OutputData\n",
            "df_nostache_nonan_w_flip.pkl  Models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyZy0sLnMlYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize Random Seed for reproducibility\n",
        "np.random.seed(13)\n",
        "\n",
        "# Load the dataframe from the pickle file\n",
        "df_nostache_nonan = pd.read_pickle(drive_path + \"df_nostache_nonan.pkl\")\n",
        "\n",
        "# Grab the last column - that is our image data for X matrix\n",
        "X = df_nostache_nonan.iloc[:, -1]\n",
        "\n",
        "# Convert from a series of arrays to an NDarray\n",
        "X = np.array([x.reshape(96,96,1) for x in X])\n",
        "\n",
        "# Grab the keypoints and stick into our y-variable\n",
        "y = np.array(df_nostache_nonan.iloc[:,:-1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oySY-Ro3QlTH",
        "colab_type": "text"
      },
      "source": [
        "## Setup our Optimizer\n",
        "\n",
        "Based on our previous studies (see cnn_notebook_tpg for details), we will focus efforts with the stochastic gradient descent (SGD - epoch based updates) and the ADAM optimizer (parameter specific gradients with decaying learning rate based on average of past gradients and squared gradients). See the image below for the results of an early optimizer study. The image shows that the Adam and SGD optimizer perform similarly. The adagrad optimizer also converges to about the same accuracy as the other two but appears to take a bit longer to get there. It was also shown to be unstable in terms of epoch training time (note the steep increase in cumulative training time for this optimizer in the third subplot). For these reasons (and to whittle down the number of parameters we continue to evaluate) we did not further consider the adagrad optimizer.\n",
        "\n",
        "![alt text](https://github.com/tomgoter/w207_finalproject/raw/master/Images/optimizer.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJx2btJLQleM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the adam optimizer with the default learning rate\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "# Set up an sgd optimizer as well\n",
        "sgd = optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xLztoT7lsr5",
        "colab_type": "text"
      },
      "source": [
        "## Create a Epoch Timing Callback\n",
        "\n",
        "In addition to using RMSE as a metric to evaluate our models against, we also want to track model training time. If a factor of 10 increase in training time leads to a small accuracy benefit we will tend to gravitate toward the more efficient model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7wupCExb6Y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TimeHistory(callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, batch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQBMAFgtaifI",
        "colab_type": "text"
      },
      "source": [
        "## Create a CNN Model\n",
        "\n",
        "We are going to change the filter strategy from our previous CNN (see cnn_notebook_tpg.ipynb [GitHub Repo (https://github.com/tomgoter/w207_finalproject)) which used the following:\n",
        "\n",
        "32 filters --> 64 filters --> 128 filters\n",
        "\n",
        "Let's halve the number of filters and determine the effects on accuracy and run time for our model.\n",
        "\n",
        "The function below is simple and creates a fixed CNN with three convolute/pool layers and two fully connected layers before the output layer. This model is really just to get our feet wet with building a \"deep\" neural net using tensor flow and keras. Later we will parameterize our function to enable easy, straightforward sensitivity evaluations. But for this first go around, let's keep it simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMjVozNKBX24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a function that will automate the CNN model generation\n",
        "def create_cnn_model():\n",
        "  '''\n",
        "  Simple function that retruns a keras cnn model\n",
        "  Alternate between convolution and pooling\n",
        "  No dropout or regularization\n",
        "  Valid padding during convolution\n",
        "  500 hidden units in fully connected layers\n",
        "  '''\n",
        "  \n",
        "  # Instantiate our model as a Sequential model\n",
        "  cnn_model = tf.keras.models.Sequential()\n",
        "  \n",
        "  # First layer has 96,96,1 dimensions - our image is 96 by 96 pixels and we omit color channels (greyscale)\n",
        "  cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "  \n",
        "  # After this layer we will have 94,94,16 - the valid (or no) padding will reduce the size of our input\n",
        "  # dimensions\n",
        "  cnn_model.add(tf.keras.layers.Conv2D(16, (3, 3), padding='valid', activation='relu'))\n",
        "  \n",
        "  # After this pooling layer we will have 47,47,16 - pooling with a 2,2 kernel will halve our dimensions\n",
        "  cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # After this convolution layer we will have 46,46,32\n",
        "  # Again the convolution layer is not padded so we reduce in dimensionality\n",
        "  cnn_model.add(tf.keras.layers.Conv2D(32, (2, 2), padding='valid', activation='relu'))\n",
        "  \n",
        "  # After this pooling layer we will have 23,23,32 - max pooling with 2x2 kernel\n",
        "  cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # After this convolution layer we will have 22,22,64\n",
        "  # Valid padding - reduce dimensionality\n",
        "  cnn_model.add(tf.keras.layers.Conv2D(64, (2, 2), padding='valid', activation='relu'))\n",
        "  \n",
        "  # After this pooling layer we will have 23,23,64\n",
        "  # Max pooling 2x2 - halve dimension space\n",
        "  cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # When we flatten we have 33,856 nodes\n",
        "  cnn_model.add(tf.keras.layers.Flatten())\n",
        "  \n",
        "  # Reduce to 500 hidden units\n",
        "  cnn_model.add(tf.keras.layers.Dense(500))\n",
        "  cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "  \n",
        "  # Second fully connected layer\n",
        "  cnn_model.add(tf.keras.layers.Dense(500))\n",
        "  cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "  \n",
        "  # Output layer is size 30 - equal to the number of keypoint coordinates we are predicting\n",
        "  cnn_model.add(tf.keras.layers.Dense(30))\n",
        "  cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "  \n",
        "  print(50*\"=\")\n",
        "  print(cnn_model.summary())\n",
        "  print(50*\"=\")\n",
        "  \n",
        "  return cnn_model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoHVGSxZQFE7",
        "colab_type": "text"
      },
      "source": [
        "##Construct the model on the TPU\n",
        "\n",
        "Attempts were made to run our models on the TPU, and the following code is required to do so. Although the models were able to run on the TPU, there appeared to be no performance benefit relative to the GPU which was unexpected. However, as we were getting reasonable performance out of the GPU we did not further troubleshoot the TPU application. The cell below was retained in case we did want to do further evaluations on the TPU.\n",
        "\n",
        "The following information is useful to know when setting up our model for running on a TPU:\n",
        "\n",
        "**batch_size**\n",
        "Determines the number of samples in each mini batch. Its maximum is the number of all samples, which makes gradient descent accurate, the loss will decrease towards the minimum if the learning rate is small enough, but iterations are slower. Its minimum is 1, resulting in stochastic gradient descent: Fast but the direction of the gradient step is based only on one example, the loss may jump around. batch_size allows to adjust between the two extremes: accurate gradient direction and fast iteration. Also, the maximum value for batch_size may be limited if your model + data set does not fit into the available (GPU) memory.\n",
        "\n",
        "**steps_per_epoch**\n",
        "The number of batch iterations before a training epoch is considered finished. If you have a training set of fixed size you can ignore it but it may be useful if you have a huge data set or if you are generating random data augmentations on the fly, i.e. if your training set has a (generated) infinite size. If you have the time to go through your whole training data set I recommend to skip this parameter.\n",
        "\n",
        "**validation_steps**\n",
        "similar to steps_per_epoch but on the validation data set instead on the training data. If you have the time to go through your whole validation data set I recommend to skip this parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL9gcVwUMSdR",
        "colab_type": "code",
        "outputId": "06eb286e-a020-46a5-b00c-dca34aaf8d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "# Grab the hardware address of the TPU and get a list of all of the assigned devices\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:' + devices)\n",
        "\n",
        "# Access the tpu and create a distribution strategy\n",
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_address)\n",
        "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "\n",
        "# Compile the model using the TPU distribution strategy\n",
        "with strategy.scope():\n",
        "  model = create_cnn_model()\n",
        "  model.compile(\n",
        "      optimizer=adam,\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['mean_squared_error'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0719 18:13:38.583853 140366703122304 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 94, 94, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 46, 46, 32)        2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 22, 22, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,148,526\n",
            "Trainable params: 4,148,526\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6OnteR3ZZCl",
        "colab_type": "code",
        "outputId": "58adf5b3-d54f-4831-995a-bbc3424aa1d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "model = create_cnn_model()\n",
        "model.compile(\n",
        "      optimizer=adam,\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['mean_squared_error'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 16:38:04.514947 139916523657088 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 94, 94, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 46, 46, 32)        2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 22, 22, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,148,526\n",
            "Trainable params: 4,148,526\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WlfGrDiBYGZ",
        "colab_type": "text"
      },
      "source": [
        "## Try out our first CNN Model\n",
        "\n",
        "The code below is a simple example of how a model is run on a GPU. The output from this model is basically our baseline CNN. Storing the results of the model fitting to the history object allows us to access epoch data such as timing and loss data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jv9cmYfNA9Q",
        "colab_type": "code",
        "outputId": "6e64596f-8136-4914-fd84-f3a2b7e9a21d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fit the basic CNN\n",
        "history = model.fit(\n",
        "    train_X.astype(np.float32), train_y.astype(np.float32),\n",
        "    epochs=200,\n",
        "    validation_split=0.15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1523 samples, validate on 269 samples\n",
            "Epoch 1/200\n",
            "1523/1523 [==============================] - 4s 3ms/sample - loss: 2186.5384 - mean_squared_error: 2186.5391 - val_loss: 85.1574 - val_mean_squared_error: 85.1573\n",
            "Epoch 2/200\n",
            "1523/1523 [==============================] - 0s 270us/sample - loss: 21.9787 - mean_squared_error: 21.9787 - val_loss: 12.5287 - val_mean_squared_error: 12.5287\n",
            "Epoch 3/200\n",
            "1523/1523 [==============================] - 0s 267us/sample - loss: 11.4104 - mean_squared_error: 11.4104 - val_loss: 11.7648 - val_mean_squared_error: 11.7648\n",
            "Epoch 4/200\n",
            "1523/1523 [==============================] - 0s 258us/sample - loss: 10.9332 - mean_squared_error: 10.9332 - val_loss: 11.3901 - val_mean_squared_error: 11.3901\n",
            "Epoch 5/200\n",
            "1523/1523 [==============================] - 0s 260us/sample - loss: 10.8059 - mean_squared_error: 10.8059 - val_loss: 13.0320 - val_mean_squared_error: 13.0320\n",
            "Epoch 6/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 10.9308 - mean_squared_error: 10.9308 - val_loss: 11.2774 - val_mean_squared_error: 11.2774\n",
            "Epoch 7/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 10.9633 - mean_squared_error: 10.9633 - val_loss: 12.3929 - val_mean_squared_error: 12.3929\n",
            "Epoch 8/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 10.8163 - mean_squared_error: 10.8163 - val_loss: 11.5844 - val_mean_squared_error: 11.5844\n",
            "Epoch 9/200\n",
            "1523/1523 [==============================] - 0s 246us/sample - loss: 10.5549 - mean_squared_error: 10.5549 - val_loss: 11.3282 - val_mean_squared_error: 11.3282\n",
            "Epoch 10/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 10.7388 - mean_squared_error: 10.7388 - val_loss: 11.6792 - val_mean_squared_error: 11.6792\n",
            "Epoch 11/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 10.8855 - mean_squared_error: 10.8855 - val_loss: 12.5035 - val_mean_squared_error: 12.5035\n",
            "Epoch 12/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.5909 - mean_squared_error: 11.5909 - val_loss: 14.0177 - val_mean_squared_error: 14.0177\n",
            "Epoch 13/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 11.2929 - mean_squared_error: 11.2929 - val_loss: 11.3373 - val_mean_squared_error: 11.3373\n",
            "Epoch 14/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 11.1431 - mean_squared_error: 11.1431 - val_loss: 11.4211 - val_mean_squared_error: 11.4211\n",
            "Epoch 15/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.2539 - mean_squared_error: 11.2539 - val_loss: 11.2227 - val_mean_squared_error: 11.2227\n",
            "Epoch 16/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.9072 - mean_squared_error: 10.9072 - val_loss: 11.3502 - val_mean_squared_error: 11.3502\n",
            "Epoch 17/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.8393 - mean_squared_error: 10.8393 - val_loss: 11.3428 - val_mean_squared_error: 11.3428\n",
            "Epoch 18/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.1583 - mean_squared_error: 11.1583 - val_loss: 13.3583 - val_mean_squared_error: 13.3583\n",
            "Epoch 19/200\n",
            "1523/1523 [==============================] - 0s 258us/sample - loss: 12.4589 - mean_squared_error: 12.4589 - val_loss: 12.9226 - val_mean_squared_error: 12.9226\n",
            "Epoch 20/200\n",
            "1523/1523 [==============================] - 0s 247us/sample - loss: 11.2134 - mean_squared_error: 11.2134 - val_loss: 12.2681 - val_mean_squared_error: 12.2681\n",
            "Epoch 21/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 10.6060 - mean_squared_error: 10.6060 - val_loss: 11.3355 - val_mean_squared_error: 11.3356\n",
            "Epoch 22/200\n",
            "1523/1523 [==============================] - 0s 247us/sample - loss: 10.7449 - mean_squared_error: 10.7449 - val_loss: 11.9970 - val_mean_squared_error: 11.9970\n",
            "Epoch 23/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 10.8729 - mean_squared_error: 10.8729 - val_loss: 11.3347 - val_mean_squared_error: 11.3347\n",
            "Epoch 24/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 10.5434 - mean_squared_error: 10.5434 - val_loss: 11.2721 - val_mean_squared_error: 11.2721\n",
            "Epoch 25/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.9073 - mean_squared_error: 10.9073 - val_loss: 11.3886 - val_mean_squared_error: 11.3886\n",
            "Epoch 26/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 11.6348 - mean_squared_error: 11.6348 - val_loss: 11.8605 - val_mean_squared_error: 11.8605\n",
            "Epoch 27/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.1554 - mean_squared_error: 11.1554 - val_loss: 12.4512 - val_mean_squared_error: 12.4512\n",
            "Epoch 28/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.2434 - mean_squared_error: 11.2434 - val_loss: 14.1728 - val_mean_squared_error: 14.1728\n",
            "Epoch 29/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 11.2802 - mean_squared_error: 11.2802 - val_loss: 11.4020 - val_mean_squared_error: 11.4020\n",
            "Epoch 30/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.9479 - mean_squared_error: 10.9479 - val_loss: 11.7527 - val_mean_squared_error: 11.7527\n",
            "Epoch 31/200\n",
            "1523/1523 [==============================] - 0s 246us/sample - loss: 11.5825 - mean_squared_error: 11.5825 - val_loss: 11.0840 - val_mean_squared_error: 11.0840\n",
            "Epoch 32/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 10.5958 - mean_squared_error: 10.5958 - val_loss: 11.1692 - val_mean_squared_error: 11.1692\n",
            "Epoch 33/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 11.1690 - mean_squared_error: 11.1690 - val_loss: 11.5418 - val_mean_squared_error: 11.5418\n",
            "Epoch 34/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 10.8846 - mean_squared_error: 10.8846 - val_loss: 11.6986 - val_mean_squared_error: 11.6986\n",
            "Epoch 35/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 11.7534 - mean_squared_error: 11.7534 - val_loss: 13.9421 - val_mean_squared_error: 13.9421\n",
            "Epoch 36/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.3234 - mean_squared_error: 11.3234 - val_loss: 11.2780 - val_mean_squared_error: 11.2780\n",
            "Epoch 37/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 11.4605 - mean_squared_error: 11.4605 - val_loss: 10.9001 - val_mean_squared_error: 10.9001\n",
            "Epoch 38/200\n",
            "1523/1523 [==============================] - 0s 244us/sample - loss: 12.0751 - mean_squared_error: 12.0751 - val_loss: 16.1726 - val_mean_squared_error: 16.1726\n",
            "Epoch 39/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 11.8547 - mean_squared_error: 11.8547 - val_loss: 10.8940 - val_mean_squared_error: 10.8940\n",
            "Epoch 40/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 10.8181 - mean_squared_error: 10.8181 - val_loss: 12.0226 - val_mean_squared_error: 12.0226\n",
            "Epoch 41/200\n",
            "1523/1523 [==============================] - 0s 247us/sample - loss: 11.7239 - mean_squared_error: 11.7239 - val_loss: 11.3769 - val_mean_squared_error: 11.3769\n",
            "Epoch 42/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 11.2483 - mean_squared_error: 11.2483 - val_loss: 10.6776 - val_mean_squared_error: 10.6776\n",
            "Epoch 43/200\n",
            "1523/1523 [==============================] - 0s 246us/sample - loss: 11.9855 - mean_squared_error: 11.9855 - val_loss: 11.9220 - val_mean_squared_error: 11.9220\n",
            "Epoch 44/200\n",
            "1523/1523 [==============================] - 0s 247us/sample - loss: 10.5701 - mean_squared_error: 10.5701 - val_loss: 11.0283 - val_mean_squared_error: 11.0283\n",
            "Epoch 45/200\n",
            "1523/1523 [==============================] - 0s 260us/sample - loss: 10.6288 - mean_squared_error: 10.6288 - val_loss: 10.6101 - val_mean_squared_error: 10.6101\n",
            "Epoch 46/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.3491 - mean_squared_error: 10.3491 - val_loss: 11.7852 - val_mean_squared_error: 11.7852\n",
            "Epoch 47/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 10.7313 - mean_squared_error: 10.7313 - val_loss: 12.0018 - val_mean_squared_error: 12.0018\n",
            "Epoch 48/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 10.9584 - mean_squared_error: 10.9584 - val_loss: 17.6078 - val_mean_squared_error: 17.6078\n",
            "Epoch 49/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 10.5421 - mean_squared_error: 10.5421 - val_loss: 10.2922 - val_mean_squared_error: 10.2922\n",
            "Epoch 50/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.6156 - mean_squared_error: 10.6156 - val_loss: 10.8067 - val_mean_squared_error: 10.8067\n",
            "Epoch 51/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 10.6448 - mean_squared_error: 10.6448 - val_loss: 10.5169 - val_mean_squared_error: 10.5169\n",
            "Epoch 52/200\n",
            "1523/1523 [==============================] - 0s 246us/sample - loss: 9.6362 - mean_squared_error: 9.6362 - val_loss: 10.2055 - val_mean_squared_error: 10.2055\n",
            "Epoch 53/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 10.2991 - mean_squared_error: 10.2991 - val_loss: 10.6341 - val_mean_squared_error: 10.6341\n",
            "Epoch 54/200\n",
            "1523/1523 [==============================] - 0s 247us/sample - loss: 10.0194 - mean_squared_error: 10.0194 - val_loss: 11.9481 - val_mean_squared_error: 11.9481\n",
            "Epoch 55/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.4444 - mean_squared_error: 10.4444 - val_loss: 9.8573 - val_mean_squared_error: 9.8573\n",
            "Epoch 56/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 9.8739 - mean_squared_error: 9.8739 - val_loss: 10.1485 - val_mean_squared_error: 10.1485\n",
            "Epoch 57/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 9.8409 - mean_squared_error: 9.8409 - val_loss: 10.1568 - val_mean_squared_error: 10.1568\n",
            "Epoch 58/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 11.7345 - mean_squared_error: 11.7345 - val_loss: 11.1454 - val_mean_squared_error: 11.1454\n",
            "Epoch 59/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 9.1059 - mean_squared_error: 9.1059 - val_loss: 9.4817 - val_mean_squared_error: 9.4817\n",
            "Epoch 60/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 9.7425 - mean_squared_error: 9.7425 - val_loss: 9.3625 - val_mean_squared_error: 9.3625\n",
            "Epoch 61/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 9.1704 - mean_squared_error: 9.1704 - val_loss: 9.6178 - val_mean_squared_error: 9.6178\n",
            "Epoch 62/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 9.4638 - mean_squared_error: 9.4638 - val_loss: 9.0279 - val_mean_squared_error: 9.0279\n",
            "Epoch 63/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 8.9360 - mean_squared_error: 8.9360 - val_loss: 9.3182 - val_mean_squared_error: 9.3182\n",
            "Epoch 64/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 8.5014 - mean_squared_error: 8.5014 - val_loss: 9.5323 - val_mean_squared_error: 9.5323\n",
            "Epoch 65/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 8.5894 - mean_squared_error: 8.5894 - val_loss: 9.0202 - val_mean_squared_error: 9.0202\n",
            "Epoch 66/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 8.7035 - mean_squared_error: 8.7035 - val_loss: 11.0113 - val_mean_squared_error: 11.0113\n",
            "Epoch 67/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 8.6125 - mean_squared_error: 8.6125 - val_loss: 9.6424 - val_mean_squared_error: 9.6424\n",
            "Epoch 68/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 8.7846 - mean_squared_error: 8.7846 - val_loss: 9.7801 - val_mean_squared_error: 9.7801\n",
            "Epoch 69/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 9.1472 - mean_squared_error: 9.1472 - val_loss: 10.3220 - val_mean_squared_error: 10.3220\n",
            "Epoch 70/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 9.6579 - mean_squared_error: 9.6579 - val_loss: 9.1459 - val_mean_squared_error: 9.1459\n",
            "Epoch 71/200\n",
            "1523/1523 [==============================] - 0s 259us/sample - loss: 7.9400 - mean_squared_error: 7.9400 - val_loss: 12.5291 - val_mean_squared_error: 12.5291\n",
            "Epoch 72/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 8.7742 - mean_squared_error: 8.7742 - val_loss: 10.1852 - val_mean_squared_error: 10.1852\n",
            "Epoch 73/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.9332 - mean_squared_error: 11.9332 - val_loss: 8.5253 - val_mean_squared_error: 8.5253\n",
            "Epoch 74/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 8.0548 - mean_squared_error: 8.0548 - val_loss: 8.3496 - val_mean_squared_error: 8.3496\n",
            "Epoch 75/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 7.6513 - mean_squared_error: 7.6513 - val_loss: 8.3298 - val_mean_squared_error: 8.3298\n",
            "Epoch 76/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 7.6619 - mean_squared_error: 7.6619 - val_loss: 8.1450 - val_mean_squared_error: 8.1450\n",
            "Epoch 77/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 7.6146 - mean_squared_error: 7.6146 - val_loss: 8.0492 - val_mean_squared_error: 8.0492\n",
            "Epoch 78/200\n",
            "1523/1523 [==============================] - 0s 247us/sample - loss: 7.5626 - mean_squared_error: 7.5626 - val_loss: 11.0281 - val_mean_squared_error: 11.0281\n",
            "Epoch 79/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 7.3256 - mean_squared_error: 7.3256 - val_loss: 7.7061 - val_mean_squared_error: 7.7061\n",
            "Epoch 80/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 6.9129 - mean_squared_error: 6.9129 - val_loss: 7.3109 - val_mean_squared_error: 7.3109\n",
            "Epoch 81/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 6.5433 - mean_squared_error: 6.5433 - val_loss: 7.1938 - val_mean_squared_error: 7.1938\n",
            "Epoch 82/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 6.1104 - mean_squared_error: 6.1104 - val_loss: 7.0612 - val_mean_squared_error: 7.0612\n",
            "Epoch 83/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 6.4696 - mean_squared_error: 6.4696 - val_loss: 6.8015 - val_mean_squared_error: 6.8015\n",
            "Epoch 84/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 6.1186 - mean_squared_error: 6.1186 - val_loss: 7.0542 - val_mean_squared_error: 7.0542\n",
            "Epoch 85/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 6.0993 - mean_squared_error: 6.0993 - val_loss: 7.1176 - val_mean_squared_error: 7.1176\n",
            "Epoch 86/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 6.4485 - mean_squared_error: 6.4485 - val_loss: 6.5254 - val_mean_squared_error: 6.5254\n",
            "Epoch 87/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 6.0172 - mean_squared_error: 6.0172 - val_loss: 6.3240 - val_mean_squared_error: 6.3240\n",
            "Epoch 88/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 5.6972 - mean_squared_error: 5.6972 - val_loss: 6.2878 - val_mean_squared_error: 6.2878\n",
            "Epoch 89/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 5.8656 - mean_squared_error: 5.8656 - val_loss: 6.1162 - val_mean_squared_error: 6.1162\n",
            "Epoch 90/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 5.8477 - mean_squared_error: 5.8477 - val_loss: 5.9035 - val_mean_squared_error: 5.9035\n",
            "Epoch 91/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 5.1237 - mean_squared_error: 5.1237 - val_loss: 6.0044 - val_mean_squared_error: 6.0044\n",
            "Epoch 92/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 5.8330 - mean_squared_error: 5.8330 - val_loss: 5.8483 - val_mean_squared_error: 5.8483\n",
            "Epoch 93/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 5.8411 - mean_squared_error: 5.8411 - val_loss: 6.2803 - val_mean_squared_error: 6.2803\n",
            "Epoch 94/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 5.0654 - mean_squared_error: 5.0654 - val_loss: 5.7353 - val_mean_squared_error: 5.7353\n",
            "Epoch 95/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 5.6155 - mean_squared_error: 5.6155 - val_loss: 6.0768 - val_mean_squared_error: 6.0768\n",
            "Epoch 96/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 5.3107 - mean_squared_error: 5.3107 - val_loss: 5.4001 - val_mean_squared_error: 5.4001\n",
            "Epoch 97/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 4.6326 - mean_squared_error: 4.6326 - val_loss: 5.3812 - val_mean_squared_error: 5.3812\n",
            "Epoch 98/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 5.0380 - mean_squared_error: 5.0380 - val_loss: 5.6562 - val_mean_squared_error: 5.6562\n",
            "Epoch 99/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 4.9512 - mean_squared_error: 4.9512 - val_loss: 5.3829 - val_mean_squared_error: 5.3829\n",
            "Epoch 100/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 4.7277 - mean_squared_error: 4.7277 - val_loss: 5.9268 - val_mean_squared_error: 5.9268\n",
            "Epoch 101/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 5.5963 - mean_squared_error: 5.5963 - val_loss: 5.3027 - val_mean_squared_error: 5.3027\n",
            "Epoch 102/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 4.9725 - mean_squared_error: 4.9725 - val_loss: 5.7850 - val_mean_squared_error: 5.7850\n",
            "Epoch 103/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 4.6928 - mean_squared_error: 4.6928 - val_loss: 5.3264 - val_mean_squared_error: 5.3264\n",
            "Epoch 104/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 4.3827 - mean_squared_error: 4.3827 - val_loss: 5.8130 - val_mean_squared_error: 5.8130\n",
            "Epoch 105/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 4.0511 - mean_squared_error: 4.0511 - val_loss: 5.8714 - val_mean_squared_error: 5.8714\n",
            "Epoch 106/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 4.2892 - mean_squared_error: 4.2892 - val_loss: 4.9869 - val_mean_squared_error: 4.9869\n",
            "Epoch 107/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 4.1399 - mean_squared_error: 4.1399 - val_loss: 5.3128 - val_mean_squared_error: 5.3128\n",
            "Epoch 108/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 3.9579 - mean_squared_error: 3.9579 - val_loss: 4.9781 - val_mean_squared_error: 4.9781\n",
            "Epoch 109/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 4.1932 - mean_squared_error: 4.1932 - val_loss: 5.5343 - val_mean_squared_error: 5.5343\n",
            "Epoch 110/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 4.1194 - mean_squared_error: 4.1194 - val_loss: 4.5430 - val_mean_squared_error: 4.5430\n",
            "Epoch 111/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 3.6807 - mean_squared_error: 3.6807 - val_loss: 4.4029 - val_mean_squared_error: 4.4029\n",
            "Epoch 112/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 3.4742 - mean_squared_error: 3.4742 - val_loss: 4.8416 - val_mean_squared_error: 4.8416\n",
            "Epoch 113/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 4.1988 - mean_squared_error: 4.1988 - val_loss: 6.0491 - val_mean_squared_error: 6.0491\n",
            "Epoch 114/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 3.7302 - mean_squared_error: 3.7302 - val_loss: 4.2168 - val_mean_squared_error: 4.2168\n",
            "Epoch 115/200\n",
            "1523/1523 [==============================] - 0s 257us/sample - loss: 3.9120 - mean_squared_error: 3.9120 - val_loss: 4.2305 - val_mean_squared_error: 4.2305\n",
            "Epoch 116/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 3.5498 - mean_squared_error: 3.5498 - val_loss: 4.8664 - val_mean_squared_error: 4.8664\n",
            "Epoch 117/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 3.8020 - mean_squared_error: 3.8020 - val_loss: 4.2361 - val_mean_squared_error: 4.2361\n",
            "Epoch 118/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 3.3981 - mean_squared_error: 3.3981 - val_loss: 4.3540 - val_mean_squared_error: 4.3540\n",
            "Epoch 119/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 3.3746 - mean_squared_error: 3.3746 - val_loss: 4.2422 - val_mean_squared_error: 4.2422\n",
            "Epoch 120/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 3.0419 - mean_squared_error: 3.0419 - val_loss: 4.1896 - val_mean_squared_error: 4.1896\n",
            "Epoch 121/200\n",
            "1523/1523 [==============================] - 0s 259us/sample - loss: 3.0100 - mean_squared_error: 3.0100 - val_loss: 4.4095 - val_mean_squared_error: 4.4095\n",
            "Epoch 122/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 3.1114 - mean_squared_error: 3.1114 - val_loss: 4.2619 - val_mean_squared_error: 4.2619\n",
            "Epoch 123/200\n",
            "1523/1523 [==============================] - 0s 260us/sample - loss: 2.9106 - mean_squared_error: 2.9106 - val_loss: 4.1942 - val_mean_squared_error: 4.1942\n",
            "Epoch 124/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 3.0467 - mean_squared_error: 3.0467 - val_loss: 4.6457 - val_mean_squared_error: 4.6457\n",
            "Epoch 125/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 3.2866 - mean_squared_error: 3.2866 - val_loss: 5.4107 - val_mean_squared_error: 5.4107\n",
            "Epoch 126/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 2.9748 - mean_squared_error: 2.9748 - val_loss: 3.8916 - val_mean_squared_error: 3.8916\n",
            "Epoch 127/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 2.7590 - mean_squared_error: 2.7590 - val_loss: 3.8440 - val_mean_squared_error: 3.8440\n",
            "Epoch 128/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 2.7930 - mean_squared_error: 2.7930 - val_loss: 3.8961 - val_mean_squared_error: 3.8961\n",
            "Epoch 129/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 2.5477 - mean_squared_error: 2.5477 - val_loss: 3.9098 - val_mean_squared_error: 3.9098\n",
            "Epoch 130/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 2.5119 - mean_squared_error: 2.5119 - val_loss: 4.0160 - val_mean_squared_error: 4.0160\n",
            "Epoch 131/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 2.7305 - mean_squared_error: 2.7305 - val_loss: 4.6461 - val_mean_squared_error: 4.6461\n",
            "Epoch 132/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 2.5016 - mean_squared_error: 2.5016 - val_loss: 4.2591 - val_mean_squared_error: 4.2591\n",
            "Epoch 133/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 2.5971 - mean_squared_error: 2.5971 - val_loss: 4.5622 - val_mean_squared_error: 4.5622\n",
            "Epoch 134/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 2.4486 - mean_squared_error: 2.4486 - val_loss: 3.9677 - val_mean_squared_error: 3.9677\n",
            "Epoch 135/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 2.3128 - mean_squared_error: 2.3128 - val_loss: 3.6781 - val_mean_squared_error: 3.6781\n",
            "Epoch 136/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 2.1649 - mean_squared_error: 2.1649 - val_loss: 3.5597 - val_mean_squared_error: 3.5597\n",
            "Epoch 137/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 2.1406 - mean_squared_error: 2.1406 - val_loss: 4.2139 - val_mean_squared_error: 4.2139\n",
            "Epoch 138/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 2.3293 - mean_squared_error: 2.3293 - val_loss: 3.8239 - val_mean_squared_error: 3.8239\n",
            "Epoch 139/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 2.5016 - mean_squared_error: 2.5016 - val_loss: 4.7531 - val_mean_squared_error: 4.7531\n",
            "Epoch 140/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 2.2518 - mean_squared_error: 2.2518 - val_loss: 4.2025 - val_mean_squared_error: 4.2025\n",
            "Epoch 141/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 2.1436 - mean_squared_error: 2.1436 - val_loss: 4.3515 - val_mean_squared_error: 4.3515\n",
            "Epoch 142/200\n",
            "1523/1523 [==============================] - 0s 257us/sample - loss: 2.2351 - mean_squared_error: 2.2351 - val_loss: 3.8939 - val_mean_squared_error: 3.8939\n",
            "Epoch 143/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 2.0506 - mean_squared_error: 2.0506 - val_loss: 3.7393 - val_mean_squared_error: 3.7393\n",
            "Epoch 144/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 2.1602 - mean_squared_error: 2.1602 - val_loss: 4.2231 - val_mean_squared_error: 4.2231\n",
            "Epoch 145/200\n",
            "1523/1523 [==============================] - 0s 260us/sample - loss: 2.1634 - mean_squared_error: 2.1634 - val_loss: 3.7979 - val_mean_squared_error: 3.7979\n",
            "Epoch 146/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.9778 - mean_squared_error: 1.9778 - val_loss: 4.0015 - val_mean_squared_error: 4.0015\n",
            "Epoch 147/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.8597 - mean_squared_error: 1.8597 - val_loss: 3.5484 - val_mean_squared_error: 3.5484\n",
            "Epoch 148/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 1.7534 - mean_squared_error: 1.7534 - val_loss: 3.9050 - val_mean_squared_error: 3.9050\n",
            "Epoch 149/200\n",
            "1523/1523 [==============================] - 0s 259us/sample - loss: 1.8591 - mean_squared_error: 1.8591 - val_loss: 4.0597 - val_mean_squared_error: 4.0597\n",
            "Epoch 150/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 2.1221 - mean_squared_error: 2.1221 - val_loss: 4.9068 - val_mean_squared_error: 4.9068\n",
            "Epoch 151/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 2.0297 - mean_squared_error: 2.0297 - val_loss: 3.6319 - val_mean_squared_error: 3.6319\n",
            "Epoch 152/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 1.7058 - mean_squared_error: 1.7058 - val_loss: 3.6505 - val_mean_squared_error: 3.6505\n",
            "Epoch 153/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 1.6978 - mean_squared_error: 1.6978 - val_loss: 3.4419 - val_mean_squared_error: 3.4419\n",
            "Epoch 154/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.9036 - mean_squared_error: 1.9036 - val_loss: 4.2756 - val_mean_squared_error: 4.2756\n",
            "Epoch 155/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 1.8122 - mean_squared_error: 1.8122 - val_loss: 3.6204 - val_mean_squared_error: 3.6204\n",
            "Epoch 156/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.6471 - mean_squared_error: 1.6471 - val_loss: 3.6115 - val_mean_squared_error: 3.6115\n",
            "Epoch 157/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.5383 - mean_squared_error: 1.5383 - val_loss: 3.5560 - val_mean_squared_error: 3.5560\n",
            "Epoch 158/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 1.4638 - mean_squared_error: 1.4638 - val_loss: 3.5228 - val_mean_squared_error: 3.5228\n",
            "Epoch 159/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.5674 - mean_squared_error: 1.5674 - val_loss: 4.1173 - val_mean_squared_error: 4.1173\n",
            "Epoch 160/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 1.9161 - mean_squared_error: 1.9161 - val_loss: 3.9769 - val_mean_squared_error: 3.9769\n",
            "Epoch 161/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 1.6398 - mean_squared_error: 1.6398 - val_loss: 3.7663 - val_mean_squared_error: 3.7663\n",
            "Epoch 162/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 1.5981 - mean_squared_error: 1.5981 - val_loss: 3.5267 - val_mean_squared_error: 3.5267\n",
            "Epoch 163/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.5102 - mean_squared_error: 1.5102 - val_loss: 3.6904 - val_mean_squared_error: 3.6904\n",
            "Epoch 164/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 1.4467 - mean_squared_error: 1.4467 - val_loss: 3.5398 - val_mean_squared_error: 3.5398\n",
            "Epoch 165/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.3855 - mean_squared_error: 1.3855 - val_loss: 3.6115 - val_mean_squared_error: 3.6115\n",
            "Epoch 166/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 1.4055 - mean_squared_error: 1.4055 - val_loss: 4.0637 - val_mean_squared_error: 4.0637\n",
            "Epoch 167/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.6381 - mean_squared_error: 1.6381 - val_loss: 3.5681 - val_mean_squared_error: 3.5681\n",
            "Epoch 168/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 1.3605 - mean_squared_error: 1.3605 - val_loss: 3.5610 - val_mean_squared_error: 3.5610\n",
            "Epoch 169/200\n",
            "1523/1523 [==============================] - 0s 258us/sample - loss: 1.3482 - mean_squared_error: 1.3482 - val_loss: 3.6626 - val_mean_squared_error: 3.6626\n",
            "Epoch 170/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 1.3517 - mean_squared_error: 1.3517 - val_loss: 3.6798 - val_mean_squared_error: 3.6798\n",
            "Epoch 171/200\n",
            "1523/1523 [==============================] - 0s 258us/sample - loss: 1.4873 - mean_squared_error: 1.4873 - val_loss: 3.6441 - val_mean_squared_error: 3.6441\n",
            "Epoch 172/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.3080 - mean_squared_error: 1.3080 - val_loss: 3.6187 - val_mean_squared_error: 3.6187\n",
            "Epoch 173/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.1645 - mean_squared_error: 1.1645 - val_loss: 3.5470 - val_mean_squared_error: 3.5470\n",
            "Epoch 174/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.2152 - mean_squared_error: 1.2152 - val_loss: 3.5694 - val_mean_squared_error: 3.5694\n",
            "Epoch 175/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 1.2807 - mean_squared_error: 1.2807 - val_loss: 3.6412 - val_mean_squared_error: 3.6412\n",
            "Epoch 176/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.4923 - mean_squared_error: 1.4923 - val_loss: 3.8647 - val_mean_squared_error: 3.8647\n",
            "Epoch 177/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.5791 - mean_squared_error: 1.5791 - val_loss: 4.3232 - val_mean_squared_error: 4.3232\n",
            "Epoch 178/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.6231 - mean_squared_error: 1.6231 - val_loss: 3.4380 - val_mean_squared_error: 3.4380\n",
            "Epoch 179/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.2866 - mean_squared_error: 1.2866 - val_loss: 3.4734 - val_mean_squared_error: 3.4734\n",
            "Epoch 180/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 1.1311 - mean_squared_error: 1.1311 - val_loss: 3.4142 - val_mean_squared_error: 3.4142\n",
            "Epoch 181/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 1.0921 - mean_squared_error: 1.0921 - val_loss: 3.5626 - val_mean_squared_error: 3.5626\n",
            "Epoch 182/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 1.3488 - mean_squared_error: 1.3488 - val_loss: 4.4737 - val_mean_squared_error: 4.4737\n",
            "Epoch 183/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 1.7049 - mean_squared_error: 1.7049 - val_loss: 3.7555 - val_mean_squared_error: 3.7555\n",
            "Epoch 184/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.2116 - mean_squared_error: 1.2116 - val_loss: 3.4356 - val_mean_squared_error: 3.4356\n",
            "Epoch 185/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.1320 - mean_squared_error: 1.1320 - val_loss: 3.6670 - val_mean_squared_error: 3.6670\n",
            "Epoch 186/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 1.1757 - mean_squared_error: 1.1757 - val_loss: 3.6879 - val_mean_squared_error: 3.6878\n",
            "Epoch 187/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 1.2657 - mean_squared_error: 1.2657 - val_loss: 3.5455 - val_mean_squared_error: 3.5455\n",
            "Epoch 188/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 1.0694 - mean_squared_error: 1.0694 - val_loss: 3.7443 - val_mean_squared_error: 3.7443\n",
            "Epoch 189/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 3.4032 - val_mean_squared_error: 3.4032\n",
            "Epoch 190/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 3.7105 - val_mean_squared_error: 3.7105\n",
            "Epoch 191/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 3.4146 - val_mean_squared_error: 3.4146\n",
            "Epoch 192/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 3.5050 - val_mean_squared_error: 3.5050\n",
            "Epoch 193/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 1.0255 - mean_squared_error: 1.0255 - val_loss: 3.5907 - val_mean_squared_error: 3.5907\n",
            "Epoch 194/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 1.0169 - mean_squared_error: 1.0169 - val_loss: 3.5567 - val_mean_squared_error: 3.5567\n",
            "Epoch 195/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 0.9713 - mean_squared_error: 0.9713 - val_loss: 3.6207 - val_mean_squared_error: 3.6207\n",
            "Epoch 196/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 0.9154 - mean_squared_error: 0.9154 - val_loss: 3.6059 - val_mean_squared_error: 3.6059\n",
            "Epoch 197/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 0.9677 - mean_squared_error: 0.9677 - val_loss: 3.6231 - val_mean_squared_error: 3.6231\n",
            "Epoch 198/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 1.3251 - mean_squared_error: 1.3251 - val_loss: 3.9969 - val_mean_squared_error: 3.9969\n",
            "Epoch 199/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 1.1739 - mean_squared_error: 1.1739 - val_loss: 3.6905 - val_mean_squared_error: 3.6905\n",
            "Epoch 200/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 0.9650 - mean_squared_error: 0.9650 - val_loss: 3.7492 - val_mean_squared_error: 3.7492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxQYX1btaBRH",
        "colab_type": "text"
      },
      "source": [
        "## Stride versus Pool Sensitivity\n",
        "\n",
        "The following code section will look at using a stride of 2 instead of using a pooling layer in order to determine which performs better for our model. Both cases perform the same task of reducing our input matrix size by a factor of 2. Based on some reading, it was indicated that the use of a stride greater than one might be a good alternative to adding a pooling layer.\n",
        "\n",
        "However the results of our limited study indicated that max pooling actually performed better. From this point on all studies maintained a stride of one. In other words when performing the convolution, our kernel slides one feature or matrix cell at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuhUYt20RRFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_stride_cnn_model(start_filter, d, step):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model \n",
        "    '''\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "    cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), strides=(2,2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (2, 2), strides=(2,2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (2, 2), strides=(2,2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+2*step))\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(500))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(500))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(30))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb-0PZfjahBg",
        "colab_type": "code",
        "outputId": "5455601c-76f8-402b-e8ec-db51cea8e150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Redefine optimizer list to just focus on adam and sgd\n",
        "opt_list = {'adam':adam, 'sgd':sgd}\n",
        "\n",
        "# Use an early stopping callback and our timing callback\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                              patience=100, mode='auto')\n",
        "time_callback = TimeHistory()\n",
        "\n",
        "# Initialize a new data frame to hold our output data\n",
        "cnn_stride_df = pd.DataFrame()\n",
        "\n",
        "# Start filter list\n",
        "start_filters = [16]\n",
        "\n",
        "# Create a list of initial dropout values and steps to increase\n",
        "dropouts = [(0.00,0.00), (0.0,0.01), (0.00,0.02)]\n",
        "\n",
        "for opt_name, opt in opt_list.items():\n",
        "    for start_filter in start_filters:\n",
        "        for d in dropouts:\n",
        "            model = create_stride_cnn_model(start_filter,d[0], d[1])\n",
        "            model.compile(\n",
        "                  optimizer=opt,\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_squared_error'])\n",
        "            history = model.fit(\n",
        "                X.astype(np.float32), y.astype(np.float32),\n",
        "                epochs=200,\n",
        "                validation_split=0.15, callbacks=[time_callback, early_stop])\n",
        "            times = time_callback.times\n",
        "\n",
        "            # Convert to dataframe\n",
        "            hist = pd.DataFrame(history.history)\n",
        "            hist['epoch'] = history.epoch\n",
        "            hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "            hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "            hist['times'] = times\n",
        "            hist['starting_filter'] = start_filter\n",
        "            hist['layers'] = 3\n",
        "            hist['pooling'] = 'yes'\n",
        "            hist['fc_layer'] = 500\n",
        "            hist['activation'] = 'relu'\n",
        "            hist['optimizer'] = opt_name\n",
        "            hist['lrate'] = opt.get_config()['learning_rate']\n",
        "            hist['dropout_initial'] = d[0]\n",
        "            hist['dropout_step'] = d[1]\n",
        "            hist['batch_norm'] = 1\n",
        "            hist['bias'] = 0\n",
        "            hist['stride'] = 2\n",
        "\n",
        "            # Keep concatenating to dataframe\n",
        "            cnn_stride_df = pd.concat([cnn_stride_df,hist])\n",
        "\n",
        "            # Re-pickle after every model to retain progress\n",
        "            cnn_stride_df.to_pickle(drive_path+\"OutputData/cnn_stride_df.pkl\")\n",
        "\n",
        "            # Save models.\n",
        "            filename = \"cnn_stride_model_{}_d{}_s{}_sf{}_stride2\".format(opt_name, d[0], d[1], start_filter)\n",
        "            model.save(drive_path+\"Models/\"+filename+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 47, 47, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 47, 47, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 23, 23, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 23, 23, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 11, 11, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 636us/sample - loss: 2049.1691 - mean_squared_error: 2049.1689 - val_loss: 2008.9767 - val_mean_squared_error: 2008.9767\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 971.5344 - mean_squared_error: 971.5344 - val_loss: 982.7558 - val_mean_squared_error: 982.7559\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 340.4282 - mean_squared_error: 340.4282 - val_loss: 181.6316 - val_mean_squared_error: 181.6317\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 106.4754 - mean_squared_error: 106.4754 - val_loss: 59.0815 - val_mean_squared_error: 59.0815\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 43.1735 - mean_squared_error: 43.1735 - val_loss: 64.9920 - val_mean_squared_error: 64.9920\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 21.8872 - mean_squared_error: 21.8872 - val_loss: 25.0446 - val_mean_squared_error: 25.0446\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 19.1502 - mean_squared_error: 19.1502 - val_loss: 15.0909 - val_mean_squared_error: 15.0909\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 17.6137 - mean_squared_error: 17.6137 - val_loss: 17.1161 - val_mean_squared_error: 17.1161\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 12.8488 - mean_squared_error: 12.8488 - val_loss: 16.3345 - val_mean_squared_error: 16.3345\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 13.0733 - mean_squared_error: 13.0733 - val_loss: 12.6076 - val_mean_squared_error: 12.6076\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 11.4412 - mean_squared_error: 11.4412 - val_loss: 12.4693 - val_mean_squared_error: 12.4693\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 11.5235 - mean_squared_error: 11.5235 - val_loss: 11.6923 - val_mean_squared_error: 11.6923\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 10.3447 - mean_squared_error: 10.3446 - val_loss: 11.7521 - val_mean_squared_error: 11.7521\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 9.8211 - mean_squared_error: 9.8211 - val_loss: 10.7797 - val_mean_squared_error: 10.7797\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 10.1403 - mean_squared_error: 10.1403 - val_loss: 16.1372 - val_mean_squared_error: 16.1372\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 9.4024 - mean_squared_error: 9.4024 - val_loss: 12.8495 - val_mean_squared_error: 12.8495\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 8.6638 - mean_squared_error: 8.6638 - val_loss: 8.5557 - val_mean_squared_error: 8.5557\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 7.7820 - mean_squared_error: 7.7820 - val_loss: 8.0109 - val_mean_squared_error: 8.0109\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 7.2886 - mean_squared_error: 7.2886 - val_loss: 7.5669 - val_mean_squared_error: 7.5669\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 7.1572 - mean_squared_error: 7.1572 - val_loss: 7.9953 - val_mean_squared_error: 7.9953\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 6.8958 - mean_squared_error: 6.8958 - val_loss: 9.2246 - val_mean_squared_error: 9.2246\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 7.2268 - mean_squared_error: 7.2268 - val_loss: 7.6976 - val_mean_squared_error: 7.6976\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 6.3183 - mean_squared_error: 6.3183 - val_loss: 6.3677 - val_mean_squared_error: 6.3677\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 6.2205 - mean_squared_error: 6.2205 - val_loss: 6.3799 - val_mean_squared_error: 6.3799\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 5.9314 - mean_squared_error: 5.9314 - val_loss: 6.3843 - val_mean_squared_error: 6.3843\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 5.4099 - mean_squared_error: 5.4099 - val_loss: 6.1325 - val_mean_squared_error: 6.1325\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 5.2052 - mean_squared_error: 5.2052 - val_loss: 6.4031 - val_mean_squared_error: 6.4031\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 5.7661 - mean_squared_error: 5.7661 - val_loss: 5.3838 - val_mean_squared_error: 5.3838\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 5.0298 - mean_squared_error: 5.0298 - val_loss: 6.7613 - val_mean_squared_error: 6.7613\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 4.6147 - mean_squared_error: 4.6147 - val_loss: 7.4356 - val_mean_squared_error: 7.4357\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 4.5930 - mean_squared_error: 4.5930 - val_loss: 7.3788 - val_mean_squared_error: 7.3788\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 4.8447 - mean_squared_error: 4.8447 - val_loss: 5.6912 - val_mean_squared_error: 5.6912\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 4.3010 - mean_squared_error: 4.3010 - val_loss: 5.2225 - val_mean_squared_error: 5.2225\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 3.9815 - mean_squared_error: 3.9815 - val_loss: 5.3625 - val_mean_squared_error: 5.3625\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 3.6900 - mean_squared_error: 3.6900 - val_loss: 5.2586 - val_mean_squared_error: 5.2586\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 3.5207 - mean_squared_error: 3.5207 - val_loss: 4.7961 - val_mean_squared_error: 4.7961\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 3.6323 - mean_squared_error: 3.6323 - val_loss: 4.6863 - val_mean_squared_error: 4.6863\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 3.4422 - mean_squared_error: 3.4422 - val_loss: 4.3749 - val_mean_squared_error: 4.3749\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 3.5371 - mean_squared_error: 3.5371 - val_loss: 4.6471 - val_mean_squared_error: 4.6471\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 3.5114 - mean_squared_error: 3.5114 - val_loss: 5.0990 - val_mean_squared_error: 5.0990\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 3.3855 - mean_squared_error: 3.3855 - val_loss: 4.0196 - val_mean_squared_error: 4.0196\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 3.0621 - mean_squared_error: 3.0621 - val_loss: 4.6513 - val_mean_squared_error: 4.6513\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 3.2321 - mean_squared_error: 3.2321 - val_loss: 5.0443 - val_mean_squared_error: 5.0443\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 3.0244 - mean_squared_error: 3.0244 - val_loss: 4.7235 - val_mean_squared_error: 4.7235\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.8338 - mean_squared_error: 2.8338 - val_loss: 4.7589 - val_mean_squared_error: 4.7589\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.9092 - mean_squared_error: 2.9092 - val_loss: 4.2882 - val_mean_squared_error: 4.2882\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 2.9760 - mean_squared_error: 2.9760 - val_loss: 4.3009 - val_mean_squared_error: 4.3009\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.8549 - mean_squared_error: 2.8549 - val_loss: 3.9676 - val_mean_squared_error: 3.9676\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 2.7016 - mean_squared_error: 2.7016 - val_loss: 4.0619 - val_mean_squared_error: 4.0619\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.7550 - mean_squared_error: 2.7550 - val_loss: 5.2858 - val_mean_squared_error: 5.2858\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.9871 - mean_squared_error: 2.9871 - val_loss: 3.7768 - val_mean_squared_error: 3.7768\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.7841 - mean_squared_error: 2.7841 - val_loss: 3.9382 - val_mean_squared_error: 3.9382\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.7132 - mean_squared_error: 2.7132 - val_loss: 4.2698 - val_mean_squared_error: 4.2698\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 2.6212 - mean_squared_error: 2.6212 - val_loss: 3.5228 - val_mean_squared_error: 3.5228\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 2.7717 - mean_squared_error: 2.7717 - val_loss: 3.6308 - val_mean_squared_error: 3.6308\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.4362 - mean_squared_error: 2.4362 - val_loss: 3.6323 - val_mean_squared_error: 3.6323\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 0s 259us/sample - loss: 2.4964 - mean_squared_error: 2.4964 - val_loss: 4.1736 - val_mean_squared_error: 4.1736\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.5899 - mean_squared_error: 2.5899 - val_loss: 3.5068 - val_mean_squared_error: 3.5068\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 2.3613 - mean_squared_error: 2.3613 - val_loss: 3.5913 - val_mean_squared_error: 3.5913\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.1814 - mean_squared_error: 2.1814 - val_loss: 3.3884 - val_mean_squared_error: 3.3884\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.2443 - mean_squared_error: 2.2443 - val_loss: 4.7800 - val_mean_squared_error: 4.7800\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.2582 - mean_squared_error: 2.2582 - val_loss: 3.4790 - val_mean_squared_error: 3.4790\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 2.3073 - mean_squared_error: 2.3073 - val_loss: 3.5033 - val_mean_squared_error: 3.5033\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 2.1433 - mean_squared_error: 2.1433 - val_loss: 3.6507 - val_mean_squared_error: 3.6507\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.0835 - mean_squared_error: 2.0835 - val_loss: 3.6283 - val_mean_squared_error: 3.6283\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 2.2265 - mean_squared_error: 2.2265 - val_loss: 3.5066 - val_mean_squared_error: 3.5066\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.3094 - mean_squared_error: 2.3094 - val_loss: 3.4184 - val_mean_squared_error: 3.4184\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 2.2691 - mean_squared_error: 2.2691 - val_loss: 5.1261 - val_mean_squared_error: 5.1261\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.1888 - mean_squared_error: 2.1888 - val_loss: 3.2838 - val_mean_squared_error: 3.2838\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 2.2583 - mean_squared_error: 2.2583 - val_loss: 4.1016 - val_mean_squared_error: 4.1016\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.1493 - mean_squared_error: 2.1493 - val_loss: 4.5967 - val_mean_squared_error: 4.5967\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 2.3242 - mean_squared_error: 2.3242 - val_loss: 3.5842 - val_mean_squared_error: 3.5842\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.1695 - mean_squared_error: 2.1695 - val_loss: 3.8381 - val_mean_squared_error: 3.8381\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 2.1535 - mean_squared_error: 2.1535 - val_loss: 3.9764 - val_mean_squared_error: 3.9764\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.0386 - mean_squared_error: 2.0386 - val_loss: 5.0688 - val_mean_squared_error: 5.0688\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 2.1396 - mean_squared_error: 2.1396 - val_loss: 4.1027 - val_mean_squared_error: 4.1027\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.8949 - mean_squared_error: 1.8949 - val_loss: 3.4398 - val_mean_squared_error: 3.4398\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.9256 - mean_squared_error: 1.9256 - val_loss: 4.2186 - val_mean_squared_error: 4.2186\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.8847 - mean_squared_error: 1.8847 - val_loss: 3.5578 - val_mean_squared_error: 3.5578\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 1.8467 - mean_squared_error: 1.8467 - val_loss: 3.4126 - val_mean_squared_error: 3.4126\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.8597 - mean_squared_error: 1.8597 - val_loss: 3.4995 - val_mean_squared_error: 3.4995\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.9790 - mean_squared_error: 1.9790 - val_loss: 3.4614 - val_mean_squared_error: 3.4614\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.8450 - mean_squared_error: 1.8450 - val_loss: 3.8350 - val_mean_squared_error: 3.8350\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.9170 - mean_squared_error: 1.9170 - val_loss: 3.5201 - val_mean_squared_error: 3.5201\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.7526 - mean_squared_error: 1.7526 - val_loss: 3.6452 - val_mean_squared_error: 3.6452\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.9153 - mean_squared_error: 1.9153 - val_loss: 3.3258 - val_mean_squared_error: 3.3258\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.7535 - mean_squared_error: 1.7535 - val_loss: 3.1821 - val_mean_squared_error: 3.1821\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.8133 - mean_squared_error: 1.8133 - val_loss: 3.3732 - val_mean_squared_error: 3.3732\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 2.1274 - mean_squared_error: 2.1274 - val_loss: 3.2372 - val_mean_squared_error: 3.2372\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.9386 - mean_squared_error: 1.9386 - val_loss: 3.5564 - val_mean_squared_error: 3.5564\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.8841 - mean_squared_error: 1.8841 - val_loss: 3.5756 - val_mean_squared_error: 3.5756\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.7348 - mean_squared_error: 1.7348 - val_loss: 3.5380 - val_mean_squared_error: 3.5380\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.7349 - mean_squared_error: 1.7349 - val_loss: 3.6041 - val_mean_squared_error: 3.6041\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.8005 - mean_squared_error: 1.8005 - val_loss: 3.5942 - val_mean_squared_error: 3.5942\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 1.7077 - mean_squared_error: 1.7077 - val_loss: 4.9999 - val_mean_squared_error: 4.9999\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.7467 - mean_squared_error: 1.7467 - val_loss: 3.6253 - val_mean_squared_error: 3.6253\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.5296 - mean_squared_error: 1.5296 - val_loss: 3.0965 - val_mean_squared_error: 3.0965\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.5226 - mean_squared_error: 1.5226 - val_loss: 3.1056 - val_mean_squared_error: 3.1056\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.5178 - mean_squared_error: 1.5178 - val_loss: 4.8810 - val_mean_squared_error: 4.8810\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.6850 - mean_squared_error: 1.6850 - val_loss: 3.2978 - val_mean_squared_error: 3.2978\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.5879 - mean_squared_error: 1.5879 - val_loss: 3.9906 - val_mean_squared_error: 3.9906\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.5147 - mean_squared_error: 1.5147 - val_loss: 3.2957 - val_mean_squared_error: 3.2957\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 0s 259us/sample - loss: 1.6706 - mean_squared_error: 1.6706 - val_loss: 3.4307 - val_mean_squared_error: 3.4307\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.5482 - mean_squared_error: 1.5482 - val_loss: 3.4853 - val_mean_squared_error: 3.4853\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.5028 - mean_squared_error: 1.5028 - val_loss: 3.0381 - val_mean_squared_error: 3.0381\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.5800 - mean_squared_error: 1.5800 - val_loss: 3.1995 - val_mean_squared_error: 3.1995\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 1.5430 - mean_squared_error: 1.5430 - val_loss: 3.1540 - val_mean_squared_error: 3.1540\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.4933 - mean_squared_error: 1.4933 - val_loss: 3.7560 - val_mean_squared_error: 3.7560\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.4160 - mean_squared_error: 1.4160 - val_loss: 3.7233 - val_mean_squared_error: 3.7233\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.3385 - mean_squared_error: 1.3385 - val_loss: 3.5470 - val_mean_squared_error: 3.5470\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 1.4341 - mean_squared_error: 1.4341 - val_loss: 3.3458 - val_mean_squared_error: 3.3458\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.4379 - mean_squared_error: 1.4379 - val_loss: 3.4171 - val_mean_squared_error: 3.4171\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.3854 - mean_squared_error: 1.3854 - val_loss: 3.4555 - val_mean_squared_error: 3.4555\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.4648 - mean_squared_error: 1.4648 - val_loss: 3.5208 - val_mean_squared_error: 3.5208\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.4023 - mean_squared_error: 1.4023 - val_loss: 3.0822 - val_mean_squared_error: 3.0822\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.4930 - mean_squared_error: 1.4930 - val_loss: 3.4225 - val_mean_squared_error: 3.4225\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.4775 - mean_squared_error: 1.4775 - val_loss: 3.1812 - val_mean_squared_error: 3.1812\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.3829 - mean_squared_error: 1.3829 - val_loss: 4.1106 - val_mean_squared_error: 4.1106\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.3309 - mean_squared_error: 1.3309 - val_loss: 3.1292 - val_mean_squared_error: 3.1292\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.3482 - mean_squared_error: 1.3482 - val_loss: 3.3282 - val_mean_squared_error: 3.3282\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.2774 - mean_squared_error: 1.2774 - val_loss: 3.3332 - val_mean_squared_error: 3.3332\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.3409 - mean_squared_error: 1.3409 - val_loss: 3.2846 - val_mean_squared_error: 3.2846\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.3253 - mean_squared_error: 1.3253 - val_loss: 3.6148 - val_mean_squared_error: 3.6148\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.3960 - mean_squared_error: 1.3960 - val_loss: 3.0988 - val_mean_squared_error: 3.0988\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.3048 - mean_squared_error: 1.3048 - val_loss: 3.2159 - val_mean_squared_error: 3.2159\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.2614 - mean_squared_error: 1.2614 - val_loss: 3.7238 - val_mean_squared_error: 3.7238\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.2661 - mean_squared_error: 1.2661 - val_loss: 3.2757 - val_mean_squared_error: 3.2757\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.2135 - mean_squared_error: 1.2135 - val_loss: 3.2404 - val_mean_squared_error: 3.2404\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2448 - mean_squared_error: 1.2448 - val_loss: 3.5274 - val_mean_squared_error: 3.5274\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.3672 - mean_squared_error: 1.3672 - val_loss: 3.2678 - val_mean_squared_error: 3.2678\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.2277 - mean_squared_error: 1.2277 - val_loss: 3.2395 - val_mean_squared_error: 3.2395\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.2607 - mean_squared_error: 1.2607 - val_loss: 3.1941 - val_mean_squared_error: 3.1941\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.2605 - mean_squared_error: 1.2605 - val_loss: 3.4215 - val_mean_squared_error: 3.4215\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.3181 - mean_squared_error: 1.3181 - val_loss: 3.0600 - val_mean_squared_error: 3.0600\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.4322 - mean_squared_error: 1.4322 - val_loss: 3.2017 - val_mean_squared_error: 3.2017\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 0s 259us/sample - loss: 1.2901 - mean_squared_error: 1.2901 - val_loss: 3.2578 - val_mean_squared_error: 3.2578\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.2496 - mean_squared_error: 1.2496 - val_loss: 3.3711 - val_mean_squared_error: 3.3711\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 1.2330 - mean_squared_error: 1.2330 - val_loss: 3.1952 - val_mean_squared_error: 3.1952\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.2125 - mean_squared_error: 1.2125 - val_loss: 3.2523 - val_mean_squared_error: 3.2523\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.2546 - mean_squared_error: 1.2546 - val_loss: 3.0099 - val_mean_squared_error: 3.0099\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.1554 - mean_squared_error: 1.1554 - val_loss: 3.2956 - val_mean_squared_error: 3.2956\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.1697 - mean_squared_error: 1.1697 - val_loss: 3.0540 - val_mean_squared_error: 3.0540\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.4842 - mean_squared_error: 1.4842 - val_loss: 3.9968 - val_mean_squared_error: 3.9968\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.3077 - mean_squared_error: 1.3077 - val_loss: 3.2600 - val_mean_squared_error: 3.2600\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.2051 - mean_squared_error: 1.2051 - val_loss: 3.0099 - val_mean_squared_error: 3.0099\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.2337 - mean_squared_error: 1.2337 - val_loss: 3.8897 - val_mean_squared_error: 3.8897\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.5919 - mean_squared_error: 1.5919 - val_loss: 4.5350 - val_mean_squared_error: 4.5350\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.3623 - mean_squared_error: 1.3623 - val_loss: 3.2515 - val_mean_squared_error: 3.2515\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 1.3013 - mean_squared_error: 1.3013 - val_loss: 3.1506 - val_mean_squared_error: 3.1506\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.1692 - mean_squared_error: 1.1692 - val_loss: 3.1673 - val_mean_squared_error: 3.1673\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.1539 - mean_squared_error: 1.1539 - val_loss: 3.2208 - val_mean_squared_error: 3.2208\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.0251 - mean_squared_error: 1.0251 - val_loss: 3.3957 - val_mean_squared_error: 3.3957\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 1.1529 - mean_squared_error: 1.1529 - val_loss: 3.2374 - val_mean_squared_error: 3.2374\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 1.1617 - mean_squared_error: 1.1617 - val_loss: 3.3251 - val_mean_squared_error: 3.3251\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 0s 259us/sample - loss: 1.1803 - mean_squared_error: 1.1803 - val_loss: 3.3907 - val_mean_squared_error: 3.3907\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.0735 - mean_squared_error: 1.0735 - val_loss: 3.4379 - val_mean_squared_error: 3.4379\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 3.2955 - val_mean_squared_error: 3.2955\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.0901 - mean_squared_error: 1.0901 - val_loss: 3.1763 - val_mean_squared_error: 3.1763\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.1201 - mean_squared_error: 1.1201 - val_loss: 3.2283 - val_mean_squared_error: 3.2283\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.1995 - mean_squared_error: 1.1995 - val_loss: 3.3825 - val_mean_squared_error: 3.3825\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.2389 - mean_squared_error: 1.2389 - val_loss: 3.1573 - val_mean_squared_error: 3.1573\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 1.0922 - mean_squared_error: 1.0922 - val_loss: 3.0806 - val_mean_squared_error: 3.0806\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.0818 - mean_squared_error: 1.0818 - val_loss: 2.8946 - val_mean_squared_error: 2.8946\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.0211 - mean_squared_error: 1.0211 - val_loss: 3.1678 - val_mean_squared_error: 3.1678\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.0494 - mean_squared_error: 1.0494 - val_loss: 3.3109 - val_mean_squared_error: 3.3109\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 1.1266 - mean_squared_error: 1.1266 - val_loss: 3.1444 - val_mean_squared_error: 3.1444\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 0.9707 - mean_squared_error: 0.9707 - val_loss: 3.1765 - val_mean_squared_error: 3.1765\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.0909 - mean_squared_error: 1.0909 - val_loss: 3.1025 - val_mean_squared_error: 3.1025\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 0.9708 - mean_squared_error: 0.9708 - val_loss: 3.4532 - val_mean_squared_error: 3.4532\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.0565 - mean_squared_error: 1.0565 - val_loss: 4.1342 - val_mean_squared_error: 4.1342\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.0611 - mean_squared_error: 1.0611 - val_loss: 3.5898 - val_mean_squared_error: 3.5898\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.0596 - mean_squared_error: 1.0596 - val_loss: 2.9996 - val_mean_squared_error: 2.9996\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.0575 - mean_squared_error: 1.0575 - val_loss: 3.1887 - val_mean_squared_error: 3.1887\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 1.0649 - mean_squared_error: 1.0649 - val_loss: 3.1812 - val_mean_squared_error: 3.1812\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.0287 - mean_squared_error: 1.0287 - val_loss: 3.0099 - val_mean_squared_error: 3.0099\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 3.5218 - val_mean_squared_error: 3.5218\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 0.9838 - mean_squared_error: 0.9838 - val_loss: 3.5429 - val_mean_squared_error: 3.5429\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 1.1833 - mean_squared_error: 1.1833 - val_loss: 3.4616 - val_mean_squared_error: 3.4616\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.0547 - mean_squared_error: 1.0547 - val_loss: 3.2239 - val_mean_squared_error: 3.2239\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.0271 - mean_squared_error: 1.0271 - val_loss: 2.9289 - val_mean_squared_error: 2.9289\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 0.9961 - mean_squared_error: 0.9961 - val_loss: 3.0439 - val_mean_squared_error: 3.0439\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.0517 - mean_squared_error: 1.0517 - val_loss: 2.8955 - val_mean_squared_error: 2.8955\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.0030 - mean_squared_error: 1.0030 - val_loss: 3.0473 - val_mean_squared_error: 3.0473\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 0.9863 - mean_squared_error: 0.9863 - val_loss: 3.0948 - val_mean_squared_error: 3.0948\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 0.9844 - mean_squared_error: 0.9844 - val_loss: 3.0820 - val_mean_squared_error: 3.0820\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 0.9577 - mean_squared_error: 0.9577 - val_loss: 3.0971 - val_mean_squared_error: 3.0971\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 0.9179 - mean_squared_error: 0.9179 - val_loss: 3.2698 - val_mean_squared_error: 3.2698\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 0.9672 - mean_squared_error: 0.9672 - val_loss: 3.0824 - val_mean_squared_error: 3.0824\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 0.9448 - mean_squared_error: 0.9448 - val_loss: 2.9945 - val_mean_squared_error: 2.9945\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 0.9570 - mean_squared_error: 0.9570 - val_loss: 2.8524 - val_mean_squared_error: 2.8524\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 0.9737 - mean_squared_error: 0.9737 - val_loss: 3.2529 - val_mean_squared_error: 3.2529\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 0.9648 - mean_squared_error: 0.9648 - val_loss: 3.2654 - val_mean_squared_error: 3.2654\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 0.9296 - mean_squared_error: 0.9296 - val_loss: 3.1039 - val_mean_squared_error: 3.1039\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 0.9700 - mean_squared_error: 0.9700 - val_loss: 3.0817 - val_mean_squared_error: 3.0817\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 0.9452 - mean_squared_error: 0.9452 - val_loss: 2.9810 - val_mean_squared_error: 2.9810\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 0.9052 - mean_squared_error: 0.9052 - val_loss: 2.9796 - val_mean_squared_error: 2.9796\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 0.8669 - mean_squared_error: 0.8669 - val_loss: 2.9057 - val_mean_squared_error: 2.9057\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 0.9495 - mean_squared_error: 0.9495 - val_loss: 2.9174 - val_mean_squared_error: 2.9174\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 0.9711 - mean_squared_error: 0.9711 - val_loss: 2.9775 - val_mean_squared_error: 2.9775\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 0.8725 - mean_squared_error: 0.8725 - val_loss: 3.2153 - val_mean_squared_error: 3.2153\n",
            "==================================================\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 47, 47, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 47, 47, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 23, 23, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 23, 23, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 11, 11, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 680us/sample - loss: 659.3698 - mean_squared_error: 659.3699 - val_loss: 513.1532 - val_mean_squared_error: 513.1533\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 25.8767 - mean_squared_error: 25.8767 - val_loss: 187.0349 - val_mean_squared_error: 187.0349\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 19.1004 - mean_squared_error: 19.1004 - val_loss: 73.3706 - val_mean_squared_error: 73.3706\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 19.1033 - mean_squared_error: 19.1033 - val_loss: 36.2824 - val_mean_squared_error: 36.2824\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 286us/sample - loss: 17.5863 - mean_squared_error: 17.5863 - val_loss: 22.4220 - val_mean_squared_error: 22.4220\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 296us/sample - loss: 16.3921 - mean_squared_error: 16.3921 - val_loss: 21.1731 - val_mean_squared_error: 21.1731\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 313us/sample - loss: 14.2596 - mean_squared_error: 14.2596 - val_loss: 12.0758 - val_mean_squared_error: 12.0757\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 14.6091 - mean_squared_error: 14.6091 - val_loss: 10.8359 - val_mean_squared_error: 10.8359\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 14.0447 - mean_squared_error: 14.0447 - val_loss: 20.4389 - val_mean_squared_error: 20.4389\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 14.6168 - mean_squared_error: 14.6168 - val_loss: 10.7891 - val_mean_squared_error: 10.7891\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 12.1657 - mean_squared_error: 12.1657 - val_loss: 9.9050 - val_mean_squared_error: 9.9050\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 10.9459 - mean_squared_error: 10.9459 - val_loss: 9.4184 - val_mean_squared_error: 9.4184\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 11.7551 - mean_squared_error: 11.7551 - val_loss: 10.0355 - val_mean_squared_error: 10.0355\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 10.6724 - mean_squared_error: 10.6724 - val_loss: 10.9424 - val_mean_squared_error: 10.9424\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 10.3946 - mean_squared_error: 10.3946 - val_loss: 12.9927 - val_mean_squared_error: 12.9927\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 10.2394 - mean_squared_error: 10.2394 - val_loss: 10.1253 - val_mean_squared_error: 10.1253\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 10.3514 - mean_squared_error: 10.3514 - val_loss: 9.8423 - val_mean_squared_error: 9.8423\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 9.6846 - mean_squared_error: 9.6846 - val_loss: 13.8745 - val_mean_squared_error: 13.8745\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 9.4305 - mean_squared_error: 9.4305 - val_loss: 8.3052 - val_mean_squared_error: 8.3052\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 291us/sample - loss: 8.7847 - mean_squared_error: 8.7847 - val_loss: 8.8239 - val_mean_squared_error: 8.8239\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 286us/sample - loss: 8.7242 - mean_squared_error: 8.7242 - val_loss: 11.2603 - val_mean_squared_error: 11.2603\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 8.7765 - mean_squared_error: 8.7765 - val_loss: 8.9575 - val_mean_squared_error: 8.9575\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 290us/sample - loss: 7.8278 - mean_squared_error: 7.8278 - val_loss: 9.1722 - val_mean_squared_error: 9.1722\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 7.5385 - mean_squared_error: 7.5385 - val_loss: 7.5951 - val_mean_squared_error: 7.5951\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 7.3524 - mean_squared_error: 7.3524 - val_loss: 7.9848 - val_mean_squared_error: 7.9848\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 7.2953 - mean_squared_error: 7.2953 - val_loss: 8.9933 - val_mean_squared_error: 8.9933\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 285us/sample - loss: 6.6967 - mean_squared_error: 6.6967 - val_loss: 6.8073 - val_mean_squared_error: 6.8073\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 6.2704 - mean_squared_error: 6.2704 - val_loss: 6.9493 - val_mean_squared_error: 6.9493\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 293us/sample - loss: 6.1721 - mean_squared_error: 6.1721 - val_loss: 8.3320 - val_mean_squared_error: 8.3320\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 6.3073 - mean_squared_error: 6.3073 - val_loss: 7.5080 - val_mean_squared_error: 7.5080\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 5.4580 - mean_squared_error: 5.4580 - val_loss: 6.2087 - val_mean_squared_error: 6.2087\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 5.5133 - mean_squared_error: 5.5133 - val_loss: 5.8132 - val_mean_squared_error: 5.8132\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 285us/sample - loss: 5.1548 - mean_squared_error: 5.1548 - val_loss: 5.9204 - val_mean_squared_error: 5.9204\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 285us/sample - loss: 5.1581 - mean_squared_error: 5.1581 - val_loss: 6.2786 - val_mean_squared_error: 6.2786\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 4.8736 - mean_squared_error: 4.8736 - val_loss: 6.6903 - val_mean_squared_error: 6.6903\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 4.7530 - mean_squared_error: 4.7530 - val_loss: 5.2568 - val_mean_squared_error: 5.2568\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 286us/sample - loss: 4.8858 - mean_squared_error: 4.8858 - val_loss: 6.2578 - val_mean_squared_error: 6.2578\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 4.5399 - mean_squared_error: 4.5399 - val_loss: 6.1132 - val_mean_squared_error: 6.1132\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 284us/sample - loss: 4.7753 - mean_squared_error: 4.7753 - val_loss: 4.8856 - val_mean_squared_error: 4.8856\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 4.1750 - mean_squared_error: 4.1750 - val_loss: 5.4377 - val_mean_squared_error: 5.4377\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 4.3842 - mean_squared_error: 4.3842 - val_loss: 4.9493 - val_mean_squared_error: 4.9493\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 4.2187 - mean_squared_error: 4.2187 - val_loss: 5.4753 - val_mean_squared_error: 5.4753\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 3.9629 - mean_squared_error: 3.9629 - val_loss: 5.7318 - val_mean_squared_error: 5.7318\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 3.8955 - mean_squared_error: 3.8955 - val_loss: 4.9175 - val_mean_squared_error: 4.9175\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 4.0900 - mean_squared_error: 4.0900 - val_loss: 4.9306 - val_mean_squared_error: 4.9306\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 3.4866 - mean_squared_error: 3.4866 - val_loss: 4.6905 - val_mean_squared_error: 4.6905\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 3.4576 - mean_squared_error: 3.4576 - val_loss: 4.3268 - val_mean_squared_error: 4.3268\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 3.6554 - mean_squared_error: 3.6554 - val_loss: 4.3148 - val_mean_squared_error: 4.3148\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 285us/sample - loss: 3.6045 - mean_squared_error: 3.6045 - val_loss: 4.5818 - val_mean_squared_error: 4.5818\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 3.3937 - mean_squared_error: 3.3937 - val_loss: 4.2896 - val_mean_squared_error: 4.2896\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 3.3881 - mean_squared_error: 3.3881 - val_loss: 4.3314 - val_mean_squared_error: 4.3314\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 3.1576 - mean_squared_error: 3.1576 - val_loss: 4.1368 - val_mean_squared_error: 4.1368\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 3.0365 - mean_squared_error: 3.0365 - val_loss: 4.0561 - val_mean_squared_error: 4.0561\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 3.3679 - mean_squared_error: 3.3679 - val_loss: 4.6871 - val_mean_squared_error: 4.6871\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 3.3145 - mean_squared_error: 3.3145 - val_loss: 4.1231 - val_mean_squared_error: 4.1231\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 3.3628 - mean_squared_error: 3.3628 - val_loss: 3.9424 - val_mean_squared_error: 3.9424\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 2.9687 - mean_squared_error: 2.9687 - val_loss: 3.9180 - val_mean_squared_error: 3.9180\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.9448 - mean_squared_error: 2.9448 - val_loss: 3.7895 - val_mean_squared_error: 3.7895\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 3.2714 - mean_squared_error: 3.2714 - val_loss: 6.6526 - val_mean_squared_error: 6.6526\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.9130 - mean_squared_error: 2.9130 - val_loss: 3.9143 - val_mean_squared_error: 3.9143\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.8531 - mean_squared_error: 2.8531 - val_loss: 4.4284 - val_mean_squared_error: 4.4284\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.8156 - mean_squared_error: 2.8156 - val_loss: 3.5911 - val_mean_squared_error: 3.5911\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 2.7073 - mean_squared_error: 2.7073 - val_loss: 3.9628 - val_mean_squared_error: 3.9628\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.7188 - mean_squared_error: 2.7188 - val_loss: 3.7007 - val_mean_squared_error: 3.7007\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 2.7969 - mean_squared_error: 2.7969 - val_loss: 3.4469 - val_mean_squared_error: 3.4469\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.4883 - mean_squared_error: 2.4883 - val_loss: 4.2252 - val_mean_squared_error: 4.2252\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.3896 - mean_squared_error: 2.3896 - val_loss: 3.4976 - val_mean_squared_error: 3.4976\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.4826 - mean_squared_error: 2.4826 - val_loss: 3.7043 - val_mean_squared_error: 3.7043\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 2.5175 - mean_squared_error: 2.5175 - val_loss: 3.9103 - val_mean_squared_error: 3.9103\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.4320 - mean_squared_error: 2.4320 - val_loss: 3.4495 - val_mean_squared_error: 3.4495\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.4296 - mean_squared_error: 2.4296 - val_loss: 3.5664 - val_mean_squared_error: 3.5664\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.5832 - mean_squared_error: 2.5832 - val_loss: 3.6877 - val_mean_squared_error: 3.6877\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.5340 - mean_squared_error: 2.5340 - val_loss: 3.6902 - val_mean_squared_error: 3.6902\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.4158 - mean_squared_error: 2.4158 - val_loss: 3.7894 - val_mean_squared_error: 3.7894\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.2845 - mean_squared_error: 2.2845 - val_loss: 3.1815 - val_mean_squared_error: 3.1815\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.3072 - mean_squared_error: 2.3072 - val_loss: 3.3901 - val_mean_squared_error: 3.3901\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 2.4883 - mean_squared_error: 2.4883 - val_loss: 3.5686 - val_mean_squared_error: 3.5686\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.2621 - mean_squared_error: 2.2621 - val_loss: 3.2592 - val_mean_squared_error: 3.2592\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.1030 - mean_squared_error: 2.1030 - val_loss: 3.3292 - val_mean_squared_error: 3.3292\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 2.1467 - mean_squared_error: 2.1467 - val_loss: 3.6425 - val_mean_squared_error: 3.6425\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.1393 - mean_squared_error: 2.1393 - val_loss: 3.2512 - val_mean_squared_error: 3.2512\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 2.1127 - mean_squared_error: 2.1127 - val_loss: 3.2395 - val_mean_squared_error: 3.2395\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.2944 - mean_squared_error: 2.2944 - val_loss: 3.9942 - val_mean_squared_error: 3.9942\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 2.2783 - mean_squared_error: 2.2783 - val_loss: 3.8218 - val_mean_squared_error: 3.8218\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.2037 - mean_squared_error: 2.2037 - val_loss: 3.3707 - val_mean_squared_error: 3.3707\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 2.1968 - mean_squared_error: 2.1968 - val_loss: 3.3114 - val_mean_squared_error: 3.3114\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 2.1694 - mean_squared_error: 2.1694 - val_loss: 3.2300 - val_mean_squared_error: 3.2300\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 2.2674 - mean_squared_error: 2.2674 - val_loss: 3.2778 - val_mean_squared_error: 3.2778\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 2.0229 - mean_squared_error: 2.0229 - val_loss: 3.4083 - val_mean_squared_error: 3.4083\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 2.1458 - mean_squared_error: 2.1458 - val_loss: 3.8107 - val_mean_squared_error: 3.8107\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.0327 - mean_squared_error: 2.0327 - val_loss: 3.6203 - val_mean_squared_error: 3.6203\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.9426 - mean_squared_error: 1.9426 - val_loss: 3.1981 - val_mean_squared_error: 3.1981\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.9561 - mean_squared_error: 1.9561 - val_loss: 3.2556 - val_mean_squared_error: 3.2556\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.9560 - mean_squared_error: 1.9560 - val_loss: 3.1729 - val_mean_squared_error: 3.1729\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.8612 - mean_squared_error: 1.8612 - val_loss: 3.3626 - val_mean_squared_error: 3.3626\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.8681 - mean_squared_error: 1.8681 - val_loss: 3.3389 - val_mean_squared_error: 3.3389\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.9120 - mean_squared_error: 1.9120 - val_loss: 3.0698 - val_mean_squared_error: 3.0698\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.8447 - mean_squared_error: 1.8447 - val_loss: 3.5020 - val_mean_squared_error: 3.5020\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.8290 - mean_squared_error: 1.8290 - val_loss: 3.1300 - val_mean_squared_error: 3.1300\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.9110 - mean_squared_error: 1.9110 - val_loss: 3.0400 - val_mean_squared_error: 3.0400\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.8228 - mean_squared_error: 1.8228 - val_loss: 3.0054 - val_mean_squared_error: 3.0054\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.8431 - mean_squared_error: 1.8431 - val_loss: 3.2053 - val_mean_squared_error: 3.2053\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.8483 - mean_squared_error: 1.8483 - val_loss: 3.3704 - val_mean_squared_error: 3.3704\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.7949 - mean_squared_error: 1.7949 - val_loss: 3.2085 - val_mean_squared_error: 3.2085\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.8352 - mean_squared_error: 1.8352 - val_loss: 3.4324 - val_mean_squared_error: 3.4324\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.9036 - mean_squared_error: 1.9036 - val_loss: 2.9613 - val_mean_squared_error: 2.9613\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.7133 - mean_squared_error: 1.7133 - val_loss: 3.1131 - val_mean_squared_error: 3.1131\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.7007 - mean_squared_error: 1.7007 - val_loss: 3.2731 - val_mean_squared_error: 3.2731\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.7247 - mean_squared_error: 1.7247 - val_loss: 2.9587 - val_mean_squared_error: 2.9587\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 285us/sample - loss: 1.6305 - mean_squared_error: 1.6305 - val_loss: 3.1655 - val_mean_squared_error: 3.1655\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.7948 - mean_squared_error: 1.7948 - val_loss: 3.6298 - val_mean_squared_error: 3.6298\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.7986 - mean_squared_error: 1.7986 - val_loss: 3.6616 - val_mean_squared_error: 3.6616\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.7916 - mean_squared_error: 1.7916 - val_loss: 3.2063 - val_mean_squared_error: 3.2063\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 1.7883 - mean_squared_error: 1.7883 - val_loss: 3.5868 - val_mean_squared_error: 3.5868\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.6178 - mean_squared_error: 1.6178 - val_loss: 3.1612 - val_mean_squared_error: 3.1612\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.8174 - mean_squared_error: 1.8174 - val_loss: 3.0607 - val_mean_squared_error: 3.0607\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.6723 - mean_squared_error: 1.6723 - val_loss: 3.0601 - val_mean_squared_error: 3.0601\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.5816 - mean_squared_error: 1.5816 - val_loss: 3.0963 - val_mean_squared_error: 3.0963\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.6449 - mean_squared_error: 1.6449 - val_loss: 3.1157 - val_mean_squared_error: 3.1157\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.7288 - mean_squared_error: 1.7288 - val_loss: 2.9552 - val_mean_squared_error: 2.9552\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.6435 - mean_squared_error: 1.6435 - val_loss: 2.9209 - val_mean_squared_error: 2.9209\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.5276 - mean_squared_error: 1.5276 - val_loss: 3.0290 - val_mean_squared_error: 3.0290\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.6371 - mean_squared_error: 1.6371 - val_loss: 3.2716 - val_mean_squared_error: 3.2716\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.5307 - mean_squared_error: 1.5307 - val_loss: 2.9299 - val_mean_squared_error: 2.9299\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.4634 - mean_squared_error: 1.4634 - val_loss: 2.9627 - val_mean_squared_error: 2.9627\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.5141 - mean_squared_error: 1.5141 - val_loss: 2.9261 - val_mean_squared_error: 2.9261\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.5220 - mean_squared_error: 1.5220 - val_loss: 3.0705 - val_mean_squared_error: 3.0705\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.6131 - mean_squared_error: 1.6131 - val_loss: 3.1765 - val_mean_squared_error: 3.1765\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.5022 - mean_squared_error: 1.5022 - val_loss: 2.8382 - val_mean_squared_error: 2.8382\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 287us/sample - loss: 1.5017 - mean_squared_error: 1.5017 - val_loss: 3.0735 - val_mean_squared_error: 3.0735\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.5243 - mean_squared_error: 1.5243 - val_loss: 2.9996 - val_mean_squared_error: 2.9996\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.6970 - mean_squared_error: 1.6970 - val_loss: 3.1996 - val_mean_squared_error: 3.1996\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.5044 - mean_squared_error: 1.5044 - val_loss: 2.9407 - val_mean_squared_error: 2.9407\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.4585 - mean_squared_error: 1.4585 - val_loss: 2.9264 - val_mean_squared_error: 2.9264\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.5093 - mean_squared_error: 1.5093 - val_loss: 2.9994 - val_mean_squared_error: 2.9994\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.3524 - mean_squared_error: 1.3524 - val_loss: 2.9062 - val_mean_squared_error: 2.9062\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.4117 - mean_squared_error: 1.4117 - val_loss: 2.8826 - val_mean_squared_error: 2.8826\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 1.3736 - mean_squared_error: 1.3736 - val_loss: 3.0447 - val_mean_squared_error: 3.0447\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.4140 - mean_squared_error: 1.4140 - val_loss: 3.1324 - val_mean_squared_error: 3.1324\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.4548 - mean_squared_error: 1.4548 - val_loss: 2.9798 - val_mean_squared_error: 2.9798\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.4319 - mean_squared_error: 1.4319 - val_loss: 2.9281 - val_mean_squared_error: 2.9281\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.3698 - mean_squared_error: 1.3698 - val_loss: 3.0548 - val_mean_squared_error: 3.0548\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.3504 - mean_squared_error: 1.3504 - val_loss: 2.7826 - val_mean_squared_error: 2.7826\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.4328 - mean_squared_error: 1.4328 - val_loss: 3.1919 - val_mean_squared_error: 3.1919\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.3433 - mean_squared_error: 1.3433 - val_loss: 3.0759 - val_mean_squared_error: 3.0759\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.5905 - mean_squared_error: 1.5905 - val_loss: 3.0916 - val_mean_squared_error: 3.0916\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.3057 - mean_squared_error: 1.3057 - val_loss: 2.8730 - val_mean_squared_error: 2.8730\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.3301 - mean_squared_error: 1.3301 - val_loss: 2.9188 - val_mean_squared_error: 2.9188\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.4177 - mean_squared_error: 1.4177 - val_loss: 3.0622 - val_mean_squared_error: 3.0622\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.4214 - mean_squared_error: 1.4214 - val_loss: 2.9185 - val_mean_squared_error: 2.9185\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.3392 - mean_squared_error: 1.3392 - val_loss: 2.9177 - val_mean_squared_error: 2.9177\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.3535 - mean_squared_error: 1.3535 - val_loss: 2.7693 - val_mean_squared_error: 2.7693\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.3440 - mean_squared_error: 1.3440 - val_loss: 3.2663 - val_mean_squared_error: 3.2663\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.2981 - mean_squared_error: 1.2981 - val_loss: 2.9512 - val_mean_squared_error: 2.9512\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.2605 - mean_squared_error: 1.2605 - val_loss: 3.1548 - val_mean_squared_error: 3.1548\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.2529 - mean_squared_error: 1.2529 - val_loss: 2.7875 - val_mean_squared_error: 2.7875\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.2510 - mean_squared_error: 1.2510 - val_loss: 2.9835 - val_mean_squared_error: 2.9835\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.2847 - mean_squared_error: 1.2847 - val_loss: 2.8218 - val_mean_squared_error: 2.8218\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2667 - mean_squared_error: 1.2667 - val_loss: 2.9213 - val_mean_squared_error: 2.9213\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2640 - mean_squared_error: 1.2640 - val_loss: 2.9127 - val_mean_squared_error: 2.9127\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.3770 - mean_squared_error: 1.3770 - val_loss: 3.1925 - val_mean_squared_error: 3.1925\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.3102 - mean_squared_error: 1.3102 - val_loss: 2.8848 - val_mean_squared_error: 2.8848\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.2588 - mean_squared_error: 1.2588 - val_loss: 3.0142 - val_mean_squared_error: 3.0142\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.2288 - mean_squared_error: 1.2288 - val_loss: 2.8055 - val_mean_squared_error: 2.8055\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2613 - mean_squared_error: 1.2613 - val_loss: 2.7578 - val_mean_squared_error: 2.7578\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.3006 - mean_squared_error: 1.3006 - val_loss: 2.8286 - val_mean_squared_error: 2.8286\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.2871 - mean_squared_error: 1.2871 - val_loss: 2.7641 - val_mean_squared_error: 2.7641\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.2887 - mean_squared_error: 1.2887 - val_loss: 2.9298 - val_mean_squared_error: 2.9298\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.2336 - mean_squared_error: 1.2336 - val_loss: 3.0422 - val_mean_squared_error: 3.0422\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.4152 - mean_squared_error: 1.4152 - val_loss: 3.3211 - val_mean_squared_error: 3.3211\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.2072 - mean_squared_error: 1.2072 - val_loss: 3.2424 - val_mean_squared_error: 3.2424\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2155 - mean_squared_error: 1.2155 - val_loss: 3.2794 - val_mean_squared_error: 3.2794\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.2750 - mean_squared_error: 1.2750 - val_loss: 2.8636 - val_mean_squared_error: 2.8636\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.2374 - mean_squared_error: 1.2374 - val_loss: 2.9672 - val_mean_squared_error: 2.9672\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.2020 - mean_squared_error: 1.2020 - val_loss: 2.8320 - val_mean_squared_error: 2.8320\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.1526 - mean_squared_error: 1.1526 - val_loss: 2.7560 - val_mean_squared_error: 2.7560\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.1845 - mean_squared_error: 1.1845 - val_loss: 2.9294 - val_mean_squared_error: 2.9294\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.1637 - mean_squared_error: 1.1637 - val_loss: 2.8677 - val_mean_squared_error: 2.8677\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.1693 - mean_squared_error: 1.1693 - val_loss: 2.8893 - val_mean_squared_error: 2.8893\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.1735 - mean_squared_error: 1.1735 - val_loss: 3.1143 - val_mean_squared_error: 3.1143\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.2084 - mean_squared_error: 1.2084 - val_loss: 3.0420 - val_mean_squared_error: 3.0420\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.2613 - mean_squared_error: 1.2613 - val_loss: 2.7555 - val_mean_squared_error: 2.7555\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.2047 - mean_squared_error: 1.2047 - val_loss: 3.4434 - val_mean_squared_error: 3.4434\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.1834 - mean_squared_error: 1.1834 - val_loss: 2.8813 - val_mean_squared_error: 2.8813\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.2295 - mean_squared_error: 1.2295 - val_loss: 2.7468 - val_mean_squared_error: 2.7468\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.2155 - mean_squared_error: 1.2155 - val_loss: 2.8637 - val_mean_squared_error: 2.8637\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.2146 - mean_squared_error: 1.2146 - val_loss: 2.9788 - val_mean_squared_error: 2.9788\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.2282 - mean_squared_error: 1.2282 - val_loss: 3.1662 - val_mean_squared_error: 3.1662\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.0426 - mean_squared_error: 1.0426 - val_loss: 2.7944 - val_mean_squared_error: 2.7944\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.1812 - mean_squared_error: 1.1812 - val_loss: 2.9256 - val_mean_squared_error: 2.9256\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.1862 - mean_squared_error: 1.1862 - val_loss: 2.8209 - val_mean_squared_error: 2.8209\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.1030 - mean_squared_error: 1.1030 - val_loss: 2.7328 - val_mean_squared_error: 2.7328\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.1938 - mean_squared_error: 1.1938 - val_loss: 2.8251 - val_mean_squared_error: 2.8251\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.1224 - mean_squared_error: 1.1224 - val_loss: 2.7264 - val_mean_squared_error: 2.7264\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 289us/sample - loss: 1.1866 - mean_squared_error: 1.1866 - val_loss: 2.9803 - val_mean_squared_error: 2.9803\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.0709 - mean_squared_error: 1.0709 - val_loss: 2.9021 - val_mean_squared_error: 2.9021\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 1.1714 - mean_squared_error: 1.1714 - val_loss: 2.8735 - val_mean_squared_error: 2.8735\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.0955 - mean_squared_error: 1.0955 - val_loss: 2.8618 - val_mean_squared_error: 2.8618\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 1.0924 - mean_squared_error: 1.0924 - val_loss: 2.8886 - val_mean_squared_error: 2.8886\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.0044 - mean_squared_error: 1.0044 - val_loss: 2.7458 - val_mean_squared_error: 2.7458\n",
            "==================================================\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 47, 47, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 47, 47, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 23, 23, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 23, 23, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 11, 11, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 719us/sample - loss: 658.2473 - mean_squared_error: 658.2473 - val_loss: 290.2615 - val_mean_squared_error: 290.2615\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 291us/sample - loss: 24.7004 - mean_squared_error: 24.7004 - val_loss: 165.3872 - val_mean_squared_error: 165.3872\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 19.1928 - mean_squared_error: 19.1928 - val_loss: 67.7393 - val_mean_squared_error: 67.7393\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 15.7305 - mean_squared_error: 15.7305 - val_loss: 29.8510 - val_mean_squared_error: 29.8510\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 312us/sample - loss: 15.1602 - mean_squared_error: 15.1602 - val_loss: 16.7579 - val_mean_squared_error: 16.7579\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 323us/sample - loss: 13.5012 - mean_squared_error: 13.5012 - val_loss: 18.0298 - val_mean_squared_error: 18.0298\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 295us/sample - loss: 13.5276 - mean_squared_error: 13.5276 - val_loss: 14.3137 - val_mean_squared_error: 14.3137\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 12.2554 - mean_squared_error: 12.2554 - val_loss: 10.6344 - val_mean_squared_error: 10.6344\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 10.8583 - mean_squared_error: 10.8583 - val_loss: 9.3793 - val_mean_squared_error: 9.3793\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 11.1603 - mean_squared_error: 11.1603 - val_loss: 11.1263 - val_mean_squared_error: 11.1263\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 11.2308 - mean_squared_error: 11.2308 - val_loss: 14.6836 - val_mean_squared_error: 14.6836\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 290us/sample - loss: 10.7420 - mean_squared_error: 10.7420 - val_loss: 9.8294 - val_mean_squared_error: 9.8294\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 10.6394 - mean_squared_error: 10.6394 - val_loss: 12.7667 - val_mean_squared_error: 12.7667\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 10.4115 - mean_squared_error: 10.4115 - val_loss: 9.8517 - val_mean_squared_error: 9.8517\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 10.1806 - mean_squared_error: 10.1806 - val_loss: 10.8530 - val_mean_squared_error: 10.8530\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 9.4574 - mean_squared_error: 9.4574 - val_loss: 8.7040 - val_mean_squared_error: 8.7040\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 9.0333 - mean_squared_error: 9.0333 - val_loss: 8.5232 - val_mean_squared_error: 8.5232\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 9.2281 - mean_squared_error: 9.2281 - val_loss: 8.9112 - val_mean_squared_error: 8.9112\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 8.5386 - mean_squared_error: 8.5386 - val_loss: 7.9539 - val_mean_squared_error: 7.9539\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 8.0634 - mean_squared_error: 8.0634 - val_loss: 7.9245 - val_mean_squared_error: 7.9245\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 7.6665 - mean_squared_error: 7.6665 - val_loss: 8.4996 - val_mean_squared_error: 8.4996\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 7.6312 - mean_squared_error: 7.6312 - val_loss: 7.7865 - val_mean_squared_error: 7.7865\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 7.5598 - mean_squared_error: 7.5598 - val_loss: 12.1380 - val_mean_squared_error: 12.1380\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 7.1799 - mean_squared_error: 7.1799 - val_loss: 7.2031 - val_mean_squared_error: 7.2031\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 6.9570 - mean_squared_error: 6.9570 - val_loss: 7.4392 - val_mean_squared_error: 7.4392\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 6.4042 - mean_squared_error: 6.4042 - val_loss: 7.0091 - val_mean_squared_error: 7.0091\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 5.9344 - mean_squared_error: 5.9344 - val_loss: 6.6878 - val_mean_squared_error: 6.6878\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 5.8264 - mean_squared_error: 5.8264 - val_loss: 6.6282 - val_mean_squared_error: 6.6282\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 5.8336 - mean_squared_error: 5.8336 - val_loss: 6.9478 - val_mean_squared_error: 6.9478\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 5.9867 - mean_squared_error: 5.9867 - val_loss: 6.2487 - val_mean_squared_error: 6.2487\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 5.3375 - mean_squared_error: 5.3375 - val_loss: 5.9082 - val_mean_squared_error: 5.9082\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 5.3917 - mean_squared_error: 5.3917 - val_loss: 5.5851 - val_mean_squared_error: 5.5851\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 5.0428 - mean_squared_error: 5.0428 - val_loss: 5.8880 - val_mean_squared_error: 5.8880\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 4.7819 - mean_squared_error: 4.7819 - val_loss: 5.3983 - val_mean_squared_error: 5.3983\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 4.7949 - mean_squared_error: 4.7949 - val_loss: 5.8007 - val_mean_squared_error: 5.8007\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 4.8737 - mean_squared_error: 4.8737 - val_loss: 6.0340 - val_mean_squared_error: 6.0340\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 4.5782 - mean_squared_error: 4.5782 - val_loss: 5.9766 - val_mean_squared_error: 5.9766\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 4.8295 - mean_squared_error: 4.8295 - val_loss: 6.1286 - val_mean_squared_error: 6.1286\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 4.2083 - mean_squared_error: 4.2083 - val_loss: 4.8243 - val_mean_squared_error: 4.8243\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 4.4910 - mean_squared_error: 4.4910 - val_loss: 5.7710 - val_mean_squared_error: 5.7710\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 4.3516 - mean_squared_error: 4.3516 - val_loss: 4.9558 - val_mean_squared_error: 4.9558\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 4.2062 - mean_squared_error: 4.2062 - val_loss: 5.6800 - val_mean_squared_error: 5.6800\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 4.0245 - mean_squared_error: 4.0245 - val_loss: 4.8134 - val_mean_squared_error: 4.8134\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 4.1142 - mean_squared_error: 4.1142 - val_loss: 5.1719 - val_mean_squared_error: 5.1719\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 3.8275 - mean_squared_error: 3.8275 - val_loss: 4.5545 - val_mean_squared_error: 4.5545\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 3.7186 - mean_squared_error: 3.7186 - val_loss: 4.4705 - val_mean_squared_error: 4.4705\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 3.7386 - mean_squared_error: 3.7386 - val_loss: 4.8782 - val_mean_squared_error: 4.8782\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 3.6879 - mean_squared_error: 3.6879 - val_loss: 5.1754 - val_mean_squared_error: 5.1754\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 3.7001 - mean_squared_error: 3.7001 - val_loss: 4.2629 - val_mean_squared_error: 4.2629\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 3.8649 - mean_squared_error: 3.8649 - val_loss: 4.5981 - val_mean_squared_error: 4.5981\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 3.3927 - mean_squared_error: 3.3927 - val_loss: 4.2730 - val_mean_squared_error: 4.2730\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 3.4769 - mean_squared_error: 3.4769 - val_loss: 4.3010 - val_mean_squared_error: 4.3010\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 3.3282 - mean_squared_error: 3.3282 - val_loss: 4.4291 - val_mean_squared_error: 4.4291\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 3.3409 - mean_squared_error: 3.3409 - val_loss: 4.1623 - val_mean_squared_error: 4.1623\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 3.1618 - mean_squared_error: 3.1618 - val_loss: 3.8821 - val_mean_squared_error: 3.8821\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 3.1948 - mean_squared_error: 3.1948 - val_loss: 5.3485 - val_mean_squared_error: 5.3485\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 3.0832 - mean_squared_error: 3.0832 - val_loss: 4.1945 - val_mean_squared_error: 4.1945\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 3.1883 - mean_squared_error: 3.1883 - val_loss: 3.8582 - val_mean_squared_error: 3.8582\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 3.0475 - mean_squared_error: 3.0475 - val_loss: 4.1328 - val_mean_squared_error: 4.1328\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.9834 - mean_squared_error: 2.9834 - val_loss: 4.9528 - val_mean_squared_error: 4.9528\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 2.9890 - mean_squared_error: 2.9890 - val_loss: 3.5659 - val_mean_squared_error: 3.5659\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 3.2585 - mean_squared_error: 3.2585 - val_loss: 4.4982 - val_mean_squared_error: 4.4982\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.7720 - mean_squared_error: 2.7720 - val_loss: 3.6995 - val_mean_squared_error: 3.6995\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 3.2163 - mean_squared_error: 3.2163 - val_loss: 5.7120 - val_mean_squared_error: 5.7120\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 3.0197 - mean_squared_error: 3.0197 - val_loss: 3.7812 - val_mean_squared_error: 3.7812\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 2.8850 - mean_squared_error: 2.8850 - val_loss: 4.8447 - val_mean_squared_error: 4.8447\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 2.6042 - mean_squared_error: 2.6042 - val_loss: 3.4615 - val_mean_squared_error: 3.4615\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 2.7618 - mean_squared_error: 2.7618 - val_loss: 4.1844 - val_mean_squared_error: 4.1844\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 2.8514 - mean_squared_error: 2.8514 - val_loss: 3.8925 - val_mean_squared_error: 3.8925\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 3.1474 - mean_squared_error: 3.1474 - val_loss: 4.7078 - val_mean_squared_error: 4.7078\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.8264 - mean_squared_error: 2.8264 - val_loss: 3.5852 - val_mean_squared_error: 3.5852\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 2.6835 - mean_squared_error: 2.6835 - val_loss: 4.4014 - val_mean_squared_error: 4.4014\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.3817 - mean_squared_error: 2.3817 - val_loss: 3.5277 - val_mean_squared_error: 3.5277\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 2.5358 - mean_squared_error: 2.5358 - val_loss: 4.0008 - val_mean_squared_error: 4.0008\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 2.4694 - mean_squared_error: 2.4694 - val_loss: 3.6489 - val_mean_squared_error: 3.6489\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.4563 - mean_squared_error: 2.4563 - val_loss: 3.5015 - val_mean_squared_error: 3.5015\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.3972 - mean_squared_error: 2.3972 - val_loss: 3.7888 - val_mean_squared_error: 3.7888\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.4052 - mean_squared_error: 2.4052 - val_loss: 3.3531 - val_mean_squared_error: 3.3531\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 2.3837 - mean_squared_error: 2.3837 - val_loss: 3.5083 - val_mean_squared_error: 3.5083\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.2429 - mean_squared_error: 2.2429 - val_loss: 3.4156 - val_mean_squared_error: 3.4156\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.1363 - mean_squared_error: 2.1363 - val_loss: 3.3198 - val_mean_squared_error: 3.3198\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.3832 - mean_squared_error: 2.3832 - val_loss: 3.5755 - val_mean_squared_error: 3.5755\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.3378 - mean_squared_error: 2.3378 - val_loss: 3.2669 - val_mean_squared_error: 3.2669\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 2.2816 - mean_squared_error: 2.2816 - val_loss: 3.4298 - val_mean_squared_error: 3.4298\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.4229 - mean_squared_error: 2.4229 - val_loss: 3.6307 - val_mean_squared_error: 3.6307\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.2964 - mean_squared_error: 2.2964 - val_loss: 3.3799 - val_mean_squared_error: 3.3799\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.1407 - mean_squared_error: 2.1407 - val_loss: 3.3825 - val_mean_squared_error: 3.3825\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.3484 - mean_squared_error: 2.3484 - val_loss: 3.2427 - val_mean_squared_error: 3.2427\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 2.0671 - mean_squared_error: 2.0671 - val_loss: 3.3138 - val_mean_squared_error: 3.3138\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.0718 - mean_squared_error: 2.0718 - val_loss: 3.5415 - val_mean_squared_error: 3.5415\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.1978 - mean_squared_error: 2.1978 - val_loss: 3.3167 - val_mean_squared_error: 3.3167\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 2.1241 - mean_squared_error: 2.1241 - val_loss: 3.3545 - val_mean_squared_error: 3.3545\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.9897 - mean_squared_error: 1.9897 - val_loss: 3.4338 - val_mean_squared_error: 3.4338\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 2.0363 - mean_squared_error: 2.0363 - val_loss: 3.0422 - val_mean_squared_error: 3.0422\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 2.1968 - mean_squared_error: 2.1968 - val_loss: 3.5604 - val_mean_squared_error: 3.5604\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.2268 - mean_squared_error: 2.2268 - val_loss: 3.2434 - val_mean_squared_error: 3.2434\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.0695 - mean_squared_error: 2.0695 - val_loss: 3.2518 - val_mean_squared_error: 3.2518\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.0286 - mean_squared_error: 2.0286 - val_loss: 3.3284 - val_mean_squared_error: 3.3284\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.8944 - mean_squared_error: 1.8944 - val_loss: 3.6493 - val_mean_squared_error: 3.6493\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.9773 - mean_squared_error: 1.9773 - val_loss: 3.3334 - val_mean_squared_error: 3.3334\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.9458 - mean_squared_error: 1.9458 - val_loss: 3.0350 - val_mean_squared_error: 3.0350\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.9169 - mean_squared_error: 1.9169 - val_loss: 3.3636 - val_mean_squared_error: 3.3636\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.0373 - mean_squared_error: 2.0373 - val_loss: 3.4078 - val_mean_squared_error: 3.4078\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.1152 - mean_squared_error: 2.1152 - val_loss: 3.5602 - val_mean_squared_error: 3.5602\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.8443 - mean_squared_error: 1.8443 - val_loss: 3.3380 - val_mean_squared_error: 3.3380\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.8649 - mean_squared_error: 1.8649 - val_loss: 3.0509 - val_mean_squared_error: 3.0509\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.8560 - mean_squared_error: 1.8560 - val_loss: 3.0519 - val_mean_squared_error: 3.0519\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.8285 - mean_squared_error: 1.8285 - val_loss: 3.1977 - val_mean_squared_error: 3.1977\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.8605 - mean_squared_error: 1.8605 - val_loss: 2.9555 - val_mean_squared_error: 2.9555\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.9110 - mean_squared_error: 1.9110 - val_loss: 3.3939 - val_mean_squared_error: 3.3939\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.8035 - mean_squared_error: 1.8035 - val_loss: 3.1150 - val_mean_squared_error: 3.1150\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.7669 - mean_squared_error: 1.7669 - val_loss: 3.1013 - val_mean_squared_error: 3.1013\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.7423 - mean_squared_error: 1.7423 - val_loss: 3.0163 - val_mean_squared_error: 3.0163\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.8027 - mean_squared_error: 1.8027 - val_loss: 3.5013 - val_mean_squared_error: 3.5013\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.7590 - mean_squared_error: 1.7590 - val_loss: 3.2760 - val_mean_squared_error: 3.2760\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.7923 - mean_squared_error: 1.7923 - val_loss: 3.1092 - val_mean_squared_error: 3.1092\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.8076 - mean_squared_error: 1.8076 - val_loss: 2.9420 - val_mean_squared_error: 2.9420\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.7093 - mean_squared_error: 1.7093 - val_loss: 3.2427 - val_mean_squared_error: 3.2427\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.6871 - mean_squared_error: 1.6871 - val_loss: 3.5771 - val_mean_squared_error: 3.5771\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.6640 - mean_squared_error: 1.6640 - val_loss: 3.4010 - val_mean_squared_error: 3.4010\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.7552 - mean_squared_error: 1.7552 - val_loss: 2.9477 - val_mean_squared_error: 2.9477\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.6491 - mean_squared_error: 1.6491 - val_loss: 2.9121 - val_mean_squared_error: 2.9121\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.7596 - mean_squared_error: 1.7596 - val_loss: 3.4841 - val_mean_squared_error: 3.4841\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.7406 - mean_squared_error: 1.7406 - val_loss: 3.3041 - val_mean_squared_error: 3.3041\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.8101 - mean_squared_error: 1.8101 - val_loss: 3.5848 - val_mean_squared_error: 3.5848\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.6377 - mean_squared_error: 1.6377 - val_loss: 3.1560 - val_mean_squared_error: 3.1560\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.6973 - mean_squared_error: 1.6973 - val_loss: 2.9123 - val_mean_squared_error: 2.9123\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.5947 - mean_squared_error: 1.5947 - val_loss: 2.9298 - val_mean_squared_error: 2.9298\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.7590 - mean_squared_error: 1.7590 - val_loss: 3.2344 - val_mean_squared_error: 3.2344\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.6389 - mean_squared_error: 1.6389 - val_loss: 3.6390 - val_mean_squared_error: 3.6390\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.7180 - mean_squared_error: 1.7180 - val_loss: 3.0059 - val_mean_squared_error: 3.0059\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.6800 - mean_squared_error: 1.6800 - val_loss: 3.0557 - val_mean_squared_error: 3.0557\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.6518 - mean_squared_error: 1.6518 - val_loss: 3.3344 - val_mean_squared_error: 3.3344\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.6061 - mean_squared_error: 1.6061 - val_loss: 2.9767 - val_mean_squared_error: 2.9767\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.6251 - mean_squared_error: 1.6251 - val_loss: 3.3506 - val_mean_squared_error: 3.3506\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.5767 - mean_squared_error: 1.5767 - val_loss: 3.3153 - val_mean_squared_error: 3.3153\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.7178 - mean_squared_error: 1.7178 - val_loss: 3.8299 - val_mean_squared_error: 3.8299\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.5316 - mean_squared_error: 1.5316 - val_loss: 3.1496 - val_mean_squared_error: 3.1496\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.6263 - mean_squared_error: 1.6263 - val_loss: 3.4267 - val_mean_squared_error: 3.4267\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.4975 - mean_squared_error: 1.4975 - val_loss: 3.0484 - val_mean_squared_error: 3.0484\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.6077 - mean_squared_error: 1.6077 - val_loss: 3.3202 - val_mean_squared_error: 3.3202\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.4394 - mean_squared_error: 1.4394 - val_loss: 2.8525 - val_mean_squared_error: 2.8525\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.4372 - mean_squared_error: 1.4372 - val_loss: 2.8951 - val_mean_squared_error: 2.8951\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.4580 - mean_squared_error: 1.4580 - val_loss: 3.1049 - val_mean_squared_error: 3.1049\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.4775 - mean_squared_error: 1.4775 - val_loss: 2.9317 - val_mean_squared_error: 2.9317\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.4852 - mean_squared_error: 1.4852 - val_loss: 2.8955 - val_mean_squared_error: 2.8955\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.4594 - mean_squared_error: 1.4594 - val_loss: 3.3986 - val_mean_squared_error: 3.3986\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.4128 - mean_squared_error: 1.4128 - val_loss: 2.8683 - val_mean_squared_error: 2.8683\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.4854 - mean_squared_error: 1.4854 - val_loss: 3.3116 - val_mean_squared_error: 3.3116\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.5002 - mean_squared_error: 1.5002 - val_loss: 3.1849 - val_mean_squared_error: 3.1849\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.4840 - mean_squared_error: 1.4840 - val_loss: 3.0787 - val_mean_squared_error: 3.0787\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.4583 - mean_squared_error: 1.4583 - val_loss: 3.1143 - val_mean_squared_error: 3.1143\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 1.3758 - mean_squared_error: 1.3758 - val_loss: 3.4813 - val_mean_squared_error: 3.4813\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.4895 - mean_squared_error: 1.4895 - val_loss: 3.1469 - val_mean_squared_error: 3.1469\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.4954 - mean_squared_error: 1.4954 - val_loss: 3.0018 - val_mean_squared_error: 3.0018\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.4181 - mean_squared_error: 1.4181 - val_loss: 3.0652 - val_mean_squared_error: 3.0652\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.5544 - mean_squared_error: 1.5544 - val_loss: 3.1618 - val_mean_squared_error: 3.1618\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.3553 - mean_squared_error: 1.3553 - val_loss: 3.3041 - val_mean_squared_error: 3.3041\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.4052 - mean_squared_error: 1.4052 - val_loss: 3.0844 - val_mean_squared_error: 3.0844\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.3666 - mean_squared_error: 1.3666 - val_loss: 2.9906 - val_mean_squared_error: 2.9906\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.5024 - mean_squared_error: 1.5024 - val_loss: 3.0908 - val_mean_squared_error: 3.0908\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.3949 - mean_squared_error: 1.3949 - val_loss: 3.3345 - val_mean_squared_error: 3.3345\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.4993 - mean_squared_error: 1.4993 - val_loss: 3.0950 - val_mean_squared_error: 3.0950\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.3377 - mean_squared_error: 1.3377 - val_loss: 2.9253 - val_mean_squared_error: 2.9253\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.4229 - mean_squared_error: 1.4229 - val_loss: 2.8192 - val_mean_squared_error: 2.8192\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.4685 - mean_squared_error: 1.4685 - val_loss: 2.9352 - val_mean_squared_error: 2.9352\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.3648 - mean_squared_error: 1.3648 - val_loss: 2.9422 - val_mean_squared_error: 2.9422\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.4084 - mean_squared_error: 1.4084 - val_loss: 2.9218 - val_mean_squared_error: 2.9218\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.3251 - mean_squared_error: 1.3251 - val_loss: 2.8779 - val_mean_squared_error: 2.8779\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.3546 - mean_squared_error: 1.3546 - val_loss: 3.1600 - val_mean_squared_error: 3.1600\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.3461 - mean_squared_error: 1.3461 - val_loss: 2.9680 - val_mean_squared_error: 2.9680\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.3642 - mean_squared_error: 1.3642 - val_loss: 2.9219 - val_mean_squared_error: 2.9219\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.3619 - mean_squared_error: 1.3619 - val_loss: 2.7976 - val_mean_squared_error: 2.7976\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2728 - mean_squared_error: 1.2728 - val_loss: 2.9788 - val_mean_squared_error: 2.9788\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.2778 - mean_squared_error: 1.2778 - val_loss: 2.8290 - val_mean_squared_error: 2.8290\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.3978 - mean_squared_error: 1.3978 - val_loss: 3.0273 - val_mean_squared_error: 3.0273\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.2691 - mean_squared_error: 1.2691 - val_loss: 3.1371 - val_mean_squared_error: 3.1371\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.3536 - mean_squared_error: 1.3536 - val_loss: 3.2754 - val_mean_squared_error: 3.2754\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.3093 - mean_squared_error: 1.3093 - val_loss: 2.9544 - val_mean_squared_error: 2.9544\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 1.3510 - mean_squared_error: 1.3510 - val_loss: 2.9378 - val_mean_squared_error: 2.9378\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.2352 - mean_squared_error: 1.2352 - val_loss: 3.1165 - val_mean_squared_error: 3.1165\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.2923 - mean_squared_error: 1.2923 - val_loss: 2.8035 - val_mean_squared_error: 2.8035\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.3194 - mean_squared_error: 1.3194 - val_loss: 3.1109 - val_mean_squared_error: 3.1109\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2897 - mean_squared_error: 1.2897 - val_loss: 2.9016 - val_mean_squared_error: 2.9016\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.2551 - mean_squared_error: 1.2551 - val_loss: 3.0160 - val_mean_squared_error: 3.0160\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.2286 - mean_squared_error: 1.2286 - val_loss: 2.9512 - val_mean_squared_error: 2.9512\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.1834 - mean_squared_error: 1.1834 - val_loss: 3.2849 - val_mean_squared_error: 3.2849\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.2188 - mean_squared_error: 1.2188 - val_loss: 2.9106 - val_mean_squared_error: 2.9106\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.1956 - mean_squared_error: 1.1956 - val_loss: 3.0473 - val_mean_squared_error: 3.0473\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.2654 - mean_squared_error: 1.2654 - val_loss: 3.1335 - val_mean_squared_error: 3.1335\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.2006 - mean_squared_error: 1.2006 - val_loss: 2.7628 - val_mean_squared_error: 2.7628\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.2510 - mean_squared_error: 1.2510 - val_loss: 2.8453 - val_mean_squared_error: 2.8453\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.2205 - mean_squared_error: 1.2205 - val_loss: 2.9722 - val_mean_squared_error: 2.9722\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 1.1973 - mean_squared_error: 1.1973 - val_loss: 2.9052 - val_mean_squared_error: 2.9052\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.1711 - mean_squared_error: 1.1711 - val_loss: 3.0387 - val_mean_squared_error: 3.0387\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 1.1837 - mean_squared_error: 1.1837 - val_loss: 3.3280 - val_mean_squared_error: 3.3280\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.2116 - mean_squared_error: 1.2116 - val_loss: 2.9807 - val_mean_squared_error: 2.9807\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 1.2192 - mean_squared_error: 1.2192 - val_loss: 2.9459 - val_mean_squared_error: 2.9459\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2151 - mean_squared_error: 1.2151 - val_loss: 3.0766 - val_mean_squared_error: 3.0766\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.1565 - mean_squared_error: 1.1565 - val_loss: 2.9416 - val_mean_squared_error: 2.9416\n",
            "==================================================\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_15 (Conv2D)           (None, 47, 47, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 47, 47, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 23, 23, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 23, 23, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 11, 11, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 649us/sample - loss: 179.4249 - mean_squared_error: 179.4249 - val_loss: 17926.0858 - val_mean_squared_error: 17926.0879\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 14.1257 - mean_squared_error: 14.1257 - val_loss: 102.8252 - val_mean_squared_error: 102.8252\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 11.7555 - mean_squared_error: 11.7555 - val_loss: 28.7266 - val_mean_squared_error: 28.7266\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 0s 254us/sample - loss: 11.2626 - mean_squared_error: 11.2626 - val_loss: 12.1707 - val_mean_squared_error: 12.1707\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 10.2960 - mean_squared_error: 10.2960 - val_loss: 10.3585 - val_mean_squared_error: 10.3585\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 9.8333 - mean_squared_error: 9.8333 - val_loss: 9.3637 - val_mean_squared_error: 9.3637\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 0s 231us/sample - loss: 9.7206 - mean_squared_error: 9.7206 - val_loss: 9.5083 - val_mean_squared_error: 9.5083\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 9.5473 - mean_squared_error: 9.5473 - val_loss: 8.8492 - val_mean_squared_error: 8.8492\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 9.2938 - mean_squared_error: 9.2938 - val_loss: 8.8506 - val_mean_squared_error: 8.8506\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 9.1435 - mean_squared_error: 9.1435 - val_loss: 8.7753 - val_mean_squared_error: 8.7753\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 8.8466 - mean_squared_error: 8.8466 - val_loss: 8.7503 - val_mean_squared_error: 8.7503\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 8.5122 - mean_squared_error: 8.5122 - val_loss: 8.3786 - val_mean_squared_error: 8.3786\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 8.3589 - mean_squared_error: 8.3589 - val_loss: 8.2799 - val_mean_squared_error: 8.2799\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 7.9806 - mean_squared_error: 7.9806 - val_loss: 8.2974 - val_mean_squared_error: 8.2974\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 7.8225 - mean_squared_error: 7.8225 - val_loss: 7.7742 - val_mean_squared_error: 7.7742\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 7.5502 - mean_squared_error: 7.5502 - val_loss: 7.8660 - val_mean_squared_error: 7.8660\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 7.2534 - mean_squared_error: 7.2534 - val_loss: 7.7319 - val_mean_squared_error: 7.7319\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 7.0930 - mean_squared_error: 7.0930 - val_loss: 7.2594 - val_mean_squared_error: 7.2594\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 6.8527 - mean_squared_error: 6.8527 - val_loss: 7.3855 - val_mean_squared_error: 7.3855\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 6.4943 - mean_squared_error: 6.4943 - val_loss: 7.1198 - val_mean_squared_error: 7.1198\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 6.3526 - mean_squared_error: 6.3526 - val_loss: 6.8479 - val_mean_squared_error: 6.8479\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 5.9916 - mean_squared_error: 5.9916 - val_loss: 6.6541 - val_mean_squared_error: 6.6541\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 0s 229us/sample - loss: 5.7174 - mean_squared_error: 5.7174 - val_loss: 6.4733 - val_mean_squared_error: 6.4733\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 5.4282 - mean_squared_error: 5.4281 - val_loss: 6.3782 - val_mean_squared_error: 6.3782\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 5.1734 - mean_squared_error: 5.1734 - val_loss: 6.0480 - val_mean_squared_error: 6.0480\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 4.8901 - mean_squared_error: 4.8901 - val_loss: 5.4586 - val_mean_squared_error: 5.4586\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 4.7259 - mean_squared_error: 4.7259 - val_loss: 5.3376 - val_mean_squared_error: 5.3376\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 4.5995 - mean_squared_error: 4.5995 - val_loss: 5.4281 - val_mean_squared_error: 5.4281\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 4.4890 - mean_squared_error: 4.4890 - val_loss: 5.2161 - val_mean_squared_error: 5.2161\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 4.4597 - mean_squared_error: 4.4597 - val_loss: 5.2042 - val_mean_squared_error: 5.2042\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 4.1684 - mean_squared_error: 4.1684 - val_loss: 4.9728 - val_mean_squared_error: 4.9728\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 4.1223 - mean_squared_error: 4.1223 - val_loss: 5.0366 - val_mean_squared_error: 5.0366\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 4.1088 - mean_squared_error: 4.1088 - val_loss: 4.7529 - val_mean_squared_error: 4.7529\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 3.8363 - mean_squared_error: 3.8363 - val_loss: 4.6140 - val_mean_squared_error: 4.6140\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 3.7866 - mean_squared_error: 3.7866 - val_loss: 4.7448 - val_mean_squared_error: 4.7448\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 3.6902 - mean_squared_error: 3.6902 - val_loss: 4.5134 - val_mean_squared_error: 4.5134\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 3.6435 - mean_squared_error: 3.6435 - val_loss: 4.6755 - val_mean_squared_error: 4.6755\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 3.5734 - mean_squared_error: 3.5734 - val_loss: 4.6800 - val_mean_squared_error: 4.6800\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 0s 231us/sample - loss: 3.4868 - mean_squared_error: 3.4868 - val_loss: 4.6703 - val_mean_squared_error: 4.6703\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 3.3892 - mean_squared_error: 3.3892 - val_loss: 4.7840 - val_mean_squared_error: 4.7840\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 3.2650 - mean_squared_error: 3.2650 - val_loss: 4.4893 - val_mean_squared_error: 4.4893\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 3.3334 - mean_squared_error: 3.3334 - val_loss: 4.2488 - val_mean_squared_error: 4.2488\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 3.1442 - mean_squared_error: 3.1442 - val_loss: 4.7698 - val_mean_squared_error: 4.7698\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 3.0749 - mean_squared_error: 3.0749 - val_loss: 4.4396 - val_mean_squared_error: 4.4396\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 3.0371 - mean_squared_error: 3.0371 - val_loss: 4.4485 - val_mean_squared_error: 4.4485\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 2.9892 - mean_squared_error: 2.9892 - val_loss: 4.1086 - val_mean_squared_error: 4.1086\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 2.9881 - mean_squared_error: 2.9881 - val_loss: 4.1764 - val_mean_squared_error: 4.1764\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 2.8752 - mean_squared_error: 2.8752 - val_loss: 4.1598 - val_mean_squared_error: 4.1598\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 2.8059 - mean_squared_error: 2.8059 - val_loss: 4.2483 - val_mean_squared_error: 4.2483\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 2.7630 - mean_squared_error: 2.7630 - val_loss: 4.1353 - val_mean_squared_error: 4.1353\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 2.8002 - mean_squared_error: 2.8002 - val_loss: 4.0119 - val_mean_squared_error: 4.0119\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 2.7147 - mean_squared_error: 2.7147 - val_loss: 4.0884 - val_mean_squared_error: 4.0884\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 0s 231us/sample - loss: 2.6642 - mean_squared_error: 2.6642 - val_loss: 3.9968 - val_mean_squared_error: 3.9968\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 2.5082 - mean_squared_error: 2.5082 - val_loss: 4.0375 - val_mean_squared_error: 4.0375\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 2.5549 - mean_squared_error: 2.5549 - val_loss: 4.1073 - val_mean_squared_error: 4.1073\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 2.4734 - mean_squared_error: 2.4734 - val_loss: 4.0535 - val_mean_squared_error: 4.0535\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 2.5513 - mean_squared_error: 2.5513 - val_loss: 3.9440 - val_mean_squared_error: 3.9440\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 2.4503 - mean_squared_error: 2.4503 - val_loss: 3.9907 - val_mean_squared_error: 3.9907\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 2.3567 - mean_squared_error: 2.3567 - val_loss: 3.9294 - val_mean_squared_error: 3.9294\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 2.3449 - mean_squared_error: 2.3449 - val_loss: 4.2700 - val_mean_squared_error: 4.2700\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 2.2824 - mean_squared_error: 2.2824 - val_loss: 3.9318 - val_mean_squared_error: 3.9318\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 2.3598 - mean_squared_error: 2.3598 - val_loss: 3.7987 - val_mean_squared_error: 3.7987\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 2.2885 - mean_squared_error: 2.2885 - val_loss: 3.8139 - val_mean_squared_error: 3.8139\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 2.2803 - mean_squared_error: 2.2803 - val_loss: 3.8525 - val_mean_squared_error: 3.8525\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 2.2440 - mean_squared_error: 2.2440 - val_loss: 3.8870 - val_mean_squared_error: 3.8870\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 2.1734 - mean_squared_error: 2.1734 - val_loss: 4.2321 - val_mean_squared_error: 4.2321\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 0s 231us/sample - loss: 2.2340 - mean_squared_error: 2.2340 - val_loss: 3.6870 - val_mean_squared_error: 3.6870\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 2.2501 - mean_squared_error: 2.2501 - val_loss: 3.7973 - val_mean_squared_error: 3.7973\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 2.1454 - mean_squared_error: 2.1454 - val_loss: 3.9022 - val_mean_squared_error: 3.9022\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 2.1042 - mean_squared_error: 2.1042 - val_loss: 3.6460 - val_mean_squared_error: 3.6460\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 2.0712 - mean_squared_error: 2.0712 - val_loss: 3.7299 - val_mean_squared_error: 3.7299\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 2.0601 - mean_squared_error: 2.0601 - val_loss: 3.9832 - val_mean_squared_error: 3.9832\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 2.0383 - mean_squared_error: 2.0383 - val_loss: 3.8797 - val_mean_squared_error: 3.8797\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.9850 - mean_squared_error: 1.9850 - val_loss: 3.6526 - val_mean_squared_error: 3.6526\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 2.0026 - mean_squared_error: 2.0026 - val_loss: 3.9009 - val_mean_squared_error: 3.9009\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.8960 - mean_squared_error: 1.8960 - val_loss: 3.7444 - val_mean_squared_error: 3.7444\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.8861 - mean_squared_error: 1.8861 - val_loss: 3.7514 - val_mean_squared_error: 3.7514\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.8812 - mean_squared_error: 1.8812 - val_loss: 3.9597 - val_mean_squared_error: 3.9597\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.8315 - mean_squared_error: 1.8315 - val_loss: 4.1159 - val_mean_squared_error: 4.1159\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.9060 - mean_squared_error: 1.9060 - val_loss: 3.6476 - val_mean_squared_error: 3.6476\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.8636 - mean_squared_error: 1.8636 - val_loss: 3.7725 - val_mean_squared_error: 3.7725\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.7809 - mean_squared_error: 1.7809 - val_loss: 3.8643 - val_mean_squared_error: 3.8643\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.7801 - mean_squared_error: 1.7801 - val_loss: 3.6653 - val_mean_squared_error: 3.6653\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.8353 - mean_squared_error: 1.8353 - val_loss: 3.7458 - val_mean_squared_error: 3.7458\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.7727 - mean_squared_error: 1.7727 - val_loss: 3.8620 - val_mean_squared_error: 3.8620\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 1.7186 - mean_squared_error: 1.7186 - val_loss: 3.6131 - val_mean_squared_error: 3.6131\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.7034 - mean_squared_error: 1.7034 - val_loss: 4.0721 - val_mean_squared_error: 4.0721\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.7376 - mean_squared_error: 1.7376 - val_loss: 3.6413 - val_mean_squared_error: 3.6413\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.7889 - mean_squared_error: 1.7889 - val_loss: 3.5704 - val_mean_squared_error: 3.5704\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.7674 - mean_squared_error: 1.7674 - val_loss: 4.0798 - val_mean_squared_error: 4.0798\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 1.6779 - mean_squared_error: 1.6779 - val_loss: 3.6446 - val_mean_squared_error: 3.6446\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.7004 - mean_squared_error: 1.7004 - val_loss: 3.5126 - val_mean_squared_error: 3.5126\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 1.7293 - mean_squared_error: 1.7293 - val_loss: 3.6849 - val_mean_squared_error: 3.6849\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.7478 - mean_squared_error: 1.7478 - val_loss: 3.6119 - val_mean_squared_error: 3.6119\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.6260 - mean_squared_error: 1.6260 - val_loss: 3.6794 - val_mean_squared_error: 3.6794\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.6095 - mean_squared_error: 1.6095 - val_loss: 3.7372 - val_mean_squared_error: 3.7372\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.6248 - mean_squared_error: 1.6248 - val_loss: 3.6614 - val_mean_squared_error: 3.6614\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 1.6155 - mean_squared_error: 1.6155 - val_loss: 3.7116 - val_mean_squared_error: 3.7116\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.5324 - mean_squared_error: 1.5324 - val_loss: 3.6616 - val_mean_squared_error: 3.6616\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.5780 - mean_squared_error: 1.5780 - val_loss: 3.5554 - val_mean_squared_error: 3.5554\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.6460 - mean_squared_error: 1.6460 - val_loss: 3.6298 - val_mean_squared_error: 3.6298\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.5613 - mean_squared_error: 1.5613 - val_loss: 3.6263 - val_mean_squared_error: 3.6263\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.5581 - mean_squared_error: 1.5581 - val_loss: 3.6889 - val_mean_squared_error: 3.6889\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.5674 - mean_squared_error: 1.5674 - val_loss: 3.7916 - val_mean_squared_error: 3.7916\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.4412 - mean_squared_error: 1.4412 - val_loss: 3.7156 - val_mean_squared_error: 3.7156\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.5113 - mean_squared_error: 1.5113 - val_loss: 3.6500 - val_mean_squared_error: 3.6500\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.4987 - mean_squared_error: 1.4987 - val_loss: 3.9866 - val_mean_squared_error: 3.9866\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.5735 - mean_squared_error: 1.5735 - val_loss: 3.5458 - val_mean_squared_error: 3.5458\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.4139 - mean_squared_error: 1.4139 - val_loss: 3.5835 - val_mean_squared_error: 3.5835\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.4759 - mean_squared_error: 1.4759 - val_loss: 3.4966 - val_mean_squared_error: 3.4966\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.4767 - mean_squared_error: 1.4767 - val_loss: 3.5987 - val_mean_squared_error: 3.5987\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.4724 - mean_squared_error: 1.4724 - val_loss: 3.6030 - val_mean_squared_error: 3.6030\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.4855 - mean_squared_error: 1.4855 - val_loss: 3.6866 - val_mean_squared_error: 3.6866\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.4852 - mean_squared_error: 1.4852 - val_loss: 3.9422 - val_mean_squared_error: 3.9422\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.4474 - mean_squared_error: 1.4474 - val_loss: 3.6296 - val_mean_squared_error: 3.6296\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.4612 - mean_squared_error: 1.4612 - val_loss: 3.6329 - val_mean_squared_error: 3.6329\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.3796 - mean_squared_error: 1.3796 - val_loss: 3.6672 - val_mean_squared_error: 3.6672\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.4327 - mean_squared_error: 1.4327 - val_loss: 3.6312 - val_mean_squared_error: 3.6312\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.4538 - mean_squared_error: 1.4538 - val_loss: 3.5960 - val_mean_squared_error: 3.5960\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.4018 - mean_squared_error: 1.4018 - val_loss: 3.6535 - val_mean_squared_error: 3.6535\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.4606 - mean_squared_error: 1.4606 - val_loss: 3.8179 - val_mean_squared_error: 3.8179\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.4127 - mean_squared_error: 1.4127 - val_loss: 3.6113 - val_mean_squared_error: 3.6113\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.3179 - mean_squared_error: 1.3179 - val_loss: 3.5388 - val_mean_squared_error: 3.5388\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.3228 - mean_squared_error: 1.3228 - val_loss: 3.5148 - val_mean_squared_error: 3.5148\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.3389 - mean_squared_error: 1.3389 - val_loss: 3.6876 - val_mean_squared_error: 3.6876\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 1.3194 - mean_squared_error: 1.3194 - val_loss: 3.5055 - val_mean_squared_error: 3.5055\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.3893 - mean_squared_error: 1.3893 - val_loss: 3.5540 - val_mean_squared_error: 3.5540\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.4302 - mean_squared_error: 1.4302 - val_loss: 3.6229 - val_mean_squared_error: 3.6229\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.3562 - mean_squared_error: 1.3562 - val_loss: 3.7298 - val_mean_squared_error: 3.7298\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.3171 - mean_squared_error: 1.3171 - val_loss: 3.6553 - val_mean_squared_error: 3.6553\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.2876 - mean_squared_error: 1.2876 - val_loss: 3.5040 - val_mean_squared_error: 3.5040\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.3079 - mean_squared_error: 1.3079 - val_loss: 3.5632 - val_mean_squared_error: 3.5632\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.3270 - mean_squared_error: 1.3270 - val_loss: 3.6363 - val_mean_squared_error: 3.6363\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.2780 - mean_squared_error: 1.2780 - val_loss: 3.9181 - val_mean_squared_error: 3.9181\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.3329 - mean_squared_error: 1.3329 - val_loss: 3.4720 - val_mean_squared_error: 3.4720\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.2645 - mean_squared_error: 1.2645 - val_loss: 3.6574 - val_mean_squared_error: 3.6574\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.3168 - mean_squared_error: 1.3168 - val_loss: 3.5610 - val_mean_squared_error: 3.5610\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.3612 - mean_squared_error: 1.3612 - val_loss: 3.4930 - val_mean_squared_error: 3.4930\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 0s 229us/sample - loss: 1.3065 - mean_squared_error: 1.3065 - val_loss: 3.5787 - val_mean_squared_error: 3.5787\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.2993 - mean_squared_error: 1.2993 - val_loss: 3.6622 - val_mean_squared_error: 3.6622\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.2280 - mean_squared_error: 1.2280 - val_loss: 3.6376 - val_mean_squared_error: 3.6376\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.2860 - mean_squared_error: 1.2860 - val_loss: 3.5799 - val_mean_squared_error: 3.5799\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 1.2358 - mean_squared_error: 1.2358 - val_loss: 3.6595 - val_mean_squared_error: 3.6595\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.2604 - mean_squared_error: 1.2604 - val_loss: 3.5853 - val_mean_squared_error: 3.5853\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.1727 - mean_squared_error: 1.1727 - val_loss: 3.6009 - val_mean_squared_error: 3.6009\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 1.2115 - mean_squared_error: 1.2115 - val_loss: 3.8180 - val_mean_squared_error: 3.8180\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2004 - mean_squared_error: 1.2004 - val_loss: 3.5566 - val_mean_squared_error: 3.5566\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 1.1777 - mean_squared_error: 1.1777 - val_loss: 3.5072 - val_mean_squared_error: 3.5072\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.2322 - mean_squared_error: 1.2322 - val_loss: 3.5788 - val_mean_squared_error: 3.5788\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.2487 - mean_squared_error: 1.2487 - val_loss: 3.6734 - val_mean_squared_error: 3.6734\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 1.2449 - mean_squared_error: 1.2449 - val_loss: 3.5635 - val_mean_squared_error: 3.5635\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 1.2311 - mean_squared_error: 1.2311 - val_loss: 3.5368 - val_mean_squared_error: 3.5368\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.3288 - mean_squared_error: 1.3288 - val_loss: 3.9318 - val_mean_squared_error: 3.9318\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.2588 - mean_squared_error: 1.2588 - val_loss: 3.4809 - val_mean_squared_error: 3.4809\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.1693 - mean_squared_error: 1.1693 - val_loss: 3.6026 - val_mean_squared_error: 3.6026\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.1530 - mean_squared_error: 1.1530 - val_loss: 3.7899 - val_mean_squared_error: 3.7899\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.2685 - mean_squared_error: 1.2685 - val_loss: 3.7008 - val_mean_squared_error: 3.7008\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.1636 - mean_squared_error: 1.1636 - val_loss: 3.4564 - val_mean_squared_error: 3.4564\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.0910 - mean_squared_error: 1.0910 - val_loss: 3.4437 - val_mean_squared_error: 3.4437\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.2016 - mean_squared_error: 1.2016 - val_loss: 3.5778 - val_mean_squared_error: 3.5778\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 0s 231us/sample - loss: 1.1744 - mean_squared_error: 1.1744 - val_loss: 3.6280 - val_mean_squared_error: 3.6280\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.1245 - mean_squared_error: 1.1245 - val_loss: 3.5793 - val_mean_squared_error: 3.5793\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.1291 - mean_squared_error: 1.1291 - val_loss: 3.5813 - val_mean_squared_error: 3.5813\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.1701 - mean_squared_error: 1.1701 - val_loss: 3.6168 - val_mean_squared_error: 3.6168\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.1397 - mean_squared_error: 1.1397 - val_loss: 3.6349 - val_mean_squared_error: 3.6349\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.1518 - mean_squared_error: 1.1518 - val_loss: 3.8907 - val_mean_squared_error: 3.8907\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.1375 - mean_squared_error: 1.1375 - val_loss: 3.5185 - val_mean_squared_error: 3.5185\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.1537 - mean_squared_error: 1.1537 - val_loss: 3.6644 - val_mean_squared_error: 3.6644\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.2005 - mean_squared_error: 1.2005 - val_loss: 3.5393 - val_mean_squared_error: 3.5393\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.2067 - mean_squared_error: 1.2067 - val_loss: 3.5069 - val_mean_squared_error: 3.5069\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.1164 - mean_squared_error: 1.1164 - val_loss: 3.4991 - val_mean_squared_error: 3.4991\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.1372 - mean_squared_error: 1.1372 - val_loss: 3.5558 - val_mean_squared_error: 3.5558\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.1631 - mean_squared_error: 1.1631 - val_loss: 3.4968 - val_mean_squared_error: 3.4968\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.1311 - mean_squared_error: 1.1311 - val_loss: 4.2234 - val_mean_squared_error: 4.2234\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.1051 - mean_squared_error: 1.1051 - val_loss: 3.5135 - val_mean_squared_error: 3.5135\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.0471 - mean_squared_error: 1.0471 - val_loss: 3.5774 - val_mean_squared_error: 3.5774\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.1399 - mean_squared_error: 1.1399 - val_loss: 3.6318 - val_mean_squared_error: 3.6318\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.0513 - mean_squared_error: 1.0513 - val_loss: 3.7294 - val_mean_squared_error: 3.7294\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.1117 - mean_squared_error: 1.1117 - val_loss: 3.5940 - val_mean_squared_error: 3.5940\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.0769 - mean_squared_error: 1.0769 - val_loss: 3.4549 - val_mean_squared_error: 3.4549\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.0703 - mean_squared_error: 1.0703 - val_loss: 3.5168 - val_mean_squared_error: 3.5168\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.0773 - mean_squared_error: 1.0773 - val_loss: 3.4992 - val_mean_squared_error: 3.4992\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.0885 - mean_squared_error: 1.0885 - val_loss: 3.5842 - val_mean_squared_error: 3.5842\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 0s 231us/sample - loss: 1.0735 - mean_squared_error: 1.0735 - val_loss: 3.5242 - val_mean_squared_error: 3.5242\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.0535 - mean_squared_error: 1.0535 - val_loss: 3.5929 - val_mean_squared_error: 3.5929\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.0732 - mean_squared_error: 1.0732 - val_loss: 3.5915 - val_mean_squared_error: 3.5915\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.0771 - mean_squared_error: 1.0771 - val_loss: 3.5046 - val_mean_squared_error: 3.5046\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.0599 - mean_squared_error: 1.0599 - val_loss: 3.5294 - val_mean_squared_error: 3.5294\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.0418 - mean_squared_error: 1.0418 - val_loss: 3.5124 - val_mean_squared_error: 3.5124\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.0530 - mean_squared_error: 1.0530 - val_loss: 3.6881 - val_mean_squared_error: 3.6881\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.0584 - mean_squared_error: 1.0584 - val_loss: 3.5945 - val_mean_squared_error: 3.5945\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.0530 - mean_squared_error: 1.0530 - val_loss: 3.5418 - val_mean_squared_error: 3.5418\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.0265 - mean_squared_error: 1.0265 - val_loss: 3.6008 - val_mean_squared_error: 3.6008\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.0855 - mean_squared_error: 1.0855 - val_loss: 3.5549 - val_mean_squared_error: 3.5549\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.0833 - mean_squared_error: 1.0833 - val_loss: 3.6988 - val_mean_squared_error: 3.6988\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.0280 - mean_squared_error: 1.0280 - val_loss: 3.6301 - val_mean_squared_error: 3.6301\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.1070 - mean_squared_error: 1.1070 - val_loss: 3.5379 - val_mean_squared_error: 3.5379\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.0323 - mean_squared_error: 1.0323 - val_loss: 3.4751 - val_mean_squared_error: 3.4751\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.0050 - mean_squared_error: 1.0050 - val_loss: 3.5065 - val_mean_squared_error: 3.5065\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.0440 - mean_squared_error: 1.0440 - val_loss: 3.6437 - val_mean_squared_error: 3.6437\n",
            "==================================================\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 47, 47, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 47, 47, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 23, 23, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 23, 23, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 11, 11, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 728us/sample - loss: 184.6800 - mean_squared_error: 184.6800 - val_loss: 130311.5926 - val_mean_squared_error: 130311.5859\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 14.3958 - mean_squared_error: 14.3958 - val_loss: 908.1665 - val_mean_squared_error: 908.1664\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 12.6987 - mean_squared_error: 12.6987 - val_loss: 40.1760 - val_mean_squared_error: 40.1760\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 10.7887 - mean_squared_error: 10.7887 - val_loss: 12.0602 - val_mean_squared_error: 12.0602\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 10.7258 - mean_squared_error: 10.7258 - val_loss: 9.5127 - val_mean_squared_error: 9.5127\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 9.8158 - mean_squared_error: 9.8158 - val_loss: 9.3846 - val_mean_squared_error: 9.3846\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 9.8203 - mean_squared_error: 9.8203 - val_loss: 9.1056 - val_mean_squared_error: 9.1056\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 9.6421 - mean_squared_error: 9.6421 - val_loss: 9.0155 - val_mean_squared_error: 9.0155\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 9.3581 - mean_squared_error: 9.3581 - val_loss: 8.9525 - val_mean_squared_error: 8.9525\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 9.1277 - mean_squared_error: 9.1277 - val_loss: 8.9577 - val_mean_squared_error: 8.9577\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 8.9481 - mean_squared_error: 8.9481 - val_loss: 8.6335 - val_mean_squared_error: 8.6335\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 8.7904 - mean_squared_error: 8.7904 - val_loss: 8.6487 - val_mean_squared_error: 8.6487\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 8.4898 - mean_squared_error: 8.4898 - val_loss: 8.2964 - val_mean_squared_error: 8.2964\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 8.2842 - mean_squared_error: 8.2842 - val_loss: 8.1457 - val_mean_squared_error: 8.1457\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 7.9665 - mean_squared_error: 7.9665 - val_loss: 8.2282 - val_mean_squared_error: 8.2282\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 7.8931 - mean_squared_error: 7.8931 - val_loss: 7.8479 - val_mean_squared_error: 7.8479\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 7.6534 - mean_squared_error: 7.6534 - val_loss: 7.4943 - val_mean_squared_error: 7.4943\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 7.5451 - mean_squared_error: 7.5451 - val_loss: 7.4564 - val_mean_squared_error: 7.4564\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 7.3636 - mean_squared_error: 7.3636 - val_loss: 7.6419 - val_mean_squared_error: 7.6419\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 7.2509 - mean_squared_error: 7.2509 - val_loss: 7.9272 - val_mean_squared_error: 7.9272\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 7.0128 - mean_squared_error: 7.0128 - val_loss: 7.3677 - val_mean_squared_error: 7.3677\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 6.8834 - mean_squared_error: 6.8834 - val_loss: 7.0133 - val_mean_squared_error: 7.0133\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 6.6326 - mean_squared_error: 6.6326 - val_loss: 6.9059 - val_mean_squared_error: 6.9059\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 6.3620 - mean_squared_error: 6.3620 - val_loss: 6.5187 - val_mean_squared_error: 6.5187\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 6.1150 - mean_squared_error: 6.1150 - val_loss: 6.0454 - val_mean_squared_error: 6.0454\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 0s 256us/sample - loss: 5.9012 - mean_squared_error: 5.9012 - val_loss: 5.9152 - val_mean_squared_error: 5.9152\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 5.7433 - mean_squared_error: 5.7433 - val_loss: 5.6882 - val_mean_squared_error: 5.6882\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 5.4387 - mean_squared_error: 5.4387 - val_loss: 5.9573 - val_mean_squared_error: 5.9573\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 5.3453 - mean_squared_error: 5.3453 - val_loss: 5.3839 - val_mean_squared_error: 5.3839\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 5.0615 - mean_squared_error: 5.0615 - val_loss: 5.1864 - val_mean_squared_error: 5.1864\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 4.9790 - mean_squared_error: 4.9790 - val_loss: 5.1477 - val_mean_squared_error: 5.1477\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 4.8249 - mean_squared_error: 4.8249 - val_loss: 5.0489 - val_mean_squared_error: 5.0489\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 4.6788 - mean_squared_error: 4.6788 - val_loss: 4.9817 - val_mean_squared_error: 4.9817\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 4.4604 - mean_squared_error: 4.4604 - val_loss: 4.7619 - val_mean_squared_error: 4.7619\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 4.4167 - mean_squared_error: 4.4167 - val_loss: 4.6722 - val_mean_squared_error: 4.6722\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 4.3563 - mean_squared_error: 4.3563 - val_loss: 4.8965 - val_mean_squared_error: 4.8965\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 4.2227 - mean_squared_error: 4.2227 - val_loss: 4.4644 - val_mean_squared_error: 4.4644\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 4.1086 - mean_squared_error: 4.1086 - val_loss: 4.3650 - val_mean_squared_error: 4.3650\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 4.1164 - mean_squared_error: 4.1164 - val_loss: 4.4162 - val_mean_squared_error: 4.4162\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 3.8665 - mean_squared_error: 3.8665 - val_loss: 4.4579 - val_mean_squared_error: 4.4579\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 3.8745 - mean_squared_error: 3.8745 - val_loss: 4.4044 - val_mean_squared_error: 4.4044\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 3.8050 - mean_squared_error: 3.8050 - val_loss: 4.2309 - val_mean_squared_error: 4.2309\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 3.6565 - mean_squared_error: 3.6565 - val_loss: 4.0979 - val_mean_squared_error: 4.0979\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 3.6626 - mean_squared_error: 3.6626 - val_loss: 4.2451 - val_mean_squared_error: 4.2451\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 3.6533 - mean_squared_error: 3.6533 - val_loss: 4.1671 - val_mean_squared_error: 4.1671\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 0s 257us/sample - loss: 3.5159 - mean_squared_error: 3.5159 - val_loss: 4.1329 - val_mean_squared_error: 4.1329\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 3.4262 - mean_squared_error: 3.4262 - val_loss: 4.1868 - val_mean_squared_error: 4.1868\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 3.2886 - mean_squared_error: 3.2886 - val_loss: 3.9676 - val_mean_squared_error: 3.9676\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 3.2307 - mean_squared_error: 3.2307 - val_loss: 3.8724 - val_mean_squared_error: 3.8724\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 3.1751 - mean_squared_error: 3.1751 - val_loss: 3.9490 - val_mean_squared_error: 3.9490\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 3.0851 - mean_squared_error: 3.0851 - val_loss: 3.8199 - val_mean_squared_error: 3.8199\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 3.0827 - mean_squared_error: 3.0827 - val_loss: 3.9954 - val_mean_squared_error: 3.9954\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 3.0051 - mean_squared_error: 3.0051 - val_loss: 3.8253 - val_mean_squared_error: 3.8253\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 2.9953 - mean_squared_error: 2.9953 - val_loss: 3.6891 - val_mean_squared_error: 3.6891\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 2.8607 - mean_squared_error: 2.8607 - val_loss: 3.8354 - val_mean_squared_error: 3.8354\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 2.8401 - mean_squared_error: 2.8401 - val_loss: 4.3935 - val_mean_squared_error: 4.3935\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 0s 254us/sample - loss: 2.8286 - mean_squared_error: 2.8286 - val_loss: 3.6326 - val_mean_squared_error: 3.6326\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 2.7612 - mean_squared_error: 2.7612 - val_loss: 3.7490 - val_mean_squared_error: 3.7490\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 0s 257us/sample - loss: 2.7465 - mean_squared_error: 2.7465 - val_loss: 3.5127 - val_mean_squared_error: 3.5127\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 2.6542 - mean_squared_error: 2.6542 - val_loss: 3.4971 - val_mean_squared_error: 3.4971\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 0s 256us/sample - loss: 2.6685 - mean_squared_error: 2.6685 - val_loss: 3.5069 - val_mean_squared_error: 3.5069\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 2.6291 - mean_squared_error: 2.6291 - val_loss: 4.2185 - val_mean_squared_error: 4.2185\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 0s 257us/sample - loss: 2.5918 - mean_squared_error: 2.5918 - val_loss: 3.5135 - val_mean_squared_error: 3.5135\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 0s 259us/sample - loss: 2.6125 - mean_squared_error: 2.6125 - val_loss: 3.5170 - val_mean_squared_error: 3.5170\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 0s 254us/sample - loss: 2.5359 - mean_squared_error: 2.5359 - val_loss: 3.5723 - val_mean_squared_error: 3.5723\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 2.5516 - mean_squared_error: 2.5516 - val_loss: 3.5874 - val_mean_squared_error: 3.5874\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.4701 - mean_squared_error: 2.4701 - val_loss: 3.3965 - val_mean_squared_error: 3.3965\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 2.4712 - mean_squared_error: 2.4712 - val_loss: 3.4370 - val_mean_squared_error: 3.4370\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.5166 - mean_squared_error: 2.5166 - val_loss: 3.4998 - val_mean_squared_error: 3.4998\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.4709 - mean_squared_error: 2.4709 - val_loss: 3.4769 - val_mean_squared_error: 3.4769\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 2.4223 - mean_squared_error: 2.4223 - val_loss: 3.3280 - val_mean_squared_error: 3.3280\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 2.3753 - mean_squared_error: 2.3753 - val_loss: 3.2924 - val_mean_squared_error: 3.2924\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 2.3064 - mean_squared_error: 2.3064 - val_loss: 3.6519 - val_mean_squared_error: 3.6519\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.3204 - mean_squared_error: 2.3204 - val_loss: 3.4275 - val_mean_squared_error: 3.4275\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.3563 - mean_squared_error: 2.3563 - val_loss: 3.3551 - val_mean_squared_error: 3.3551\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 2.2838 - mean_squared_error: 2.2838 - val_loss: 3.6899 - val_mean_squared_error: 3.6899\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.2797 - mean_squared_error: 2.2797 - val_loss: 3.4288 - val_mean_squared_error: 3.4288\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 2.3255 - mean_squared_error: 2.3255 - val_loss: 3.4104 - val_mean_squared_error: 3.4104\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 2.2334 - mean_squared_error: 2.2334 - val_loss: 3.2567 - val_mean_squared_error: 3.2567\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.2483 - mean_squared_error: 2.2483 - val_loss: 3.4566 - val_mean_squared_error: 3.4566\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 2.2363 - mean_squared_error: 2.2363 - val_loss: 3.3900 - val_mean_squared_error: 3.3900\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.1780 - mean_squared_error: 2.1780 - val_loss: 3.2744 - val_mean_squared_error: 3.2744\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 2.2169 - mean_squared_error: 2.2169 - val_loss: 3.3430 - val_mean_squared_error: 3.3430\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.1219 - mean_squared_error: 2.1219 - val_loss: 3.6573 - val_mean_squared_error: 3.6573\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.0733 - mean_squared_error: 2.0733 - val_loss: 3.2621 - val_mean_squared_error: 3.2621\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.1551 - mean_squared_error: 2.1551 - val_loss: 3.2259 - val_mean_squared_error: 3.2259\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.9842 - mean_squared_error: 1.9842 - val_loss: 3.2532 - val_mean_squared_error: 3.2532\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 2.0700 - mean_squared_error: 2.0700 - val_loss: 3.5407 - val_mean_squared_error: 3.5407\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 2.1606 - mean_squared_error: 2.1606 - val_loss: 3.3231 - val_mean_squared_error: 3.3231\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.9938 - mean_squared_error: 1.9938 - val_loss: 3.2685 - val_mean_squared_error: 3.2685\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.9975 - mean_squared_error: 1.9975 - val_loss: 3.1902 - val_mean_squared_error: 3.1902\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.9900 - mean_squared_error: 1.9900 - val_loss: 3.3776 - val_mean_squared_error: 3.3776\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 2.0203 - mean_squared_error: 2.0203 - val_loss: 3.3749 - val_mean_squared_error: 3.3749\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.9436 - mean_squared_error: 1.9436 - val_loss: 3.1598 - val_mean_squared_error: 3.1598\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.8890 - mean_squared_error: 1.8890 - val_loss: 3.2655 - val_mean_squared_error: 3.2655\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.9394 - mean_squared_error: 1.9394 - val_loss: 3.2935 - val_mean_squared_error: 3.2935\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.8948 - mean_squared_error: 1.8948 - val_loss: 3.1628 - val_mean_squared_error: 3.1628\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.8746 - mean_squared_error: 1.8746 - val_loss: 3.4524 - val_mean_squared_error: 3.4524\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.9194 - mean_squared_error: 1.9194 - val_loss: 3.2582 - val_mean_squared_error: 3.2582\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.8580 - mean_squared_error: 1.8580 - val_loss: 3.2199 - val_mean_squared_error: 3.2199\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.8085 - mean_squared_error: 1.8085 - val_loss: 3.2243 - val_mean_squared_error: 3.2243\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.7322 - mean_squared_error: 1.7322 - val_loss: 3.2036 - val_mean_squared_error: 3.2036\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.8217 - mean_squared_error: 1.8217 - val_loss: 3.2824 - val_mean_squared_error: 3.2824\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.8050 - mean_squared_error: 1.8050 - val_loss: 3.2566 - val_mean_squared_error: 3.2566\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.7747 - mean_squared_error: 1.7747 - val_loss: 3.3022 - val_mean_squared_error: 3.3022\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.7641 - mean_squared_error: 1.7641 - val_loss: 3.2110 - val_mean_squared_error: 3.2110\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.8239 - mean_squared_error: 1.8239 - val_loss: 3.2069 - val_mean_squared_error: 3.2069\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.7546 - mean_squared_error: 1.7546 - val_loss: 3.5654 - val_mean_squared_error: 3.5654\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.7245 - mean_squared_error: 1.7245 - val_loss: 3.1192 - val_mean_squared_error: 3.1192\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.7356 - mean_squared_error: 1.7356 - val_loss: 3.1932 - val_mean_squared_error: 3.1932\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.7497 - mean_squared_error: 1.7497 - val_loss: 3.1684 - val_mean_squared_error: 3.1684\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.7053 - mean_squared_error: 1.7053 - val_loss: 3.3306 - val_mean_squared_error: 3.3306\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.7403 - mean_squared_error: 1.7403 - val_loss: 3.1768 - val_mean_squared_error: 3.1768\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.6370 - mean_squared_error: 1.6370 - val_loss: 3.1350 - val_mean_squared_error: 3.1350\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.7115 - mean_squared_error: 1.7115 - val_loss: 3.2109 - val_mean_squared_error: 3.2109\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.7362 - mean_squared_error: 1.7362 - val_loss: 3.2580 - val_mean_squared_error: 3.2580\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.7234 - mean_squared_error: 1.7234 - val_loss: 3.1597 - val_mean_squared_error: 3.1597\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.6358 - mean_squared_error: 1.6358 - val_loss: 3.1722 - val_mean_squared_error: 3.1722\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.6003 - mean_squared_error: 1.6003 - val_loss: 3.0443 - val_mean_squared_error: 3.0443\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.6582 - mean_squared_error: 1.6582 - val_loss: 3.2313 - val_mean_squared_error: 3.2313\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.5837 - mean_squared_error: 1.5837 - val_loss: 3.0261 - val_mean_squared_error: 3.0261\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.5500 - mean_squared_error: 1.5500 - val_loss: 3.1039 - val_mean_squared_error: 3.1039\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.6619 - mean_squared_error: 1.6619 - val_loss: 3.1228 - val_mean_squared_error: 3.1228\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.5418 - mean_squared_error: 1.5418 - val_loss: 3.2079 - val_mean_squared_error: 3.2079\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.5042 - mean_squared_error: 1.5042 - val_loss: 3.2104 - val_mean_squared_error: 3.2104\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.6590 - mean_squared_error: 1.6590 - val_loss: 3.1210 - val_mean_squared_error: 3.1210\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.5771 - mean_squared_error: 1.5771 - val_loss: 3.0919 - val_mean_squared_error: 3.0919\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.4988 - mean_squared_error: 1.4988 - val_loss: 3.1473 - val_mean_squared_error: 3.1472\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.5225 - mean_squared_error: 1.5225 - val_loss: 3.1442 - val_mean_squared_error: 3.1442\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.5373 - mean_squared_error: 1.5373 - val_loss: 3.1028 - val_mean_squared_error: 3.1028\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.5280 - mean_squared_error: 1.5280 - val_loss: 3.0905 - val_mean_squared_error: 3.0905\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.4900 - mean_squared_error: 1.4900 - val_loss: 3.1245 - val_mean_squared_error: 3.1245\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.5121 - mean_squared_error: 1.5121 - val_loss: 3.1712 - val_mean_squared_error: 3.1712\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.4541 - mean_squared_error: 1.4541 - val_loss: 3.2433 - val_mean_squared_error: 3.2433\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.5326 - mean_squared_error: 1.5326 - val_loss: 3.1197 - val_mean_squared_error: 3.1197\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.4729 - mean_squared_error: 1.4729 - val_loss: 3.0506 - val_mean_squared_error: 3.0506\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.5770 - mean_squared_error: 1.5770 - val_loss: 3.1727 - val_mean_squared_error: 3.1727\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.4620 - mean_squared_error: 1.4620 - val_loss: 2.9923 - val_mean_squared_error: 2.9923\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.5423 - mean_squared_error: 1.5423 - val_loss: 3.3125 - val_mean_squared_error: 3.3125\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.4925 - mean_squared_error: 1.4925 - val_loss: 3.1428 - val_mean_squared_error: 3.1428\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 1.4238 - mean_squared_error: 1.4238 - val_loss: 3.1511 - val_mean_squared_error: 3.1511\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.4733 - mean_squared_error: 1.4733 - val_loss: 3.0507 - val_mean_squared_error: 3.0507\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.5279 - mean_squared_error: 1.5279 - val_loss: 3.1598 - val_mean_squared_error: 3.1598\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.4576 - mean_squared_error: 1.4576 - val_loss: 3.0687 - val_mean_squared_error: 3.0687\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.4446 - mean_squared_error: 1.4446 - val_loss: 3.4184 - val_mean_squared_error: 3.4184\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.4467 - mean_squared_error: 1.4467 - val_loss: 3.0220 - val_mean_squared_error: 3.0220\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.4436 - mean_squared_error: 1.4436 - val_loss: 3.1413 - val_mean_squared_error: 3.1413\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.4577 - mean_squared_error: 1.4577 - val_loss: 3.1035 - val_mean_squared_error: 3.1035\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.4099 - mean_squared_error: 1.4099 - val_loss: 3.0102 - val_mean_squared_error: 3.0102\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.4845 - mean_squared_error: 1.4845 - val_loss: 3.1713 - val_mean_squared_error: 3.1713\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.4285 - mean_squared_error: 1.4285 - val_loss: 3.1296 - val_mean_squared_error: 3.1296\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.4204 - mean_squared_error: 1.4204 - val_loss: 3.0091 - val_mean_squared_error: 3.0091\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 1.4195 - mean_squared_error: 1.4195 - val_loss: 3.1773 - val_mean_squared_error: 3.1773\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.4329 - mean_squared_error: 1.4329 - val_loss: 2.9335 - val_mean_squared_error: 2.9335\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.4359 - mean_squared_error: 1.4359 - val_loss: 2.9756 - val_mean_squared_error: 2.9756\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.3533 - mean_squared_error: 1.3533 - val_loss: 2.9801 - val_mean_squared_error: 2.9801\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.3761 - mean_squared_error: 1.3761 - val_loss: 3.1093 - val_mean_squared_error: 3.1093\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.3533 - mean_squared_error: 1.3533 - val_loss: 3.0269 - val_mean_squared_error: 3.0269\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3735 - mean_squared_error: 1.3735 - val_loss: 3.0150 - val_mean_squared_error: 3.0150\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.3328 - mean_squared_error: 1.3328 - val_loss: 3.0153 - val_mean_squared_error: 3.0153\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3982 - mean_squared_error: 1.3982 - val_loss: 3.2284 - val_mean_squared_error: 3.2284\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.3379 - mean_squared_error: 1.3379 - val_loss: 3.0317 - val_mean_squared_error: 3.0317\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3557 - mean_squared_error: 1.3557 - val_loss: 3.1731 - val_mean_squared_error: 3.1731\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.3301 - mean_squared_error: 1.3301 - val_loss: 3.0711 - val_mean_squared_error: 3.0711\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3366 - mean_squared_error: 1.3366 - val_loss: 3.0067 - val_mean_squared_error: 3.0067\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.2788 - mean_squared_error: 1.2788 - val_loss: 3.0031 - val_mean_squared_error: 3.0031\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.3260 - mean_squared_error: 1.3260 - val_loss: 3.0444 - val_mean_squared_error: 3.0444\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.3546 - mean_squared_error: 1.3546 - val_loss: 3.3490 - val_mean_squared_error: 3.3490\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.3266 - mean_squared_error: 1.3266 - val_loss: 3.0130 - val_mean_squared_error: 3.0130\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.2783 - mean_squared_error: 1.2783 - val_loss: 2.9777 - val_mean_squared_error: 2.9777\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.3000 - mean_squared_error: 1.3000 - val_loss: 3.1665 - val_mean_squared_error: 3.1665\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3174 - mean_squared_error: 1.3174 - val_loss: 3.1293 - val_mean_squared_error: 3.1293\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.2591 - mean_squared_error: 1.2591 - val_loss: 3.0372 - val_mean_squared_error: 3.0372\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.3488 - mean_squared_error: 1.3488 - val_loss: 3.1624 - val_mean_squared_error: 3.1624\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.3242 - mean_squared_error: 1.3242 - val_loss: 3.1311 - val_mean_squared_error: 3.1311\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.3275 - mean_squared_error: 1.3275 - val_loss: 3.1667 - val_mean_squared_error: 3.1667\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.2656 - mean_squared_error: 1.2656 - val_loss: 2.9988 - val_mean_squared_error: 2.9988\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2521 - mean_squared_error: 1.2521 - val_loss: 3.3468 - val_mean_squared_error: 3.3468\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3064 - mean_squared_error: 1.3064 - val_loss: 3.0172 - val_mean_squared_error: 3.0172\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.2828 - mean_squared_error: 1.2828 - val_loss: 3.1095 - val_mean_squared_error: 3.1095\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.2365 - mean_squared_error: 1.2365 - val_loss: 3.1000 - val_mean_squared_error: 3.1000\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2923 - mean_squared_error: 1.2923 - val_loss: 3.0214 - val_mean_squared_error: 3.0214\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2804 - mean_squared_error: 1.2804 - val_loss: 3.0876 - val_mean_squared_error: 3.0876\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.2249 - mean_squared_error: 1.2249 - val_loss: 3.0449 - val_mean_squared_error: 3.0449\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.2209 - mean_squared_error: 1.2209 - val_loss: 3.0539 - val_mean_squared_error: 3.0539\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.2483 - mean_squared_error: 1.2483 - val_loss: 3.1226 - val_mean_squared_error: 3.1226\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.2448 - mean_squared_error: 1.2448 - val_loss: 2.9525 - val_mean_squared_error: 2.9525\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.2666 - mean_squared_error: 1.2666 - val_loss: 2.9692 - val_mean_squared_error: 2.9692\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.2192 - mean_squared_error: 1.2192 - val_loss: 2.9035 - val_mean_squared_error: 2.9035\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2639 - mean_squared_error: 1.2639 - val_loss: 2.9348 - val_mean_squared_error: 2.9348\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.2237 - mean_squared_error: 1.2237 - val_loss: 2.9725 - val_mean_squared_error: 2.9725\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.1842 - mean_squared_error: 1.1842 - val_loss: 3.1095 - val_mean_squared_error: 3.1095\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.2469 - mean_squared_error: 1.2469 - val_loss: 3.0464 - val_mean_squared_error: 3.0464\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.2302 - mean_squared_error: 1.2302 - val_loss: 3.0344 - val_mean_squared_error: 3.0344\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.2516 - mean_squared_error: 1.2516 - val_loss: 3.0627 - val_mean_squared_error: 3.0627\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.3128 - mean_squared_error: 1.3128 - val_loss: 3.0456 - val_mean_squared_error: 3.0456\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.2395 - mean_squared_error: 1.2395 - val_loss: 2.9453 - val_mean_squared_error: 2.9453\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 1.1628 - mean_squared_error: 1.1628 - val_loss: 2.9139 - val_mean_squared_error: 2.9139\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.1650 - mean_squared_error: 1.1650 - val_loss: 2.9294 - val_mean_squared_error: 2.9294\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.1196 - mean_squared_error: 1.1196 - val_loss: 3.0638 - val_mean_squared_error: 3.0638\n",
            "==================================================\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 47, 47, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 47, 47, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 23, 23, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 23, 23, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 11, 11, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 742us/sample - loss: 185.0017 - mean_squared_error: 185.0017 - val_loss: 128974.6745 - val_mean_squared_error: 128974.6797\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 0s 259us/sample - loss: 12.4920 - mean_squared_error: 12.4920 - val_loss: 886.7932 - val_mean_squared_error: 886.7932\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 11.0854 - mean_squared_error: 11.0854 - val_loss: 42.0045 - val_mean_squared_error: 42.0045\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 10.4580 - mean_squared_error: 10.4580 - val_loss: 10.8134 - val_mean_squared_error: 10.8134\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 10.1914 - mean_squared_error: 10.1914 - val_loss: 10.0731 - val_mean_squared_error: 10.0731\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 9.6159 - mean_squared_error: 9.6159 - val_loss: 10.3317 - val_mean_squared_error: 10.3317\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 9.5706 - mean_squared_error: 9.5706 - val_loss: 9.0666 - val_mean_squared_error: 9.0666\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 9.2587 - mean_squared_error: 9.2587 - val_loss: 9.1672 - val_mean_squared_error: 9.1672\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 9.3279 - mean_squared_error: 9.3279 - val_loss: 9.2992 - val_mean_squared_error: 9.2992\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 8.9914 - mean_squared_error: 8.9914 - val_loss: 8.8214 - val_mean_squared_error: 8.8214\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 8.9942 - mean_squared_error: 8.9942 - val_loss: 8.7586 - val_mean_squared_error: 8.7586\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 0s 256us/sample - loss: 8.7529 - mean_squared_error: 8.7529 - val_loss: 8.7933 - val_mean_squared_error: 8.7933\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 8.5962 - mean_squared_error: 8.5962 - val_loss: 8.4378 - val_mean_squared_error: 8.4378\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 8.3463 - mean_squared_error: 8.3463 - val_loss: 8.3310 - val_mean_squared_error: 8.3310\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 8.0841 - mean_squared_error: 8.0841 - val_loss: 8.2121 - val_mean_squared_error: 8.2121\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 7.9839 - mean_squared_error: 7.9839 - val_loss: 8.3990 - val_mean_squared_error: 8.3990\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 7.8222 - mean_squared_error: 7.8222 - val_loss: 7.9972 - val_mean_squared_error: 7.9972\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 7.6555 - mean_squared_error: 7.6555 - val_loss: 7.8877 - val_mean_squared_error: 7.8877\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 7.6186 - mean_squared_error: 7.6186 - val_loss: 7.6407 - val_mean_squared_error: 7.6407\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 7.2094 - mean_squared_error: 7.2094 - val_loss: 7.2891 - val_mean_squared_error: 7.2891\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 6.9369 - mean_squared_error: 6.9369 - val_loss: 6.7540 - val_mean_squared_error: 6.7540\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 6.7177 - mean_squared_error: 6.7177 - val_loss: 6.3974 - val_mean_squared_error: 6.3974\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 6.5039 - mean_squared_error: 6.5039 - val_loss: 6.1806 - val_mean_squared_error: 6.1806\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 6.2344 - mean_squared_error: 6.2344 - val_loss: 5.9341 - val_mean_squared_error: 5.9341\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 6.0388 - mean_squared_error: 6.0388 - val_loss: 6.5853 - val_mean_squared_error: 6.5853\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 5.8173 - mean_squared_error: 5.8173 - val_loss: 5.8543 - val_mean_squared_error: 5.8543\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 5.6988 - mean_squared_error: 5.6988 - val_loss: 5.5736 - val_mean_squared_error: 5.5736\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 5.5324 - mean_squared_error: 5.5324 - val_loss: 5.3968 - val_mean_squared_error: 5.3968\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 5.2114 - mean_squared_error: 5.2114 - val_loss: 5.1489 - val_mean_squared_error: 5.1489\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 5.1708 - mean_squared_error: 5.1708 - val_loss: 5.0488 - val_mean_squared_error: 5.0488\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 4.9215 - mean_squared_error: 4.9215 - val_loss: 5.1005 - val_mean_squared_error: 5.1005\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 4.9261 - mean_squared_error: 4.9261 - val_loss: 5.1947 - val_mean_squared_error: 5.1947\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 4.7901 - mean_squared_error: 4.7901 - val_loss: 5.4684 - val_mean_squared_error: 5.4684\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 4.7415 - mean_squared_error: 4.7415 - val_loss: 4.9472 - val_mean_squared_error: 4.9472\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 4.5804 - mean_squared_error: 4.5804 - val_loss: 4.8919 - val_mean_squared_error: 4.8919\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 4.5847 - mean_squared_error: 4.5847 - val_loss: 4.8110 - val_mean_squared_error: 4.8110\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 4.4240 - mean_squared_error: 4.4240 - val_loss: 4.7459 - val_mean_squared_error: 4.7459\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 4.3788 - mean_squared_error: 4.3788 - val_loss: 4.4390 - val_mean_squared_error: 4.4390\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 4.2530 - mean_squared_error: 4.2530 - val_loss: 4.6247 - val_mean_squared_error: 4.6247\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 4.2368 - mean_squared_error: 4.2368 - val_loss: 4.6526 - val_mean_squared_error: 4.6526\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 4.1543 - mean_squared_error: 4.1543 - val_loss: 4.7660 - val_mean_squared_error: 4.7660\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 4.0550 - mean_squared_error: 4.0550 - val_loss: 4.6045 - val_mean_squared_error: 4.6045\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 3.9576 - mean_squared_error: 3.9576 - val_loss: 4.3755 - val_mean_squared_error: 4.3755\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 3.9044 - mean_squared_error: 3.9044 - val_loss: 4.6612 - val_mean_squared_error: 4.6612\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 3.7206 - mean_squared_error: 3.7206 - val_loss: 4.2184 - val_mean_squared_error: 4.2184\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 3.7408 - mean_squared_error: 3.7408 - val_loss: 4.0310 - val_mean_squared_error: 4.0310\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 3.6390 - mean_squared_error: 3.6390 - val_loss: 4.1144 - val_mean_squared_error: 4.1144\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 3.5778 - mean_squared_error: 3.5778 - val_loss: 4.0533 - val_mean_squared_error: 4.0533\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 3.5435 - mean_squared_error: 3.5435 - val_loss: 4.2856 - val_mean_squared_error: 4.2856\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 3.5151 - mean_squared_error: 3.5151 - val_loss: 3.7982 - val_mean_squared_error: 3.7982\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 3.4011 - mean_squared_error: 3.4011 - val_loss: 3.9859 - val_mean_squared_error: 3.9859\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 3.3907 - mean_squared_error: 3.3907 - val_loss: 3.8104 - val_mean_squared_error: 3.8104\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 3.2740 - mean_squared_error: 3.2740 - val_loss: 4.0880 - val_mean_squared_error: 4.0880\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 3.2780 - mean_squared_error: 3.2781 - val_loss: 3.8251 - val_mean_squared_error: 3.8251\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 3.2631 - mean_squared_error: 3.2631 - val_loss: 3.7723 - val_mean_squared_error: 3.7723\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 3.1051 - mean_squared_error: 3.1051 - val_loss: 3.6090 - val_mean_squared_error: 3.6090\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 3.0534 - mean_squared_error: 3.0534 - val_loss: 3.4661 - val_mean_squared_error: 3.4661\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 2.9956 - mean_squared_error: 2.9956 - val_loss: 3.5424 - val_mean_squared_error: 3.5424\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.8938 - mean_squared_error: 2.8938 - val_loss: 3.5383 - val_mean_squared_error: 3.5383\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 2.8464 - mean_squared_error: 2.8464 - val_loss: 3.3869 - val_mean_squared_error: 3.3869\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 2.8506 - mean_squared_error: 2.8506 - val_loss: 3.4559 - val_mean_squared_error: 3.4559\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.8254 - mean_squared_error: 2.8254 - val_loss: 3.3757 - val_mean_squared_error: 3.3757\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.7997 - mean_squared_error: 2.7997 - val_loss: 3.3783 - val_mean_squared_error: 3.3783\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.7090 - mean_squared_error: 2.7090 - val_loss: 3.5480 - val_mean_squared_error: 3.5480\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.7061 - mean_squared_error: 2.7061 - val_loss: 3.5242 - val_mean_squared_error: 3.5242\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 2.6554 - mean_squared_error: 2.6554 - val_loss: 3.4113 - val_mean_squared_error: 3.4113\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.5590 - mean_squared_error: 2.5590 - val_loss: 3.4027 - val_mean_squared_error: 3.4027\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.6103 - mean_squared_error: 2.6103 - val_loss: 3.2488 - val_mean_squared_error: 3.2488\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.5148 - mean_squared_error: 2.5148 - val_loss: 3.3270 - val_mean_squared_error: 3.3270\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.4649 - mean_squared_error: 2.4649 - val_loss: 3.3231 - val_mean_squared_error: 3.3231\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 2.4640 - mean_squared_error: 2.4640 - val_loss: 3.3972 - val_mean_squared_error: 3.3972\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 2.5117 - mean_squared_error: 2.5117 - val_loss: 3.3609 - val_mean_squared_error: 3.3609\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 2.4153 - mean_squared_error: 2.4153 - val_loss: 3.2700 - val_mean_squared_error: 3.2700\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 2.3968 - mean_squared_error: 2.3968 - val_loss: 3.3015 - val_mean_squared_error: 3.3015\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 0s 256us/sample - loss: 2.3683 - mean_squared_error: 2.3683 - val_loss: 3.3628 - val_mean_squared_error: 3.3628\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.3520 - mean_squared_error: 2.3520 - val_loss: 3.2279 - val_mean_squared_error: 3.2279\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 2.2230 - mean_squared_error: 2.2230 - val_loss: 3.2133 - val_mean_squared_error: 3.2133\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 2.2479 - mean_squared_error: 2.2479 - val_loss: 3.1909 - val_mean_squared_error: 3.1909\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 2.2713 - mean_squared_error: 2.2713 - val_loss: 3.4556 - val_mean_squared_error: 3.4556\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 2.1972 - mean_squared_error: 2.1972 - val_loss: 3.0919 - val_mean_squared_error: 3.0919\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.2534 - mean_squared_error: 2.2534 - val_loss: 3.1911 - val_mean_squared_error: 3.1911\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.1621 - mean_squared_error: 2.1621 - val_loss: 3.1788 - val_mean_squared_error: 3.1788\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 2.1952 - mean_squared_error: 2.1952 - val_loss: 3.2974 - val_mean_squared_error: 3.2974\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.1161 - mean_squared_error: 2.1161 - val_loss: 3.1389 - val_mean_squared_error: 3.1389\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.1949 - mean_squared_error: 2.1949 - val_loss: 3.0914 - val_mean_squared_error: 3.0914\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 2.1575 - mean_squared_error: 2.1575 - val_loss: 3.1712 - val_mean_squared_error: 3.1712\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.0907 - mean_squared_error: 2.0907 - val_loss: 3.2652 - val_mean_squared_error: 3.2652\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.1208 - mean_squared_error: 2.1208 - val_loss: 3.1792 - val_mean_squared_error: 3.1792\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 2.0270 - mean_squared_error: 2.0270 - val_loss: 3.7164 - val_mean_squared_error: 3.7164\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 2.0803 - mean_squared_error: 2.0803 - val_loss: 3.2227 - val_mean_squared_error: 3.2227\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.9949 - mean_squared_error: 1.9949 - val_loss: 3.4948 - val_mean_squared_error: 3.4948\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 2.0446 - mean_squared_error: 2.0446 - val_loss: 3.0117 - val_mean_squared_error: 3.0117\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.9387 - mean_squared_error: 1.9387 - val_loss: 3.1031 - val_mean_squared_error: 3.1031\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.9801 - mean_squared_error: 1.9801 - val_loss: 3.0680 - val_mean_squared_error: 3.0680\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 2.0291 - mean_squared_error: 2.0291 - val_loss: 3.1766 - val_mean_squared_error: 3.1766\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.9175 - mean_squared_error: 1.9175 - val_loss: 3.2654 - val_mean_squared_error: 3.2654\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.9097 - mean_squared_error: 1.9097 - val_loss: 3.0803 - val_mean_squared_error: 3.0803\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 1.8831 - mean_squared_error: 1.8831 - val_loss: 3.1477 - val_mean_squared_error: 3.1477\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.8305 - mean_squared_error: 1.8305 - val_loss: 3.0300 - val_mean_squared_error: 3.0300\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.9702 - mean_squared_error: 1.9702 - val_loss: 3.1040 - val_mean_squared_error: 3.1040\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.9068 - mean_squared_error: 1.9068 - val_loss: 3.4078 - val_mean_squared_error: 3.4078\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.8926 - mean_squared_error: 1.8926 - val_loss: 3.1557 - val_mean_squared_error: 3.1557\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.8738 - mean_squared_error: 1.8738 - val_loss: 3.1613 - val_mean_squared_error: 3.1613\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.8374 - mean_squared_error: 1.8374 - val_loss: 3.0157 - val_mean_squared_error: 3.0157\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.8956 - mean_squared_error: 1.8956 - val_loss: 3.0607 - val_mean_squared_error: 3.0607\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.7401 - mean_squared_error: 1.7401 - val_loss: 3.0566 - val_mean_squared_error: 3.0566\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.7885 - mean_squared_error: 1.7885 - val_loss: 2.9655 - val_mean_squared_error: 2.9655\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.8242 - mean_squared_error: 1.8242 - val_loss: 2.9626 - val_mean_squared_error: 2.9626\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.7472 - mean_squared_error: 1.7472 - val_loss: 2.9911 - val_mean_squared_error: 2.9911\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.7660 - mean_squared_error: 1.7660 - val_loss: 3.0308 - val_mean_squared_error: 3.0308\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 0s 254us/sample - loss: 1.6931 - mean_squared_error: 1.6931 - val_loss: 2.9444 - val_mean_squared_error: 2.9444\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.7462 - mean_squared_error: 1.7462 - val_loss: 2.9909 - val_mean_squared_error: 2.9909\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.6732 - mean_squared_error: 1.6732 - val_loss: 3.1940 - val_mean_squared_error: 3.1940\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.7709 - mean_squared_error: 1.7709 - val_loss: 3.0695 - val_mean_squared_error: 3.0695\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.7392 - mean_squared_error: 1.7392 - val_loss: 2.9767 - val_mean_squared_error: 2.9767\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.6305 - mean_squared_error: 1.6305 - val_loss: 3.0739 - val_mean_squared_error: 3.0739\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.7025 - mean_squared_error: 1.7025 - val_loss: 3.0003 - val_mean_squared_error: 3.0003\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.6472 - mean_squared_error: 1.6472 - val_loss: 2.9682 - val_mean_squared_error: 2.9682\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.6608 - mean_squared_error: 1.6608 - val_loss: 2.9574 - val_mean_squared_error: 2.9574\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.6839 - mean_squared_error: 1.6839 - val_loss: 2.9119 - val_mean_squared_error: 2.9119\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.6633 - mean_squared_error: 1.6633 - val_loss: 3.0347 - val_mean_squared_error: 3.0347\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.5940 - mean_squared_error: 1.5940 - val_loss: 3.1993 - val_mean_squared_error: 3.1993\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.6121 - mean_squared_error: 1.6121 - val_loss: 3.0132 - val_mean_squared_error: 3.0132\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.5997 - mean_squared_error: 1.5997 - val_loss: 2.8996 - val_mean_squared_error: 2.8996\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 1.6251 - mean_squared_error: 1.6251 - val_loss: 2.9677 - val_mean_squared_error: 2.9677\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.6017 - mean_squared_error: 1.6017 - val_loss: 2.9839 - val_mean_squared_error: 2.9839\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.6345 - mean_squared_error: 1.6345 - val_loss: 2.9141 - val_mean_squared_error: 2.9141\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.5733 - mean_squared_error: 1.5733 - val_loss: 3.0973 - val_mean_squared_error: 3.0973\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.6140 - mean_squared_error: 1.6140 - val_loss: 2.9016 - val_mean_squared_error: 2.9016\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.6278 - mean_squared_error: 1.6278 - val_loss: 3.0033 - val_mean_squared_error: 3.0033\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.5386 - mean_squared_error: 1.5386 - val_loss: 3.0411 - val_mean_squared_error: 3.0411\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.5364 - mean_squared_error: 1.5364 - val_loss: 3.0294 - val_mean_squared_error: 3.0294\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.5834 - mean_squared_error: 1.5834 - val_loss: 2.9871 - val_mean_squared_error: 2.9871\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.4756 - mean_squared_error: 1.4756 - val_loss: 3.1378 - val_mean_squared_error: 3.1378\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.5420 - mean_squared_error: 1.5420 - val_loss: 3.3723 - val_mean_squared_error: 3.3723\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 1.4889 - mean_squared_error: 1.4889 - val_loss: 2.9693 - val_mean_squared_error: 2.9693\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.5282 - mean_squared_error: 1.5282 - val_loss: 2.8948 - val_mean_squared_error: 2.8948\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.4955 - mean_squared_error: 1.4955 - val_loss: 3.0516 - val_mean_squared_error: 3.0516\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.5381 - mean_squared_error: 1.5381 - val_loss: 2.9975 - val_mean_squared_error: 2.9975\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.4793 - mean_squared_error: 1.4793 - val_loss: 2.8340 - val_mean_squared_error: 2.8340\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.4191 - mean_squared_error: 1.4191 - val_loss: 2.8483 - val_mean_squared_error: 2.8483\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.4341 - mean_squared_error: 1.4341 - val_loss: 2.9209 - val_mean_squared_error: 2.9209\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 1.4700 - mean_squared_error: 1.4700 - val_loss: 2.8768 - val_mean_squared_error: 2.8768\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.5660 - mean_squared_error: 1.5660 - val_loss: 2.9278 - val_mean_squared_error: 2.9278\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.4333 - mean_squared_error: 1.4333 - val_loss: 2.9374 - val_mean_squared_error: 2.9374\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.4175 - mean_squared_error: 1.4175 - val_loss: 2.8870 - val_mean_squared_error: 2.8870\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.4648 - mean_squared_error: 1.4648 - val_loss: 2.9155 - val_mean_squared_error: 2.9155\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.4032 - mean_squared_error: 1.4032 - val_loss: 2.9366 - val_mean_squared_error: 2.9366\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3743 - mean_squared_error: 1.3743 - val_loss: 2.8834 - val_mean_squared_error: 2.8834\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.4724 - mean_squared_error: 1.4724 - val_loss: 2.9535 - val_mean_squared_error: 2.9535\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.4683 - mean_squared_error: 1.4683 - val_loss: 3.0854 - val_mean_squared_error: 3.0854\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.4197 - mean_squared_error: 1.4197 - val_loss: 2.8131 - val_mean_squared_error: 2.8131\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.4791 - mean_squared_error: 1.4791 - val_loss: 2.8855 - val_mean_squared_error: 2.8855\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.3887 - mean_squared_error: 1.3887 - val_loss: 2.8530 - val_mean_squared_error: 2.8530\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3880 - mean_squared_error: 1.3880 - val_loss: 2.8249 - val_mean_squared_error: 2.8249\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3849 - mean_squared_error: 1.3849 - val_loss: 2.8981 - val_mean_squared_error: 2.8981\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3849 - mean_squared_error: 1.3849 - val_loss: 2.8677 - val_mean_squared_error: 2.8677\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3716 - mean_squared_error: 1.3716 - val_loss: 2.9392 - val_mean_squared_error: 2.9392\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.4306 - mean_squared_error: 1.4306 - val_loss: 2.9191 - val_mean_squared_error: 2.9191\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.3557 - mean_squared_error: 1.3557 - val_loss: 2.9242 - val_mean_squared_error: 2.9242\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.3465 - mean_squared_error: 1.3465 - val_loss: 2.8556 - val_mean_squared_error: 2.8556\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.3830 - mean_squared_error: 1.3830 - val_loss: 2.8869 - val_mean_squared_error: 2.8869\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 0s 256us/sample - loss: 1.3628 - mean_squared_error: 1.3628 - val_loss: 2.8611 - val_mean_squared_error: 2.8611\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.3506 - mean_squared_error: 1.3506 - val_loss: 2.8707 - val_mean_squared_error: 2.8707\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.3514 - mean_squared_error: 1.3514 - val_loss: 2.8450 - val_mean_squared_error: 2.8450\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.4015 - mean_squared_error: 1.4015 - val_loss: 2.8240 - val_mean_squared_error: 2.8240\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.3950 - mean_squared_error: 1.3950 - val_loss: 2.8208 - val_mean_squared_error: 2.8208\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 1.2900 - mean_squared_error: 1.2900 - val_loss: 2.9131 - val_mean_squared_error: 2.9131\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3562 - mean_squared_error: 1.3562 - val_loss: 2.8153 - val_mean_squared_error: 2.8153\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.2967 - mean_squared_error: 1.2967 - val_loss: 3.1156 - val_mean_squared_error: 3.1156\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.3339 - mean_squared_error: 1.3339 - val_loss: 2.8334 - val_mean_squared_error: 2.8334\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.2791 - mean_squared_error: 1.2791 - val_loss: 2.7635 - val_mean_squared_error: 2.7635\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.3037 - mean_squared_error: 1.3037 - val_loss: 2.8884 - val_mean_squared_error: 2.8884\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3276 - mean_squared_error: 1.3276 - val_loss: 2.8670 - val_mean_squared_error: 2.8670\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.3024 - mean_squared_error: 1.3024 - val_loss: 2.8316 - val_mean_squared_error: 2.8316\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.2925 - mean_squared_error: 1.2925 - val_loss: 3.0112 - val_mean_squared_error: 3.0112\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3403 - mean_squared_error: 1.3403 - val_loss: 2.9519 - val_mean_squared_error: 2.9519\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.2931 - mean_squared_error: 1.2931 - val_loss: 2.7914 - val_mean_squared_error: 2.7914\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 1.3377 - mean_squared_error: 1.3377 - val_loss: 3.2896 - val_mean_squared_error: 3.2896\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2996 - mean_squared_error: 1.2996 - val_loss: 2.8254 - val_mean_squared_error: 2.8254\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.2350 - mean_squared_error: 1.2350 - val_loss: 2.8031 - val_mean_squared_error: 2.8031\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.3255 - mean_squared_error: 1.3255 - val_loss: 2.8916 - val_mean_squared_error: 2.8916\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 1.3620 - mean_squared_error: 1.3620 - val_loss: 2.8686 - val_mean_squared_error: 2.8686\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.3049 - mean_squared_error: 1.3049 - val_loss: 2.8289 - val_mean_squared_error: 2.8289\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.2031 - mean_squared_error: 1.2031 - val_loss: 2.8556 - val_mean_squared_error: 2.8556\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.2214 - mean_squared_error: 1.2214 - val_loss: 2.8357 - val_mean_squared_error: 2.8357\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2665 - mean_squared_error: 1.2665 - val_loss: 2.8237 - val_mean_squared_error: 2.8237\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.2997 - mean_squared_error: 1.2997 - val_loss: 2.8469 - val_mean_squared_error: 2.8469\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.2728 - mean_squared_error: 1.2728 - val_loss: 2.8688 - val_mean_squared_error: 2.8688\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 1.2729 - mean_squared_error: 1.2729 - val_loss: 2.8773 - val_mean_squared_error: 2.8773\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.2215 - mean_squared_error: 1.2215 - val_loss: 2.8229 - val_mean_squared_error: 2.8229\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.2637 - mean_squared_error: 1.2637 - val_loss: 2.8726 - val_mean_squared_error: 2.8726\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.2754 - mean_squared_error: 1.2754 - val_loss: 2.8890 - val_mean_squared_error: 2.8890\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.2041 - mean_squared_error: 1.2041 - val_loss: 2.8202 - val_mean_squared_error: 2.8202\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 0s 254us/sample - loss: 1.2843 - mean_squared_error: 1.2843 - val_loss: 2.8543 - val_mean_squared_error: 2.8543\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.2426 - mean_squared_error: 1.2426 - val_loss: 2.8896 - val_mean_squared_error: 2.8896\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 1.2819 - mean_squared_error: 1.2819 - val_loss: 2.8194 - val_mean_squared_error: 2.8194\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.2933 - mean_squared_error: 1.2933 - val_loss: 3.0076 - val_mean_squared_error: 3.0076\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 1.1984 - mean_squared_error: 1.1984 - val_loss: 2.7585 - val_mean_squared_error: 2.7585\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.2721 - mean_squared_error: 1.2721 - val_loss: 2.7961 - val_mean_squared_error: 2.7961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3-oCZ_ykWUn",
        "colab_type": "text"
      },
      "source": [
        "## Learning Rate Sensitivity\n",
        "\n",
        "Several rounds of sensitivity studies were performed outside on the Colaboratory environment on the team's personal computers. Many of these studies can be found in the cnn_notebook_tpg.ipynb file located in the team's [GitHub repository](https://github.com/tomgoter/w207_finalproject).\n",
        "\n",
        "To summarize what was learned from those studies:\n",
        "\n",
        "1.   Starting filter depth of 12 to 16 was a better balance to runtime and accuracy than starting with the initial assumption of 32. This is clearly shown below. The starting filter size of 32 is unnecessary for accuracy and runs much more slowly than a reduced starting filter depth.\n",
        "![alt text](https://github.com/tomgoter/w207_finalproject/raw/master/Images/starting_filter.png?raw=true)\n",
        "2.   It was also determined (as previously mentioned in this document) that the Adam and SGD optimizers performed the best for these models. \n",
        "3.  The use of batch normalization (without a bias term) allows us to increase the learning rate and achieve better accuracies. Batch normalization basically standardizes the feature set after every convolution/pooling layer. The image below shows that the use of batch normalization does lead to faster convergence, but comes at the cost of model runtime. \n",
        "\n",
        "![alt text](https://github.com/tomgoter/w207_finalproject/raw/master/Images/batch_norm.png?raw=true)\n",
        "\n",
        "4. Gradual dropouts starting at 0 and increasing by small amounts appeared to give the best final accuracies (but this is further explored below).\n",
        "\n",
        "This cell blocks below were used to pick-up where the aforementioned notebook left off. From here on out all models were built and run through the Colaboratory environment to make the progress easier to follow. We start with a learning rate sensitivity, which also explore different dropout rates and starting filter sizes. \n",
        "\n",
        "The results of the experiment below seemed to indicate that learning rates between 5x to 10x the default rate of 0.001 seem to perform better than the default rate, and that this is true for both starting filter depths of 12 and 16. See the plot below which was generated with the neural_net_analysis.ipynb notebook located at the GitHub repository (link above)\n",
        "\n",
        "![image](https://github.com/tomgoter/w207_finalproject/raw/master/Images/LR_Sense_16SF.png \"Learning Rate Sensitivity\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF2rTYqJkrre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_bn_cnn_model(start_filter, d, step, fc1=500, fc2=500):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model that implements\n",
        "    convolute/pool framework (similar to AlexNet). It makes use of valid padding during the convolution\n",
        "    layers with rectified linear unit activation functions. Kernels are typically 2x2. Starting filter \n",
        "    depth can be varied, but it is assumed the filter depth increases by a factor of two during every \n",
        "    convolution (inspired by AlexNet architecture)\n",
        "    '''\n",
        "    # Instantiate Sequential Model\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "    \n",
        "    # Define Input structure\n",
        "    cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    \n",
        "    # Define convolution layer - takes the start_filter parameter as the filter depth\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='valid', activation='relu', use_bias=False))\n",
        "    \n",
        "    # Standardize the convoluted features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Max Pool - reduce the feature space\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add a form of regularization through dropout - dropout rate is parameterized\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    \n",
        "    # Rinse and repeat for two more full layers - increasing filter depth and dropout rate\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (2, 2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (2, 2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+2*step))\n",
        "    \n",
        "    # Flatten the feature space and transition to fully connected layers\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc1))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc2))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    \n",
        "    # Output layer of 30 keypoint coordinates with linear activation\n",
        "    cnn_model.add(tf.keras.layers.Dense(30))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FK2GEwckcAi",
        "colab_type": "code",
        "outputId": "d0e318ee-1d67-4ac0-f8bb-92a10f9487ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Use an early stopping callback and our timing callback\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                              patience=100, mode='auto')\n",
        "time_callback = TimeHistory()\n",
        "\n",
        "# Initialize a new data frame to hold our output data\n",
        "cnn_lr_df = pd.DataFrame()\n",
        "\n",
        "# Start filter list\n",
        "start_filters = [12, 16]\n",
        "\n",
        "# Create a list of initial dropout values and steps to increase\n",
        "dropouts = [(0.00,0.00), (0.0,0.01), (0.00,0.02), (0.00,0.03)]\n",
        "\n",
        "\n",
        "# Run a parameteric set of studies\n",
        "for lr_factor in [2, 5, 10]: \n",
        "    for start_filter in start_filters:\n",
        "        for d in dropouts:\n",
        "            model = create_bn_cnn_model(start_filter, d[0], d[1])\n",
        "            # Use the adam optimizer with the default learning rate\n",
        "            adam = optimizers.Adam(lr=0.001*lr_factor, beta_1=0.9, beta_2=0.999)\n",
        "            model.compile(\n",
        "                  optimizer=adam,\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_squared_error'])\n",
        "            history = model.fit(\n",
        "                X.astype(np.float32), y.astype(np.float32),\n",
        "                epochs=200,\n",
        "                validation_split=0.15, callbacks=[time_callback, early_stop])\n",
        "            times = time_callback.times\n",
        "\n",
        "            # Convert model output to dataframe\n",
        "            hist = pd.DataFrame(history.history)\n",
        "            hist['epoch'] = history.epoch\n",
        "            hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "            hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "            hist['times'] = times\n",
        "            \n",
        "            # Add model specific metadata to differentiate between models\n",
        "            hist['starting_filter'] = start_filter\n",
        "            hist['layers'] = 3\n",
        "            hist['pooling'] = 'yes'\n",
        "            hist['fc_layer1'] = 500\n",
        "            hist['fc_layer2'] = 500\n",
        "            hist['activation'] = 'relu'\n",
        "            hist['optimizer'] = 'adam'\n",
        "            hist['lrate'] = adam.get_config()['learning_rate']\n",
        "            hist['dropout_initial'] = d[0]\n",
        "            hist['dropout_step'] = d[1]\n",
        "            hist['batch_norm'] = 1\n",
        "            hist['bias'] = 0\n",
        "            hist['stride'] = 1\n",
        "\n",
        "            # Keep concatenating to dataframe\n",
        "            cnn_lr_df = pd.concat([cnn_lr_df,hist])\n",
        "\n",
        "            # Re-pickle after every model to retain progress\n",
        "            cnn_lr_df.to_pickle(drive_path+\"OutputData/cnn_lr_df.pkl\")\n",
        "\n",
        "            # Save models.\n",
        "            filename = \"cnn_lr_{}_d{}_s{}_sf{}_lrfactor{}\".format('adam', d[0], d[1], start_filter, lr_factor)\n",
        "            model.save(drive_path+\"Models/\"+filename+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 691us/sample - loss: 1524.5925 - mean_squared_error: 1524.5928 - val_loss: 1340.4921 - val_mean_squared_error: 1340.4921\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 229.2972 - mean_squared_error: 229.2972 - val_loss: 333.2501 - val_mean_squared_error: 333.2502\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 39.2172 - mean_squared_error: 39.2172 - val_loss: 103.8055 - val_mean_squared_error: 103.8055\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 21.5871 - mean_squared_error: 21.5871 - val_loss: 47.4265 - val_mean_squared_error: 47.4265\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 15.4881 - mean_squared_error: 15.4881 - val_loss: 25.3412 - val_mean_squared_error: 25.3412\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 16.4591 - mean_squared_error: 16.4591 - val_loss: 18.1627 - val_mean_squared_error: 18.1627\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 14.7848 - mean_squared_error: 14.7848 - val_loss: 15.6381 - val_mean_squared_error: 15.6381\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 14.6200 - mean_squared_error: 14.6200 - val_loss: 10.7751 - val_mean_squared_error: 10.7751\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 12.8585 - mean_squared_error: 12.8585 - val_loss: 14.3254 - val_mean_squared_error: 14.3254\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 12.8056 - mean_squared_error: 12.8056 - val_loss: 16.0035 - val_mean_squared_error: 16.0035\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 11.9934 - mean_squared_error: 11.9934 - val_loss: 12.3756 - val_mean_squared_error: 12.3756\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 10.5664 - mean_squared_error: 10.5664 - val_loss: 11.3312 - val_mean_squared_error: 11.3312\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 11.5691 - mean_squared_error: 11.5691 - val_loss: 10.4683 - val_mean_squared_error: 10.4683\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 10.4908 - mean_squared_error: 10.4908 - val_loss: 11.0794 - val_mean_squared_error: 11.0794\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 10.7546 - mean_squared_error: 10.7546 - val_loss: 13.9518 - val_mean_squared_error: 13.9518\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 9.2093 - mean_squared_error: 9.2093 - val_loss: 10.7245 - val_mean_squared_error: 10.7245\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 8.6495 - mean_squared_error: 8.6495 - val_loss: 10.8164 - val_mean_squared_error: 10.8164\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 8.8125 - mean_squared_error: 8.8125 - val_loss: 11.7447 - val_mean_squared_error: 11.7447\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 8.0065 - mean_squared_error: 8.0065 - val_loss: 9.3142 - val_mean_squared_error: 9.3142\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 7.3384 - mean_squared_error: 7.3384 - val_loss: 8.7957 - val_mean_squared_error: 8.7957\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 6.8864 - mean_squared_error: 6.8864 - val_loss: 9.0984 - val_mean_squared_error: 9.0984\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 6.4859 - mean_squared_error: 6.4859 - val_loss: 7.9767 - val_mean_squared_error: 7.9767\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 5.8136 - mean_squared_error: 5.8136 - val_loss: 6.2399 - val_mean_squared_error: 6.2399\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 5.4800 - mean_squared_error: 5.4800 - val_loss: 7.0085 - val_mean_squared_error: 7.0085\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 5.9928 - mean_squared_error: 5.9928 - val_loss: 6.8727 - val_mean_squared_error: 6.8727\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 5.7755 - mean_squared_error: 5.7755 - val_loss: 7.4414 - val_mean_squared_error: 7.4414\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 5.2558 - mean_squared_error: 5.2558 - val_loss: 5.8596 - val_mean_squared_error: 5.8596\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 327us/sample - loss: 5.0365 - mean_squared_error: 5.0365 - val_loss: 6.0949 - val_mean_squared_error: 6.0949\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 4.4527 - mean_squared_error: 4.4527 - val_loss: 5.2767 - val_mean_squared_error: 5.2767\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 4.1954 - mean_squared_error: 4.1954 - val_loss: 5.6757 - val_mean_squared_error: 5.6757\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 4.2053 - mean_squared_error: 4.2053 - val_loss: 5.0579 - val_mean_squared_error: 5.0579\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 4.1148 - mean_squared_error: 4.1148 - val_loss: 4.5708 - val_mean_squared_error: 4.5708\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 3.9779 - mean_squared_error: 3.9779 - val_loss: 5.7409 - val_mean_squared_error: 5.7409\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 3.8312 - mean_squared_error: 3.8312 - val_loss: 4.8623 - val_mean_squared_error: 4.8623\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 3.7779 - mean_squared_error: 3.7779 - val_loss: 4.5047 - val_mean_squared_error: 4.5047\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 3.6256 - mean_squared_error: 3.6256 - val_loss: 4.7511 - val_mean_squared_error: 4.7511\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 3.5155 - mean_squared_error: 3.5155 - val_loss: 4.5789 - val_mean_squared_error: 4.5789\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 327us/sample - loss: 3.4155 - mean_squared_error: 3.4155 - val_loss: 5.0810 - val_mean_squared_error: 5.0810\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 3.4059 - mean_squared_error: 3.4059 - val_loss: 5.4563 - val_mean_squared_error: 5.4563\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 3.1935 - mean_squared_error: 3.1935 - val_loss: 3.9975 - val_mean_squared_error: 3.9975\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 3.1112 - mean_squared_error: 3.1112 - val_loss: 4.4076 - val_mean_squared_error: 4.4076\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 3.2834 - mean_squared_error: 3.2834 - val_loss: 4.8234 - val_mean_squared_error: 4.8234\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 3.4395 - mean_squared_error: 3.4395 - val_loss: 4.5921 - val_mean_squared_error: 4.5921\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 3.1076 - mean_squared_error: 3.1076 - val_loss: 3.7299 - val_mean_squared_error: 3.7299\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 3.1186 - mean_squared_error: 3.1186 - val_loss: 3.9973 - val_mean_squared_error: 3.9973\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 2.9553 - mean_squared_error: 2.9553 - val_loss: 3.6556 - val_mean_squared_error: 3.6556\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 2.7850 - mean_squared_error: 2.7850 - val_loss: 4.2310 - val_mean_squared_error: 4.2310\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 2.6972 - mean_squared_error: 2.6972 - val_loss: 3.3696 - val_mean_squared_error: 3.3696\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 2.9707 - mean_squared_error: 2.9707 - val_loss: 3.3160 - val_mean_squared_error: 3.3160\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 2.6943 - mean_squared_error: 2.6943 - val_loss: 5.7513 - val_mean_squared_error: 5.7513\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 3.0725 - mean_squared_error: 3.0725 - val_loss: 5.1434 - val_mean_squared_error: 5.1434\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 2.7004 - mean_squared_error: 2.7004 - val_loss: 4.9225 - val_mean_squared_error: 4.9225\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 2.7124 - mean_squared_error: 2.7124 - val_loss: 3.3402 - val_mean_squared_error: 3.3402\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 2.7610 - mean_squared_error: 2.7610 - val_loss: 3.6415 - val_mean_squared_error: 3.6415\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 2.7432 - mean_squared_error: 2.7432 - val_loss: 3.8502 - val_mean_squared_error: 3.8502\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 2.4309 - mean_squared_error: 2.4309 - val_loss: 4.1760 - val_mean_squared_error: 4.1760\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 2.2484 - mean_squared_error: 2.2484 - val_loss: 3.5271 - val_mean_squared_error: 3.5271\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 2.4465 - mean_squared_error: 2.4465 - val_loss: 3.6380 - val_mean_squared_error: 3.6380\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.3687 - mean_squared_error: 2.3687 - val_loss: 3.8697 - val_mean_squared_error: 3.8697\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 2.2982 - mean_squared_error: 2.2982 - val_loss: 3.5946 - val_mean_squared_error: 3.5946\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 2.1950 - mean_squared_error: 2.1950 - val_loss: 3.5468 - val_mean_squared_error: 3.5468\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 2.1717 - mean_squared_error: 2.1717 - val_loss: 4.3792 - val_mean_squared_error: 4.3792\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 2.1563 - mean_squared_error: 2.1563 - val_loss: 3.3375 - val_mean_squared_error: 3.3375\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 2.0412 - mean_squared_error: 2.0412 - val_loss: 3.4253 - val_mean_squared_error: 3.4253\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 2.2236 - mean_squared_error: 2.2236 - val_loss: 4.0542 - val_mean_squared_error: 4.0542\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 2.1583 - mean_squared_error: 2.1583 - val_loss: 4.4447 - val_mean_squared_error: 4.4447\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 2.0917 - mean_squared_error: 2.0917 - val_loss: 4.2642 - val_mean_squared_error: 4.2642\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 2.1918 - mean_squared_error: 2.1918 - val_loss: 3.1926 - val_mean_squared_error: 3.1926\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 2.0283 - mean_squared_error: 2.0283 - val_loss: 2.8176 - val_mean_squared_error: 2.8176\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.9635 - mean_squared_error: 1.9635 - val_loss: 3.3382 - val_mean_squared_error: 3.3382\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.0154 - mean_squared_error: 2.0154 - val_loss: 3.9843 - val_mean_squared_error: 3.9843\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 2.0209 - mean_squared_error: 2.0209 - val_loss: 2.9425 - val_mean_squared_error: 2.9425\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.9137 - mean_squared_error: 1.9137 - val_loss: 3.3100 - val_mean_squared_error: 3.3100\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.7703 - mean_squared_error: 1.7703 - val_loss: 2.9951 - val_mean_squared_error: 2.9951\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.9670 - mean_squared_error: 1.9670 - val_loss: 3.6787 - val_mean_squared_error: 3.6787\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.9712 - mean_squared_error: 1.9712 - val_loss: 3.4390 - val_mean_squared_error: 3.4390\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.1967 - mean_squared_error: 2.1967 - val_loss: 3.3732 - val_mean_squared_error: 3.3732\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 1.9861 - mean_squared_error: 1.9861 - val_loss: 2.9091 - val_mean_squared_error: 2.9091\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.8522 - mean_squared_error: 1.8522 - val_loss: 3.0388 - val_mean_squared_error: 3.0388\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.7615 - mean_squared_error: 1.7615 - val_loss: 2.8752 - val_mean_squared_error: 2.8752\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.7972 - mean_squared_error: 1.7972 - val_loss: 3.1531 - val_mean_squared_error: 3.1531\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.6786 - mean_squared_error: 1.6786 - val_loss: 2.8551 - val_mean_squared_error: 2.8551\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.8268 - mean_squared_error: 1.8268 - val_loss: 3.5829 - val_mean_squared_error: 3.5829\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.9200 - mean_squared_error: 1.9200 - val_loss: 4.4824 - val_mean_squared_error: 4.4824\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 1.9233 - mean_squared_error: 1.9233 - val_loss: 3.1965 - val_mean_squared_error: 3.1965\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.6678 - mean_squared_error: 1.6678 - val_loss: 2.7654 - val_mean_squared_error: 2.7654\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.6699 - mean_squared_error: 1.6699 - val_loss: 3.4868 - val_mean_squared_error: 3.4868\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.7726 - mean_squared_error: 1.7726 - val_loss: 3.0454 - val_mean_squared_error: 3.0454\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.5334 - mean_squared_error: 1.5334 - val_loss: 3.0022 - val_mean_squared_error: 3.0022\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.9744 - mean_squared_error: 1.9744 - val_loss: 2.8378 - val_mean_squared_error: 2.8378\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 1.8268 - mean_squared_error: 1.8268 - val_loss: 3.1858 - val_mean_squared_error: 3.1858\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 1.7038 - mean_squared_error: 1.7038 - val_loss: 3.6120 - val_mean_squared_error: 3.6120\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 1.5929 - mean_squared_error: 1.5929 - val_loss: 2.8858 - val_mean_squared_error: 2.8858\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.6499 - mean_squared_error: 1.6499 - val_loss: 3.1895 - val_mean_squared_error: 3.1896\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.5100 - mean_squared_error: 1.5100 - val_loss: 3.3787 - val_mean_squared_error: 3.3787\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.5992 - mean_squared_error: 1.5992 - val_loss: 2.7657 - val_mean_squared_error: 2.7657\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.5502 - mean_squared_error: 1.5502 - val_loss: 2.8314 - val_mean_squared_error: 2.8314\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.4696 - mean_squared_error: 1.4696 - val_loss: 3.1666 - val_mean_squared_error: 3.1666\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.3915 - mean_squared_error: 1.3915 - val_loss: 2.8946 - val_mean_squared_error: 2.8946\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.5075 - mean_squared_error: 1.5075 - val_loss: 3.0499 - val_mean_squared_error: 3.0499\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.5333 - mean_squared_error: 1.5333 - val_loss: 3.3448 - val_mean_squared_error: 3.3448\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.6521 - mean_squared_error: 1.6521 - val_loss: 3.6577 - val_mean_squared_error: 3.6577\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 1.5652 - mean_squared_error: 1.5652 - val_loss: 2.6486 - val_mean_squared_error: 2.6486\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.4510 - mean_squared_error: 1.4510 - val_loss: 3.0655 - val_mean_squared_error: 3.0655\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.4722 - mean_squared_error: 1.4722 - val_loss: 3.0636 - val_mean_squared_error: 3.0636\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.5761 - mean_squared_error: 1.5761 - val_loss: 3.2183 - val_mean_squared_error: 3.2183\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.6860 - mean_squared_error: 1.6860 - val_loss: 3.0852 - val_mean_squared_error: 3.0852\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 1.6515 - mean_squared_error: 1.6515 - val_loss: 3.0542 - val_mean_squared_error: 3.0542\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.5147 - mean_squared_error: 1.5147 - val_loss: 3.2559 - val_mean_squared_error: 3.2559\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.5189 - mean_squared_error: 1.5189 - val_loss: 3.0748 - val_mean_squared_error: 3.0748\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.5855 - mean_squared_error: 1.5855 - val_loss: 3.1558 - val_mean_squared_error: 3.1558\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.3288 - mean_squared_error: 1.3288 - val_loss: 3.1580 - val_mean_squared_error: 3.1580\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.3528 - mean_squared_error: 1.3528 - val_loss: 2.9205 - val_mean_squared_error: 2.9205\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.3930 - mean_squared_error: 1.3930 - val_loss: 3.4026 - val_mean_squared_error: 3.4026\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.4290 - mean_squared_error: 1.4290 - val_loss: 3.0492 - val_mean_squared_error: 3.0492\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.2499 - mean_squared_error: 1.2499 - val_loss: 2.9560 - val_mean_squared_error: 2.9560\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.3829 - mean_squared_error: 1.3829 - val_loss: 2.6087 - val_mean_squared_error: 2.6087\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.2697 - mean_squared_error: 1.2697 - val_loss: 2.8086 - val_mean_squared_error: 2.8086\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.3186 - mean_squared_error: 1.3186 - val_loss: 3.2032 - val_mean_squared_error: 3.2032\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.3838 - mean_squared_error: 1.3838 - val_loss: 2.9809 - val_mean_squared_error: 2.9809\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.3503 - mean_squared_error: 1.3503 - val_loss: 2.9820 - val_mean_squared_error: 2.9820\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 1.3109 - mean_squared_error: 1.3109 - val_loss: 2.8117 - val_mean_squared_error: 2.8117\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.4020 - mean_squared_error: 1.4020 - val_loss: 4.1778 - val_mean_squared_error: 4.1778\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.4137 - mean_squared_error: 1.4137 - val_loss: 3.2657 - val_mean_squared_error: 3.2657\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.3992 - mean_squared_error: 1.3992 - val_loss: 2.7730 - val_mean_squared_error: 2.7730\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.2853 - mean_squared_error: 1.2853 - val_loss: 3.1351 - val_mean_squared_error: 3.1351\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.2874 - mean_squared_error: 1.2874 - val_loss: 2.8040 - val_mean_squared_error: 2.8040\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.2224 - mean_squared_error: 1.2224 - val_loss: 2.9431 - val_mean_squared_error: 2.9431\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.2373 - mean_squared_error: 1.2373 - val_loss: 2.7026 - val_mean_squared_error: 2.7026\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 1.2033 - mean_squared_error: 1.2033 - val_loss: 2.6911 - val_mean_squared_error: 2.6911\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.2562 - mean_squared_error: 1.2562 - val_loss: 3.6669 - val_mean_squared_error: 3.6669\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.3466 - mean_squared_error: 1.3466 - val_loss: 3.0587 - val_mean_squared_error: 3.0587\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.2520 - mean_squared_error: 1.2520 - val_loss: 3.3521 - val_mean_squared_error: 3.3521\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.2556 - mean_squared_error: 1.2556 - val_loss: 3.1148 - val_mean_squared_error: 3.1148\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.2919 - mean_squared_error: 1.2919 - val_loss: 3.5194 - val_mean_squared_error: 3.5194\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.1034 - mean_squared_error: 1.1034 - val_loss: 2.6932 - val_mean_squared_error: 2.6932\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.3507 - mean_squared_error: 1.3507 - val_loss: 2.7983 - val_mean_squared_error: 2.7983\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 1.1841 - mean_squared_error: 1.1841 - val_loss: 2.7635 - val_mean_squared_error: 2.7635\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.2228 - mean_squared_error: 1.2228 - val_loss: 2.9916 - val_mean_squared_error: 2.9916\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.2590 - mean_squared_error: 1.2590 - val_loss: 2.9638 - val_mean_squared_error: 2.9638\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.1848 - mean_squared_error: 1.1848 - val_loss: 3.2261 - val_mean_squared_error: 3.2261\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.3669 - mean_squared_error: 1.3669 - val_loss: 2.6746 - val_mean_squared_error: 2.6746\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.2367 - mean_squared_error: 1.2367 - val_loss: 2.6942 - val_mean_squared_error: 2.6942\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 1.2440 - mean_squared_error: 1.2440 - val_loss: 2.8893 - val_mean_squared_error: 2.8893\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 1.1372 - mean_squared_error: 1.1372 - val_loss: 3.4356 - val_mean_squared_error: 3.4356\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.1430 - mean_squared_error: 1.1430 - val_loss: 2.7737 - val_mean_squared_error: 2.7737\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.1281 - mean_squared_error: 1.1281 - val_loss: 2.7658 - val_mean_squared_error: 2.7658\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 1.2148 - mean_squared_error: 1.2148 - val_loss: 2.6524 - val_mean_squared_error: 2.6524\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.1351 - mean_squared_error: 1.1351 - val_loss: 2.7441 - val_mean_squared_error: 2.7441\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.1403 - mean_squared_error: 1.1403 - val_loss: 2.5393 - val_mean_squared_error: 2.5393\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.1392 - mean_squared_error: 1.1392 - val_loss: 3.0913 - val_mean_squared_error: 3.0913\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.1939 - mean_squared_error: 1.1939 - val_loss: 2.5334 - val_mean_squared_error: 2.5334\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.0369 - mean_squared_error: 1.0369 - val_loss: 2.8408 - val_mean_squared_error: 2.8408\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.0257 - mean_squared_error: 1.0257 - val_loss: 3.0412 - val_mean_squared_error: 3.0412\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 1.1442 - mean_squared_error: 1.1442 - val_loss: 3.0601 - val_mean_squared_error: 3.0601\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.1605 - mean_squared_error: 1.1605 - val_loss: 2.8725 - val_mean_squared_error: 2.8725\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.2256 - mean_squared_error: 1.2256 - val_loss: 2.7384 - val_mean_squared_error: 2.7384\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.0667 - mean_squared_error: 1.0667 - val_loss: 2.8791 - val_mean_squared_error: 2.8791\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.2401 - mean_squared_error: 1.2401 - val_loss: 2.8068 - val_mean_squared_error: 2.8068\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 1.0490 - mean_squared_error: 1.0490 - val_loss: 2.8483 - val_mean_squared_error: 2.8483\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.0272 - mean_squared_error: 1.0272 - val_loss: 2.5878 - val_mean_squared_error: 2.5878\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 2.4772 - val_mean_squared_error: 2.4772\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 0.9846 - mean_squared_error: 0.9846 - val_loss: 2.7602 - val_mean_squared_error: 2.7602\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 0.9718 - mean_squared_error: 0.9718 - val_loss: 2.5430 - val_mean_squared_error: 2.5430\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.0677 - mean_squared_error: 1.0677 - val_loss: 2.5938 - val_mean_squared_error: 2.5938\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.0692 - mean_squared_error: 1.0692 - val_loss: 2.5925 - val_mean_squared_error: 2.5925\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 0.9344 - mean_squared_error: 0.9344 - val_loss: 2.7897 - val_mean_squared_error: 2.7897\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 0.9988 - mean_squared_error: 0.9988 - val_loss: 3.4614 - val_mean_squared_error: 3.4614\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 1.0472 - mean_squared_error: 1.0472 - val_loss: 3.3565 - val_mean_squared_error: 3.3565\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 1.1021 - mean_squared_error: 1.1021 - val_loss: 2.4928 - val_mean_squared_error: 2.4928\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.0261 - mean_squared_error: 1.0261 - val_loss: 2.5720 - val_mean_squared_error: 2.5720\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 0.9979 - mean_squared_error: 0.9979 - val_loss: 2.6140 - val_mean_squared_error: 2.6140\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 2.4827 - val_mean_squared_error: 2.4827\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.0035 - mean_squared_error: 1.0035 - val_loss: 2.7606 - val_mean_squared_error: 2.7606\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 2.9191 - val_mean_squared_error: 2.9191\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 0.9622 - mean_squared_error: 0.9622 - val_loss: 2.9331 - val_mean_squared_error: 2.9331\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.0396 - mean_squared_error: 1.0396 - val_loss: 2.7388 - val_mean_squared_error: 2.7388\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.1263 - mean_squared_error: 1.1263 - val_loss: 2.7988 - val_mean_squared_error: 2.7988\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.0204 - mean_squared_error: 1.0204 - val_loss: 2.4584 - val_mean_squared_error: 2.4584\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 324us/sample - loss: 0.9972 - mean_squared_error: 0.9972 - val_loss: 2.4462 - val_mean_squared_error: 2.4462\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 1.0616 - mean_squared_error: 1.0616 - val_loss: 3.0488 - val_mean_squared_error: 3.0488\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.1434 - mean_squared_error: 1.1434 - val_loss: 2.8065 - val_mean_squared_error: 2.8065\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.0747 - mean_squared_error: 1.0747 - val_loss: 2.8111 - val_mean_squared_error: 2.8111\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 0.9592 - mean_squared_error: 0.9592 - val_loss: 2.4511 - val_mean_squared_error: 2.4511\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 0.9603 - mean_squared_error: 0.9603 - val_loss: 3.1299 - val_mean_squared_error: 3.1299\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.0444 - mean_squared_error: 1.0444 - val_loss: 2.6308 - val_mean_squared_error: 2.6308\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 0.8967 - mean_squared_error: 0.8967 - val_loss: 2.3878 - val_mean_squared_error: 2.3878\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 0.9619 - mean_squared_error: 0.9619 - val_loss: 2.7498 - val_mean_squared_error: 2.7498\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 0.8410 - mean_squared_error: 0.8410 - val_loss: 2.4158 - val_mean_squared_error: 2.4158\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 0.9353 - mean_squared_error: 0.9353 - val_loss: 2.5129 - val_mean_squared_error: 2.5129\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 0.8498 - mean_squared_error: 0.8498 - val_loss: 2.5320 - val_mean_squared_error: 2.5320\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 0.8835 - mean_squared_error: 0.8835 - val_loss: 2.5566 - val_mean_squared_error: 2.5566\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 0.8192 - mean_squared_error: 0.8192 - val_loss: 2.5539 - val_mean_squared_error: 2.5539\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 0.9380 - mean_squared_error: 0.9380 - val_loss: 2.6786 - val_mean_squared_error: 2.6786\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 0.9260 - mean_squared_error: 0.9260 - val_loss: 2.9203 - val_mean_squared_error: 2.9203\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 0.8660 - mean_squared_error: 0.8660 - val_loss: 2.5545 - val_mean_squared_error: 2.5545\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 0.8759 - mean_squared_error: 0.8759 - val_loss: 2.5923 - val_mean_squared_error: 2.5923\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 0.9031 - mean_squared_error: 0.9031 - val_loss: 2.8666 - val_mean_squared_error: 2.8666\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 0.9407 - mean_squared_error: 0.9407 - val_loss: 2.9140 - val_mean_squared_error: 2.9140\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 2.7811 - val_mean_squared_error: 2.7811\n",
            "==================================================\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 746us/sample - loss: 1531.7508 - mean_squared_error: 1531.7510 - val_loss: 1833.5960 - val_mean_squared_error: 1833.5958\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 227.3205 - mean_squared_error: 227.3205 - val_loss: 636.7475 - val_mean_squared_error: 636.7476\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 34.3137 - mean_squared_error: 34.3137 - val_loss: 129.6406 - val_mean_squared_error: 129.6406\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 20.7294 - mean_squared_error: 20.7294 - val_loss: 36.6403 - val_mean_squared_error: 36.6403\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 17.7408 - mean_squared_error: 17.7408 - val_loss: 31.2636 - val_mean_squared_error: 31.2636\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 15.8775 - mean_squared_error: 15.8775 - val_loss: 17.1555 - val_mean_squared_error: 17.1555\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 15.7749 - mean_squared_error: 15.7749 - val_loss: 12.8060 - val_mean_squared_error: 12.8060\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 13.3198 - mean_squared_error: 13.3198 - val_loss: 13.6501 - val_mean_squared_error: 13.6501\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 14.8992 - mean_squared_error: 14.8992 - val_loss: 13.7562 - val_mean_squared_error: 13.7562\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 13.5944 - mean_squared_error: 13.5944 - val_loss: 11.2093 - val_mean_squared_error: 11.2093\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 10.8711 - mean_squared_error: 10.8711 - val_loss: 10.1716 - val_mean_squared_error: 10.1716\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 10.9808 - mean_squared_error: 10.9808 - val_loss: 12.4758 - val_mean_squared_error: 12.4758\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 10.7591 - mean_squared_error: 10.7591 - val_loss: 11.0797 - val_mean_squared_error: 11.0797\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 9.9483 - mean_squared_error: 9.9483 - val_loss: 14.1063 - val_mean_squared_error: 14.1063\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 9.0034 - mean_squared_error: 9.0034 - val_loss: 12.7847 - val_mean_squared_error: 12.7847\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 8.5551 - mean_squared_error: 8.5551 - val_loss: 9.5947 - val_mean_squared_error: 9.5947\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 8.8986 - mean_squared_error: 8.8986 - val_loss: 8.7610 - val_mean_squared_error: 8.7610\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 7.7325 - mean_squared_error: 7.7325 - val_loss: 7.1777 - val_mean_squared_error: 7.1777\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 7.1611 - mean_squared_error: 7.1611 - val_loss: 7.4120 - val_mean_squared_error: 7.4120\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 6.9081 - mean_squared_error: 6.9081 - val_loss: 6.9830 - val_mean_squared_error: 6.9830\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 7.3473 - mean_squared_error: 7.3473 - val_loss: 9.4985 - val_mean_squared_error: 9.4985\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 6.2172 - mean_squared_error: 6.2172 - val_loss: 8.0752 - val_mean_squared_error: 8.0752\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 6.1281 - mean_squared_error: 6.1281 - val_loss: 7.2919 - val_mean_squared_error: 7.2919\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 5.3750 - mean_squared_error: 5.3750 - val_loss: 6.7647 - val_mean_squared_error: 6.7647\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 5.6228 - mean_squared_error: 5.6228 - val_loss: 5.7332 - val_mean_squared_error: 5.7332\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 5.1281 - mean_squared_error: 5.1281 - val_loss: 5.5741 - val_mean_squared_error: 5.5741\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 4.9573 - mean_squared_error: 4.9573 - val_loss: 6.4072 - val_mean_squared_error: 6.4072\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 5.0629 - mean_squared_error: 5.0629 - val_loss: 5.0215 - val_mean_squared_error: 5.0215\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 4.8806 - mean_squared_error: 4.8806 - val_loss: 5.6619 - val_mean_squared_error: 5.6619\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 4.1556 - mean_squared_error: 4.1556 - val_loss: 6.4258 - val_mean_squared_error: 6.4258\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 4.1947 - mean_squared_error: 4.1947 - val_loss: 6.2721 - val_mean_squared_error: 6.2721\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 3.9906 - mean_squared_error: 3.9906 - val_loss: 5.6023 - val_mean_squared_error: 5.6023\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 4.0203 - mean_squared_error: 4.0203 - val_loss: 4.9937 - val_mean_squared_error: 4.9937\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 3.6927 - mean_squared_error: 3.6927 - val_loss: 4.2784 - val_mean_squared_error: 4.2784\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 3.6724 - mean_squared_error: 3.6724 - val_loss: 4.5915 - val_mean_squared_error: 4.5915\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 4.0681 - mean_squared_error: 4.0681 - val_loss: 3.9745 - val_mean_squared_error: 3.9745\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 3.7794 - mean_squared_error: 3.7794 - val_loss: 4.0804 - val_mean_squared_error: 4.0804\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 3.7756 - mean_squared_error: 3.7756 - val_loss: 4.9983 - val_mean_squared_error: 4.9983\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 3.6434 - mean_squared_error: 3.6434 - val_loss: 4.1991 - val_mean_squared_error: 4.1991\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 3.3269 - mean_squared_error: 3.3269 - val_loss: 5.1763 - val_mean_squared_error: 5.1763\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 3.5580 - mean_squared_error: 3.5580 - val_loss: 4.1809 - val_mean_squared_error: 4.1809\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 3.6196 - mean_squared_error: 3.6196 - val_loss: 4.2060 - val_mean_squared_error: 4.2060\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 3.5793 - mean_squared_error: 3.5793 - val_loss: 4.2517 - val_mean_squared_error: 4.2517\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 3.2795 - mean_squared_error: 3.2795 - val_loss: 3.8887 - val_mean_squared_error: 3.8887\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.9832 - mean_squared_error: 2.9832 - val_loss: 4.0836 - val_mean_squared_error: 4.0836\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 3.1435 - mean_squared_error: 3.1435 - val_loss: 3.5348 - val_mean_squared_error: 3.5348\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 2.8675 - mean_squared_error: 2.8675 - val_loss: 3.4064 - val_mean_squared_error: 3.4064\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 2.7854 - mean_squared_error: 2.7854 - val_loss: 4.0605 - val_mean_squared_error: 4.0605\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 2.7528 - mean_squared_error: 2.7528 - val_loss: 4.0674 - val_mean_squared_error: 4.0674\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.6106 - mean_squared_error: 2.6106 - val_loss: 3.2367 - val_mean_squared_error: 3.2367\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.6025 - mean_squared_error: 2.6025 - val_loss: 3.2029 - val_mean_squared_error: 3.2029\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 3.0152 - mean_squared_error: 3.0152 - val_loss: 3.5455 - val_mean_squared_error: 3.5455\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 2.6341 - mean_squared_error: 2.6341 - val_loss: 3.2474 - val_mean_squared_error: 3.2474\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.8424 - mean_squared_error: 2.8424 - val_loss: 4.9503 - val_mean_squared_error: 4.9503\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.6412 - mean_squared_error: 2.6412 - val_loss: 3.3565 - val_mean_squared_error: 3.3565\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.6999 - mean_squared_error: 2.6999 - val_loss: 3.5115 - val_mean_squared_error: 3.5115\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.5216 - mean_squared_error: 2.5216 - val_loss: 3.8212 - val_mean_squared_error: 3.8212\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 2.5677 - mean_squared_error: 2.5677 - val_loss: 3.7734 - val_mean_squared_error: 3.7734\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 2.7973 - mean_squared_error: 2.7973 - val_loss: 3.3221 - val_mean_squared_error: 3.3221\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.5895 - mean_squared_error: 2.5895 - val_loss: 3.4111 - val_mean_squared_error: 3.4111\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.5085 - mean_squared_error: 2.5085 - val_loss: 4.0086 - val_mean_squared_error: 4.0086\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 2.3536 - mean_squared_error: 2.3536 - val_loss: 3.1289 - val_mean_squared_error: 3.1289\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 2.2753 - mean_squared_error: 2.2753 - val_loss: 3.7759 - val_mean_squared_error: 3.7759\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.3875 - mean_squared_error: 2.3875 - val_loss: 3.8786 - val_mean_squared_error: 3.8786\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 2.3129 - mean_squared_error: 2.3129 - val_loss: 4.0096 - val_mean_squared_error: 4.0096\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.3944 - mean_squared_error: 2.3944 - val_loss: 4.1020 - val_mean_squared_error: 4.1020\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.1674 - mean_squared_error: 2.1674 - val_loss: 3.2697 - val_mean_squared_error: 3.2697\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.3325 - mean_squared_error: 2.3325 - val_loss: 3.1854 - val_mean_squared_error: 3.1854\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 2.1679 - mean_squared_error: 2.1679 - val_loss: 3.4384 - val_mean_squared_error: 3.4384\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 2.2473 - mean_squared_error: 2.2473 - val_loss: 3.1735 - val_mean_squared_error: 3.1735\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.2524 - mean_squared_error: 2.2524 - val_loss: 3.6364 - val_mean_squared_error: 3.6364\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 2.0567 - mean_squared_error: 2.0567 - val_loss: 3.1127 - val_mean_squared_error: 3.1127\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 2.1227 - mean_squared_error: 2.1227 - val_loss: 3.1082 - val_mean_squared_error: 3.1082\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 2.0594 - mean_squared_error: 2.0594 - val_loss: 2.9600 - val_mean_squared_error: 2.9600\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.9603 - mean_squared_error: 1.9603 - val_loss: 3.2447 - val_mean_squared_error: 3.2447\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 2.1427 - mean_squared_error: 2.1427 - val_loss: 3.1463 - val_mean_squared_error: 3.1463\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 2.1888 - mean_squared_error: 2.1888 - val_loss: 2.9011 - val_mean_squared_error: 2.9011\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.9067 - mean_squared_error: 1.9067 - val_loss: 2.7594 - val_mean_squared_error: 2.7594\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.7529 - mean_squared_error: 1.7529 - val_loss: 3.0492 - val_mean_squared_error: 3.0492\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.8576 - mean_squared_error: 1.8576 - val_loss: 3.6980 - val_mean_squared_error: 3.6980\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.9385 - mean_squared_error: 1.9385 - val_loss: 3.2157 - val_mean_squared_error: 3.2157\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.9608 - mean_squared_error: 1.9608 - val_loss: 2.9709 - val_mean_squared_error: 2.9709\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.8596 - mean_squared_error: 1.8596 - val_loss: 2.9752 - val_mean_squared_error: 2.9752\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.9284 - mean_squared_error: 1.9284 - val_loss: 2.8694 - val_mean_squared_error: 2.8694\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.8748 - mean_squared_error: 1.8748 - val_loss: 3.0896 - val_mean_squared_error: 3.0896\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.9133 - mean_squared_error: 1.9133 - val_loss: 2.8866 - val_mean_squared_error: 2.8866\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.8594 - mean_squared_error: 1.8594 - val_loss: 3.0264 - val_mean_squared_error: 3.0264\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.7998 - mean_squared_error: 1.7998 - val_loss: 3.6406 - val_mean_squared_error: 3.6406\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.8177 - mean_squared_error: 1.8177 - val_loss: 3.2685 - val_mean_squared_error: 3.2685\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.8832 - mean_squared_error: 1.8832 - val_loss: 3.1390 - val_mean_squared_error: 3.1390\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.8344 - mean_squared_error: 1.8344 - val_loss: 3.2387 - val_mean_squared_error: 3.2387\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.7053 - mean_squared_error: 1.7053 - val_loss: 3.0298 - val_mean_squared_error: 3.0298\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.7629 - mean_squared_error: 1.7629 - val_loss: 2.8983 - val_mean_squared_error: 2.8983\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.7862 - mean_squared_error: 1.7862 - val_loss: 2.8197 - val_mean_squared_error: 2.8197\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.7655 - mean_squared_error: 1.7655 - val_loss: 2.7140 - val_mean_squared_error: 2.7140\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.6807 - mean_squared_error: 1.6807 - val_loss: 2.8759 - val_mean_squared_error: 2.8759\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.7159 - mean_squared_error: 1.7159 - val_loss: 2.9226 - val_mean_squared_error: 2.9226\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.6338 - mean_squared_error: 1.6338 - val_loss: 2.8084 - val_mean_squared_error: 2.8084\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.6550 - mean_squared_error: 1.6550 - val_loss: 3.0734 - val_mean_squared_error: 3.0734\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.7801 - mean_squared_error: 1.7801 - val_loss: 3.1237 - val_mean_squared_error: 3.1237\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.7050 - mean_squared_error: 1.7050 - val_loss: 3.1416 - val_mean_squared_error: 3.1416\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.6976 - mean_squared_error: 1.6976 - val_loss: 2.9897 - val_mean_squared_error: 2.9897\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.6797 - mean_squared_error: 1.6797 - val_loss: 2.8713 - val_mean_squared_error: 2.8713\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.7926 - mean_squared_error: 1.7926 - val_loss: 3.2137 - val_mean_squared_error: 3.2137\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.9228 - mean_squared_error: 1.9228 - val_loss: 3.1132 - val_mean_squared_error: 3.1132\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.7678 - mean_squared_error: 1.7678 - val_loss: 3.0888 - val_mean_squared_error: 3.0888\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.7468 - mean_squared_error: 1.7468 - val_loss: 3.4390 - val_mean_squared_error: 3.4390\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.6636 - mean_squared_error: 1.6636 - val_loss: 2.9194 - val_mean_squared_error: 2.9194\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.5578 - mean_squared_error: 1.5578 - val_loss: 2.8632 - val_mean_squared_error: 2.8632\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.5727 - mean_squared_error: 1.5727 - val_loss: 2.5893 - val_mean_squared_error: 2.5893\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.5924 - mean_squared_error: 1.5924 - val_loss: 3.1592 - val_mean_squared_error: 3.1592\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.5921 - mean_squared_error: 1.5921 - val_loss: 2.7343 - val_mean_squared_error: 2.7343\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.8415 - mean_squared_error: 1.8415 - val_loss: 2.8866 - val_mean_squared_error: 2.8866\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.5994 - mean_squared_error: 1.5994 - val_loss: 3.3479 - val_mean_squared_error: 3.3479\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.5063 - mean_squared_error: 1.5063 - val_loss: 2.7707 - val_mean_squared_error: 2.7707\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.6251 - mean_squared_error: 1.6251 - val_loss: 2.8279 - val_mean_squared_error: 2.8279\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.9298 - mean_squared_error: 1.9298 - val_loss: 3.3635 - val_mean_squared_error: 3.3635\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.6606 - mean_squared_error: 1.6606 - val_loss: 2.7797 - val_mean_squared_error: 2.7797\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.6366 - mean_squared_error: 1.6366 - val_loss: 2.8445 - val_mean_squared_error: 2.8445\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.5016 - mean_squared_error: 1.5016 - val_loss: 2.8955 - val_mean_squared_error: 2.8955\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.4426 - mean_squared_error: 1.4426 - val_loss: 2.6126 - val_mean_squared_error: 2.6126\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.3267 - mean_squared_error: 1.3267 - val_loss: 2.9418 - val_mean_squared_error: 2.9418\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.5202 - mean_squared_error: 1.5202 - val_loss: 2.4002 - val_mean_squared_error: 2.4002\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.5574 - mean_squared_error: 1.5574 - val_loss: 3.2675 - val_mean_squared_error: 3.2675\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.5757 - mean_squared_error: 1.5757 - val_loss: 2.7796 - val_mean_squared_error: 2.7796\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.3995 - mean_squared_error: 1.3995 - val_loss: 2.5042 - val_mean_squared_error: 2.5042\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.2981 - mean_squared_error: 1.2981 - val_loss: 2.5214 - val_mean_squared_error: 2.5214\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.4248 - mean_squared_error: 1.4248 - val_loss: 2.8333 - val_mean_squared_error: 2.8333\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.3885 - mean_squared_error: 1.3885 - val_loss: 2.4288 - val_mean_squared_error: 2.4288\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4475 - mean_squared_error: 1.4475 - val_loss: 2.6834 - val_mean_squared_error: 2.6834\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.4824 - mean_squared_error: 1.4824 - val_loss: 2.9444 - val_mean_squared_error: 2.9444\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4742 - mean_squared_error: 1.4742 - val_loss: 2.6404 - val_mean_squared_error: 2.6404\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.3760 - mean_squared_error: 1.3760 - val_loss: 2.5610 - val_mean_squared_error: 2.5610\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3043 - mean_squared_error: 1.3043 - val_loss: 2.8319 - val_mean_squared_error: 2.8319\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.2822 - mean_squared_error: 1.2822 - val_loss: 2.4961 - val_mean_squared_error: 2.4961\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.4975 - mean_squared_error: 1.4975 - val_loss: 2.9093 - val_mean_squared_error: 2.9093\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.2920 - mean_squared_error: 1.2920 - val_loss: 2.8443 - val_mean_squared_error: 2.8443\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.3635 - mean_squared_error: 1.3635 - val_loss: 2.5012 - val_mean_squared_error: 2.5012\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.2478 - mean_squared_error: 1.2478 - val_loss: 2.8683 - val_mean_squared_error: 2.8683\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.3766 - mean_squared_error: 1.3766 - val_loss: 3.7227 - val_mean_squared_error: 3.7227\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.4704 - mean_squared_error: 1.4704 - val_loss: 2.6212 - val_mean_squared_error: 2.6212\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.3666 - mean_squared_error: 1.3666 - val_loss: 2.7865 - val_mean_squared_error: 2.7865\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.2630 - mean_squared_error: 1.2630 - val_loss: 2.8477 - val_mean_squared_error: 2.8477\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3704 - mean_squared_error: 1.3704 - val_loss: 2.8854 - val_mean_squared_error: 2.8854\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.3192 - mean_squared_error: 1.3192 - val_loss: 2.6678 - val_mean_squared_error: 2.6678\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.2514 - mean_squared_error: 1.2514 - val_loss: 2.6758 - val_mean_squared_error: 2.6758\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.1823 - mean_squared_error: 1.1823 - val_loss: 2.4725 - val_mean_squared_error: 2.4725\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3750 - mean_squared_error: 1.3750 - val_loss: 2.6369 - val_mean_squared_error: 2.6369\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3311 - mean_squared_error: 1.3311 - val_loss: 2.8039 - val_mean_squared_error: 2.8039\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.3235 - mean_squared_error: 1.3235 - val_loss: 2.8059 - val_mean_squared_error: 2.8059\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.2958 - mean_squared_error: 1.2958 - val_loss: 2.3819 - val_mean_squared_error: 2.3819\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.1764 - mean_squared_error: 1.1764 - val_loss: 2.4208 - val_mean_squared_error: 2.4208\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.2341 - mean_squared_error: 1.2341 - val_loss: 2.5228 - val_mean_squared_error: 2.5228\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.2022 - mean_squared_error: 1.2022 - val_loss: 2.3769 - val_mean_squared_error: 2.3769\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.2511 - mean_squared_error: 1.2511 - val_loss: 2.8382 - val_mean_squared_error: 2.8382\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.1425 - mean_squared_error: 1.1425 - val_loss: 2.7583 - val_mean_squared_error: 2.7583\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.2313 - mean_squared_error: 1.2313 - val_loss: 2.7232 - val_mean_squared_error: 2.7232\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.2363 - mean_squared_error: 1.2363 - val_loss: 2.6317 - val_mean_squared_error: 2.6317\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.1710 - mean_squared_error: 1.1710 - val_loss: 2.4829 - val_mean_squared_error: 2.4829\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.2638 - mean_squared_error: 1.2638 - val_loss: 2.7688 - val_mean_squared_error: 2.7688\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4417 - mean_squared_error: 1.4417 - val_loss: 2.5435 - val_mean_squared_error: 2.5435\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.4134 - mean_squared_error: 1.4134 - val_loss: 2.9569 - val_mean_squared_error: 2.9569\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.2296 - mean_squared_error: 1.2296 - val_loss: 2.4083 - val_mean_squared_error: 2.4083\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.1556 - mean_squared_error: 1.1556 - val_loss: 2.6134 - val_mean_squared_error: 2.6134\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.1098 - mean_squared_error: 1.1098 - val_loss: 2.8618 - val_mean_squared_error: 2.8618\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.2037 - mean_squared_error: 1.2037 - val_loss: 2.7587 - val_mean_squared_error: 2.7587\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.1781 - mean_squared_error: 1.1781 - val_loss: 2.4546 - val_mean_squared_error: 2.4546\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.2038 - mean_squared_error: 1.2038 - val_loss: 2.5379 - val_mean_squared_error: 2.5379\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.2559 - mean_squared_error: 1.2559 - val_loss: 2.9326 - val_mean_squared_error: 2.9326\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.1161 - mean_squared_error: 1.1161 - val_loss: 2.3828 - val_mean_squared_error: 2.3828\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.0972 - mean_squared_error: 1.0972 - val_loss: 2.7361 - val_mean_squared_error: 2.7361\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.0726 - mean_squared_error: 1.0726 - val_loss: 2.3746 - val_mean_squared_error: 2.3746\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.1085 - mean_squared_error: 1.1085 - val_loss: 3.0910 - val_mean_squared_error: 3.0910\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.1364 - mean_squared_error: 1.1364 - val_loss: 3.2407 - val_mean_squared_error: 3.2407\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.1556 - mean_squared_error: 1.1556 - val_loss: 2.5414 - val_mean_squared_error: 2.5414\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.0648 - mean_squared_error: 1.0648 - val_loss: 3.0395 - val_mean_squared_error: 3.0395\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.1127 - mean_squared_error: 1.1127 - val_loss: 2.4982 - val_mean_squared_error: 2.4982\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.1607 - mean_squared_error: 1.1607 - val_loss: 2.6727 - val_mean_squared_error: 2.6727\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.1442 - mean_squared_error: 1.1442 - val_loss: 2.5081 - val_mean_squared_error: 2.5081\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.2102 - mean_squared_error: 1.2102 - val_loss: 2.6670 - val_mean_squared_error: 2.6670\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.1277 - mean_squared_error: 1.1277 - val_loss: 2.5181 - val_mean_squared_error: 2.5181\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.1226 - mean_squared_error: 1.1226 - val_loss: 2.5793 - val_mean_squared_error: 2.5793\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.1583 - mean_squared_error: 1.1583 - val_loss: 2.7873 - val_mean_squared_error: 2.7873\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.2088 - mean_squared_error: 1.2088 - val_loss: 2.3078 - val_mean_squared_error: 2.3078\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.1859 - mean_squared_error: 1.1859 - val_loss: 2.5505 - val_mean_squared_error: 2.5505\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.1410 - mean_squared_error: 1.1410 - val_loss: 2.4683 - val_mean_squared_error: 2.4683\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.0798 - mean_squared_error: 1.0798 - val_loss: 2.5065 - val_mean_squared_error: 2.5065\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 2.4983 - val_mean_squared_error: 2.4983\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0418 - mean_squared_error: 1.0418 - val_loss: 2.6166 - val_mean_squared_error: 2.6166\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0597 - mean_squared_error: 1.0597 - val_loss: 2.7839 - val_mean_squared_error: 2.7839\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.0239 - mean_squared_error: 1.0239 - val_loss: 2.3438 - val_mean_squared_error: 2.3438\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.1178 - mean_squared_error: 1.1178 - val_loss: 2.5696 - val_mean_squared_error: 2.5696\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.1536 - mean_squared_error: 1.1536 - val_loss: 2.5550 - val_mean_squared_error: 2.5550\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0307 - mean_squared_error: 1.0307 - val_loss: 2.3767 - val_mean_squared_error: 2.3767\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.0822 - mean_squared_error: 1.0822 - val_loss: 2.4491 - val_mean_squared_error: 2.4491\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.0530 - mean_squared_error: 1.0530 - val_loss: 2.4692 - val_mean_squared_error: 2.4692\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.1326 - mean_squared_error: 1.1326 - val_loss: 2.4393 - val_mean_squared_error: 2.4393\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.1159 - mean_squared_error: 1.1159 - val_loss: 2.5482 - val_mean_squared_error: 2.5482\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.0698 - mean_squared_error: 1.0698 - val_loss: 2.2544 - val_mean_squared_error: 2.2544\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.1206 - mean_squared_error: 1.1206 - val_loss: 2.5271 - val_mean_squared_error: 2.5271\n",
            "==================================================\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1542.6482 - mean_squared_error: 1542.6478 - val_loss: 1595.8480 - val_mean_squared_error: 1595.8479\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 236.2562 - mean_squared_error: 236.2562 - val_loss: 501.9699 - val_mean_squared_error: 501.9699\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 28.7331 - mean_squared_error: 28.7331 - val_loss: 126.5701 - val_mean_squared_error: 126.5701\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 23.9120 - mean_squared_error: 23.9120 - val_loss: 56.7752 - val_mean_squared_error: 56.7752\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 19.2990 - mean_squared_error: 19.2990 - val_loss: 36.4718 - val_mean_squared_error: 36.4718\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 17.0369 - mean_squared_error: 17.0369 - val_loss: 21.4212 - val_mean_squared_error: 21.4212\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 17.2341 - mean_squared_error: 17.2341 - val_loss: 20.9821 - val_mean_squared_error: 20.9821\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 12.9664 - mean_squared_error: 12.9664 - val_loss: 10.8653 - val_mean_squared_error: 10.8653\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 12.7966 - mean_squared_error: 12.7966 - val_loss: 12.7495 - val_mean_squared_error: 12.7495\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 11.9209 - mean_squared_error: 11.9209 - val_loss: 12.5547 - val_mean_squared_error: 12.5547\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 12.8122 - mean_squared_error: 12.8122 - val_loss: 10.7831 - val_mean_squared_error: 10.7831\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 11.3683 - mean_squared_error: 11.3683 - val_loss: 10.2853 - val_mean_squared_error: 10.2853\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 10.7981 - mean_squared_error: 10.7981 - val_loss: 11.0279 - val_mean_squared_error: 11.0279\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 10.1402 - mean_squared_error: 10.1402 - val_loss: 10.2144 - val_mean_squared_error: 10.2144\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 10.6085 - mean_squared_error: 10.6085 - val_loss: 10.5334 - val_mean_squared_error: 10.5334\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 8.7892 - mean_squared_error: 8.7892 - val_loss: 10.7382 - val_mean_squared_error: 10.7382\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 8.4159 - mean_squared_error: 8.4159 - val_loss: 8.2618 - val_mean_squared_error: 8.2618\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 8.7013 - mean_squared_error: 8.7013 - val_loss: 7.6274 - val_mean_squared_error: 7.6274\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 8.1637 - mean_squared_error: 8.1637 - val_loss: 9.6942 - val_mean_squared_error: 9.6942\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 7.6025 - mean_squared_error: 7.6025 - val_loss: 7.0039 - val_mean_squared_error: 7.0039\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 7.0122 - mean_squared_error: 7.0122 - val_loss: 7.0823 - val_mean_squared_error: 7.0823\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 6.6777 - mean_squared_error: 6.6777 - val_loss: 9.0166 - val_mean_squared_error: 9.0166\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 6.0895 - mean_squared_error: 6.0895 - val_loss: 6.3072 - val_mean_squared_error: 6.3072\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 6.9554 - mean_squared_error: 6.9554 - val_loss: 8.5893 - val_mean_squared_error: 8.5893\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 6.1056 - mean_squared_error: 6.1056 - val_loss: 5.8660 - val_mean_squared_error: 5.8660\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 5.8550 - mean_squared_error: 5.8550 - val_loss: 8.9422 - val_mean_squared_error: 8.9422\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 5.1085 - mean_squared_error: 5.1085 - val_loss: 5.8311 - val_mean_squared_error: 5.8311\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 5.2098 - mean_squared_error: 5.2098 - val_loss: 5.9749 - val_mean_squared_error: 5.9749\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 5.7929 - mean_squared_error: 5.7929 - val_loss: 7.9519 - val_mean_squared_error: 7.9519\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 5.2112 - mean_squared_error: 5.2112 - val_loss: 5.9666 - val_mean_squared_error: 5.9666\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 4.6705 - mean_squared_error: 4.6705 - val_loss: 5.1367 - val_mean_squared_error: 5.1367\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 4.5308 - mean_squared_error: 4.5308 - val_loss: 4.2603 - val_mean_squared_error: 4.2603\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 4.3667 - mean_squared_error: 4.3667 - val_loss: 4.3887 - val_mean_squared_error: 4.3887\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 4.1120 - mean_squared_error: 4.1120 - val_loss: 4.7155 - val_mean_squared_error: 4.7155\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 4.5517 - mean_squared_error: 4.5517 - val_loss: 4.5787 - val_mean_squared_error: 4.5787\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 3.9382 - mean_squared_error: 3.9382 - val_loss: 4.8112 - val_mean_squared_error: 4.8112\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 4.0561 - mean_squared_error: 4.0561 - val_loss: 4.4159 - val_mean_squared_error: 4.4159\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 3.7561 - mean_squared_error: 3.7561 - val_loss: 4.8827 - val_mean_squared_error: 4.8827\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 3.7039 - mean_squared_error: 3.7039 - val_loss: 8.5294 - val_mean_squared_error: 8.5294\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 3.5960 - mean_squared_error: 3.5960 - val_loss: 4.5265 - val_mean_squared_error: 4.5265\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 3.5867 - mean_squared_error: 3.5867 - val_loss: 3.7064 - val_mean_squared_error: 3.7064\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 3.5989 - mean_squared_error: 3.5989 - val_loss: 3.7031 - val_mean_squared_error: 3.7031\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 3.4821 - mean_squared_error: 3.4821 - val_loss: 4.2741 - val_mean_squared_error: 4.2741\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 3.4676 - mean_squared_error: 3.4676 - val_loss: 4.3028 - val_mean_squared_error: 4.3028\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 3.3237 - mean_squared_error: 3.3237 - val_loss: 3.8111 - val_mean_squared_error: 3.8111\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 3.2385 - mean_squared_error: 3.2385 - val_loss: 3.6701 - val_mean_squared_error: 3.6701\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 3.6148 - mean_squared_error: 3.6148 - val_loss: 3.4275 - val_mean_squared_error: 3.4275\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 3.2436 - mean_squared_error: 3.2436 - val_loss: 3.5922 - val_mean_squared_error: 3.5922\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 3.0842 - mean_squared_error: 3.0842 - val_loss: 3.7136 - val_mean_squared_error: 3.7136\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 2.9751 - mean_squared_error: 2.9751 - val_loss: 3.7579 - val_mean_squared_error: 3.7579\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.7787 - mean_squared_error: 2.7787 - val_loss: 3.5467 - val_mean_squared_error: 3.5467\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.8986 - mean_squared_error: 2.8986 - val_loss: 3.4574 - val_mean_squared_error: 3.4574\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 3.1836 - mean_squared_error: 3.1836 - val_loss: 4.4540 - val_mean_squared_error: 4.4540\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 3.1433 - mean_squared_error: 3.1433 - val_loss: 4.1540 - val_mean_squared_error: 4.1540\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.8627 - mean_squared_error: 2.8627 - val_loss: 3.7064 - val_mean_squared_error: 3.7064\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.9369 - mean_squared_error: 2.9369 - val_loss: 5.1505 - val_mean_squared_error: 5.1505\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.7003 - mean_squared_error: 2.7003 - val_loss: 3.2724 - val_mean_squared_error: 3.2724\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 2.8117 - mean_squared_error: 2.8117 - val_loss: 3.2321 - val_mean_squared_error: 3.2321\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 2.5327 - mean_squared_error: 2.5327 - val_loss: 3.3423 - val_mean_squared_error: 3.3423\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.8317 - mean_squared_error: 2.8317 - val_loss: 4.0122 - val_mean_squared_error: 4.0122\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.7584 - mean_squared_error: 2.7584 - val_loss: 4.1921 - val_mean_squared_error: 4.1921\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.6050 - mean_squared_error: 2.6050 - val_loss: 4.5777 - val_mean_squared_error: 4.5777\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 2.5572 - mean_squared_error: 2.5572 - val_loss: 3.1960 - val_mean_squared_error: 3.1960\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.4946 - mean_squared_error: 2.4946 - val_loss: 3.1531 - val_mean_squared_error: 3.1531\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.3324 - mean_squared_error: 2.3324 - val_loss: 3.0898 - val_mean_squared_error: 3.0898\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.4651 - mean_squared_error: 2.4651 - val_loss: 3.5752 - val_mean_squared_error: 3.5752\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 3.0569 - mean_squared_error: 3.0569 - val_loss: 3.6045 - val_mean_squared_error: 3.6045\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 2.5633 - mean_squared_error: 2.5633 - val_loss: 3.4937 - val_mean_squared_error: 3.4937\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 2.4474 - mean_squared_error: 2.4474 - val_loss: 2.9549 - val_mean_squared_error: 2.9549\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.3934 - mean_squared_error: 2.3934 - val_loss: 3.3352 - val_mean_squared_error: 3.3352\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 2.2869 - mean_squared_error: 2.2869 - val_loss: 3.0342 - val_mean_squared_error: 3.0342\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 2.3549 - mean_squared_error: 2.3549 - val_loss: 3.3616 - val_mean_squared_error: 3.3616\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.2709 - mean_squared_error: 2.2709 - val_loss: 3.1181 - val_mean_squared_error: 3.1181\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 2.1954 - mean_squared_error: 2.1954 - val_loss: 3.1733 - val_mean_squared_error: 3.1733\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.2518 - mean_squared_error: 2.2518 - val_loss: 3.4403 - val_mean_squared_error: 3.4403\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.4013 - mean_squared_error: 2.4013 - val_loss: 3.1547 - val_mean_squared_error: 3.1547\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.2832 - mean_squared_error: 2.2832 - val_loss: 3.4134 - val_mean_squared_error: 3.4134\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.2198 - mean_squared_error: 2.2198 - val_loss: 2.7337 - val_mean_squared_error: 2.7337\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.3358 - mean_squared_error: 2.3358 - val_loss: 3.8921 - val_mean_squared_error: 3.8921\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.2349 - mean_squared_error: 2.2349 - val_loss: 3.1271 - val_mean_squared_error: 3.1271\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.4054 - mean_squared_error: 2.4054 - val_loss: 3.4849 - val_mean_squared_error: 3.4849\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.4378 - mean_squared_error: 2.4378 - val_loss: 2.8600 - val_mean_squared_error: 2.8600\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 2.3998 - mean_squared_error: 2.3998 - val_loss: 2.9210 - val_mean_squared_error: 2.9210\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.2751 - mean_squared_error: 2.2751 - val_loss: 4.2610 - val_mean_squared_error: 4.2610\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.9923 - mean_squared_error: 1.9923 - val_loss: 2.8520 - val_mean_squared_error: 2.8520\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.2022 - mean_squared_error: 2.2022 - val_loss: 3.1824 - val_mean_squared_error: 3.1824\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.1490 - mean_squared_error: 2.1490 - val_loss: 2.8225 - val_mean_squared_error: 2.8225\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 2.0674 - mean_squared_error: 2.0674 - val_loss: 3.0899 - val_mean_squared_error: 3.0899\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.0128 - mean_squared_error: 2.0128 - val_loss: 3.3872 - val_mean_squared_error: 3.3872\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.1897 - mean_squared_error: 2.1897 - val_loss: 2.8824 - val_mean_squared_error: 2.8824\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 2.0123 - mean_squared_error: 2.0123 - val_loss: 2.6829 - val_mean_squared_error: 2.6829\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.8738 - mean_squared_error: 1.8738 - val_loss: 2.8663 - val_mean_squared_error: 2.8663\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.0037 - mean_squared_error: 2.0037 - val_loss: 3.0091 - val_mean_squared_error: 3.0091\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.9785 - mean_squared_error: 1.9785 - val_loss: 2.8517 - val_mean_squared_error: 2.8517\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.9409 - mean_squared_error: 1.9409 - val_loss: 2.8816 - val_mean_squared_error: 2.8816\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.9561 - mean_squared_error: 1.9561 - val_loss: 2.8009 - val_mean_squared_error: 2.8009\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.9203 - mean_squared_error: 1.9203 - val_loss: 3.4112 - val_mean_squared_error: 3.4112\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.9829 - mean_squared_error: 1.9829 - val_loss: 2.8969 - val_mean_squared_error: 2.8969\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.7671 - mean_squared_error: 1.7671 - val_loss: 2.6110 - val_mean_squared_error: 2.6110\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.9052 - mean_squared_error: 1.9052 - val_loss: 2.5860 - val_mean_squared_error: 2.5860\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.8579 - mean_squared_error: 1.8579 - val_loss: 2.7161 - val_mean_squared_error: 2.7161\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 2.2779 - mean_squared_error: 2.2779 - val_loss: 3.6312 - val_mean_squared_error: 3.6312\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.8899 - mean_squared_error: 1.8899 - val_loss: 3.0711 - val_mean_squared_error: 3.0711\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.7707 - mean_squared_error: 1.7707 - val_loss: 2.7938 - val_mean_squared_error: 2.7938\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 2.0288 - mean_squared_error: 2.0288 - val_loss: 2.8966 - val_mean_squared_error: 2.8966\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.7112 - mean_squared_error: 1.7112 - val_loss: 2.7409 - val_mean_squared_error: 2.7409\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 2.0220 - mean_squared_error: 2.0220 - val_loss: 2.4151 - val_mean_squared_error: 2.4151\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.7363 - mean_squared_error: 1.7363 - val_loss: 2.5251 - val_mean_squared_error: 2.5251\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.7872 - mean_squared_error: 1.7872 - val_loss: 2.6397 - val_mean_squared_error: 2.6397\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.8206 - mean_squared_error: 1.8206 - val_loss: 2.9949 - val_mean_squared_error: 2.9949\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.7679 - mean_squared_error: 1.7679 - val_loss: 2.5709 - val_mean_squared_error: 2.5709\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.6141 - mean_squared_error: 1.6141 - val_loss: 2.6423 - val_mean_squared_error: 2.6423\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.5621 - mean_squared_error: 1.5621 - val_loss: 2.5731 - val_mean_squared_error: 2.5731\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.6771 - mean_squared_error: 1.6771 - val_loss: 2.5609 - val_mean_squared_error: 2.5609\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.5521 - mean_squared_error: 1.5521 - val_loss: 3.0669 - val_mean_squared_error: 3.0669\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.7863 - mean_squared_error: 1.7863 - val_loss: 2.7035 - val_mean_squared_error: 2.7035\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.6196 - mean_squared_error: 1.6196 - val_loss: 3.1030 - val_mean_squared_error: 3.1030\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.7252 - mean_squared_error: 1.7252 - val_loss: 2.8772 - val_mean_squared_error: 2.8772\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.7637 - mean_squared_error: 1.7637 - val_loss: 3.9922 - val_mean_squared_error: 3.9922\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.7699 - mean_squared_error: 1.7699 - val_loss: 2.8543 - val_mean_squared_error: 2.8543\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.5903 - mean_squared_error: 1.5903 - val_loss: 2.7017 - val_mean_squared_error: 2.7017\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.5002 - mean_squared_error: 1.5002 - val_loss: 2.8467 - val_mean_squared_error: 2.8467\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.6565 - mean_squared_error: 1.6565 - val_loss: 2.5334 - val_mean_squared_error: 2.5334\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.5648 - mean_squared_error: 1.5648 - val_loss: 2.8196 - val_mean_squared_error: 2.8196\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.6101 - mean_squared_error: 1.6101 - val_loss: 2.4455 - val_mean_squared_error: 2.4455\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.4955 - mean_squared_error: 1.4955 - val_loss: 2.6431 - val_mean_squared_error: 2.6431\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.5051 - mean_squared_error: 1.5051 - val_loss: 2.7922 - val_mean_squared_error: 2.7922\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.4664 - mean_squared_error: 1.4664 - val_loss: 2.4943 - val_mean_squared_error: 2.4943\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.4392 - mean_squared_error: 1.4392 - val_loss: 2.6077 - val_mean_squared_error: 2.6077\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.5827 - mean_squared_error: 1.5827 - val_loss: 2.8065 - val_mean_squared_error: 2.8065\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.5864 - mean_squared_error: 1.5864 - val_loss: 2.6773 - val_mean_squared_error: 2.6773\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4677 - mean_squared_error: 1.4677 - val_loss: 2.8950 - val_mean_squared_error: 2.8950\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.4936 - mean_squared_error: 1.4936 - val_loss: 2.8028 - val_mean_squared_error: 2.8028\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.4274 - mean_squared_error: 1.4274 - val_loss: 2.5161 - val_mean_squared_error: 2.5161\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.5562 - mean_squared_error: 1.5562 - val_loss: 2.4193 - val_mean_squared_error: 2.4193\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.6154 - mean_squared_error: 1.6154 - val_loss: 3.3194 - val_mean_squared_error: 3.3194\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.5342 - mean_squared_error: 1.5342 - val_loss: 2.5376 - val_mean_squared_error: 2.5376\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.5774 - mean_squared_error: 1.5774 - val_loss: 2.9324 - val_mean_squared_error: 2.9324\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.5847 - mean_squared_error: 1.5847 - val_loss: 3.2837 - val_mean_squared_error: 3.2837\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.6731 - mean_squared_error: 1.6731 - val_loss: 3.3260 - val_mean_squared_error: 3.3260\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.5581 - mean_squared_error: 1.5581 - val_loss: 2.6942 - val_mean_squared_error: 2.6942\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.4677 - mean_squared_error: 1.4677 - val_loss: 2.5559 - val_mean_squared_error: 2.5559\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.5138 - mean_squared_error: 1.5138 - val_loss: 2.6891 - val_mean_squared_error: 2.6891\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.4484 - mean_squared_error: 1.4484 - val_loss: 2.6222 - val_mean_squared_error: 2.6222\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.4582 - mean_squared_error: 1.4582 - val_loss: 2.9324 - val_mean_squared_error: 2.9324\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.4416 - mean_squared_error: 1.4416 - val_loss: 2.5222 - val_mean_squared_error: 2.5222\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.4356 - mean_squared_error: 1.4356 - val_loss: 2.5261 - val_mean_squared_error: 2.5261\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4348 - mean_squared_error: 1.4348 - val_loss: 2.8564 - val_mean_squared_error: 2.8564\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.5065 - mean_squared_error: 1.5065 - val_loss: 2.6221 - val_mean_squared_error: 2.6221\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3385 - mean_squared_error: 1.3385 - val_loss: 2.9355 - val_mean_squared_error: 2.9355\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.3834 - mean_squared_error: 1.3834 - val_loss: 2.5723 - val_mean_squared_error: 2.5723\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.4293 - mean_squared_error: 1.4293 - val_loss: 2.6209 - val_mean_squared_error: 2.6209\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.5332 - mean_squared_error: 1.5332 - val_loss: 2.9660 - val_mean_squared_error: 2.9660\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.5402 - mean_squared_error: 1.5402 - val_loss: 3.3313 - val_mean_squared_error: 3.3313\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.4326 - mean_squared_error: 1.4326 - val_loss: 2.8200 - val_mean_squared_error: 2.8200\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.4914 - mean_squared_error: 1.4914 - val_loss: 3.4441 - val_mean_squared_error: 3.4441\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.5477 - mean_squared_error: 1.5477 - val_loss: 2.7196 - val_mean_squared_error: 2.7196\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.4350 - mean_squared_error: 1.4350 - val_loss: 2.4621 - val_mean_squared_error: 2.4621\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.2573 - mean_squared_error: 1.2573 - val_loss: 2.6056 - val_mean_squared_error: 2.6056\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.4298 - mean_squared_error: 1.4298 - val_loss: 2.6677 - val_mean_squared_error: 2.6677\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.4482 - mean_squared_error: 1.4482 - val_loss: 2.8753 - val_mean_squared_error: 2.8753\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.4071 - mean_squared_error: 1.4071 - val_loss: 2.5214 - val_mean_squared_error: 2.5214\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3710 - mean_squared_error: 1.3710 - val_loss: 2.6164 - val_mean_squared_error: 2.6164\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.3524 - mean_squared_error: 1.3524 - val_loss: 2.5349 - val_mean_squared_error: 2.5349\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3249 - mean_squared_error: 1.3249 - val_loss: 2.7735 - val_mean_squared_error: 2.7735\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3583 - mean_squared_error: 1.3583 - val_loss: 2.4534 - val_mean_squared_error: 2.4534\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.3470 - mean_squared_error: 1.3470 - val_loss: 2.4335 - val_mean_squared_error: 2.4335\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.1967 - mean_squared_error: 1.1967 - val_loss: 2.6099 - val_mean_squared_error: 2.6099\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.3637 - mean_squared_error: 1.3637 - val_loss: 2.5884 - val_mean_squared_error: 2.5884\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.3245 - mean_squared_error: 1.3245 - val_loss: 2.6564 - val_mean_squared_error: 2.6564\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.4723 - mean_squared_error: 1.4723 - val_loss: 3.0289 - val_mean_squared_error: 3.0289\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.3126 - mean_squared_error: 1.3126 - val_loss: 2.4132 - val_mean_squared_error: 2.4132\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.4132 - mean_squared_error: 1.4132 - val_loss: 2.8929 - val_mean_squared_error: 2.8929\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.2793 - mean_squared_error: 1.2793 - val_loss: 2.4196 - val_mean_squared_error: 2.4196\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.2790 - mean_squared_error: 1.2790 - val_loss: 2.4231 - val_mean_squared_error: 2.4231\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.2619 - mean_squared_error: 1.2619 - val_loss: 2.4584 - val_mean_squared_error: 2.4584\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.3153 - mean_squared_error: 1.3153 - val_loss: 2.8401 - val_mean_squared_error: 2.8401\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3874 - mean_squared_error: 1.3874 - val_loss: 2.8626 - val_mean_squared_error: 2.8626\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.3254 - mean_squared_error: 1.3254 - val_loss: 2.5281 - val_mean_squared_error: 2.5281\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.2882 - mean_squared_error: 1.2882 - val_loss: 2.5168 - val_mean_squared_error: 2.5168\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.2418 - mean_squared_error: 1.2418 - val_loss: 2.4297 - val_mean_squared_error: 2.4297\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.2042 - mean_squared_error: 1.2042 - val_loss: 2.3443 - val_mean_squared_error: 2.3443\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.1960 - mean_squared_error: 1.1960 - val_loss: 2.7529 - val_mean_squared_error: 2.7529\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.2984 - mean_squared_error: 1.2984 - val_loss: 2.7948 - val_mean_squared_error: 2.7948\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.2985 - mean_squared_error: 1.2985 - val_loss: 2.4300 - val_mean_squared_error: 2.4300\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3228 - mean_squared_error: 1.3228 - val_loss: 2.4404 - val_mean_squared_error: 2.4404\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.2366 - mean_squared_error: 1.2366 - val_loss: 3.1691 - val_mean_squared_error: 3.1691\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.2210 - mean_squared_error: 1.2210 - val_loss: 2.6451 - val_mean_squared_error: 2.6451\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.2469 - mean_squared_error: 1.2469 - val_loss: 2.7089 - val_mean_squared_error: 2.7089\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.2105 - mean_squared_error: 1.2105 - val_loss: 2.7190 - val_mean_squared_error: 2.7190\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.2076 - mean_squared_error: 1.2076 - val_loss: 2.5527 - val_mean_squared_error: 2.5527\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.1963 - mean_squared_error: 1.1963 - val_loss: 2.4928 - val_mean_squared_error: 2.4928\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.1872 - mean_squared_error: 1.1872 - val_loss: 2.2246 - val_mean_squared_error: 2.2246\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.2806 - mean_squared_error: 1.2806 - val_loss: 2.4246 - val_mean_squared_error: 2.4246\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.1299 - mean_squared_error: 1.1299 - val_loss: 2.6571 - val_mean_squared_error: 2.6571\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.1948 - mean_squared_error: 1.1948 - val_loss: 2.6697 - val_mean_squared_error: 2.6697\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.1968 - mean_squared_error: 1.1968 - val_loss: 2.5676 - val_mean_squared_error: 2.5676\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.0704 - mean_squared_error: 1.0704 - val_loss: 2.4225 - val_mean_squared_error: 2.4225\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.0745 - mean_squared_error: 1.0745 - val_loss: 2.4638 - val_mean_squared_error: 2.4638\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.1380 - mean_squared_error: 1.1380 - val_loss: 2.6232 - val_mean_squared_error: 2.6232\n",
            "==================================================\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 860us/sample - loss: 1555.1615 - mean_squared_error: 1555.1613 - val_loss: 1614.6392 - val_mean_squared_error: 1614.6392\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 232.7503 - mean_squared_error: 232.7503 - val_loss: 384.8420 - val_mean_squared_error: 384.8420\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 38.0865 - mean_squared_error: 38.0865 - val_loss: 138.1138 - val_mean_squared_error: 138.1138\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 19.6731 - mean_squared_error: 19.6731 - val_loss: 77.7957 - val_mean_squared_error: 77.7957\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 16.3299 - mean_squared_error: 16.3299 - val_loss: 23.3999 - val_mean_squared_error: 23.3999\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 19.2439 - mean_squared_error: 19.2439 - val_loss: 26.4006 - val_mean_squared_error: 26.4006\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 13.4431 - mean_squared_error: 13.4431 - val_loss: 13.7353 - val_mean_squared_error: 13.7353\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 13.5774 - mean_squared_error: 13.5774 - val_loss: 12.3626 - val_mean_squared_error: 12.3626\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 12.1562 - mean_squared_error: 12.1562 - val_loss: 10.3349 - val_mean_squared_error: 10.3349\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 12.4768 - mean_squared_error: 12.4768 - val_loss: 11.3044 - val_mean_squared_error: 11.3044\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 11.6733 - mean_squared_error: 11.6733 - val_loss: 11.3043 - val_mean_squared_error: 11.3043\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 11.3115 - mean_squared_error: 11.3115 - val_loss: 12.4261 - val_mean_squared_error: 12.4261\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 10.0064 - mean_squared_error: 10.0064 - val_loss: 9.6214 - val_mean_squared_error: 9.6214\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 10.7705 - mean_squared_error: 10.7705 - val_loss: 11.3102 - val_mean_squared_error: 11.3102\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 9.4085 - mean_squared_error: 9.4085 - val_loss: 8.7827 - val_mean_squared_error: 8.7827\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 9.3029 - mean_squared_error: 9.3029 - val_loss: 9.3318 - val_mean_squared_error: 9.3318\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 9.1082 - mean_squared_error: 9.1082 - val_loss: 10.2413 - val_mean_squared_error: 10.2413\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 8.2137 - mean_squared_error: 8.2137 - val_loss: 8.8956 - val_mean_squared_error: 8.8956\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 8.4855 - mean_squared_error: 8.4855 - val_loss: 7.4718 - val_mean_squared_error: 7.4718\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 7.9313 - mean_squared_error: 7.9313 - val_loss: 8.3051 - val_mean_squared_error: 8.3051\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 7.0562 - mean_squared_error: 7.0562 - val_loss: 7.9368 - val_mean_squared_error: 7.9368\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 6.7548 - mean_squared_error: 6.7548 - val_loss: 7.7180 - val_mean_squared_error: 7.7180\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 6.5925 - mean_squared_error: 6.5925 - val_loss: 6.1244 - val_mean_squared_error: 6.1244\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 6.2164 - mean_squared_error: 6.2164 - val_loss: 6.8401 - val_mean_squared_error: 6.8401\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 5.8638 - mean_squared_error: 5.8638 - val_loss: 5.8653 - val_mean_squared_error: 5.8653\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 5.4193 - mean_squared_error: 5.4193 - val_loss: 5.1049 - val_mean_squared_error: 5.1049\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 5.3379 - mean_squared_error: 5.3379 - val_loss: 5.5164 - val_mean_squared_error: 5.5164\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 5.5173 - mean_squared_error: 5.5173 - val_loss: 6.6790 - val_mean_squared_error: 6.6790\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 5.2289 - mean_squared_error: 5.2289 - val_loss: 8.7638 - val_mean_squared_error: 8.7638\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 4.8368 - mean_squared_error: 4.8368 - val_loss: 4.5763 - val_mean_squared_error: 4.5763\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 4.4792 - mean_squared_error: 4.4792 - val_loss: 4.4763 - val_mean_squared_error: 4.4763\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 4.9099 - mean_squared_error: 4.9099 - val_loss: 4.5058 - val_mean_squared_error: 4.5058\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 4.1847 - mean_squared_error: 4.1847 - val_loss: 4.5126 - val_mean_squared_error: 4.5126\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 4.7974 - mean_squared_error: 4.7974 - val_loss: 6.2929 - val_mean_squared_error: 6.2929\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 4.7159 - mean_squared_error: 4.7159 - val_loss: 4.9556 - val_mean_squared_error: 4.9556\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 4.1803 - mean_squared_error: 4.1803 - val_loss: 4.6357 - val_mean_squared_error: 4.6357\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 3.7098 - mean_squared_error: 3.7098 - val_loss: 4.0260 - val_mean_squared_error: 4.0260\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 3.7178 - mean_squared_error: 3.7178 - val_loss: 4.5959 - val_mean_squared_error: 4.5959\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 3.8129 - mean_squared_error: 3.8129 - val_loss: 4.3705 - val_mean_squared_error: 4.3705\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 4.0502 - mean_squared_error: 4.0502 - val_loss: 5.5506 - val_mean_squared_error: 5.5506\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 3.6621 - mean_squared_error: 3.6621 - val_loss: 3.6704 - val_mean_squared_error: 3.6704\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 3.5267 - mean_squared_error: 3.5267 - val_loss: 4.2984 - val_mean_squared_error: 4.2984\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 3.3358 - mean_squared_error: 3.3358 - val_loss: 3.5702 - val_mean_squared_error: 3.5702\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 3.8814 - mean_squared_error: 3.8814 - val_loss: 3.3111 - val_mean_squared_error: 3.3111\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 3.5931 - mean_squared_error: 3.5931 - val_loss: 4.3222 - val_mean_squared_error: 4.3222\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 3.2695 - mean_squared_error: 3.2695 - val_loss: 4.9654 - val_mean_squared_error: 4.9654\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 3.2189 - mean_squared_error: 3.2189 - val_loss: 3.9661 - val_mean_squared_error: 3.9661\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 3.0978 - mean_squared_error: 3.0978 - val_loss: 3.7265 - val_mean_squared_error: 3.7265\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 3.0518 - mean_squared_error: 3.0518 - val_loss: 3.2623 - val_mean_squared_error: 3.2623\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.8630 - mean_squared_error: 2.8630 - val_loss: 4.0057 - val_mean_squared_error: 4.0057\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 3.0924 - mean_squared_error: 3.0924 - val_loss: 3.2962 - val_mean_squared_error: 3.2962\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.9974 - mean_squared_error: 2.9974 - val_loss: 3.9773 - val_mean_squared_error: 3.9773\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 3.0943 - mean_squared_error: 3.0943 - val_loss: 3.9900 - val_mean_squared_error: 3.9900\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 3.1164 - mean_squared_error: 3.1164 - val_loss: 3.3836 - val_mean_squared_error: 3.3836\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 3.1654 - mean_squared_error: 3.1654 - val_loss: 3.4465 - val_mean_squared_error: 3.4465\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 3.1882 - mean_squared_error: 3.1882 - val_loss: 3.2689 - val_mean_squared_error: 3.2689\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.8097 - mean_squared_error: 2.8097 - val_loss: 3.0727 - val_mean_squared_error: 3.0727\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 3.0704 - mean_squared_error: 3.0704 - val_loss: 3.5507 - val_mean_squared_error: 3.5507\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 2.6766 - mean_squared_error: 2.6766 - val_loss: 3.3365 - val_mean_squared_error: 3.3365\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.6841 - mean_squared_error: 2.6841 - val_loss: 3.3368 - val_mean_squared_error: 3.3368\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.8678 - mean_squared_error: 2.8678 - val_loss: 3.8722 - val_mean_squared_error: 3.8722\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.6998 - mean_squared_error: 2.6998 - val_loss: 3.0093 - val_mean_squared_error: 3.0093\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 2.4737 - mean_squared_error: 2.4737 - val_loss: 2.8838 - val_mean_squared_error: 2.8838\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 2.5252 - mean_squared_error: 2.5252 - val_loss: 2.8406 - val_mean_squared_error: 2.8406\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.6198 - mean_squared_error: 2.6198 - val_loss: 3.1922 - val_mean_squared_error: 3.1922\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.7698 - mean_squared_error: 2.7698 - val_loss: 3.3188 - val_mean_squared_error: 3.3188\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.5000 - mean_squared_error: 2.5000 - val_loss: 3.2382 - val_mean_squared_error: 3.2382\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 2.5684 - mean_squared_error: 2.5684 - val_loss: 2.8096 - val_mean_squared_error: 2.8096\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.4858 - mean_squared_error: 2.4858 - val_loss: 2.8358 - val_mean_squared_error: 2.8358\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.5076 - mean_squared_error: 2.5076 - val_loss: 3.0881 - val_mean_squared_error: 3.0881\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.5047 - mean_squared_error: 2.5047 - val_loss: 2.8844 - val_mean_squared_error: 2.8844\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.4886 - mean_squared_error: 2.4886 - val_loss: 2.7873 - val_mean_squared_error: 2.7873\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.5693 - mean_squared_error: 2.5693 - val_loss: 3.0706 - val_mean_squared_error: 3.0706\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.6606 - mean_squared_error: 2.6606 - val_loss: 3.5300 - val_mean_squared_error: 3.5300\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 2.3147 - mean_squared_error: 2.3147 - val_loss: 2.9865 - val_mean_squared_error: 2.9865\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.3020 - mean_squared_error: 2.3020 - val_loss: 2.9822 - val_mean_squared_error: 2.9822\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.1473 - mean_squared_error: 2.1473 - val_loss: 2.6397 - val_mean_squared_error: 2.6397\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.2817 - mean_squared_error: 2.2817 - val_loss: 3.0052 - val_mean_squared_error: 3.0052\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.1956 - mean_squared_error: 2.1956 - val_loss: 2.9701 - val_mean_squared_error: 2.9701\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 2.1546 - mean_squared_error: 2.1546 - val_loss: 3.0716 - val_mean_squared_error: 3.0716\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.2535 - mean_squared_error: 2.2535 - val_loss: 3.0918 - val_mean_squared_error: 3.0918\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.1399 - mean_squared_error: 2.1399 - val_loss: 3.2695 - val_mean_squared_error: 3.2695\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.0815 - mean_squared_error: 2.0815 - val_loss: 2.9978 - val_mean_squared_error: 2.9978\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.1982 - mean_squared_error: 2.1982 - val_loss: 2.8056 - val_mean_squared_error: 2.8056\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.0524 - mean_squared_error: 2.0524 - val_loss: 2.8245 - val_mean_squared_error: 2.8245\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.3002 - mean_squared_error: 2.3002 - val_loss: 2.5705 - val_mean_squared_error: 2.5705\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.0832 - mean_squared_error: 2.0832 - val_loss: 2.6438 - val_mean_squared_error: 2.6438\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.1236 - mean_squared_error: 2.1236 - val_loss: 3.2392 - val_mean_squared_error: 3.2392\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.0109 - mean_squared_error: 2.0109 - val_loss: 2.5588 - val_mean_squared_error: 2.5588\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.0002 - mean_squared_error: 2.0002 - val_loss: 2.8633 - val_mean_squared_error: 2.8633\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.1372 - mean_squared_error: 2.1372 - val_loss: 3.0272 - val_mean_squared_error: 3.0272\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 2.0061 - mean_squared_error: 2.0061 - val_loss: 2.5486 - val_mean_squared_error: 2.5486\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.8893 - mean_squared_error: 1.8893 - val_loss: 2.7336 - val_mean_squared_error: 2.7336\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 2.0647 - mean_squared_error: 2.0647 - val_loss: 3.0724 - val_mean_squared_error: 3.0724\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 2.2851 - mean_squared_error: 2.2851 - val_loss: 3.6278 - val_mean_squared_error: 3.6278\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.0462 - mean_squared_error: 2.0462 - val_loss: 2.6992 - val_mean_squared_error: 2.6992\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.0581 - mean_squared_error: 2.0581 - val_loss: 2.4761 - val_mean_squared_error: 2.4761\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.9929 - mean_squared_error: 1.9929 - val_loss: 2.8908 - val_mean_squared_error: 2.8908\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 2.0616 - mean_squared_error: 2.0616 - val_loss: 3.6275 - val_mean_squared_error: 3.6275\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.0475 - mean_squared_error: 2.0475 - val_loss: 3.5683 - val_mean_squared_error: 3.5683\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.0730 - mean_squared_error: 2.0730 - val_loss: 2.9986 - val_mean_squared_error: 2.9986\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.9209 - mean_squared_error: 1.9209 - val_loss: 2.4947 - val_mean_squared_error: 2.4947\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.9612 - mean_squared_error: 1.9612 - val_loss: 2.4391 - val_mean_squared_error: 2.4391\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.8609 - mean_squared_error: 1.8609 - val_loss: 2.8457 - val_mean_squared_error: 2.8457\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.9416 - mean_squared_error: 1.9416 - val_loss: 2.7366 - val_mean_squared_error: 2.7366\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.8992 - mean_squared_error: 1.8992 - val_loss: 2.9952 - val_mean_squared_error: 2.9952\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.9747 - mean_squared_error: 1.9747 - val_loss: 2.9776 - val_mean_squared_error: 2.9776\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.9786 - mean_squared_error: 1.9786 - val_loss: 3.2061 - val_mean_squared_error: 3.2061\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.8566 - mean_squared_error: 1.8566 - val_loss: 2.9172 - val_mean_squared_error: 2.9172\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.6894 - mean_squared_error: 1.6894 - val_loss: 2.7207 - val_mean_squared_error: 2.7207\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.8249 - mean_squared_error: 1.8249 - val_loss: 2.6695 - val_mean_squared_error: 2.6695\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.8207 - mean_squared_error: 1.8207 - val_loss: 2.6100 - val_mean_squared_error: 2.6100\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.8116 - mean_squared_error: 1.8116 - val_loss: 2.5400 - val_mean_squared_error: 2.5400\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.7772 - mean_squared_error: 1.7772 - val_loss: 2.6740 - val_mean_squared_error: 2.6740\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.7616 - mean_squared_error: 1.7616 - val_loss: 2.4197 - val_mean_squared_error: 2.4197\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.7229 - mean_squared_error: 1.7229 - val_loss: 2.6452 - val_mean_squared_error: 2.6452\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.6778 - mean_squared_error: 1.6778 - val_loss: 2.8509 - val_mean_squared_error: 2.8509\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.6609 - mean_squared_error: 1.6609 - val_loss: 2.4339 - val_mean_squared_error: 2.4339\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.7646 - mean_squared_error: 1.7646 - val_loss: 2.7914 - val_mean_squared_error: 2.7914\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.7160 - mean_squared_error: 1.7160 - val_loss: 2.3871 - val_mean_squared_error: 2.3871\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.7223 - mean_squared_error: 1.7223 - val_loss: 2.5659 - val_mean_squared_error: 2.5659\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.7179 - mean_squared_error: 1.7179 - val_loss: 2.4932 - val_mean_squared_error: 2.4932\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.6088 - mean_squared_error: 1.6088 - val_loss: 2.2575 - val_mean_squared_error: 2.2575\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.7150 - mean_squared_error: 1.7150 - val_loss: 2.4668 - val_mean_squared_error: 2.4668\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.8164 - mean_squared_error: 1.8164 - val_loss: 2.7185 - val_mean_squared_error: 2.7185\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.6799 - mean_squared_error: 1.6799 - val_loss: 2.5581 - val_mean_squared_error: 2.5581\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.8075 - mean_squared_error: 1.8075 - val_loss: 3.3600 - val_mean_squared_error: 3.3600\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.6065 - mean_squared_error: 1.6065 - val_loss: 2.6061 - val_mean_squared_error: 2.6061\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.6692 - mean_squared_error: 1.6692 - val_loss: 2.9635 - val_mean_squared_error: 2.9635\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.6234 - mean_squared_error: 1.6234 - val_loss: 2.6387 - val_mean_squared_error: 2.6387\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.7801 - mean_squared_error: 1.7801 - val_loss: 2.6070 - val_mean_squared_error: 2.6070\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.5179 - mean_squared_error: 1.5179 - val_loss: 2.7603 - val_mean_squared_error: 2.7603\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.8949 - mean_squared_error: 1.8949 - val_loss: 2.7629 - val_mean_squared_error: 2.7629\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.6578 - mean_squared_error: 1.6578 - val_loss: 2.6059 - val_mean_squared_error: 2.6059\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.6863 - mean_squared_error: 1.6863 - val_loss: 2.5510 - val_mean_squared_error: 2.5510\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.6579 - mean_squared_error: 1.6579 - val_loss: 2.4628 - val_mean_squared_error: 2.4628\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.6160 - mean_squared_error: 1.6160 - val_loss: 2.4128 - val_mean_squared_error: 2.4128\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.7996 - mean_squared_error: 1.7996 - val_loss: 2.5953 - val_mean_squared_error: 2.5953\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.6917 - mean_squared_error: 1.6917 - val_loss: 2.5613 - val_mean_squared_error: 2.5613\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.5638 - mean_squared_error: 1.5638 - val_loss: 2.6576 - val_mean_squared_error: 2.6576\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.4618 - mean_squared_error: 1.4618 - val_loss: 2.5431 - val_mean_squared_error: 2.5431\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.6200 - mean_squared_error: 1.6200 - val_loss: 2.4235 - val_mean_squared_error: 2.4235\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.6584 - mean_squared_error: 1.6584 - val_loss: 2.2994 - val_mean_squared_error: 2.2994\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.5951 - mean_squared_error: 1.5951 - val_loss: 3.1898 - val_mean_squared_error: 3.1898\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.6432 - mean_squared_error: 1.6432 - val_loss: 2.9670 - val_mean_squared_error: 2.9670\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.5887 - mean_squared_error: 1.5887 - val_loss: 2.4466 - val_mean_squared_error: 2.4466\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.4501 - mean_squared_error: 1.4501 - val_loss: 2.5661 - val_mean_squared_error: 2.5661\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.4705 - mean_squared_error: 1.4705 - val_loss: 2.6547 - val_mean_squared_error: 2.6547\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.4628 - mean_squared_error: 1.4628 - val_loss: 2.4906 - val_mean_squared_error: 2.4906\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.4770 - mean_squared_error: 1.4770 - val_loss: 2.4641 - val_mean_squared_error: 2.4641\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.3521 - mean_squared_error: 1.3521 - val_loss: 2.6487 - val_mean_squared_error: 2.6487\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.5236 - mean_squared_error: 1.5236 - val_loss: 2.5465 - val_mean_squared_error: 2.5465\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4563 - mean_squared_error: 1.4563 - val_loss: 2.2371 - val_mean_squared_error: 2.2371\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4823 - mean_squared_error: 1.4823 - val_loss: 2.2540 - val_mean_squared_error: 2.2540\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.4110 - mean_squared_error: 1.4110 - val_loss: 2.3638 - val_mean_squared_error: 2.3638\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.4077 - mean_squared_error: 1.4077 - val_loss: 2.5580 - val_mean_squared_error: 2.5580\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.4708 - mean_squared_error: 1.4708 - val_loss: 2.8186 - val_mean_squared_error: 2.8186\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.4262 - mean_squared_error: 1.4262 - val_loss: 2.1208 - val_mean_squared_error: 2.1208\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.4133 - mean_squared_error: 1.4133 - val_loss: 2.1902 - val_mean_squared_error: 2.1902\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.5409 - mean_squared_error: 1.5409 - val_loss: 2.4034 - val_mean_squared_error: 2.4034\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.4398 - mean_squared_error: 1.4398 - val_loss: 2.5052 - val_mean_squared_error: 2.5052\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.4429 - mean_squared_error: 1.4429 - val_loss: 2.4107 - val_mean_squared_error: 2.4107\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.4836 - mean_squared_error: 1.4836 - val_loss: 2.5164 - val_mean_squared_error: 2.5164\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.4212 - mean_squared_error: 1.4212 - val_loss: 2.1584 - val_mean_squared_error: 2.1584\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.3700 - mean_squared_error: 1.3700 - val_loss: 2.4373 - val_mean_squared_error: 2.4373\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3839 - mean_squared_error: 1.3839 - val_loss: 2.3354 - val_mean_squared_error: 2.3354\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.3228 - mean_squared_error: 1.3228 - val_loss: 2.4240 - val_mean_squared_error: 2.4240\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.3596 - mean_squared_error: 1.3596 - val_loss: 2.3576 - val_mean_squared_error: 2.3576\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3455 - mean_squared_error: 1.3455 - val_loss: 2.2457 - val_mean_squared_error: 2.2457\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.3061 - mean_squared_error: 1.3061 - val_loss: 2.5040 - val_mean_squared_error: 2.5040\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.3285 - mean_squared_error: 1.3285 - val_loss: 2.4703 - val_mean_squared_error: 2.4703\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3307 - mean_squared_error: 1.3307 - val_loss: 2.2785 - val_mean_squared_error: 2.2785\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.3068 - mean_squared_error: 1.3068 - val_loss: 2.2008 - val_mean_squared_error: 2.2008\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.4499 - mean_squared_error: 1.4499 - val_loss: 2.4906 - val_mean_squared_error: 2.4906\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.3224 - mean_squared_error: 1.3224 - val_loss: 2.3323 - val_mean_squared_error: 2.3323\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.3432 - mean_squared_error: 1.3432 - val_loss: 2.2810 - val_mean_squared_error: 2.2810\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.3509 - mean_squared_error: 1.3509 - val_loss: 2.5700 - val_mean_squared_error: 2.5700\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.3682 - mean_squared_error: 1.3682 - val_loss: 2.4810 - val_mean_squared_error: 2.4810\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.3692 - mean_squared_error: 1.3692 - val_loss: 2.2864 - val_mean_squared_error: 2.2864\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.2897 - mean_squared_error: 1.2897 - val_loss: 2.2055 - val_mean_squared_error: 2.2055\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.3122 - mean_squared_error: 1.3122 - val_loss: 2.2126 - val_mean_squared_error: 2.2126\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.3404 - mean_squared_error: 1.3404 - val_loss: 2.5120 - val_mean_squared_error: 2.5120\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.3254 - mean_squared_error: 1.3254 - val_loss: 2.4422 - val_mean_squared_error: 2.4422\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.3046 - mean_squared_error: 1.3046 - val_loss: 2.4611 - val_mean_squared_error: 2.4611\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.3621 - mean_squared_error: 1.3621 - val_loss: 2.5299 - val_mean_squared_error: 2.5299\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.4233 - mean_squared_error: 1.4233 - val_loss: 2.3485 - val_mean_squared_error: 2.3485\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.4170 - mean_squared_error: 1.4170 - val_loss: 2.7058 - val_mean_squared_error: 2.7058\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.3077 - mean_squared_error: 1.3077 - val_loss: 2.5342 - val_mean_squared_error: 2.5342\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.3472 - mean_squared_error: 1.3472 - val_loss: 2.6681 - val_mean_squared_error: 2.6681\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.2475 - mean_squared_error: 1.2475 - val_loss: 2.5054 - val_mean_squared_error: 2.5054\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.3304 - mean_squared_error: 1.3304 - val_loss: 2.4046 - val_mean_squared_error: 2.4046\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4560 - mean_squared_error: 1.4560 - val_loss: 2.1383 - val_mean_squared_error: 2.1383\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.2013 - mean_squared_error: 1.2013 - val_loss: 2.2316 - val_mean_squared_error: 2.2316\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.2825 - mean_squared_error: 1.2825 - val_loss: 2.2018 - val_mean_squared_error: 2.2018\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.3109 - mean_squared_error: 1.3109 - val_loss: 2.9414 - val_mean_squared_error: 2.9414\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.2489 - mean_squared_error: 1.2489 - val_loss: 2.1163 - val_mean_squared_error: 2.1163\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.2009 - mean_squared_error: 1.2009 - val_loss: 2.5886 - val_mean_squared_error: 2.5886\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.2050 - mean_squared_error: 1.2050 - val_loss: 2.3470 - val_mean_squared_error: 2.3470\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.3356 - mean_squared_error: 1.3356 - val_loss: 2.4519 - val_mean_squared_error: 2.4519\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.4238 - mean_squared_error: 1.4238 - val_loss: 2.8586 - val_mean_squared_error: 2.8586\n",
            "==================================================\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_15 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 967us/sample - loss: 1530.3863 - mean_squared_error: 1530.3864 - val_loss: 1192.0035 - val_mean_squared_error: 1192.0035\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 233.8412 - mean_squared_error: 233.8412 - val_loss: 271.1752 - val_mean_squared_error: 271.1752\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 33.4862 - mean_squared_error: 33.4862 - val_loss: 91.7978 - val_mean_squared_error: 91.7978\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 23.8187 - mean_squared_error: 23.8186 - val_loss: 78.7706 - val_mean_squared_error: 78.7706\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 18.3343 - mean_squared_error: 18.3343 - val_loss: 37.6090 - val_mean_squared_error: 37.6090\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 16.5556 - mean_squared_error: 16.5556 - val_loss: 30.7180 - val_mean_squared_error: 30.7180\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 16.2933 - mean_squared_error: 16.2933 - val_loss: 16.1901 - val_mean_squared_error: 16.1901\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 14.9922 - mean_squared_error: 14.9922 - val_loss: 17.4007 - val_mean_squared_error: 17.4007\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 13.9644 - mean_squared_error: 13.9644 - val_loss: 14.8769 - val_mean_squared_error: 14.8769\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 13.4619 - mean_squared_error: 13.4619 - val_loss: 14.1344 - val_mean_squared_error: 14.1344\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 12.6056 - mean_squared_error: 12.6056 - val_loss: 11.1634 - val_mean_squared_error: 11.1634\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 11.9298 - mean_squared_error: 11.9298 - val_loss: 12.1391 - val_mean_squared_error: 12.1391\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 11.3997 - mean_squared_error: 11.3997 - val_loss: 9.9755 - val_mean_squared_error: 9.9755\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 10.2750 - mean_squared_error: 10.2750 - val_loss: 9.4383 - val_mean_squared_error: 9.4383\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 10.1002 - mean_squared_error: 10.1002 - val_loss: 10.9194 - val_mean_squared_error: 10.9194\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 9.6049 - mean_squared_error: 9.6049 - val_loss: 13.5874 - val_mean_squared_error: 13.5874\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 9.2947 - mean_squared_error: 9.2947 - val_loss: 10.8512 - val_mean_squared_error: 10.8512\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 8.4631 - mean_squared_error: 8.4631 - val_loss: 11.4087 - val_mean_squared_error: 11.4087\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 7.8603 - mean_squared_error: 7.8603 - val_loss: 8.9462 - val_mean_squared_error: 8.9462\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 7.8403 - mean_squared_error: 7.8403 - val_loss: 7.6377 - val_mean_squared_error: 7.6376\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 7.2988 - mean_squared_error: 7.2988 - val_loss: 8.6265 - val_mean_squared_error: 8.6265\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 6.3727 - mean_squared_error: 6.3727 - val_loss: 7.4293 - val_mean_squared_error: 7.4293\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 6.4458 - mean_squared_error: 6.4458 - val_loss: 7.2046 - val_mean_squared_error: 7.2046\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 6.9760 - mean_squared_error: 6.9760 - val_loss: 11.1852 - val_mean_squared_error: 11.1852\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 6.9458 - mean_squared_error: 6.9458 - val_loss: 7.1318 - val_mean_squared_error: 7.1318\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 5.4990 - mean_squared_error: 5.4990 - val_loss: 5.3559 - val_mean_squared_error: 5.3559\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 5.7184 - mean_squared_error: 5.7184 - val_loss: 7.4961 - val_mean_squared_error: 7.4961\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 5.5028 - mean_squared_error: 5.5028 - val_loss: 6.8460 - val_mean_squared_error: 6.8460\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 4.8222 - mean_squared_error: 4.8222 - val_loss: 5.7080 - val_mean_squared_error: 5.7080\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 4.2481 - mean_squared_error: 4.2481 - val_loss: 4.8382 - val_mean_squared_error: 4.8382\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 4.2163 - mean_squared_error: 4.2163 - val_loss: 4.9412 - val_mean_squared_error: 4.9412\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 3.9890 - mean_squared_error: 3.9890 - val_loss: 4.6096 - val_mean_squared_error: 4.6096\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 3.8756 - mean_squared_error: 3.8756 - val_loss: 6.0651 - val_mean_squared_error: 6.0651\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 3.7906 - mean_squared_error: 3.7906 - val_loss: 4.6778 - val_mean_squared_error: 4.6778\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 3.6914 - mean_squared_error: 3.6914 - val_loss: 5.4482 - val_mean_squared_error: 5.4482\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 3.8782 - mean_squared_error: 3.8782 - val_loss: 4.2759 - val_mean_squared_error: 4.2759\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 3.5882 - mean_squared_error: 3.5882 - val_loss: 3.7806 - val_mean_squared_error: 3.7806\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 3.4192 - mean_squared_error: 3.4192 - val_loss: 4.8491 - val_mean_squared_error: 4.8491\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 3.4945 - mean_squared_error: 3.4945 - val_loss: 4.0207 - val_mean_squared_error: 4.0207\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 3.2948 - mean_squared_error: 3.2948 - val_loss: 3.6990 - val_mean_squared_error: 3.6990\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 3.0617 - mean_squared_error: 3.0617 - val_loss: 4.2440 - val_mean_squared_error: 4.2440\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 3.3109 - mean_squared_error: 3.3109 - val_loss: 4.6082 - val_mean_squared_error: 4.6082\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 3.0515 - mean_squared_error: 3.0515 - val_loss: 6.6823 - val_mean_squared_error: 6.6823\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 3.4705 - mean_squared_error: 3.4705 - val_loss: 3.8199 - val_mean_squared_error: 3.8199\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 3.1178 - mean_squared_error: 3.1178 - val_loss: 4.9685 - val_mean_squared_error: 4.9685\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 2.9762 - mean_squared_error: 2.9762 - val_loss: 3.5138 - val_mean_squared_error: 3.5138\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 2.5213 - mean_squared_error: 2.5213 - val_loss: 4.3865 - val_mean_squared_error: 4.3865\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.7756 - mean_squared_error: 2.7756 - val_loss: 5.7904 - val_mean_squared_error: 5.7904\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.9538 - mean_squared_error: 2.9538 - val_loss: 3.6822 - val_mean_squared_error: 3.6822\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.5916 - mean_squared_error: 2.5916 - val_loss: 3.4622 - val_mean_squared_error: 3.4622\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.5230 - mean_squared_error: 2.5230 - val_loss: 3.3589 - val_mean_squared_error: 3.3589\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.5552 - mean_squared_error: 2.5552 - val_loss: 4.0321 - val_mean_squared_error: 4.0321\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.5015 - mean_squared_error: 2.5015 - val_loss: 3.8073 - val_mean_squared_error: 3.8073\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.3907 - mean_squared_error: 2.3907 - val_loss: 3.3058 - val_mean_squared_error: 3.3058\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.5351 - mean_squared_error: 2.5351 - val_loss: 4.3235 - val_mean_squared_error: 4.3235\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 2.2766 - mean_squared_error: 2.2766 - val_loss: 3.2634 - val_mean_squared_error: 3.2634\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.4411 - mean_squared_error: 2.4411 - val_loss: 5.3048 - val_mean_squared_error: 5.3048\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.2359 - mean_squared_error: 2.2359 - val_loss: 3.1392 - val_mean_squared_error: 3.1392\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 2.3757 - mean_squared_error: 2.3757 - val_loss: 4.1535 - val_mean_squared_error: 4.1535\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.2879 - mean_squared_error: 2.2879 - val_loss: 3.0112 - val_mean_squared_error: 3.0112\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.2659 - mean_squared_error: 2.2659 - val_loss: 3.8895 - val_mean_squared_error: 3.8895\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.1151 - mean_squared_error: 2.1151 - val_loss: 3.2098 - val_mean_squared_error: 3.2098\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 2.1166 - mean_squared_error: 2.1166 - val_loss: 3.1872 - val_mean_squared_error: 3.1872\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.1582 - mean_squared_error: 2.1582 - val_loss: 3.1148 - val_mean_squared_error: 3.1148\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.9890 - mean_squared_error: 1.9890 - val_loss: 3.0413 - val_mean_squared_error: 3.0413\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 2.0814 - mean_squared_error: 2.0814 - val_loss: 3.0292 - val_mean_squared_error: 3.0292\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.2400 - mean_squared_error: 2.2400 - val_loss: 3.1020 - val_mean_squared_error: 3.1020\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.2517 - mean_squared_error: 2.2517 - val_loss: 3.3288 - val_mean_squared_error: 3.3288\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 2.0646 - mean_squared_error: 2.0646 - val_loss: 3.0405 - val_mean_squared_error: 3.0405\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 2.0242 - mean_squared_error: 2.0242 - val_loss: 2.9005 - val_mean_squared_error: 2.9005\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.0126 - mean_squared_error: 2.0126 - val_loss: 2.9949 - val_mean_squared_error: 2.9949\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.9279 - mean_squared_error: 1.9279 - val_loss: 3.0159 - val_mean_squared_error: 3.0159\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.0131 - mean_squared_error: 2.0131 - val_loss: 3.2709 - val_mean_squared_error: 3.2709\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.9005 - mean_squared_error: 1.9005 - val_loss: 3.1165 - val_mean_squared_error: 3.1165\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.7238 - mean_squared_error: 1.7238 - val_loss: 3.3480 - val_mean_squared_error: 3.3480\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.7196 - mean_squared_error: 1.7196 - val_loss: 2.8657 - val_mean_squared_error: 2.8657\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.8053 - mean_squared_error: 1.8053 - val_loss: 3.7603 - val_mean_squared_error: 3.7603\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.9301 - mean_squared_error: 1.9301 - val_loss: 3.5042 - val_mean_squared_error: 3.5042\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.7627 - mean_squared_error: 1.7627 - val_loss: 2.9826 - val_mean_squared_error: 2.9826\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.8048 - mean_squared_error: 1.8048 - val_loss: 2.9327 - val_mean_squared_error: 2.9327\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.8986 - mean_squared_error: 1.8986 - val_loss: 2.8656 - val_mean_squared_error: 2.8656\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.7858 - mean_squared_error: 1.7858 - val_loss: 3.2334 - val_mean_squared_error: 3.2334\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.7286 - mean_squared_error: 1.7286 - val_loss: 3.2754 - val_mean_squared_error: 3.2754\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 2.0291 - mean_squared_error: 2.0291 - val_loss: 3.2944 - val_mean_squared_error: 3.2944\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.7899 - mean_squared_error: 1.7899 - val_loss: 2.9347 - val_mean_squared_error: 2.9347\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.6929 - mean_squared_error: 1.6929 - val_loss: 2.8672 - val_mean_squared_error: 2.8672\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.8073 - mean_squared_error: 1.8073 - val_loss: 2.7510 - val_mean_squared_error: 2.7510\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.7853 - mean_squared_error: 1.7853 - val_loss: 2.8849 - val_mean_squared_error: 2.8849\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.7786 - mean_squared_error: 1.7786 - val_loss: 2.6559 - val_mean_squared_error: 2.6559\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.6383 - mean_squared_error: 1.6383 - val_loss: 3.4030 - val_mean_squared_error: 3.4030\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.6851 - mean_squared_error: 1.6851 - val_loss: 3.0381 - val_mean_squared_error: 3.0381\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.6127 - mean_squared_error: 1.6127 - val_loss: 2.9481 - val_mean_squared_error: 2.9481\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.6476 - mean_squared_error: 1.6476 - val_loss: 3.0746 - val_mean_squared_error: 3.0746\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.6807 - mean_squared_error: 1.6807 - val_loss: 3.2870 - val_mean_squared_error: 3.2870\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.7635 - mean_squared_error: 1.7635 - val_loss: 3.1899 - val_mean_squared_error: 3.1899\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.7217 - mean_squared_error: 1.7217 - val_loss: 3.8511 - val_mean_squared_error: 3.8511\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.4882 - mean_squared_error: 1.4882 - val_loss: 2.9503 - val_mean_squared_error: 2.9503\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.6844 - mean_squared_error: 1.6844 - val_loss: 2.6730 - val_mean_squared_error: 2.6730\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.5769 - mean_squared_error: 1.5769 - val_loss: 3.3625 - val_mean_squared_error: 3.3625\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.4988 - mean_squared_error: 1.4988 - val_loss: 2.7071 - val_mean_squared_error: 2.7071\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.4405 - mean_squared_error: 1.4405 - val_loss: 2.8514 - val_mean_squared_error: 2.8514\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.4958 - mean_squared_error: 1.4958 - val_loss: 3.0514 - val_mean_squared_error: 3.0514\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.4764 - mean_squared_error: 1.4764 - val_loss: 2.4802 - val_mean_squared_error: 2.4802\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.3256 - mean_squared_error: 1.3256 - val_loss: 3.0789 - val_mean_squared_error: 3.0789\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.4236 - mean_squared_error: 1.4236 - val_loss: 3.3583 - val_mean_squared_error: 3.3583\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.5382 - mean_squared_error: 1.5382 - val_loss: 3.3323 - val_mean_squared_error: 3.3323\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.4815 - mean_squared_error: 1.4815 - val_loss: 3.5163 - val_mean_squared_error: 3.5163\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.5863 - mean_squared_error: 1.5863 - val_loss: 2.6253 - val_mean_squared_error: 2.6253\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.5250 - mean_squared_error: 1.5250 - val_loss: 2.5491 - val_mean_squared_error: 2.5491\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.3926 - mean_squared_error: 1.3926 - val_loss: 2.8529 - val_mean_squared_error: 2.8529\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.3275 - mean_squared_error: 1.3275 - val_loss: 3.0120 - val_mean_squared_error: 3.0120\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.4029 - mean_squared_error: 1.4029 - val_loss: 2.8029 - val_mean_squared_error: 2.8029\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.5801 - mean_squared_error: 1.5801 - val_loss: 3.9030 - val_mean_squared_error: 3.9030\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.3742 - mean_squared_error: 1.3742 - val_loss: 2.5296 - val_mean_squared_error: 2.5296\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.2243 - mean_squared_error: 1.2243 - val_loss: 2.5056 - val_mean_squared_error: 2.5056\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.4369 - mean_squared_error: 1.4369 - val_loss: 3.0220 - val_mean_squared_error: 3.0220\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.3754 - mean_squared_error: 1.3754 - val_loss: 2.9553 - val_mean_squared_error: 2.9553\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.2976 - mean_squared_error: 1.2976 - val_loss: 2.7946 - val_mean_squared_error: 2.7946\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.3475 - mean_squared_error: 1.3475 - val_loss: 2.7409 - val_mean_squared_error: 2.7409\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.3286 - mean_squared_error: 1.3286 - val_loss: 3.7146 - val_mean_squared_error: 3.7146\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.2822 - mean_squared_error: 1.2822 - val_loss: 2.6421 - val_mean_squared_error: 2.6421\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.2876 - mean_squared_error: 1.2876 - val_loss: 2.8078 - val_mean_squared_error: 2.8078\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.3493 - mean_squared_error: 1.3493 - val_loss: 2.7907 - val_mean_squared_error: 2.7907\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.3392 - mean_squared_error: 1.3392 - val_loss: 2.7318 - val_mean_squared_error: 2.7318\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.2059 - mean_squared_error: 1.2059 - val_loss: 2.7827 - val_mean_squared_error: 2.7827\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.2375 - mean_squared_error: 1.2375 - val_loss: 3.0248 - val_mean_squared_error: 3.0248\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.2977 - mean_squared_error: 1.2977 - val_loss: 2.9588 - val_mean_squared_error: 2.9588\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.1724 - mean_squared_error: 1.1724 - val_loss: 2.8743 - val_mean_squared_error: 2.8743\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.1216 - mean_squared_error: 1.1216 - val_loss: 2.4502 - val_mean_squared_error: 2.4502\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.2713 - mean_squared_error: 1.2713 - val_loss: 2.5107 - val_mean_squared_error: 2.5107\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.2234 - mean_squared_error: 1.2234 - val_loss: 2.6196 - val_mean_squared_error: 2.6196\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.2923 - mean_squared_error: 1.2923 - val_loss: 2.6332 - val_mean_squared_error: 2.6332\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.3206 - mean_squared_error: 1.3206 - val_loss: 2.5577 - val_mean_squared_error: 2.5577\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.2221 - mean_squared_error: 1.2221 - val_loss: 2.7095 - val_mean_squared_error: 2.7095\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.1476 - mean_squared_error: 1.1476 - val_loss: 2.7311 - val_mean_squared_error: 2.7311\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.3532 - mean_squared_error: 1.3532 - val_loss: 4.2770 - val_mean_squared_error: 4.2770\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.3209 - mean_squared_error: 1.3209 - val_loss: 2.7934 - val_mean_squared_error: 2.7934\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.1118 - mean_squared_error: 1.1118 - val_loss: 2.5689 - val_mean_squared_error: 2.5689\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.2408 - mean_squared_error: 1.2408 - val_loss: 2.5306 - val_mean_squared_error: 2.5306\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.1859 - mean_squared_error: 1.1859 - val_loss: 2.3951 - val_mean_squared_error: 2.3951\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.3438 - mean_squared_error: 1.3438 - val_loss: 2.8269 - val_mean_squared_error: 2.8269\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.3306 - mean_squared_error: 1.3306 - val_loss: 2.8763 - val_mean_squared_error: 2.8763\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.3357 - mean_squared_error: 1.3357 - val_loss: 2.9770 - val_mean_squared_error: 2.9770\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.3269 - mean_squared_error: 1.3269 - val_loss: 2.6921 - val_mean_squared_error: 2.6921\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.2716 - mean_squared_error: 1.2716 - val_loss: 2.5899 - val_mean_squared_error: 2.5899\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.1159 - mean_squared_error: 1.1159 - val_loss: 2.5471 - val_mean_squared_error: 2.5471\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.2099 - mean_squared_error: 1.2099 - val_loss: 2.6891 - val_mean_squared_error: 2.6891\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.1239 - mean_squared_error: 1.1239 - val_loss: 2.6009 - val_mean_squared_error: 2.6009\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.1448 - mean_squared_error: 1.1448 - val_loss: 2.5045 - val_mean_squared_error: 2.5045\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.0398 - mean_squared_error: 1.0398 - val_loss: 2.4693 - val_mean_squared_error: 2.4693\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.1381 - mean_squared_error: 1.1381 - val_loss: 2.4873 - val_mean_squared_error: 2.4873\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.1221 - mean_squared_error: 1.1221 - val_loss: 2.5647 - val_mean_squared_error: 2.5647\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 1.0641 - mean_squared_error: 1.0641 - val_loss: 2.5330 - val_mean_squared_error: 2.5330\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 2.4096 - val_mean_squared_error: 2.4096\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.0001 - mean_squared_error: 1.0001 - val_loss: 2.7065 - val_mean_squared_error: 2.7065\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.0538 - mean_squared_error: 1.0538 - val_loss: 2.3824 - val_mean_squared_error: 2.3824\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 0.9784 - mean_squared_error: 0.9784 - val_loss: 2.5915 - val_mean_squared_error: 2.5915\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.0592 - mean_squared_error: 1.0592 - val_loss: 2.7778 - val_mean_squared_error: 2.7778\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 0.9745 - mean_squared_error: 0.9745 - val_loss: 2.4855 - val_mean_squared_error: 2.4855\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.0874 - mean_squared_error: 1.0874 - val_loss: 2.4316 - val_mean_squared_error: 2.4316\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.0615 - mean_squared_error: 1.0615 - val_loss: 2.7687 - val_mean_squared_error: 2.7687\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.0316 - mean_squared_error: 1.0316 - val_loss: 2.5719 - val_mean_squared_error: 2.5719\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.0761 - mean_squared_error: 1.0761 - val_loss: 2.5263 - val_mean_squared_error: 2.5263\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.0740 - mean_squared_error: 1.0740 - val_loss: 2.5780 - val_mean_squared_error: 2.5780\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.1300 - mean_squared_error: 1.1300 - val_loss: 2.4398 - val_mean_squared_error: 2.4398\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.0556 - mean_squared_error: 1.0556 - val_loss: 2.5574 - val_mean_squared_error: 2.5574\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.0847 - mean_squared_error: 1.0847 - val_loss: 2.3961 - val_mean_squared_error: 2.3961\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.0314 - mean_squared_error: 1.0314 - val_loss: 2.4741 - val_mean_squared_error: 2.4741\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.0862 - mean_squared_error: 1.0862 - val_loss: 3.3869 - val_mean_squared_error: 3.3869\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.0548 - mean_squared_error: 1.0548 - val_loss: 2.5455 - val_mean_squared_error: 2.5455\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 0.9916 - mean_squared_error: 0.9916 - val_loss: 2.3334 - val_mean_squared_error: 2.3334\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 0.9849 - mean_squared_error: 0.9849 - val_loss: 2.2450 - val_mean_squared_error: 2.2450\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.0282 - mean_squared_error: 1.0281 - val_loss: 2.5090 - val_mean_squared_error: 2.5089\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 0.9845 - mean_squared_error: 0.9845 - val_loss: 2.7302 - val_mean_squared_error: 2.7302\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 0.9720 - mean_squared_error: 0.9720 - val_loss: 2.4838 - val_mean_squared_error: 2.4838\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.0309 - mean_squared_error: 1.0309 - val_loss: 2.3570 - val_mean_squared_error: 2.3570\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.0474 - mean_squared_error: 1.0474 - val_loss: 2.3638 - val_mean_squared_error: 2.3638\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 0.9576 - mean_squared_error: 0.9576 - val_loss: 2.8438 - val_mean_squared_error: 2.8438\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.0356 - mean_squared_error: 1.0356 - val_loss: 2.6602 - val_mean_squared_error: 2.6602\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.0526 - mean_squared_error: 1.0526 - val_loss: 2.9604 - val_mean_squared_error: 2.9604\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.9639 - mean_squared_error: 0.9639 - val_loss: 2.4999 - val_mean_squared_error: 2.4999\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 0.9980 - mean_squared_error: 0.9980 - val_loss: 2.3608 - val_mean_squared_error: 2.3608\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 0.9644 - mean_squared_error: 0.9644 - val_loss: 2.6158 - val_mean_squared_error: 2.6158\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 0.9973 - mean_squared_error: 0.9973 - val_loss: 2.4804 - val_mean_squared_error: 2.4804\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 0.9271 - mean_squared_error: 0.9271 - val_loss: 2.3509 - val_mean_squared_error: 2.3509\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 0.9275 - mean_squared_error: 0.9275 - val_loss: 2.2991 - val_mean_squared_error: 2.2991\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 0.8182 - mean_squared_error: 0.8182 - val_loss: 2.6219 - val_mean_squared_error: 2.6219\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 2.5358 - val_mean_squared_error: 2.5358\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 0.9446 - mean_squared_error: 0.9446 - val_loss: 2.9698 - val_mean_squared_error: 2.9698\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 2.3721 - val_mean_squared_error: 2.3721\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 0.9638 - mean_squared_error: 0.9638 - val_loss: 3.0823 - val_mean_squared_error: 3.0823\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 0.9611 - mean_squared_error: 0.9611 - val_loss: 2.5520 - val_mean_squared_error: 2.5520\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 0.9400 - mean_squared_error: 0.9400 - val_loss: 2.4093 - val_mean_squared_error: 2.4093\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 0.8643 - mean_squared_error: 0.8643 - val_loss: 2.5583 - val_mean_squared_error: 2.5583\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 0.8739 - mean_squared_error: 0.8739 - val_loss: 2.6267 - val_mean_squared_error: 2.6267\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 0.9894 - mean_squared_error: 0.9894 - val_loss: 2.7365 - val_mean_squared_error: 2.7365\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 0.9633 - mean_squared_error: 0.9633 - val_loss: 2.4468 - val_mean_squared_error: 2.4468\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 0.8501 - mean_squared_error: 0.8501 - val_loss: 2.5219 - val_mean_squared_error: 2.5219\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 0.9506 - mean_squared_error: 0.9506 - val_loss: 2.4134 - val_mean_squared_error: 2.4134\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 0.8153 - mean_squared_error: 0.8153 - val_loss: 2.3163 - val_mean_squared_error: 2.3163\n",
            "==================================================\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 969us/sample - loss: 1533.7654 - mean_squared_error: 1533.7649 - val_loss: 1231.5241 - val_mean_squared_error: 1231.5240\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 240.8387 - mean_squared_error: 240.8387 - val_loss: 331.6874 - val_mean_squared_error: 331.6873\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 35.3428 - mean_squared_error: 35.3428 - val_loss: 170.1576 - val_mean_squared_error: 170.1576\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 21.8876 - mean_squared_error: 21.8876 - val_loss: 78.8138 - val_mean_squared_error: 78.8138\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 18.1811 - mean_squared_error: 18.1811 - val_loss: 32.7347 - val_mean_squared_error: 32.7347\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 15.9610 - mean_squared_error: 15.9610 - val_loss: 18.7201 - val_mean_squared_error: 18.7201\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 15.7953 - mean_squared_error: 15.7952 - val_loss: 11.9901 - val_mean_squared_error: 11.9901\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 440us/sample - loss: 17.2002 - mean_squared_error: 17.2002 - val_loss: 21.8495 - val_mean_squared_error: 21.8494\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 13.8156 - mean_squared_error: 13.8156 - val_loss: 11.2696 - val_mean_squared_error: 11.2696\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 13.9395 - mean_squared_error: 13.9395 - val_loss: 13.9962 - val_mean_squared_error: 13.9962\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 12.1562 - mean_squared_error: 12.1562 - val_loss: 13.0655 - val_mean_squared_error: 13.0655\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 13.1128 - mean_squared_error: 13.1129 - val_loss: 12.0792 - val_mean_squared_error: 12.0792\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 12.1415 - mean_squared_error: 12.1415 - val_loss: 11.4310 - val_mean_squared_error: 11.4310\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 11.5253 - mean_squared_error: 11.5253 - val_loss: 11.7839 - val_mean_squared_error: 11.7839\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 10.7868 - mean_squared_error: 10.7868 - val_loss: 9.4027 - val_mean_squared_error: 9.4027\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 10.4351 - mean_squared_error: 10.4351 - val_loss: 11.3655 - val_mean_squared_error: 11.3655\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 9.7331 - mean_squared_error: 9.7331 - val_loss: 8.9788 - val_mean_squared_error: 8.9788\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 9.1780 - mean_squared_error: 9.1780 - val_loss: 10.9107 - val_mean_squared_error: 10.9107\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 8.5767 - mean_squared_error: 8.5767 - val_loss: 9.2150 - val_mean_squared_error: 9.2150\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 8.7579 - mean_squared_error: 8.7579 - val_loss: 8.4100 - val_mean_squared_error: 8.4100\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 8.1224 - mean_squared_error: 8.1224 - val_loss: 12.0821 - val_mean_squared_error: 12.0821\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 8.1493 - mean_squared_error: 8.1493 - val_loss: 8.8818 - val_mean_squared_error: 8.8818\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 7.4912 - mean_squared_error: 7.4912 - val_loss: 8.7295 - val_mean_squared_error: 8.7295\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 7.3997 - mean_squared_error: 7.3997 - val_loss: 11.1630 - val_mean_squared_error: 11.1630\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 7.0723 - mean_squared_error: 7.0723 - val_loss: 7.3098 - val_mean_squared_error: 7.3098\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 6.1455 - mean_squared_error: 6.1455 - val_loss: 6.4079 - val_mean_squared_error: 6.4079\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 5.9803 - mean_squared_error: 5.9803 - val_loss: 8.9216 - val_mean_squared_error: 8.9216\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 6.0519 - mean_squared_error: 6.0519 - val_loss: 5.4808 - val_mean_squared_error: 5.4808\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 5.2827 - mean_squared_error: 5.2827 - val_loss: 5.4374 - val_mean_squared_error: 5.4374\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 5.0661 - mean_squared_error: 5.0661 - val_loss: 5.7183 - val_mean_squared_error: 5.7183\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 4.8551 - mean_squared_error: 4.8551 - val_loss: 5.6741 - val_mean_squared_error: 5.6741\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 5.1782 - mean_squared_error: 5.1782 - val_loss: 4.7463 - val_mean_squared_error: 4.7463\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 4.1942 - mean_squared_error: 4.1942 - val_loss: 4.7770 - val_mean_squared_error: 4.7770\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 4.3776 - mean_squared_error: 4.3776 - val_loss: 4.9754 - val_mean_squared_error: 4.9754\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 4.4387 - mean_squared_error: 4.4387 - val_loss: 6.2216 - val_mean_squared_error: 6.2216\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 4.5247 - mean_squared_error: 4.5247 - val_loss: 4.2996 - val_mean_squared_error: 4.2996\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 4.0241 - mean_squared_error: 4.0241 - val_loss: 4.7179 - val_mean_squared_error: 4.7179\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.8842 - mean_squared_error: 3.8842 - val_loss: 5.1363 - val_mean_squared_error: 5.1363\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 4.2723 - mean_squared_error: 4.2723 - val_loss: 4.1159 - val_mean_squared_error: 4.1159\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 3.8864 - mean_squared_error: 3.8864 - val_loss: 4.6010 - val_mean_squared_error: 4.6010\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.4972 - mean_squared_error: 3.4972 - val_loss: 4.1067 - val_mean_squared_error: 4.1067\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 3.7327 - mean_squared_error: 3.7327 - val_loss: 4.3555 - val_mean_squared_error: 4.3555\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 3.4078 - mean_squared_error: 3.4078 - val_loss: 3.5400 - val_mean_squared_error: 3.5400\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.2321 - mean_squared_error: 3.2321 - val_loss: 5.0627 - val_mean_squared_error: 5.0627\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 3.1324 - mean_squared_error: 3.1324 - val_loss: 3.6145 - val_mean_squared_error: 3.6145\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.1989 - mean_squared_error: 3.1989 - val_loss: 4.4867 - val_mean_squared_error: 4.4867\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 3.2235 - mean_squared_error: 3.2235 - val_loss: 4.4956 - val_mean_squared_error: 4.4956\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 3.1498 - mean_squared_error: 3.1498 - val_loss: 4.6845 - val_mean_squared_error: 4.6845\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 2.7866 - mean_squared_error: 2.7866 - val_loss: 4.1794 - val_mean_squared_error: 4.1794\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 3.1360 - mean_squared_error: 3.1360 - val_loss: 3.6931 - val_mean_squared_error: 3.6931\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 3.2957 - mean_squared_error: 3.2957 - val_loss: 4.8210 - val_mean_squared_error: 4.8210\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 3.0513 - mean_squared_error: 3.0513 - val_loss: 3.9327 - val_mean_squared_error: 3.9327\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 2.8796 - mean_squared_error: 2.8796 - val_loss: 3.5598 - val_mean_squared_error: 3.5598\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.7493 - mean_squared_error: 2.7493 - val_loss: 3.7861 - val_mean_squared_error: 3.7861\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.6939 - mean_squared_error: 2.6939 - val_loss: 3.9906 - val_mean_squared_error: 3.9906\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.6314 - mean_squared_error: 2.6314 - val_loss: 3.2446 - val_mean_squared_error: 3.2446\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.7011 - mean_squared_error: 2.7011 - val_loss: 3.4583 - val_mean_squared_error: 3.4583\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.6443 - mean_squared_error: 2.6443 - val_loss: 3.5879 - val_mean_squared_error: 3.5879\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.7177 - mean_squared_error: 2.7177 - val_loss: 3.6561 - val_mean_squared_error: 3.6561\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.3944 - mean_squared_error: 2.3944 - val_loss: 3.0356 - val_mean_squared_error: 3.0356\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.5490 - mean_squared_error: 2.5490 - val_loss: 3.5794 - val_mean_squared_error: 3.5794\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 2.5626 - mean_squared_error: 2.5626 - val_loss: 3.7472 - val_mean_squared_error: 3.7472\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.5156 - mean_squared_error: 2.5156 - val_loss: 3.3829 - val_mean_squared_error: 3.3829\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.4220 - mean_squared_error: 2.4220 - val_loss: 3.1498 - val_mean_squared_error: 3.1498\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.3902 - mean_squared_error: 2.3902 - val_loss: 3.1564 - val_mean_squared_error: 3.1564\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.4416 - mean_squared_error: 2.4416 - val_loss: 3.0357 - val_mean_squared_error: 3.0357\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.4318 - mean_squared_error: 2.4318 - val_loss: 2.7664 - val_mean_squared_error: 2.7664\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.3708 - mean_squared_error: 2.3708 - val_loss: 5.3623 - val_mean_squared_error: 5.3623\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.3299 - mean_squared_error: 2.3299 - val_loss: 2.9723 - val_mean_squared_error: 2.9723\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.3042 - mean_squared_error: 2.3042 - val_loss: 4.3745 - val_mean_squared_error: 4.3745\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.3780 - mean_squared_error: 2.3780 - val_loss: 3.5979 - val_mean_squared_error: 3.5979\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.1786 - mean_squared_error: 2.1786 - val_loss: 3.4690 - val_mean_squared_error: 3.4690\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.2214 - mean_squared_error: 2.2214 - val_loss: 3.4880 - val_mean_squared_error: 3.4880\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.1723 - mean_squared_error: 2.1723 - val_loss: 3.8109 - val_mean_squared_error: 3.8109\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.0809 - mean_squared_error: 2.0809 - val_loss: 3.5638 - val_mean_squared_error: 3.5638\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.1140 - mean_squared_error: 2.1140 - val_loss: 3.1817 - val_mean_squared_error: 3.1817\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.0929 - mean_squared_error: 2.0929 - val_loss: 2.9129 - val_mean_squared_error: 2.9129\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.9677 - mean_squared_error: 1.9677 - val_loss: 3.3923 - val_mean_squared_error: 3.3923\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.9022 - mean_squared_error: 1.9022 - val_loss: 2.5762 - val_mean_squared_error: 2.5762\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.0145 - mean_squared_error: 2.0145 - val_loss: 3.4232 - val_mean_squared_error: 3.4232\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.2249 - mean_squared_error: 2.2249 - val_loss: 3.4767 - val_mean_squared_error: 3.4767\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.7874 - mean_squared_error: 1.7874 - val_loss: 3.0888 - val_mean_squared_error: 3.0888\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.8491 - mean_squared_error: 1.8491 - val_loss: 2.5860 - val_mean_squared_error: 2.5860\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.8449 - mean_squared_error: 1.8449 - val_loss: 2.6427 - val_mean_squared_error: 2.6427\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.8124 - mean_squared_error: 1.8124 - val_loss: 3.0429 - val_mean_squared_error: 3.0429\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.9585 - mean_squared_error: 1.9585 - val_loss: 4.2193 - val_mean_squared_error: 4.2193\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.1252 - mean_squared_error: 2.1252 - val_loss: 3.4596 - val_mean_squared_error: 3.4596\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.8305 - mean_squared_error: 1.8305 - val_loss: 2.7827 - val_mean_squared_error: 2.7827\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.8488 - mean_squared_error: 1.8488 - val_loss: 2.7136 - val_mean_squared_error: 2.7136\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.8616 - mean_squared_error: 1.8616 - val_loss: 2.7381 - val_mean_squared_error: 2.7381\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.8210 - mean_squared_error: 1.8210 - val_loss: 3.5060 - val_mean_squared_error: 3.5060\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7829 - mean_squared_error: 1.7829 - val_loss: 3.2441 - val_mean_squared_error: 3.2441\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.9259 - mean_squared_error: 1.9259 - val_loss: 2.8134 - val_mean_squared_error: 2.8134\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.5949 - mean_squared_error: 1.5949 - val_loss: 2.8697 - val_mean_squared_error: 2.8697\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.6464 - mean_squared_error: 1.6464 - val_loss: 2.4076 - val_mean_squared_error: 2.4076\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7840 - mean_squared_error: 1.7840 - val_loss: 2.5619 - val_mean_squared_error: 2.5619\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.8462 - mean_squared_error: 1.8462 - val_loss: 3.9456 - val_mean_squared_error: 3.9456\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.6915 - mean_squared_error: 1.6915 - val_loss: 2.8477 - val_mean_squared_error: 2.8477\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.7678 - mean_squared_error: 1.7678 - val_loss: 2.9085 - val_mean_squared_error: 2.9085\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.7329 - mean_squared_error: 1.7329 - val_loss: 2.8969 - val_mean_squared_error: 2.8969\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.7846 - mean_squared_error: 1.7846 - val_loss: 2.9269 - val_mean_squared_error: 2.9269\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.7763 - mean_squared_error: 1.7763 - val_loss: 3.9303 - val_mean_squared_error: 3.9303\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.5971 - mean_squared_error: 1.5971 - val_loss: 2.5303 - val_mean_squared_error: 2.5303\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.6215 - mean_squared_error: 1.6215 - val_loss: 2.6634 - val_mean_squared_error: 2.6634\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.7688 - mean_squared_error: 1.7688 - val_loss: 2.7367 - val_mean_squared_error: 2.7367\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.5188 - mean_squared_error: 1.5188 - val_loss: 2.8909 - val_mean_squared_error: 2.8909\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.5635 - mean_squared_error: 1.5635 - val_loss: 2.5821 - val_mean_squared_error: 2.5821\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.7405 - mean_squared_error: 1.7405 - val_loss: 2.7237 - val_mean_squared_error: 2.7237\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.6286 - mean_squared_error: 1.6286 - val_loss: 2.5939 - val_mean_squared_error: 2.5939\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.6850 - mean_squared_error: 1.6850 - val_loss: 2.4530 - val_mean_squared_error: 2.4530\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.6471 - mean_squared_error: 1.6471 - val_loss: 2.5292 - val_mean_squared_error: 2.5292\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.4751 - mean_squared_error: 1.4751 - val_loss: 2.4770 - val_mean_squared_error: 2.4770\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.5640 - mean_squared_error: 1.5640 - val_loss: 4.1067 - val_mean_squared_error: 4.1067\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.7030 - mean_squared_error: 1.7030 - val_loss: 2.9384 - val_mean_squared_error: 2.9384\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.5696 - mean_squared_error: 1.5696 - val_loss: 2.8578 - val_mean_squared_error: 2.8578\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.5754 - mean_squared_error: 1.5754 - val_loss: 2.6086 - val_mean_squared_error: 2.6086\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.5776 - mean_squared_error: 1.5776 - val_loss: 3.4482 - val_mean_squared_error: 3.4482\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.5640 - mean_squared_error: 1.5640 - val_loss: 2.4505 - val_mean_squared_error: 2.4505\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.6139 - mean_squared_error: 1.6139 - val_loss: 2.6857 - val_mean_squared_error: 2.6857\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.4881 - mean_squared_error: 1.4881 - val_loss: 2.6505 - val_mean_squared_error: 2.6505\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.4014 - mean_squared_error: 1.4014 - val_loss: 2.4373 - val_mean_squared_error: 2.4373\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.6131 - mean_squared_error: 1.6131 - val_loss: 4.7234 - val_mean_squared_error: 4.7234\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.5295 - mean_squared_error: 1.5295 - val_loss: 3.2449 - val_mean_squared_error: 3.2449\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.4369 - mean_squared_error: 1.4369 - val_loss: 2.5088 - val_mean_squared_error: 2.5088\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.3880 - mean_squared_error: 1.3880 - val_loss: 2.6324 - val_mean_squared_error: 2.6324\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.3369 - mean_squared_error: 1.3369 - val_loss: 2.8268 - val_mean_squared_error: 2.8268\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.3029 - mean_squared_error: 1.3029 - val_loss: 3.0410 - val_mean_squared_error: 3.0410\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.4064 - mean_squared_error: 1.4064 - val_loss: 3.4142 - val_mean_squared_error: 3.4142\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.4131 - mean_squared_error: 1.4131 - val_loss: 2.8806 - val_mean_squared_error: 2.8806\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.3375 - mean_squared_error: 1.3375 - val_loss: 2.2944 - val_mean_squared_error: 2.2944\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.4880 - mean_squared_error: 1.4880 - val_loss: 2.5366 - val_mean_squared_error: 2.5366\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.4238 - mean_squared_error: 1.4238 - val_loss: 2.9764 - val_mean_squared_error: 2.9764\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.4593 - mean_squared_error: 1.4593 - val_loss: 2.7358 - val_mean_squared_error: 2.7358\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.4248 - mean_squared_error: 1.4248 - val_loss: 2.8495 - val_mean_squared_error: 2.8495\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.5098 - mean_squared_error: 1.5098 - val_loss: 2.5933 - val_mean_squared_error: 2.5933\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.3906 - mean_squared_error: 1.3906 - val_loss: 2.6814 - val_mean_squared_error: 2.6814\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.3466 - mean_squared_error: 1.3466 - val_loss: 2.5303 - val_mean_squared_error: 2.5303\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.3247 - mean_squared_error: 1.3247 - val_loss: 2.6571 - val_mean_squared_error: 2.6571\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.3172 - mean_squared_error: 1.3172 - val_loss: 3.4337 - val_mean_squared_error: 3.4337\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.2806 - mean_squared_error: 1.2806 - val_loss: 2.3405 - val_mean_squared_error: 2.3405\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1844 - mean_squared_error: 1.1844 - val_loss: 2.2299 - val_mean_squared_error: 2.2299\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.3238 - mean_squared_error: 1.3238 - val_loss: 2.6015 - val_mean_squared_error: 2.6015\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3066 - mean_squared_error: 1.3066 - val_loss: 2.7301 - val_mean_squared_error: 2.7301\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.3801 - mean_squared_error: 1.3801 - val_loss: 2.4186 - val_mean_squared_error: 2.4186\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.4024 - mean_squared_error: 1.4024 - val_loss: 2.5962 - val_mean_squared_error: 2.5962\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.2759 - mean_squared_error: 1.2759 - val_loss: 2.1690 - val_mean_squared_error: 2.1690\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2621 - mean_squared_error: 1.2621 - val_loss: 2.3013 - val_mean_squared_error: 2.3013\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.1907 - mean_squared_error: 1.1907 - val_loss: 2.4917 - val_mean_squared_error: 2.4917\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2174 - mean_squared_error: 1.2174 - val_loss: 3.1425 - val_mean_squared_error: 3.1425\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1969 - mean_squared_error: 1.1969 - val_loss: 3.1743 - val_mean_squared_error: 3.1743\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.1906 - mean_squared_error: 1.1906 - val_loss: 3.1785 - val_mean_squared_error: 3.1785\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.2563 - mean_squared_error: 1.2563 - val_loss: 3.0067 - val_mean_squared_error: 3.0067\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2297 - mean_squared_error: 1.2297 - val_loss: 2.3925 - val_mean_squared_error: 2.3925\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.2445 - mean_squared_error: 1.2445 - val_loss: 2.6027 - val_mean_squared_error: 2.6027\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.2309 - mean_squared_error: 1.2309 - val_loss: 2.4587 - val_mean_squared_error: 2.4587\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1966 - mean_squared_error: 1.1966 - val_loss: 2.6839 - val_mean_squared_error: 2.6839\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.2425 - mean_squared_error: 1.2425 - val_loss: 2.4260 - val_mean_squared_error: 2.4260\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.1516 - mean_squared_error: 1.1516 - val_loss: 2.6387 - val_mean_squared_error: 2.6387\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1498 - mean_squared_error: 1.1498 - val_loss: 2.2267 - val_mean_squared_error: 2.2267\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.1610 - mean_squared_error: 1.1610 - val_loss: 2.2431 - val_mean_squared_error: 2.2431\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.2493 - mean_squared_error: 1.2493 - val_loss: 3.5597 - val_mean_squared_error: 3.5597\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.1557 - mean_squared_error: 1.1557 - val_loss: 2.7888 - val_mean_squared_error: 2.7888\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.2064 - mean_squared_error: 1.2064 - val_loss: 2.7349 - val_mean_squared_error: 2.7349\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.2021 - mean_squared_error: 1.2021 - val_loss: 2.2151 - val_mean_squared_error: 2.2151\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1894 - mean_squared_error: 1.1894 - val_loss: 2.4200 - val_mean_squared_error: 2.4200\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.2794 - mean_squared_error: 1.2794 - val_loss: 2.3092 - val_mean_squared_error: 2.3092\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.1655 - mean_squared_error: 1.1655 - val_loss: 3.0507 - val_mean_squared_error: 3.0507\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.0761 - mean_squared_error: 1.0761 - val_loss: 2.3058 - val_mean_squared_error: 2.3058\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1431 - mean_squared_error: 1.1431 - val_loss: 2.6688 - val_mean_squared_error: 2.6688\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1641 - mean_squared_error: 1.1641 - val_loss: 2.3833 - val_mean_squared_error: 2.3833\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.1259 - mean_squared_error: 1.1259 - val_loss: 2.3620 - val_mean_squared_error: 2.3620\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.0626 - mean_squared_error: 1.0626 - val_loss: 2.3616 - val_mean_squared_error: 2.3616\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.0913 - mean_squared_error: 1.0913 - val_loss: 2.3888 - val_mean_squared_error: 2.3888\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.1321 - mean_squared_error: 1.1321 - val_loss: 2.4171 - val_mean_squared_error: 2.4171\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2195 - mean_squared_error: 1.2195 - val_loss: 3.0221 - val_mean_squared_error: 3.0221\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.1784 - mean_squared_error: 1.1784 - val_loss: 2.9581 - val_mean_squared_error: 2.9581\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.0825 - mean_squared_error: 1.0825 - val_loss: 2.8772 - val_mean_squared_error: 2.8772\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.1505 - mean_squared_error: 1.1505 - val_loss: 2.2347 - val_mean_squared_error: 2.2347\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.0693 - mean_squared_error: 1.0693 - val_loss: 2.5212 - val_mean_squared_error: 2.5212\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1146 - mean_squared_error: 1.1146 - val_loss: 2.3367 - val_mean_squared_error: 2.3367\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.1069 - mean_squared_error: 1.1069 - val_loss: 2.4106 - val_mean_squared_error: 2.4106\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.0944 - mean_squared_error: 1.0944 - val_loss: 2.2041 - val_mean_squared_error: 2.2041\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.0890 - mean_squared_error: 1.0890 - val_loss: 2.5584 - val_mean_squared_error: 2.5584\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 2.5308 - val_mean_squared_error: 2.5308\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 2.2097 - val_mean_squared_error: 2.2097\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.0444 - mean_squared_error: 1.0444 - val_loss: 2.0927 - val_mean_squared_error: 2.0927\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.1864 - mean_squared_error: 1.1864 - val_loss: 2.4159 - val_mean_squared_error: 2.4159\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.0596 - mean_squared_error: 1.0596 - val_loss: 2.2562 - val_mean_squared_error: 2.2562\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.0426 - mean_squared_error: 1.0426 - val_loss: 2.3278 - val_mean_squared_error: 2.3278\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.0285 - mean_squared_error: 1.0285 - val_loss: 2.5954 - val_mean_squared_error: 2.5954\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.0684 - mean_squared_error: 1.0684 - val_loss: 2.1823 - val_mean_squared_error: 2.1823\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.0414 - mean_squared_error: 1.0414 - val_loss: 2.3048 - val_mean_squared_error: 2.3048\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.0697 - mean_squared_error: 1.0697 - val_loss: 2.5380 - val_mean_squared_error: 2.5380\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 2.3491 - val_mean_squared_error: 2.3491\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.1483 - mean_squared_error: 1.1483 - val_loss: 2.6519 - val_mean_squared_error: 2.6519\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.0374 - mean_squared_error: 1.0374 - val_loss: 2.3966 - val_mean_squared_error: 2.3966\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.0665 - mean_squared_error: 1.0665 - val_loss: 2.4726 - val_mean_squared_error: 2.4726\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.0758 - mean_squared_error: 1.0757 - val_loss: 2.2376 - val_mean_squared_error: 2.2376\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.0447 - mean_squared_error: 1.0447 - val_loss: 3.5173 - val_mean_squared_error: 3.5173\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.1456 - mean_squared_error: 1.1456 - val_loss: 2.4318 - val_mean_squared_error: 2.4318\n",
            "==================================================\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 1551.0239 - mean_squared_error: 1551.0239 - val_loss: 1531.3066 - val_mean_squared_error: 1531.3065\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 240.1167 - mean_squared_error: 240.1167 - val_loss: 385.2489 - val_mean_squared_error: 385.2489\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 31.7824 - mean_squared_error: 31.7824 - val_loss: 128.7735 - val_mean_squared_error: 128.7735\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 18.0104 - mean_squared_error: 18.0104 - val_loss: 44.3799 - val_mean_squared_error: 44.3799\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 22.0732 - mean_squared_error: 22.0732 - val_loss: 59.5999 - val_mean_squared_error: 59.5999\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 18.0292 - mean_squared_error: 18.0292 - val_loss: 31.0874 - val_mean_squared_error: 31.0874\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 435us/sample - loss: 14.6726 - mean_squared_error: 14.6726 - val_loss: 15.1309 - val_mean_squared_error: 15.1309\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 13.9488 - mean_squared_error: 13.9488 - val_loss: 12.7408 - val_mean_squared_error: 12.7408\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 14.5687 - mean_squared_error: 14.5687 - val_loss: 12.9480 - val_mean_squared_error: 12.9480\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 12.4740 - mean_squared_error: 12.4740 - val_loss: 12.0507 - val_mean_squared_error: 12.0507\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 12.3392 - mean_squared_error: 12.3392 - val_loss: 16.4123 - val_mean_squared_error: 16.4123\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 10.7002 - mean_squared_error: 10.7002 - val_loss: 10.3839 - val_mean_squared_error: 10.3839\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 10.5155 - mean_squared_error: 10.5155 - val_loss: 13.2388 - val_mean_squared_error: 13.2388\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 10.3474 - mean_squared_error: 10.3474 - val_loss: 9.6654 - val_mean_squared_error: 9.6654\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 9.1838 - mean_squared_error: 9.1838 - val_loss: 8.4679 - val_mean_squared_error: 8.4679\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 9.3229 - mean_squared_error: 9.3229 - val_loss: 10.3539 - val_mean_squared_error: 10.3539\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 8.3342 - mean_squared_error: 8.3342 - val_loss: 9.1173 - val_mean_squared_error: 9.1173\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 8.6697 - mean_squared_error: 8.6697 - val_loss: 8.3832 - val_mean_squared_error: 8.3832\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 7.4253 - mean_squared_error: 7.4253 - val_loss: 7.4177 - val_mean_squared_error: 7.4177\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 7.2177 - mean_squared_error: 7.2177 - val_loss: 8.1811 - val_mean_squared_error: 8.1811\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 6.9559 - mean_squared_error: 6.9559 - val_loss: 6.7636 - val_mean_squared_error: 6.7636\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 6.4907 - mean_squared_error: 6.4907 - val_loss: 8.1086 - val_mean_squared_error: 8.1086\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 6.4097 - mean_squared_error: 6.4097 - val_loss: 7.3438 - val_mean_squared_error: 7.3438\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 6.0938 - mean_squared_error: 6.0938 - val_loss: 6.6644 - val_mean_squared_error: 6.6644\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 5.9827 - mean_squared_error: 5.9827 - val_loss: 6.1986 - val_mean_squared_error: 6.1986\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 6.7148 - mean_squared_error: 6.7148 - val_loss: 5.9885 - val_mean_squared_error: 5.9885\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 5.6066 - mean_squared_error: 5.6066 - val_loss: 5.0307 - val_mean_squared_error: 5.0307\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 5.3892 - mean_squared_error: 5.3892 - val_loss: 7.3561 - val_mean_squared_error: 7.3561\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 5.3362 - mean_squared_error: 5.3362 - val_loss: 6.3450 - val_mean_squared_error: 6.3450\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 4.8343 - mean_squared_error: 4.8343 - val_loss: 4.5833 - val_mean_squared_error: 4.5833\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 4.5879 - mean_squared_error: 4.5879 - val_loss: 5.0756 - val_mean_squared_error: 5.0756\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 4.3517 - mean_squared_error: 4.3517 - val_loss: 4.9827 - val_mean_squared_error: 4.9827\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 4.5850 - mean_squared_error: 4.5850 - val_loss: 5.3758 - val_mean_squared_error: 5.3758\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 4.1823 - mean_squared_error: 4.1823 - val_loss: 4.9132 - val_mean_squared_error: 4.9132\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 4.0754 - mean_squared_error: 4.0754 - val_loss: 4.2378 - val_mean_squared_error: 4.2378\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 4.0340 - mean_squared_error: 4.0340 - val_loss: 4.5383 - val_mean_squared_error: 4.5383\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 3.8837 - mean_squared_error: 3.8837 - val_loss: 3.9982 - val_mean_squared_error: 3.9982\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 3.8737 - mean_squared_error: 3.8737 - val_loss: 4.3599 - val_mean_squared_error: 4.3599\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 3.9024 - mean_squared_error: 3.9024 - val_loss: 4.0480 - val_mean_squared_error: 4.0480\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 3.5331 - mean_squared_error: 3.5331 - val_loss: 4.1880 - val_mean_squared_error: 4.1880\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 3.4624 - mean_squared_error: 3.4624 - val_loss: 3.4788 - val_mean_squared_error: 3.4788\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 3.6694 - mean_squared_error: 3.6694 - val_loss: 3.9712 - val_mean_squared_error: 3.9712\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 3.2554 - mean_squared_error: 3.2554 - val_loss: 3.7342 - val_mean_squared_error: 3.7342\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 3.1744 - mean_squared_error: 3.1744 - val_loss: 3.5011 - val_mean_squared_error: 3.5011\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.2580 - mean_squared_error: 3.2580 - val_loss: 3.8553 - val_mean_squared_error: 3.8553\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 3.0386 - mean_squared_error: 3.0386 - val_loss: 3.7093 - val_mean_squared_error: 3.7093\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.8936 - mean_squared_error: 2.8936 - val_loss: 3.4981 - val_mean_squared_error: 3.4981\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 3.0916 - mean_squared_error: 3.0916 - val_loss: 3.4992 - val_mean_squared_error: 3.4992\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 3.2909 - mean_squared_error: 3.2909 - val_loss: 5.4007 - val_mean_squared_error: 5.4007\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.8627 - mean_squared_error: 2.8627 - val_loss: 4.0005 - val_mean_squared_error: 4.0005\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.0729 - mean_squared_error: 3.0729 - val_loss: 3.3911 - val_mean_squared_error: 3.3911\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.9701 - mean_squared_error: 2.9701 - val_loss: 3.4589 - val_mean_squared_error: 3.4589\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 3.0098 - mean_squared_error: 3.0098 - val_loss: 3.3628 - val_mean_squared_error: 3.3628\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.6797 - mean_squared_error: 2.6797 - val_loss: 3.7888 - val_mean_squared_error: 3.7888\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.8772 - mean_squared_error: 2.8772 - val_loss: 3.7177 - val_mean_squared_error: 3.7177\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 2.8025 - mean_squared_error: 2.8025 - val_loss: 3.3245 - val_mean_squared_error: 3.3245\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.7248 - mean_squared_error: 2.7248 - val_loss: 3.8280 - val_mean_squared_error: 3.8280\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.9976 - mean_squared_error: 2.9976 - val_loss: 3.5323 - val_mean_squared_error: 3.5323\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 3.0344 - mean_squared_error: 3.0344 - val_loss: 2.7860 - val_mean_squared_error: 2.7860\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 2.5227 - mean_squared_error: 2.5227 - val_loss: 3.0783 - val_mean_squared_error: 3.0783\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.5841 - mean_squared_error: 2.5841 - val_loss: 4.0020 - val_mean_squared_error: 4.0020\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.6605 - mean_squared_error: 2.6605 - val_loss: 5.2484 - val_mean_squared_error: 5.2484\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 2.4419 - mean_squared_error: 2.4419 - val_loss: 3.0998 - val_mean_squared_error: 3.0998\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.4911 - mean_squared_error: 2.4911 - val_loss: 3.5846 - val_mean_squared_error: 3.5846\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.4740 - mean_squared_error: 2.4740 - val_loss: 3.4792 - val_mean_squared_error: 3.4792\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.3997 - mean_squared_error: 2.3997 - val_loss: 2.9580 - val_mean_squared_error: 2.9580\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.3691 - mean_squared_error: 2.3691 - val_loss: 3.5909 - val_mean_squared_error: 3.5909\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.5197 - mean_squared_error: 2.5197 - val_loss: 2.8392 - val_mean_squared_error: 2.8392\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.6717 - mean_squared_error: 2.6717 - val_loss: 3.2901 - val_mean_squared_error: 3.2901\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.4483 - mean_squared_error: 2.4483 - val_loss: 4.2175 - val_mean_squared_error: 4.2175\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.3486 - mean_squared_error: 2.3486 - val_loss: 2.8462 - val_mean_squared_error: 2.8462\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.4862 - mean_squared_error: 2.4862 - val_loss: 3.8960 - val_mean_squared_error: 3.8960\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.3995 - mean_squared_error: 2.3995 - val_loss: 3.1223 - val_mean_squared_error: 3.1223\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.2900 - mean_squared_error: 2.2900 - val_loss: 2.8725 - val_mean_squared_error: 2.8725\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.5676 - mean_squared_error: 2.5676 - val_loss: 2.9722 - val_mean_squared_error: 2.9722\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.2354 - mean_squared_error: 2.2354 - val_loss: 3.1996 - val_mean_squared_error: 3.1996\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.2425 - mean_squared_error: 2.2425 - val_loss: 3.0013 - val_mean_squared_error: 3.0013\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 2.2377 - mean_squared_error: 2.2377 - val_loss: 3.5286 - val_mean_squared_error: 3.5286\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.1280 - mean_squared_error: 2.1280 - val_loss: 2.7657 - val_mean_squared_error: 2.7657\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.0992 - mean_squared_error: 2.0992 - val_loss: 3.3229 - val_mean_squared_error: 3.3229\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.9415 - mean_squared_error: 1.9415 - val_loss: 2.8349 - val_mean_squared_error: 2.8349\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.0532 - mean_squared_error: 2.0532 - val_loss: 3.2056 - val_mean_squared_error: 3.2056\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.0180 - mean_squared_error: 2.0180 - val_loss: 4.2600 - val_mean_squared_error: 4.2600\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.0865 - mean_squared_error: 2.0865 - val_loss: 2.9344 - val_mean_squared_error: 2.9344\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.0300 - mean_squared_error: 2.0300 - val_loss: 2.7495 - val_mean_squared_error: 2.7495\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.0809 - mean_squared_error: 2.0809 - val_loss: 2.9683 - val_mean_squared_error: 2.9683\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 2.3341 - mean_squared_error: 2.3341 - val_loss: 3.8427 - val_mean_squared_error: 3.8427\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 2.1265 - mean_squared_error: 2.1265 - val_loss: 2.5118 - val_mean_squared_error: 2.5118\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.9310 - mean_squared_error: 1.9310 - val_loss: 3.1657 - val_mean_squared_error: 3.1657\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 2.0925 - mean_squared_error: 2.0925 - val_loss: 2.5797 - val_mean_squared_error: 2.5797\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 2.0322 - mean_squared_error: 2.0322 - val_loss: 2.7110 - val_mean_squared_error: 2.7110\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.9913 - mean_squared_error: 1.9913 - val_loss: 3.2676 - val_mean_squared_error: 3.2676\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.0129 - mean_squared_error: 2.0129 - val_loss: 3.0740 - val_mean_squared_error: 3.0740\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.9097 - mean_squared_error: 1.9097 - val_loss: 2.5175 - val_mean_squared_error: 2.5175\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.9833 - mean_squared_error: 1.9833 - val_loss: 2.5083 - val_mean_squared_error: 2.5083\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.9875 - mean_squared_error: 1.9875 - val_loss: 2.7847 - val_mean_squared_error: 2.7847\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.8752 - mean_squared_error: 1.8752 - val_loss: 2.6329 - val_mean_squared_error: 2.6329\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.7549 - mean_squared_error: 1.7549 - val_loss: 3.0387 - val_mean_squared_error: 3.0387\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.8789 - mean_squared_error: 1.8789 - val_loss: 2.9679 - val_mean_squared_error: 2.9679\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.9738 - mean_squared_error: 1.9738 - val_loss: 3.4140 - val_mean_squared_error: 3.4140\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 2.0356 - mean_squared_error: 2.0356 - val_loss: 3.3760 - val_mean_squared_error: 3.3760\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.7705 - mean_squared_error: 1.7705 - val_loss: 2.8077 - val_mean_squared_error: 2.8077\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.8955 - mean_squared_error: 1.8955 - val_loss: 2.9254 - val_mean_squared_error: 2.9254\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.7404 - mean_squared_error: 1.7404 - val_loss: 3.0125 - val_mean_squared_error: 3.0125\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.8949 - mean_squared_error: 1.8949 - val_loss: 2.6334 - val_mean_squared_error: 2.6334\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.7474 - mean_squared_error: 1.7474 - val_loss: 2.8965 - val_mean_squared_error: 2.8965\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.7579 - mean_squared_error: 1.7579 - val_loss: 2.6440 - val_mean_squared_error: 2.6440\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.8640 - mean_squared_error: 1.8640 - val_loss: 2.8121 - val_mean_squared_error: 2.8121\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.7535 - mean_squared_error: 1.7535 - val_loss: 2.4671 - val_mean_squared_error: 2.4671\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.7692 - mean_squared_error: 1.7692 - val_loss: 2.8057 - val_mean_squared_error: 2.8057\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.6355 - mean_squared_error: 1.6355 - val_loss: 3.0309 - val_mean_squared_error: 3.0309\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.6445 - mean_squared_error: 1.6445 - val_loss: 2.6483 - val_mean_squared_error: 2.6483\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.5323 - mean_squared_error: 1.5323 - val_loss: 2.7213 - val_mean_squared_error: 2.7213\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.6025 - mean_squared_error: 1.6025 - val_loss: 3.8314 - val_mean_squared_error: 3.8314\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.6721 - mean_squared_error: 1.6721 - val_loss: 2.8927 - val_mean_squared_error: 2.8927\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.5745 - mean_squared_error: 1.5745 - val_loss: 2.6747 - val_mean_squared_error: 2.6747\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.7700 - mean_squared_error: 1.7700 - val_loss: 2.7501 - val_mean_squared_error: 2.7501\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.6259 - mean_squared_error: 1.6259 - val_loss: 2.8910 - val_mean_squared_error: 2.8910\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.6281 - mean_squared_error: 1.6281 - val_loss: 2.4478 - val_mean_squared_error: 2.4478\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.5460 - mean_squared_error: 1.5460 - val_loss: 3.8795 - val_mean_squared_error: 3.8795\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.5766 - mean_squared_error: 1.5766 - val_loss: 2.5229 - val_mean_squared_error: 2.5229\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.5617 - mean_squared_error: 1.5617 - val_loss: 3.3485 - val_mean_squared_error: 3.3485\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.6465 - mean_squared_error: 1.6465 - val_loss: 2.5478 - val_mean_squared_error: 2.5478\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.6020 - mean_squared_error: 1.6020 - val_loss: 2.9892 - val_mean_squared_error: 2.9892\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.5807 - mean_squared_error: 1.5807 - val_loss: 3.1029 - val_mean_squared_error: 3.1029\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.5727 - mean_squared_error: 1.5727 - val_loss: 3.3841 - val_mean_squared_error: 3.3841\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.6297 - mean_squared_error: 1.6297 - val_loss: 2.7844 - val_mean_squared_error: 2.7844\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.4565 - mean_squared_error: 1.4565 - val_loss: 2.7092 - val_mean_squared_error: 2.7092\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.6278 - mean_squared_error: 1.6278 - val_loss: 3.3265 - val_mean_squared_error: 3.3265\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.5175 - mean_squared_error: 1.5175 - val_loss: 2.3322 - val_mean_squared_error: 2.3322\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.5164 - mean_squared_error: 1.5164 - val_loss: 2.3866 - val_mean_squared_error: 2.3866\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.5894 - mean_squared_error: 1.5894 - val_loss: 2.7133 - val_mean_squared_error: 2.7133\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.5371 - mean_squared_error: 1.5371 - val_loss: 2.4886 - val_mean_squared_error: 2.4886\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.4861 - mean_squared_error: 1.4861 - val_loss: 2.6635 - val_mean_squared_error: 2.6635\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.4710 - mean_squared_error: 1.4710 - val_loss: 2.5586 - val_mean_squared_error: 2.5586\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.5337 - mean_squared_error: 1.5337 - val_loss: 2.4900 - val_mean_squared_error: 2.4900\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.5644 - mean_squared_error: 1.5644 - val_loss: 2.5283 - val_mean_squared_error: 2.5283\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.4013 - mean_squared_error: 1.4013 - val_loss: 2.6243 - val_mean_squared_error: 2.6243\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.4435 - mean_squared_error: 1.4435 - val_loss: 2.6447 - val_mean_squared_error: 2.6447\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3274 - mean_squared_error: 1.3274 - val_loss: 2.2819 - val_mean_squared_error: 2.2819\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.4245 - mean_squared_error: 1.4245 - val_loss: 2.4777 - val_mean_squared_error: 2.4777\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.3652 - mean_squared_error: 1.3652 - val_loss: 2.3819 - val_mean_squared_error: 2.3819\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.3907 - mean_squared_error: 1.3907 - val_loss: 2.3582 - val_mean_squared_error: 2.3582\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.3794 - mean_squared_error: 1.3794 - val_loss: 2.6980 - val_mean_squared_error: 2.6980\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.3927 - mean_squared_error: 1.3927 - val_loss: 2.4635 - val_mean_squared_error: 2.4635\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3564 - mean_squared_error: 1.3564 - val_loss: 2.3165 - val_mean_squared_error: 2.3165\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.4451 - mean_squared_error: 1.4451 - val_loss: 2.6445 - val_mean_squared_error: 2.6445\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.4260 - mean_squared_error: 1.4260 - val_loss: 2.7378 - val_mean_squared_error: 2.7378\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.4541 - mean_squared_error: 1.4541 - val_loss: 2.6263 - val_mean_squared_error: 2.6263\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.5385 - mean_squared_error: 1.5385 - val_loss: 2.4057 - val_mean_squared_error: 2.4057\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3564 - mean_squared_error: 1.3564 - val_loss: 3.0521 - val_mean_squared_error: 3.0521\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.4513 - mean_squared_error: 1.4513 - val_loss: 2.6703 - val_mean_squared_error: 2.6703\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.4942 - mean_squared_error: 1.4942 - val_loss: 2.4928 - val_mean_squared_error: 2.4928\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2999 - mean_squared_error: 1.2999 - val_loss: 2.4197 - val_mean_squared_error: 2.4197\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3628 - mean_squared_error: 1.3628 - val_loss: 3.4887 - val_mean_squared_error: 3.4887\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.2977 - mean_squared_error: 1.2977 - val_loss: 2.1586 - val_mean_squared_error: 2.1586\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2567 - mean_squared_error: 1.2567 - val_loss: 2.6185 - val_mean_squared_error: 2.6185\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.2828 - mean_squared_error: 1.2828 - val_loss: 2.3601 - val_mean_squared_error: 2.3601\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3519 - mean_squared_error: 1.3519 - val_loss: 2.4533 - val_mean_squared_error: 2.4533\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3584 - mean_squared_error: 1.3584 - val_loss: 2.4671 - val_mean_squared_error: 2.4671\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3677 - mean_squared_error: 1.3677 - val_loss: 2.3614 - val_mean_squared_error: 2.3614\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.3956 - mean_squared_error: 1.3956 - val_loss: 2.7726 - val_mean_squared_error: 2.7726\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3658 - mean_squared_error: 1.3658 - val_loss: 2.4168 - val_mean_squared_error: 2.4168\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2238 - mean_squared_error: 1.2238 - val_loss: 2.4884 - val_mean_squared_error: 2.4884\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.3247 - mean_squared_error: 1.3247 - val_loss: 2.4641 - val_mean_squared_error: 2.4641\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3567 - mean_squared_error: 1.3567 - val_loss: 2.8494 - val_mean_squared_error: 2.8494\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.3692 - mean_squared_error: 1.3692 - val_loss: 2.7410 - val_mean_squared_error: 2.7410\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.3087 - mean_squared_error: 1.3087 - val_loss: 2.2627 - val_mean_squared_error: 2.2627\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.1830 - mean_squared_error: 1.1830 - val_loss: 2.3021 - val_mean_squared_error: 2.3021\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2520 - mean_squared_error: 1.2520 - val_loss: 2.3531 - val_mean_squared_error: 2.3531\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.2621 - mean_squared_error: 1.2621 - val_loss: 4.2621 - val_mean_squared_error: 4.2621\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.2021 - mean_squared_error: 1.2021 - val_loss: 2.7003 - val_mean_squared_error: 2.7003\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1968 - mean_squared_error: 1.1968 - val_loss: 2.1691 - val_mean_squared_error: 2.1691\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.1847 - mean_squared_error: 1.1847 - val_loss: 2.3046 - val_mean_squared_error: 2.3046\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1776 - mean_squared_error: 1.1776 - val_loss: 2.5877 - val_mean_squared_error: 2.5877\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.2637 - mean_squared_error: 1.2637 - val_loss: 2.6132 - val_mean_squared_error: 2.6132\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.3483 - mean_squared_error: 1.3483 - val_loss: 2.9845 - val_mean_squared_error: 2.9845\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.2702 - mean_squared_error: 1.2702 - val_loss: 2.9992 - val_mean_squared_error: 2.9992\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.3695 - mean_squared_error: 1.3695 - val_loss: 2.3113 - val_mean_squared_error: 2.3113\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.1566 - mean_squared_error: 1.1566 - val_loss: 2.6710 - val_mean_squared_error: 2.6710\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.2319 - mean_squared_error: 1.2319 - val_loss: 2.5394 - val_mean_squared_error: 2.5394\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2275 - mean_squared_error: 1.2275 - val_loss: 2.4366 - val_mean_squared_error: 2.4366\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2239 - mean_squared_error: 1.2239 - val_loss: 2.5364 - val_mean_squared_error: 2.5364\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.3276 - mean_squared_error: 1.3276 - val_loss: 3.2909 - val_mean_squared_error: 3.2909\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.5259 - mean_squared_error: 1.5259 - val_loss: 2.6021 - val_mean_squared_error: 2.6021\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.2167 - mean_squared_error: 1.2167 - val_loss: 3.0366 - val_mean_squared_error: 3.0366\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.2651 - mean_squared_error: 1.2651 - val_loss: 2.2163 - val_mean_squared_error: 2.2163\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.0843 - mean_squared_error: 1.0843 - val_loss: 2.3416 - val_mean_squared_error: 2.3416\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1444 - mean_squared_error: 1.1444 - val_loss: 2.3150 - val_mean_squared_error: 2.3150\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1340 - mean_squared_error: 1.1340 - val_loss: 2.2545 - val_mean_squared_error: 2.2545\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.1370 - mean_squared_error: 1.1370 - val_loss: 2.3633 - val_mean_squared_error: 2.3633\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1544 - mean_squared_error: 1.1544 - val_loss: 2.2476 - val_mean_squared_error: 2.2476\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1134 - mean_squared_error: 1.1134 - val_loss: 2.3600 - val_mean_squared_error: 2.3600\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1362 - mean_squared_error: 1.1362 - val_loss: 2.2166 - val_mean_squared_error: 2.2166\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.1397 - mean_squared_error: 1.1397 - val_loss: 2.2627 - val_mean_squared_error: 2.2627\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2362 - mean_squared_error: 1.2362 - val_loss: 2.5442 - val_mean_squared_error: 2.5442\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.2119 - mean_squared_error: 1.2119 - val_loss: 2.2014 - val_mean_squared_error: 2.2014\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.1640 - mean_squared_error: 1.1640 - val_loss: 2.2560 - val_mean_squared_error: 2.2560\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1618 - mean_squared_error: 1.1618 - val_loss: 2.4718 - val_mean_squared_error: 2.4718\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 2.5675 - val_mean_squared_error: 2.5675\n",
            "==================================================\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_24 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 1548.3708 - mean_squared_error: 1548.3711 - val_loss: 1156.1791 - val_mean_squared_error: 1156.1791\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 245.7980 - mean_squared_error: 245.7980 - val_loss: 281.6049 - val_mean_squared_error: 281.6049\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 36.3173 - mean_squared_error: 36.3173 - val_loss: 111.4351 - val_mean_squared_error: 111.4351\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 20.0018 - mean_squared_error: 20.0018 - val_loss: 60.3180 - val_mean_squared_error: 60.3180\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 18.1614 - mean_squared_error: 18.1614 - val_loss: 32.5994 - val_mean_squared_error: 32.5994\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 438us/sample - loss: 17.8158 - mean_squared_error: 17.8158 - val_loss: 31.7266 - val_mean_squared_error: 31.7266\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 14.8116 - mean_squared_error: 14.8116 - val_loss: 14.3013 - val_mean_squared_error: 14.3013\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 14.7469 - mean_squared_error: 14.7469 - val_loss: 11.4701 - val_mean_squared_error: 11.4701\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 14.4837 - mean_squared_error: 14.4837 - val_loss: 10.7606 - val_mean_squared_error: 10.7606\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 12.6705 - mean_squared_error: 12.6706 - val_loss: 12.2870 - val_mean_squared_error: 12.2870\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 12.4599 - mean_squared_error: 12.4599 - val_loss: 15.1529 - val_mean_squared_error: 15.1529\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 11.9424 - mean_squared_error: 11.9424 - val_loss: 10.5189 - val_mean_squared_error: 10.5189\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 11.2819 - mean_squared_error: 11.2819 - val_loss: 11.5669 - val_mean_squared_error: 11.5669\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 11.7593 - mean_squared_error: 11.7593 - val_loss: 16.9834 - val_mean_squared_error: 16.9834\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 10.4187 - mean_squared_error: 10.4187 - val_loss: 14.0712 - val_mean_squared_error: 14.0712\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 10.2840 - mean_squared_error: 10.2840 - val_loss: 10.3311 - val_mean_squared_error: 10.3311\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 9.2396 - mean_squared_error: 9.2396 - val_loss: 11.2663 - val_mean_squared_error: 11.2663\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 8.5797 - mean_squared_error: 8.5797 - val_loss: 10.9380 - val_mean_squared_error: 10.9380\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 8.4702 - mean_squared_error: 8.4702 - val_loss: 9.1662 - val_mean_squared_error: 9.1662\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 7.7805 - mean_squared_error: 7.7805 - val_loss: 9.2118 - val_mean_squared_error: 9.2118\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 8.1172 - mean_squared_error: 8.1172 - val_loss: 6.9148 - val_mean_squared_error: 6.9148\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 7.9806 - mean_squared_error: 7.9806 - val_loss: 8.9237 - val_mean_squared_error: 8.9237\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 6.7613 - mean_squared_error: 6.7613 - val_loss: 8.1065 - val_mean_squared_error: 8.1065\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 6.8160 - mean_squared_error: 6.8160 - val_loss: 6.1154 - val_mean_squared_error: 6.1154\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 6.3600 - mean_squared_error: 6.3600 - val_loss: 6.6461 - val_mean_squared_error: 6.6461\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 6.5613 - mean_squared_error: 6.5613 - val_loss: 7.0762 - val_mean_squared_error: 7.0762\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 5.7338 - mean_squared_error: 5.7338 - val_loss: 9.4074 - val_mean_squared_error: 9.4074\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 5.2685 - mean_squared_error: 5.2685 - val_loss: 5.1798 - val_mean_squared_error: 5.1798\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 5.2313 - mean_squared_error: 5.2313 - val_loss: 7.1963 - val_mean_squared_error: 7.1963\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 5.1978 - mean_squared_error: 5.1978 - val_loss: 6.6431 - val_mean_squared_error: 6.6431\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 4.6756 - mean_squared_error: 4.6756 - val_loss: 6.3529 - val_mean_squared_error: 6.3529\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 4.7753 - mean_squared_error: 4.7753 - val_loss: 4.6637 - val_mean_squared_error: 4.6637\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 4.5253 - mean_squared_error: 4.5253 - val_loss: 4.4375 - val_mean_squared_error: 4.4375\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 4.4532 - mean_squared_error: 4.4532 - val_loss: 4.0647 - val_mean_squared_error: 4.0647\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 4.3140 - mean_squared_error: 4.3140 - val_loss: 4.8424 - val_mean_squared_error: 4.8424\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 4.0796 - mean_squared_error: 4.0796 - val_loss: 3.9599 - val_mean_squared_error: 3.9599\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 4.6280 - mean_squared_error: 4.6280 - val_loss: 5.0406 - val_mean_squared_error: 5.0406\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 4.1055 - mean_squared_error: 4.1055 - val_loss: 3.8462 - val_mean_squared_error: 3.8462\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 4.0658 - mean_squared_error: 4.0658 - val_loss: 6.2382 - val_mean_squared_error: 6.2382\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 4.0065 - mean_squared_error: 4.0065 - val_loss: 3.9531 - val_mean_squared_error: 3.9531\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 3.8691 - mean_squared_error: 3.8691 - val_loss: 3.9732 - val_mean_squared_error: 3.9732\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 3.8645 - mean_squared_error: 3.8645 - val_loss: 4.1480 - val_mean_squared_error: 4.1480\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 3.8625 - mean_squared_error: 3.8625 - val_loss: 3.6529 - val_mean_squared_error: 3.6529\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 3.7374 - mean_squared_error: 3.7374 - val_loss: 3.8315 - val_mean_squared_error: 3.8315\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 3.5695 - mean_squared_error: 3.5695 - val_loss: 3.6914 - val_mean_squared_error: 3.6914\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 3.3417 - mean_squared_error: 3.3417 - val_loss: 3.7414 - val_mean_squared_error: 3.7414\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 3.1031 - mean_squared_error: 3.1031 - val_loss: 3.2179 - val_mean_squared_error: 3.2179\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 3.2782 - mean_squared_error: 3.2782 - val_loss: 3.5290 - val_mean_squared_error: 3.5290\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 3.4136 - mean_squared_error: 3.4136 - val_loss: 3.7424 - val_mean_squared_error: 3.7424\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 3.7005 - mean_squared_error: 3.7005 - val_loss: 3.4395 - val_mean_squared_error: 3.4395\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 3.1194 - mean_squared_error: 3.1194 - val_loss: 3.3351 - val_mean_squared_error: 3.3351\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 3.1954 - mean_squared_error: 3.1954 - val_loss: 3.3580 - val_mean_squared_error: 3.3580\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 3.2109 - mean_squared_error: 3.2109 - val_loss: 3.8091 - val_mean_squared_error: 3.8091\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.0753 - mean_squared_error: 3.0753 - val_loss: 3.5325 - val_mean_squared_error: 3.5325\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.9187 - mean_squared_error: 2.9187 - val_loss: 3.0864 - val_mean_squared_error: 3.0864\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.9510 - mean_squared_error: 2.9510 - val_loss: 3.5854 - val_mean_squared_error: 3.5854\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.8677 - mean_squared_error: 2.8677 - val_loss: 3.2050 - val_mean_squared_error: 3.2050\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.9193 - mean_squared_error: 2.9193 - val_loss: 3.2532 - val_mean_squared_error: 3.2532\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 2.6550 - mean_squared_error: 2.6550 - val_loss: 3.0065 - val_mean_squared_error: 3.0065\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.8085 - mean_squared_error: 2.8085 - val_loss: 3.0291 - val_mean_squared_error: 3.0291\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.8654 - mean_squared_error: 2.8654 - val_loss: 3.2678 - val_mean_squared_error: 3.2678\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.7776 - mean_squared_error: 2.7776 - val_loss: 3.3535 - val_mean_squared_error: 3.3535\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.6180 - mean_squared_error: 2.6180 - val_loss: 3.5887 - val_mean_squared_error: 3.5887\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.8883 - mean_squared_error: 2.8883 - val_loss: 3.5392 - val_mean_squared_error: 3.5392\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.5739 - mean_squared_error: 2.5739 - val_loss: 3.4358 - val_mean_squared_error: 3.4358\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.9932 - mean_squared_error: 2.9932 - val_loss: 4.0325 - val_mean_squared_error: 4.0325\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.7491 - mean_squared_error: 2.7491 - val_loss: 3.0668 - val_mean_squared_error: 3.0668\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.6148 - mean_squared_error: 2.6148 - val_loss: 3.3276 - val_mean_squared_error: 3.3276\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.4949 - mean_squared_error: 2.4949 - val_loss: 3.4407 - val_mean_squared_error: 3.4407\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.5987 - mean_squared_error: 2.5987 - val_loss: 3.2845 - val_mean_squared_error: 3.2845\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.5672 - mean_squared_error: 2.5672 - val_loss: 2.9964 - val_mean_squared_error: 2.9964\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.5957 - mean_squared_error: 2.5957 - val_loss: 4.2649 - val_mean_squared_error: 4.2649\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.5104 - mean_squared_error: 2.5104 - val_loss: 3.0114 - val_mean_squared_error: 3.0114\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.3195 - mean_squared_error: 2.3195 - val_loss: 2.8834 - val_mean_squared_error: 2.8834\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.3055 - mean_squared_error: 2.3055 - val_loss: 3.0120 - val_mean_squared_error: 3.0120\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.2975 - mean_squared_error: 2.2975 - val_loss: 3.1073 - val_mean_squared_error: 3.1073\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.4107 - mean_squared_error: 2.4107 - val_loss: 3.0965 - val_mean_squared_error: 3.0965\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.3465 - mean_squared_error: 2.3465 - val_loss: 3.0390 - val_mean_squared_error: 3.0390\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.2018 - mean_squared_error: 2.2018 - val_loss: 2.7086 - val_mean_squared_error: 2.7086\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 2.2317 - mean_squared_error: 2.2317 - val_loss: 2.9926 - val_mean_squared_error: 2.9926\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.4662 - mean_squared_error: 2.4662 - val_loss: 2.9517 - val_mean_squared_error: 2.9517\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.1138 - mean_squared_error: 2.1138 - val_loss: 2.7093 - val_mean_squared_error: 2.7093\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.2426 - mean_squared_error: 2.2426 - val_loss: 3.0153 - val_mean_squared_error: 3.0153\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.2328 - mean_squared_error: 2.2328 - val_loss: 2.7399 - val_mean_squared_error: 2.7399\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.2082 - mean_squared_error: 2.2082 - val_loss: 2.7605 - val_mean_squared_error: 2.7605\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.3247 - mean_squared_error: 2.3247 - val_loss: 3.1930 - val_mean_squared_error: 3.1930\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.3283 - mean_squared_error: 2.3283 - val_loss: 2.6067 - val_mean_squared_error: 2.6067\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.1794 - mean_squared_error: 2.1794 - val_loss: 2.7591 - val_mean_squared_error: 2.7591\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.0095 - mean_squared_error: 2.0095 - val_loss: 2.4813 - val_mean_squared_error: 2.4813\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.8752 - mean_squared_error: 1.8752 - val_loss: 2.4327 - val_mean_squared_error: 2.4327\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.1642 - mean_squared_error: 2.1642 - val_loss: 3.1341 - val_mean_squared_error: 3.1341\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.1398 - mean_squared_error: 2.1398 - val_loss: 3.2910 - val_mean_squared_error: 3.2910\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.2958 - mean_squared_error: 2.2958 - val_loss: 3.0944 - val_mean_squared_error: 3.0944\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.9255 - mean_squared_error: 1.9255 - val_loss: 2.7855 - val_mean_squared_error: 2.7855\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.1187 - mean_squared_error: 2.1187 - val_loss: 2.8270 - val_mean_squared_error: 2.8270\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.0378 - mean_squared_error: 2.0378 - val_loss: 2.8427 - val_mean_squared_error: 2.8427\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.1118 - mean_squared_error: 2.1118 - val_loss: 2.8408 - val_mean_squared_error: 2.8408\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.9860 - mean_squared_error: 1.9860 - val_loss: 2.5852 - val_mean_squared_error: 2.5852\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.9533 - mean_squared_error: 1.9533 - val_loss: 2.8446 - val_mean_squared_error: 2.8446\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.9524 - mean_squared_error: 1.9524 - val_loss: 2.6007 - val_mean_squared_error: 2.6007\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.9482 - mean_squared_error: 1.9482 - val_loss: 2.7203 - val_mean_squared_error: 2.7203\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.9284 - mean_squared_error: 1.9284 - val_loss: 2.7030 - val_mean_squared_error: 2.7030\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.8988 - mean_squared_error: 1.8988 - val_loss: 2.4567 - val_mean_squared_error: 2.4567\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.0584 - mean_squared_error: 2.0584 - val_loss: 2.7595 - val_mean_squared_error: 2.7595\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.9984 - mean_squared_error: 1.9984 - val_loss: 2.6909 - val_mean_squared_error: 2.6909\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.9414 - mean_squared_error: 1.9414 - val_loss: 2.4844 - val_mean_squared_error: 2.4844\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.8803 - mean_squared_error: 1.8803 - val_loss: 2.6010 - val_mean_squared_error: 2.6010\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.9343 - mean_squared_error: 1.9344 - val_loss: 3.6242 - val_mean_squared_error: 3.6242\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.8398 - mean_squared_error: 1.8398 - val_loss: 2.5411 - val_mean_squared_error: 2.5411\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.8258 - mean_squared_error: 1.8258 - val_loss: 2.2884 - val_mean_squared_error: 2.2884\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.8818 - mean_squared_error: 1.8818 - val_loss: 2.8428 - val_mean_squared_error: 2.8428\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.9099 - mean_squared_error: 1.9099 - val_loss: 2.6582 - val_mean_squared_error: 2.6582\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.8772 - mean_squared_error: 1.8772 - val_loss: 2.6245 - val_mean_squared_error: 2.6245\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.9229 - mean_squared_error: 1.9229 - val_loss: 2.8281 - val_mean_squared_error: 2.8281\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.7317 - mean_squared_error: 1.7317 - val_loss: 2.3787 - val_mean_squared_error: 2.3787\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.8594 - mean_squared_error: 1.8594 - val_loss: 3.0363 - val_mean_squared_error: 3.0363\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.6891 - mean_squared_error: 1.6891 - val_loss: 2.4965 - val_mean_squared_error: 2.4965\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.6784 - mean_squared_error: 1.6784 - val_loss: 2.3401 - val_mean_squared_error: 2.3401\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.7296 - mean_squared_error: 1.7296 - val_loss: 2.4673 - val_mean_squared_error: 2.4673\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.6573 - mean_squared_error: 1.6573 - val_loss: 2.3501 - val_mean_squared_error: 2.3501\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.6343 - mean_squared_error: 1.6343 - val_loss: 2.5335 - val_mean_squared_error: 2.5335\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7270 - mean_squared_error: 1.7270 - val_loss: 2.2410 - val_mean_squared_error: 2.2410\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.6271 - mean_squared_error: 1.6271 - val_loss: 2.4101 - val_mean_squared_error: 2.4101\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.5422 - mean_squared_error: 1.5422 - val_loss: 2.2986 - val_mean_squared_error: 2.2986\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.6909 - mean_squared_error: 1.6909 - val_loss: 3.3017 - val_mean_squared_error: 3.3017\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.6292 - mean_squared_error: 1.6292 - val_loss: 2.8851 - val_mean_squared_error: 2.8851\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.8039 - mean_squared_error: 1.8039 - val_loss: 2.4806 - val_mean_squared_error: 2.4806\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.8329 - mean_squared_error: 1.8329 - val_loss: 2.7541 - val_mean_squared_error: 2.7541\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.6883 - mean_squared_error: 1.6883 - val_loss: 2.8282 - val_mean_squared_error: 2.8282\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.6408 - mean_squared_error: 1.6408 - val_loss: 2.3171 - val_mean_squared_error: 2.3171\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.6230 - mean_squared_error: 1.6230 - val_loss: 2.4205 - val_mean_squared_error: 2.4205\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5841 - mean_squared_error: 1.5841 - val_loss: 2.7761 - val_mean_squared_error: 2.7761\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.6956 - mean_squared_error: 1.6956 - val_loss: 2.7674 - val_mean_squared_error: 2.7674\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.5810 - mean_squared_error: 1.5810 - val_loss: 2.4463 - val_mean_squared_error: 2.4463\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.7988 - mean_squared_error: 1.7988 - val_loss: 2.5492 - val_mean_squared_error: 2.5492\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.6164 - mean_squared_error: 1.6164 - val_loss: 2.6348 - val_mean_squared_error: 2.6348\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.7030 - mean_squared_error: 1.7030 - val_loss: 2.4139 - val_mean_squared_error: 2.4139\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7286 - mean_squared_error: 1.7286 - val_loss: 2.3529 - val_mean_squared_error: 2.3529\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.5925 - mean_squared_error: 1.5925 - val_loss: 2.4574 - val_mean_squared_error: 2.4574\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.4758 - mean_squared_error: 1.4758 - val_loss: 2.9730 - val_mean_squared_error: 2.9730\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.4480 - mean_squared_error: 1.4480 - val_loss: 2.7587 - val_mean_squared_error: 2.7587\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.5394 - mean_squared_error: 1.5394 - val_loss: 2.4784 - val_mean_squared_error: 2.4784\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.4725 - mean_squared_error: 1.4725 - val_loss: 2.5789 - val_mean_squared_error: 2.5789\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4612 - mean_squared_error: 1.4612 - val_loss: 2.2357 - val_mean_squared_error: 2.2357\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.5175 - mean_squared_error: 1.5175 - val_loss: 2.8718 - val_mean_squared_error: 2.8718\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.5376 - mean_squared_error: 1.5376 - val_loss: 2.3946 - val_mean_squared_error: 2.3946\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.4616 - mean_squared_error: 1.4616 - val_loss: 3.1089 - val_mean_squared_error: 3.1089\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.4248 - mean_squared_error: 1.4248 - val_loss: 2.3458 - val_mean_squared_error: 2.3458\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.6789 - mean_squared_error: 1.6789 - val_loss: 3.0164 - val_mean_squared_error: 3.0164\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.5584 - mean_squared_error: 1.5584 - val_loss: 2.7854 - val_mean_squared_error: 2.7854\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.4458 - mean_squared_error: 1.4458 - val_loss: 2.8798 - val_mean_squared_error: 2.8798\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.4487 - mean_squared_error: 1.4487 - val_loss: 2.4868 - val_mean_squared_error: 2.4868\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.4976 - mean_squared_error: 1.4976 - val_loss: 2.4153 - val_mean_squared_error: 2.4153\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.4942 - mean_squared_error: 1.4942 - val_loss: 2.5245 - val_mean_squared_error: 2.5245\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3714 - mean_squared_error: 1.3714 - val_loss: 2.3159 - val_mean_squared_error: 2.3159\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4623 - mean_squared_error: 1.4623 - val_loss: 2.4093 - val_mean_squared_error: 2.4093\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.4493 - mean_squared_error: 1.4493 - val_loss: 2.7375 - val_mean_squared_error: 2.7375\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.4627 - mean_squared_error: 1.4627 - val_loss: 2.4074 - val_mean_squared_error: 2.4074\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.3487 - mean_squared_error: 1.3487 - val_loss: 2.2697 - val_mean_squared_error: 2.2697\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.5726 - mean_squared_error: 1.5726 - val_loss: 2.5347 - val_mean_squared_error: 2.5347\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.3764 - mean_squared_error: 1.3764 - val_loss: 2.1504 - val_mean_squared_error: 2.1504\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.3441 - mean_squared_error: 1.3441 - val_loss: 2.3102 - val_mean_squared_error: 2.3102\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.4289 - mean_squared_error: 1.4289 - val_loss: 2.3331 - val_mean_squared_error: 2.3331\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.5650 - mean_squared_error: 1.5650 - val_loss: 2.5204 - val_mean_squared_error: 2.5204\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.4865 - mean_squared_error: 1.4865 - val_loss: 2.6545 - val_mean_squared_error: 2.6545\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3779 - mean_squared_error: 1.3779 - val_loss: 2.2888 - val_mean_squared_error: 2.2888\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.4379 - mean_squared_error: 1.4379 - val_loss: 2.3527 - val_mean_squared_error: 2.3527\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.3141 - mean_squared_error: 1.3141 - val_loss: 2.2629 - val_mean_squared_error: 2.2629\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3332 - mean_squared_error: 1.3332 - val_loss: 2.2490 - val_mean_squared_error: 2.2490\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.3288 - mean_squared_error: 1.3288 - val_loss: 2.3845 - val_mean_squared_error: 2.3845\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.3817 - mean_squared_error: 1.3817 - val_loss: 2.2372 - val_mean_squared_error: 2.2372\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3642 - mean_squared_error: 1.3642 - val_loss: 2.6834 - val_mean_squared_error: 2.6834\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3434 - mean_squared_error: 1.3434 - val_loss: 2.4287 - val_mean_squared_error: 2.4287\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.4083 - mean_squared_error: 1.4083 - val_loss: 2.9880 - val_mean_squared_error: 2.9880\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.5052 - mean_squared_error: 1.5052 - val_loss: 3.1096 - val_mean_squared_error: 3.1096\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.3368 - mean_squared_error: 1.3368 - val_loss: 2.5738 - val_mean_squared_error: 2.5738\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.4075 - mean_squared_error: 1.4075 - val_loss: 2.3817 - val_mean_squared_error: 2.3817\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.3837 - mean_squared_error: 1.3837 - val_loss: 2.3551 - val_mean_squared_error: 2.3551\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.2865 - mean_squared_error: 1.2865 - val_loss: 2.1129 - val_mean_squared_error: 2.1129\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.3112 - mean_squared_error: 1.3112 - val_loss: 2.4732 - val_mean_squared_error: 2.4732\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.2904 - mean_squared_error: 1.2904 - val_loss: 2.2208 - val_mean_squared_error: 2.2208\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.3616 - mean_squared_error: 1.3616 - val_loss: 2.5728 - val_mean_squared_error: 2.5728\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.3124 - mean_squared_error: 1.3124 - val_loss: 2.7111 - val_mean_squared_error: 2.7111\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.2633 - mean_squared_error: 1.2633 - val_loss: 2.4040 - val_mean_squared_error: 2.4040\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.3398 - mean_squared_error: 1.3398 - val_loss: 2.2973 - val_mean_squared_error: 2.2973\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3226 - mean_squared_error: 1.3226 - val_loss: 2.6107 - val_mean_squared_error: 2.6107\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3733 - mean_squared_error: 1.3733 - val_loss: 2.6767 - val_mean_squared_error: 2.6766\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2286 - mean_squared_error: 1.2286 - val_loss: 2.1520 - val_mean_squared_error: 2.1520\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.2038 - mean_squared_error: 1.2038 - val_loss: 2.5645 - val_mean_squared_error: 2.5645\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2493 - mean_squared_error: 1.2493 - val_loss: 2.3292 - val_mean_squared_error: 2.3292\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.3472 - mean_squared_error: 1.3472 - val_loss: 2.2117 - val_mean_squared_error: 2.2117\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.3068 - mean_squared_error: 1.3068 - val_loss: 2.3769 - val_mean_squared_error: 2.3769\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2061 - mean_squared_error: 1.2061 - val_loss: 2.4470 - val_mean_squared_error: 2.4470\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2070 - mean_squared_error: 1.2070 - val_loss: 2.2987 - val_mean_squared_error: 2.2987\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.1670 - mean_squared_error: 1.1670 - val_loss: 2.3538 - val_mean_squared_error: 2.3538\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.2623 - mean_squared_error: 1.2623 - val_loss: 2.2430 - val_mean_squared_error: 2.2430\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2450 - mean_squared_error: 1.2450 - val_loss: 2.2644 - val_mean_squared_error: 2.2644\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1802 - mean_squared_error: 1.1802 - val_loss: 2.2200 - val_mean_squared_error: 2.2200\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1755 - mean_squared_error: 1.1755 - val_loss: 2.2876 - val_mean_squared_error: 2.2876\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.1845 - mean_squared_error: 1.1845 - val_loss: 2.5249 - val_mean_squared_error: 2.5249\n",
            "==================================================\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_27 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 750.0222 - mean_squared_error: 750.0222 - val_loss: 508.0744 - val_mean_squared_error: 508.0744\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 26.2142 - mean_squared_error: 26.2142 - val_loss: 207.2201 - val_mean_squared_error: 207.2201\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 21.1227 - mean_squared_error: 21.1227 - val_loss: 74.3040 - val_mean_squared_error: 74.3040\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 17.3442 - mean_squared_error: 17.3442 - val_loss: 54.4440 - val_mean_squared_error: 54.4440\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 17.3550 - mean_squared_error: 17.3550 - val_loss: 36.7612 - val_mean_squared_error: 36.7612\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 17.7891 - mean_squared_error: 17.7891 - val_loss: 22.5571 - val_mean_squared_error: 22.5571\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 15.5083 - mean_squared_error: 15.5083 - val_loss: 18.5126 - val_mean_squared_error: 18.5126\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 15.7265 - mean_squared_error: 15.7265 - val_loss: 15.7932 - val_mean_squared_error: 15.7932\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 15.8353 - mean_squared_error: 15.8353 - val_loss: 12.9643 - val_mean_squared_error: 12.9643\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 13.9586 - mean_squared_error: 13.9586 - val_loss: 14.7404 - val_mean_squared_error: 14.7404\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 13.3681 - mean_squared_error: 13.3681 - val_loss: 10.6379 - val_mean_squared_error: 10.6379\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 11.6257 - mean_squared_error: 11.6257 - val_loss: 10.7130 - val_mean_squared_error: 10.7130\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 11.3969 - mean_squared_error: 11.3969 - val_loss: 14.0837 - val_mean_squared_error: 14.0837\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 10.1166 - mean_squared_error: 10.1166 - val_loss: 9.9110 - val_mean_squared_error: 9.9110\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 10.3966 - mean_squared_error: 10.3966 - val_loss: 9.2410 - val_mean_squared_error: 9.2410\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 8.9836 - mean_squared_error: 8.9836 - val_loss: 8.6457 - val_mean_squared_error: 8.6457\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 10.0854 - mean_squared_error: 10.0854 - val_loss: 14.1994 - val_mean_squared_error: 14.1994\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 8.3774 - mean_squared_error: 8.3774 - val_loss: 8.0796 - val_mean_squared_error: 8.0796\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 8.6314 - mean_squared_error: 8.6314 - val_loss: 9.2630 - val_mean_squared_error: 9.2630\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 7.6774 - mean_squared_error: 7.6774 - val_loss: 8.6626 - val_mean_squared_error: 8.6626\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 7.4345 - mean_squared_error: 7.4345 - val_loss: 8.1035 - val_mean_squared_error: 8.1035\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 7.7788 - mean_squared_error: 7.7788 - val_loss: 8.3757 - val_mean_squared_error: 8.3757\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 6.8986 - mean_squared_error: 6.8986 - val_loss: 8.0825 - val_mean_squared_error: 8.0825\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 6.0799 - mean_squared_error: 6.0799 - val_loss: 7.3363 - val_mean_squared_error: 7.3363\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 5.7753 - mean_squared_error: 5.7753 - val_loss: 6.8431 - val_mean_squared_error: 6.8431\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 5.1285 - mean_squared_error: 5.1285 - val_loss: 6.2364 - val_mean_squared_error: 6.2364\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 5.4549 - mean_squared_error: 5.4549 - val_loss: 8.0194 - val_mean_squared_error: 8.0194\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 5.0713 - mean_squared_error: 5.0713 - val_loss: 4.7934 - val_mean_squared_error: 4.7934\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 4.7466 - mean_squared_error: 4.7466 - val_loss: 5.4594 - val_mean_squared_error: 5.4594\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 4.3836 - mean_squared_error: 4.3836 - val_loss: 6.0247 - val_mean_squared_error: 6.0247\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 3.9898 - mean_squared_error: 3.9898 - val_loss: 4.4498 - val_mean_squared_error: 4.4498\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 3.7611 - mean_squared_error: 3.7611 - val_loss: 4.1593 - val_mean_squared_error: 4.1593\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 3.5175 - mean_squared_error: 3.5175 - val_loss: 4.5727 - val_mean_squared_error: 4.5727\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 3.8903 - mean_squared_error: 3.8903 - val_loss: 4.7602 - val_mean_squared_error: 4.7602\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 3.3784 - mean_squared_error: 3.3784 - val_loss: 4.5352 - val_mean_squared_error: 4.5352\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 3.6691 - mean_squared_error: 3.6691 - val_loss: 4.2372 - val_mean_squared_error: 4.2372\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 3.2498 - mean_squared_error: 3.2498 - val_loss: 4.3841 - val_mean_squared_error: 4.3841\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 3.3823 - mean_squared_error: 3.3823 - val_loss: 4.0753 - val_mean_squared_error: 4.0753\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 3.3463 - mean_squared_error: 3.3463 - val_loss: 4.6787 - val_mean_squared_error: 4.6787\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 3.0667 - mean_squared_error: 3.0667 - val_loss: 3.3850 - val_mean_squared_error: 3.3850\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 3.0027 - mean_squared_error: 3.0027 - val_loss: 3.8952 - val_mean_squared_error: 3.8952\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.9101 - mean_squared_error: 2.9101 - val_loss: 3.7770 - val_mean_squared_error: 3.7770\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.7168 - mean_squared_error: 2.7168 - val_loss: 4.1582 - val_mean_squared_error: 4.1582\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.8381 - mean_squared_error: 2.8381 - val_loss: 3.6091 - val_mean_squared_error: 3.6091\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 2.4444 - mean_squared_error: 2.4444 - val_loss: 3.3356 - val_mean_squared_error: 3.3356\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.4803 - mean_squared_error: 2.4803 - val_loss: 3.5131 - val_mean_squared_error: 3.5131\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.4628 - mean_squared_error: 2.4628 - val_loss: 4.0354 - val_mean_squared_error: 4.0354\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.5419 - mean_squared_error: 2.5419 - val_loss: 3.7453 - val_mean_squared_error: 3.7453\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.6399 - mean_squared_error: 2.6399 - val_loss: 3.9999 - val_mean_squared_error: 3.9999\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.5030 - mean_squared_error: 2.5030 - val_loss: 3.6317 - val_mean_squared_error: 3.6317\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.3366 - mean_squared_error: 2.3366 - val_loss: 3.0817 - val_mean_squared_error: 3.0817\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.2354 - mean_squared_error: 2.2354 - val_loss: 4.5087 - val_mean_squared_error: 4.5087\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.7702 - mean_squared_error: 2.7702 - val_loss: 3.1052 - val_mean_squared_error: 3.1052\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.1777 - mean_squared_error: 2.1777 - val_loss: 3.3076 - val_mean_squared_error: 3.3076\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.1416 - mean_squared_error: 2.1416 - val_loss: 3.3624 - val_mean_squared_error: 3.3624\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 2.0528 - mean_squared_error: 2.0528 - val_loss: 3.4599 - val_mean_squared_error: 3.4599\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.5058 - mean_squared_error: 2.5058 - val_loss: 3.3586 - val_mean_squared_error: 3.3586\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.4079 - mean_squared_error: 2.4079 - val_loss: 3.6939 - val_mean_squared_error: 3.6939\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.9509 - mean_squared_error: 1.9509 - val_loss: 3.0994 - val_mean_squared_error: 3.0994\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.1671 - mean_squared_error: 2.1671 - val_loss: 3.0671 - val_mean_squared_error: 3.0671\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 2.1500 - mean_squared_error: 2.1500 - val_loss: 3.1184 - val_mean_squared_error: 3.1184\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.8897 - mean_squared_error: 1.8897 - val_loss: 3.1787 - val_mean_squared_error: 3.1787\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.9471 - mean_squared_error: 1.9471 - val_loss: 2.8330 - val_mean_squared_error: 2.8330\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.8183 - mean_squared_error: 1.8183 - val_loss: 2.9393 - val_mean_squared_error: 2.9393\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.8660 - mean_squared_error: 1.8660 - val_loss: 3.6948 - val_mean_squared_error: 3.6948\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.7897 - mean_squared_error: 1.7897 - val_loss: 3.4718 - val_mean_squared_error: 3.4718\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.8349 - mean_squared_error: 1.8349 - val_loss: 2.9405 - val_mean_squared_error: 2.9405\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.8318 - mean_squared_error: 1.8318 - val_loss: 2.8511 - val_mean_squared_error: 2.8511\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.7320 - mean_squared_error: 1.7320 - val_loss: 3.8205 - val_mean_squared_error: 3.8205\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.7853 - mean_squared_error: 1.7853 - val_loss: 3.0715 - val_mean_squared_error: 3.0715\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.7010 - mean_squared_error: 1.7010 - val_loss: 2.9513 - val_mean_squared_error: 2.9513\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.7290 - mean_squared_error: 1.7290 - val_loss: 3.2218 - val_mean_squared_error: 3.2218\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.7705 - mean_squared_error: 1.7705 - val_loss: 2.7534 - val_mean_squared_error: 2.7534\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.7899 - mean_squared_error: 1.7899 - val_loss: 3.4147 - val_mean_squared_error: 3.4147\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.8896 - mean_squared_error: 1.8896 - val_loss: 2.9387 - val_mean_squared_error: 2.9387\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.9830 - mean_squared_error: 1.9830 - val_loss: 3.3024 - val_mean_squared_error: 3.3024\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.8132 - mean_squared_error: 1.8132 - val_loss: 3.0557 - val_mean_squared_error: 3.0557\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.6499 - mean_squared_error: 1.6499 - val_loss: 2.9142 - val_mean_squared_error: 2.9142\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 1.6712 - mean_squared_error: 1.6712 - val_loss: 3.0751 - val_mean_squared_error: 3.0751\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.8679 - mean_squared_error: 1.8679 - val_loss: 2.8989 - val_mean_squared_error: 2.8989\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.6049 - mean_squared_error: 1.6049 - val_loss: 3.4986 - val_mean_squared_error: 3.4986\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.7137 - mean_squared_error: 1.7137 - val_loss: 2.6595 - val_mean_squared_error: 2.6595\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.5187 - mean_squared_error: 1.5187 - val_loss: 3.0428 - val_mean_squared_error: 3.0428\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.6015 - mean_squared_error: 1.6015 - val_loss: 2.8653 - val_mean_squared_error: 2.8653\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.6524 - mean_squared_error: 1.6524 - val_loss: 2.9059 - val_mean_squared_error: 2.9059\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7128 - mean_squared_error: 1.7128 - val_loss: 3.6661 - val_mean_squared_error: 3.6661\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5205 - mean_squared_error: 1.5205 - val_loss: 3.1394 - val_mean_squared_error: 3.1394\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.5453 - mean_squared_error: 1.5453 - val_loss: 2.9688 - val_mean_squared_error: 2.9688\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.4075 - mean_squared_error: 1.4075 - val_loss: 2.8151 - val_mean_squared_error: 2.8151\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.5210 - mean_squared_error: 1.5210 - val_loss: 2.8604 - val_mean_squared_error: 2.8604\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4971 - mean_squared_error: 1.4971 - val_loss: 2.7652 - val_mean_squared_error: 2.7652\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.4345 - mean_squared_error: 1.4345 - val_loss: 2.6474 - val_mean_squared_error: 2.6474\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.3443 - mean_squared_error: 1.3443 - val_loss: 2.9632 - val_mean_squared_error: 2.9632\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3613 - mean_squared_error: 1.3613 - val_loss: 2.9004 - val_mean_squared_error: 2.9004\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.4547 - mean_squared_error: 1.4547 - val_loss: 2.7684 - val_mean_squared_error: 2.7684\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.4979 - mean_squared_error: 1.4979 - val_loss: 2.6402 - val_mean_squared_error: 2.6402\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.4251 - mean_squared_error: 1.4251 - val_loss: 3.0483 - val_mean_squared_error: 3.0483\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.5185 - mean_squared_error: 1.5185 - val_loss: 2.8340 - val_mean_squared_error: 2.8340\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.3185 - mean_squared_error: 1.3185 - val_loss: 2.8643 - val_mean_squared_error: 2.8643\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.3889 - mean_squared_error: 1.3889 - val_loss: 2.9580 - val_mean_squared_error: 2.9580\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.3254 - mean_squared_error: 1.3254 - val_loss: 2.9765 - val_mean_squared_error: 2.9765\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.5492 - mean_squared_error: 1.5492 - val_loss: 3.0100 - val_mean_squared_error: 3.0100\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.3179 - mean_squared_error: 1.3179 - val_loss: 2.8567 - val_mean_squared_error: 2.8567\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.5238 - mean_squared_error: 1.5238 - val_loss: 3.4973 - val_mean_squared_error: 3.4973\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.5292 - mean_squared_error: 1.5292 - val_loss: 3.3228 - val_mean_squared_error: 3.3228\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.4371 - mean_squared_error: 1.4371 - val_loss: 3.3852 - val_mean_squared_error: 3.3852\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.3211 - mean_squared_error: 1.3211 - val_loss: 2.6537 - val_mean_squared_error: 2.6537\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.2340 - mean_squared_error: 1.2340 - val_loss: 2.7629 - val_mean_squared_error: 2.7629\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.2750 - mean_squared_error: 1.2750 - val_loss: 2.8138 - val_mean_squared_error: 2.8138\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.2638 - mean_squared_error: 1.2638 - val_loss: 2.6043 - val_mean_squared_error: 2.6043\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.2668 - mean_squared_error: 1.2668 - val_loss: 3.0289 - val_mean_squared_error: 3.0289\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3138 - mean_squared_error: 1.3138 - val_loss: 3.2353 - val_mean_squared_error: 3.2353\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.2992 - mean_squared_error: 1.2992 - val_loss: 2.9731 - val_mean_squared_error: 2.9731\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.1864 - mean_squared_error: 1.1864 - val_loss: 2.6431 - val_mean_squared_error: 2.6431\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.1581 - mean_squared_error: 1.1581 - val_loss: 2.6960 - val_mean_squared_error: 2.6960\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.2805 - mean_squared_error: 1.2805 - val_loss: 2.8545 - val_mean_squared_error: 2.8545\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.1871 - mean_squared_error: 1.1871 - val_loss: 2.7827 - val_mean_squared_error: 2.7827\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.1155 - mean_squared_error: 1.1155 - val_loss: 3.0073 - val_mean_squared_error: 3.0073\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.2522 - mean_squared_error: 1.2522 - val_loss: 2.6087 - val_mean_squared_error: 2.6087\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.3338 - mean_squared_error: 1.3338 - val_loss: 2.7257 - val_mean_squared_error: 2.7257\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.1174 - mean_squared_error: 1.1174 - val_loss: 2.7024 - val_mean_squared_error: 2.7024\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.1264 - mean_squared_error: 1.1264 - val_loss: 2.6662 - val_mean_squared_error: 2.6662\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.1048 - mean_squared_error: 1.1048 - val_loss: 2.6468 - val_mean_squared_error: 2.6468\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0453 - mean_squared_error: 1.0453 - val_loss: 2.5768 - val_mean_squared_error: 2.5768\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.1667 - mean_squared_error: 1.1667 - val_loss: 2.8576 - val_mean_squared_error: 2.8576\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.1253 - mean_squared_error: 1.1253 - val_loss: 2.6221 - val_mean_squared_error: 2.6221\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.1752 - mean_squared_error: 1.1752 - val_loss: 2.6077 - val_mean_squared_error: 2.6077\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.3141 - mean_squared_error: 1.3141 - val_loss: 2.8937 - val_mean_squared_error: 2.8937\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.2319 - mean_squared_error: 1.2319 - val_loss: 2.9501 - val_mean_squared_error: 2.9501\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.1383 - mean_squared_error: 1.1383 - val_loss: 2.8600 - val_mean_squared_error: 2.8600\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.2239 - mean_squared_error: 1.2239 - val_loss: 2.5676 - val_mean_squared_error: 2.5676\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.1185 - mean_squared_error: 1.1185 - val_loss: 2.6020 - val_mean_squared_error: 2.6020\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.0823 - mean_squared_error: 1.0823 - val_loss: 2.6541 - val_mean_squared_error: 2.6541\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.1237 - mean_squared_error: 1.1237 - val_loss: 2.4939 - val_mean_squared_error: 2.4939\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.9937 - mean_squared_error: 0.9937 - val_loss: 2.4448 - val_mean_squared_error: 2.4448\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.1005 - mean_squared_error: 1.1005 - val_loss: 2.7427 - val_mean_squared_error: 2.7427\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.0410 - mean_squared_error: 1.0410 - val_loss: 2.5636 - val_mean_squared_error: 2.5636\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.0891 - mean_squared_error: 1.0891 - val_loss: 2.5823 - val_mean_squared_error: 2.5823\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.0348 - mean_squared_error: 1.0348 - val_loss: 2.4897 - val_mean_squared_error: 2.4897\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.9419 - mean_squared_error: 0.9419 - val_loss: 2.8554 - val_mean_squared_error: 2.8554\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.0627 - mean_squared_error: 1.0627 - val_loss: 2.5111 - val_mean_squared_error: 2.5111\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.0216 - mean_squared_error: 1.0216 - val_loss: 2.4277 - val_mean_squared_error: 2.4277\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.0791 - mean_squared_error: 1.0791 - val_loss: 3.0716 - val_mean_squared_error: 3.0716\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.0480 - mean_squared_error: 1.0480 - val_loss: 2.6563 - val_mean_squared_error: 2.6563\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.9237 - mean_squared_error: 0.9237 - val_loss: 2.5239 - val_mean_squared_error: 2.5239\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.9733 - mean_squared_error: 0.9733 - val_loss: 2.5709 - val_mean_squared_error: 2.5709\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.0632 - mean_squared_error: 1.0632 - val_loss: 2.5520 - val_mean_squared_error: 2.5520\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.9922 - mean_squared_error: 0.9922 - val_loss: 2.5686 - val_mean_squared_error: 2.5686\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0278 - mean_squared_error: 1.0278 - val_loss: 2.6220 - val_mean_squared_error: 2.6220\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.2478 - mean_squared_error: 1.2478 - val_loss: 2.6088 - val_mean_squared_error: 2.6088\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 2.7655 - val_mean_squared_error: 2.7655\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.0255 - mean_squared_error: 1.0255 - val_loss: 2.4049 - val_mean_squared_error: 2.4049\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.9785 - mean_squared_error: 0.9785 - val_loss: 2.4843 - val_mean_squared_error: 2.4843\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9080 - mean_squared_error: 0.9080 - val_loss: 2.9515 - val_mean_squared_error: 2.9515\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.0244 - mean_squared_error: 1.0244 - val_loss: 2.5588 - val_mean_squared_error: 2.5588\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.9115 - mean_squared_error: 0.9115 - val_loss: 2.4771 - val_mean_squared_error: 2.4771\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.9632 - mean_squared_error: 0.9632 - val_loss: 2.5862 - val_mean_squared_error: 2.5862\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.1541 - mean_squared_error: 1.1541 - val_loss: 2.5334 - val_mean_squared_error: 2.5334\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 0.9549 - mean_squared_error: 0.9549 - val_loss: 2.4470 - val_mean_squared_error: 2.4470\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 0.8508 - mean_squared_error: 0.8508 - val_loss: 2.4415 - val_mean_squared_error: 2.4415\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 0.8647 - mean_squared_error: 0.8647 - val_loss: 2.5678 - val_mean_squared_error: 2.5678\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.8366 - mean_squared_error: 0.8366 - val_loss: 2.4085 - val_mean_squared_error: 2.4085\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.8947 - mean_squared_error: 0.8947 - val_loss: 2.5755 - val_mean_squared_error: 2.5755\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 2.4764 - val_mean_squared_error: 2.4764\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.9648 - mean_squared_error: 0.9648 - val_loss: 2.4910 - val_mean_squared_error: 2.4910\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 0.9453 - mean_squared_error: 0.9453 - val_loss: 2.4403 - val_mean_squared_error: 2.4403\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.8743 - mean_squared_error: 0.8743 - val_loss: 2.4570 - val_mean_squared_error: 2.4570\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.9713 - mean_squared_error: 0.9713 - val_loss: 2.3651 - val_mean_squared_error: 2.3651\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 0.8544 - mean_squared_error: 0.8544 - val_loss: 2.7420 - val_mean_squared_error: 2.7420\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.8888 - mean_squared_error: 0.8888 - val_loss: 2.4482 - val_mean_squared_error: 2.4482\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.8328 - mean_squared_error: 0.8328 - val_loss: 2.5783 - val_mean_squared_error: 2.5783\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 0.9088 - mean_squared_error: 0.9088 - val_loss: 2.4379 - val_mean_squared_error: 2.4379\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.8821 - mean_squared_error: 0.8821 - val_loss: 2.4559 - val_mean_squared_error: 2.4559\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 0.9110 - mean_squared_error: 0.9110 - val_loss: 2.6503 - val_mean_squared_error: 2.6503\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.7204 - mean_squared_error: 0.7204 - val_loss: 2.3517 - val_mean_squared_error: 2.3517\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.7614 - mean_squared_error: 0.7614 - val_loss: 2.3597 - val_mean_squared_error: 2.3597\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 0.8260 - mean_squared_error: 0.8260 - val_loss: 2.5155 - val_mean_squared_error: 2.5155\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.9024 - mean_squared_error: 0.9024 - val_loss: 2.4724 - val_mean_squared_error: 2.4724\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0622 - mean_squared_error: 1.0622 - val_loss: 2.7030 - val_mean_squared_error: 2.7030\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.8620 - mean_squared_error: 0.8620 - val_loss: 2.3594 - val_mean_squared_error: 2.3594\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.8540 - mean_squared_error: 0.8540 - val_loss: 2.4489 - val_mean_squared_error: 2.4489\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 0.8951 - mean_squared_error: 0.8951 - val_loss: 2.3678 - val_mean_squared_error: 2.3678\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.7774 - mean_squared_error: 0.7774 - val_loss: 2.5844 - val_mean_squared_error: 2.5844\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 0.8670 - mean_squared_error: 0.8670 - val_loss: 2.5780 - val_mean_squared_error: 2.5780\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.8875 - mean_squared_error: 0.8875 - val_loss: 2.9077 - val_mean_squared_error: 2.9077\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8862 - mean_squared_error: 0.8862 - val_loss: 2.6038 - val_mean_squared_error: 2.6038\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8175 - mean_squared_error: 0.8175 - val_loss: 2.4290 - val_mean_squared_error: 2.4290\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.8542 - mean_squared_error: 0.8542 - val_loss: 2.5618 - val_mean_squared_error: 2.5618\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.8578 - mean_squared_error: 0.8578 - val_loss: 2.3747 - val_mean_squared_error: 2.3747\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.9125 - mean_squared_error: 0.9125 - val_loss: 2.4424 - val_mean_squared_error: 2.4424\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.7717 - mean_squared_error: 0.7717 - val_loss: 2.5507 - val_mean_squared_error: 2.5507\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.7731 - mean_squared_error: 0.7731 - val_loss: 2.4202 - val_mean_squared_error: 2.4202\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 0.8248 - mean_squared_error: 0.8248 - val_loss: 2.7570 - val_mean_squared_error: 2.7570\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.7798 - mean_squared_error: 0.7798 - val_loss: 2.2949 - val_mean_squared_error: 2.2949\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.8396 - mean_squared_error: 0.8396 - val_loss: 2.2164 - val_mean_squared_error: 2.2164\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 0.7504 - mean_squared_error: 0.7504 - val_loss: 2.4139 - val_mean_squared_error: 2.4139\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.8763 - mean_squared_error: 0.8763 - val_loss: 2.4174 - val_mean_squared_error: 2.4174\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.8030 - mean_squared_error: 0.8030 - val_loss: 2.4663 - val_mean_squared_error: 2.4663\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 0.6851 - mean_squared_error: 0.6851 - val_loss: 2.4933 - val_mean_squared_error: 2.4933\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.7430 - mean_squared_error: 0.7430 - val_loss: 2.2772 - val_mean_squared_error: 2.2772\n",
            "==================================================\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_30 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_32 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 750.6943 - mean_squared_error: 750.6944 - val_loss: 176.1146 - val_mean_squared_error: 176.1146\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 31.9763 - mean_squared_error: 31.9763 - val_loss: 233.9745 - val_mean_squared_error: 233.9745\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 18.1991 - mean_squared_error: 18.1991 - val_loss: 98.2630 - val_mean_squared_error: 98.2630\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 20.4544 - mean_squared_error: 20.4545 - val_loss: 50.2282 - val_mean_squared_error: 50.2282\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 20.3456 - mean_squared_error: 20.3456 - val_loss: 56.1190 - val_mean_squared_error: 56.1190\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 17.0750 - mean_squared_error: 17.0750 - val_loss: 19.0127 - val_mean_squared_error: 19.0127\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 14.2384 - mean_squared_error: 14.2384 - val_loss: 13.6259 - val_mean_squared_error: 13.6259\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 14.7280 - mean_squared_error: 14.7280 - val_loss: 14.8727 - val_mean_squared_error: 14.8727\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 15.3908 - mean_squared_error: 15.3908 - val_loss: 15.4870 - val_mean_squared_error: 15.4870\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 14.5363 - mean_squared_error: 14.5363 - val_loss: 12.7658 - val_mean_squared_error: 12.7658\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 13.6531 - mean_squared_error: 13.6531 - val_loss: 11.7162 - val_mean_squared_error: 11.7162\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 12.1494 - mean_squared_error: 12.1494 - val_loss: 11.2893 - val_mean_squared_error: 11.2893\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 12.1299 - mean_squared_error: 12.1299 - val_loss: 12.0154 - val_mean_squared_error: 12.0154\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 11.1469 - mean_squared_error: 11.1469 - val_loss: 12.8172 - val_mean_squared_error: 12.8172\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 11.6853 - mean_squared_error: 11.6853 - val_loss: 12.7246 - val_mean_squared_error: 12.7246\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 11.3794 - mean_squared_error: 11.3794 - val_loss: 9.4222 - val_mean_squared_error: 9.4222\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 10.9480 - mean_squared_error: 10.9480 - val_loss: 16.1034 - val_mean_squared_error: 16.1034\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 9.7887 - mean_squared_error: 9.7887 - val_loss: 8.5678 - val_mean_squared_error: 8.5678\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 8.7524 - mean_squared_error: 8.7524 - val_loss: 8.0653 - val_mean_squared_error: 8.0653\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 8.3005 - mean_squared_error: 8.3005 - val_loss: 10.0111 - val_mean_squared_error: 10.0111\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 8.3456 - mean_squared_error: 8.3456 - val_loss: 13.0152 - val_mean_squared_error: 13.0152\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 7.4309 - mean_squared_error: 7.4309 - val_loss: 10.3828 - val_mean_squared_error: 10.3828\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 7.3927 - mean_squared_error: 7.3927 - val_loss: 7.6635 - val_mean_squared_error: 7.6635\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 6.8474 - mean_squared_error: 6.8474 - val_loss: 6.7520 - val_mean_squared_error: 6.7520\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 6.7235 - mean_squared_error: 6.7235 - val_loss: 7.3969 - val_mean_squared_error: 7.3969\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 5.9593 - mean_squared_error: 5.9593 - val_loss: 7.9972 - val_mean_squared_error: 7.9972\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 5.8299 - mean_squared_error: 5.8299 - val_loss: 6.0941 - val_mean_squared_error: 6.0941\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 5.3701 - mean_squared_error: 5.3701 - val_loss: 8.9710 - val_mean_squared_error: 8.9710\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 5.1579 - mean_squared_error: 5.1579 - val_loss: 5.8493 - val_mean_squared_error: 5.8493\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 5.1808 - mean_squared_error: 5.1808 - val_loss: 5.8991 - val_mean_squared_error: 5.8991\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 4.7682 - mean_squared_error: 4.7682 - val_loss: 5.7045 - val_mean_squared_error: 5.7045\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 4.4318 - mean_squared_error: 4.4318 - val_loss: 4.6154 - val_mean_squared_error: 4.6154\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 3.9792 - mean_squared_error: 3.9792 - val_loss: 4.1914 - val_mean_squared_error: 4.1914\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 4.1936 - mean_squared_error: 4.1936 - val_loss: 4.5211 - val_mean_squared_error: 4.5211\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 3.9655 - mean_squared_error: 3.9655 - val_loss: 4.6047 - val_mean_squared_error: 4.6047\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 3.8286 - mean_squared_error: 3.8286 - val_loss: 4.4305 - val_mean_squared_error: 4.4305\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 3.5757 - mean_squared_error: 3.5757 - val_loss: 6.7848 - val_mean_squared_error: 6.7848\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 3.5799 - mean_squared_error: 3.5799 - val_loss: 4.0555 - val_mean_squared_error: 4.0555\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 3.6203 - mean_squared_error: 3.6203 - val_loss: 4.2486 - val_mean_squared_error: 4.2486\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 3.1824 - mean_squared_error: 3.1824 - val_loss: 3.7674 - val_mean_squared_error: 3.7674\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 3.3912 - mean_squared_error: 3.3912 - val_loss: 3.8766 - val_mean_squared_error: 3.8766\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.9612 - mean_squared_error: 2.9612 - val_loss: 3.6261 - val_mean_squared_error: 3.6261\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.9158 - mean_squared_error: 2.9158 - val_loss: 3.4726 - val_mean_squared_error: 3.4726\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 2.8907 - mean_squared_error: 2.8907 - val_loss: 3.3452 - val_mean_squared_error: 3.3452\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.8657 - mean_squared_error: 2.8657 - val_loss: 3.8804 - val_mean_squared_error: 3.8804\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.9369 - mean_squared_error: 2.9369 - val_loss: 3.5152 - val_mean_squared_error: 3.5152\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.8168 - mean_squared_error: 2.8168 - val_loss: 3.5653 - val_mean_squared_error: 3.5653\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.7646 - mean_squared_error: 2.7646 - val_loss: 3.7598 - val_mean_squared_error: 3.7598\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.8305 - mean_squared_error: 2.8305 - val_loss: 3.6627 - val_mean_squared_error: 3.6627\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 3.0942 - mean_squared_error: 3.0942 - val_loss: 3.7171 - val_mean_squared_error: 3.7171\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.6746 - mean_squared_error: 2.6746 - val_loss: 3.4793 - val_mean_squared_error: 3.4793\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.5010 - mean_squared_error: 2.5010 - val_loss: 3.8379 - val_mean_squared_error: 3.8379\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.4536 - mean_squared_error: 2.4536 - val_loss: 3.3091 - val_mean_squared_error: 3.3091\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.6537 - mean_squared_error: 2.6537 - val_loss: 3.8125 - val_mean_squared_error: 3.8125\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.8818 - mean_squared_error: 2.8818 - val_loss: 3.8228 - val_mean_squared_error: 3.8228\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.5145 - mean_squared_error: 2.5145 - val_loss: 3.4522 - val_mean_squared_error: 3.4522\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.3661 - mean_squared_error: 2.3661 - val_loss: 3.1094 - val_mean_squared_error: 3.1094\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.3994 - mean_squared_error: 2.3994 - val_loss: 3.4409 - val_mean_squared_error: 3.4409\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.2156 - mean_squared_error: 2.2156 - val_loss: 3.3178 - val_mean_squared_error: 3.3178\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.3239 - mean_squared_error: 2.3239 - val_loss: 4.2366 - val_mean_squared_error: 4.2366\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.4764 - mean_squared_error: 2.4764 - val_loss: 3.6845 - val_mean_squared_error: 3.6845\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.4433 - mean_squared_error: 2.4433 - val_loss: 3.0879 - val_mean_squared_error: 3.0879\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.1560 - mean_squared_error: 2.1560 - val_loss: 2.9347 - val_mean_squared_error: 2.9347\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.2742 - mean_squared_error: 2.2742 - val_loss: 3.4926 - val_mean_squared_error: 3.4926\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.2382 - mean_squared_error: 2.2382 - val_loss: 3.1051 - val_mean_squared_error: 3.1051\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.9941 - mean_squared_error: 1.9941 - val_loss: 2.9807 - val_mean_squared_error: 2.9807\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.9949 - mean_squared_error: 1.9949 - val_loss: 3.8292 - val_mean_squared_error: 3.8292\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.9981 - mean_squared_error: 1.9981 - val_loss: 2.8437 - val_mean_squared_error: 2.8437\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.9591 - mean_squared_error: 1.9591 - val_loss: 2.7794 - val_mean_squared_error: 2.7794\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.0550 - mean_squared_error: 2.0550 - val_loss: 3.0465 - val_mean_squared_error: 3.0465\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.2000 - mean_squared_error: 2.2000 - val_loss: 3.5199 - val_mean_squared_error: 3.5199\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.1582 - mean_squared_error: 2.1582 - val_loss: 3.1654 - val_mean_squared_error: 3.1654\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.8890 - mean_squared_error: 1.8890 - val_loss: 3.0202 - val_mean_squared_error: 3.0202\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.9446 - mean_squared_error: 1.9446 - val_loss: 2.8342 - val_mean_squared_error: 2.8342\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.7367 - mean_squared_error: 1.7367 - val_loss: 2.6593 - val_mean_squared_error: 2.6593\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.8027 - mean_squared_error: 1.8027 - val_loss: 2.7215 - val_mean_squared_error: 2.7215\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.2481 - mean_squared_error: 2.2481 - val_loss: 3.3375 - val_mean_squared_error: 3.3375\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.9490 - mean_squared_error: 1.9490 - val_loss: 2.7256 - val_mean_squared_error: 2.7256\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.9035 - mean_squared_error: 1.9035 - val_loss: 3.0613 - val_mean_squared_error: 3.0613\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.9438 - mean_squared_error: 1.9438 - val_loss: 2.9958 - val_mean_squared_error: 2.9958\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.8891 - mean_squared_error: 1.8891 - val_loss: 3.1943 - val_mean_squared_error: 3.1943\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.9418 - mean_squared_error: 1.9418 - val_loss: 2.7360 - val_mean_squared_error: 2.7360\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.6541 - mean_squared_error: 1.6541 - val_loss: 2.6830 - val_mean_squared_error: 2.6830\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.5975 - mean_squared_error: 1.5975 - val_loss: 2.5917 - val_mean_squared_error: 2.5917\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.8404 - mean_squared_error: 1.8404 - val_loss: 3.1119 - val_mean_squared_error: 3.1119\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.9240 - mean_squared_error: 1.9240 - val_loss: 3.0878 - val_mean_squared_error: 3.0878\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.8054 - mean_squared_error: 1.8054 - val_loss: 2.9033 - val_mean_squared_error: 2.9033\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.6529 - mean_squared_error: 1.6529 - val_loss: 2.7315 - val_mean_squared_error: 2.7315\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.7335 - mean_squared_error: 1.7335 - val_loss: 2.9087 - val_mean_squared_error: 2.9087\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.7098 - mean_squared_error: 1.7098 - val_loss: 2.5153 - val_mean_squared_error: 2.5153\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.8115 - mean_squared_error: 1.8115 - val_loss: 3.1397 - val_mean_squared_error: 3.1397\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6536 - mean_squared_error: 1.6536 - val_loss: 2.8593 - val_mean_squared_error: 2.8593\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.6130 - mean_squared_error: 1.6130 - val_loss: 2.9574 - val_mean_squared_error: 2.9574\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.6144 - mean_squared_error: 1.6144 - val_loss: 3.0395 - val_mean_squared_error: 3.0395\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4584 - mean_squared_error: 1.4584 - val_loss: 2.8315 - val_mean_squared_error: 2.8315\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.4975 - mean_squared_error: 1.4975 - val_loss: 2.9186 - val_mean_squared_error: 2.9186\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.4890 - mean_squared_error: 1.4890 - val_loss: 2.7207 - val_mean_squared_error: 2.7207\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.5147 - mean_squared_error: 1.5147 - val_loss: 2.7231 - val_mean_squared_error: 2.7231\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.5591 - mean_squared_error: 1.5591 - val_loss: 2.7392 - val_mean_squared_error: 2.7392\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.5636 - mean_squared_error: 1.5636 - val_loss: 2.7766 - val_mean_squared_error: 2.7766\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.5026 - mean_squared_error: 1.5026 - val_loss: 2.5005 - val_mean_squared_error: 2.5005\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.6089 - mean_squared_error: 1.6089 - val_loss: 2.7773 - val_mean_squared_error: 2.7773\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.5849 - mean_squared_error: 1.5849 - val_loss: 2.6952 - val_mean_squared_error: 2.6952\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.5442 - mean_squared_error: 1.5442 - val_loss: 2.5356 - val_mean_squared_error: 2.5356\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.4331 - mean_squared_error: 1.4331 - val_loss: 2.6302 - val_mean_squared_error: 2.6302\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.4555 - mean_squared_error: 1.4555 - val_loss: 2.4996 - val_mean_squared_error: 2.4996\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.5254 - mean_squared_error: 1.5254 - val_loss: 2.8262 - val_mean_squared_error: 2.8262\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.4580 - mean_squared_error: 1.4580 - val_loss: 2.7809 - val_mean_squared_error: 2.7809\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4513 - mean_squared_error: 1.4513 - val_loss: 2.5539 - val_mean_squared_error: 2.5539\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.4405 - mean_squared_error: 1.4405 - val_loss: 2.7891 - val_mean_squared_error: 2.7891\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.4532 - mean_squared_error: 1.4532 - val_loss: 3.3313 - val_mean_squared_error: 3.3313\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.4729 - mean_squared_error: 1.4729 - val_loss: 2.4221 - val_mean_squared_error: 2.4221\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.3787 - mean_squared_error: 1.3787 - val_loss: 2.6683 - val_mean_squared_error: 2.6683\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.2583 - mean_squared_error: 1.2583 - val_loss: 2.4175 - val_mean_squared_error: 2.4175\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.4094 - mean_squared_error: 1.4094 - val_loss: 2.5933 - val_mean_squared_error: 2.5933\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4592 - mean_squared_error: 1.4592 - val_loss: 2.8263 - val_mean_squared_error: 2.8263\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.2504 - mean_squared_error: 1.2504 - val_loss: 2.5188 - val_mean_squared_error: 2.5188\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.4698 - mean_squared_error: 1.4698 - val_loss: 2.4461 - val_mean_squared_error: 2.4461\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.2797 - mean_squared_error: 1.2797 - val_loss: 2.4640 - val_mean_squared_error: 2.4640\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.3551 - mean_squared_error: 1.3551 - val_loss: 2.6199 - val_mean_squared_error: 2.6199\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4022 - mean_squared_error: 1.4022 - val_loss: 2.6469 - val_mean_squared_error: 2.6469\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.3938 - mean_squared_error: 1.3938 - val_loss: 2.7313 - val_mean_squared_error: 2.7313\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3724 - mean_squared_error: 1.3724 - val_loss: 2.3712 - val_mean_squared_error: 2.3712\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2034 - mean_squared_error: 1.2034 - val_loss: 2.4954 - val_mean_squared_error: 2.4954\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.1817 - mean_squared_error: 1.1817 - val_loss: 2.5777 - val_mean_squared_error: 2.5777\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3118 - mean_squared_error: 1.3118 - val_loss: 2.5267 - val_mean_squared_error: 2.5267\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.2214 - mean_squared_error: 1.2214 - val_loss: 2.7852 - val_mean_squared_error: 2.7852\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.3229 - mean_squared_error: 1.3229 - val_loss: 2.6409 - val_mean_squared_error: 2.6409\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.3282 - mean_squared_error: 1.3282 - val_loss: 2.9359 - val_mean_squared_error: 2.9359\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.3115 - mean_squared_error: 1.3115 - val_loss: 2.5067 - val_mean_squared_error: 2.5067\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.4514 - mean_squared_error: 1.4514 - val_loss: 2.5670 - val_mean_squared_error: 2.5670\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.2080 - mean_squared_error: 1.2080 - val_loss: 2.5741 - val_mean_squared_error: 2.5741\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.3065 - mean_squared_error: 1.3065 - val_loss: 2.4030 - val_mean_squared_error: 2.4030\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.2625 - mean_squared_error: 1.2625 - val_loss: 2.3700 - val_mean_squared_error: 2.3700\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.2114 - mean_squared_error: 1.2114 - val_loss: 2.5192 - val_mean_squared_error: 2.5192\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.2012 - mean_squared_error: 1.2012 - val_loss: 2.3506 - val_mean_squared_error: 2.3506\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1467 - mean_squared_error: 1.1467 - val_loss: 2.5031 - val_mean_squared_error: 2.5031\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.2745 - mean_squared_error: 1.2745 - val_loss: 2.4887 - val_mean_squared_error: 2.4887\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.2040 - mean_squared_error: 1.2040 - val_loss: 2.3739 - val_mean_squared_error: 2.3739\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0819 - mean_squared_error: 1.0819 - val_loss: 2.3062 - val_mean_squared_error: 2.3062\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.1411 - mean_squared_error: 1.1411 - val_loss: 2.3958 - val_mean_squared_error: 2.3958\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.1211 - mean_squared_error: 1.1211 - val_loss: 2.5983 - val_mean_squared_error: 2.5983\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.1631 - mean_squared_error: 1.1631 - val_loss: 2.4917 - val_mean_squared_error: 2.4917\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.1725 - mean_squared_error: 1.1725 - val_loss: 2.1739 - val_mean_squared_error: 2.1739\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.0805 - mean_squared_error: 1.0805 - val_loss: 2.3133 - val_mean_squared_error: 2.3133\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 2.3483 - val_mean_squared_error: 2.3483\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.0795 - mean_squared_error: 1.0795 - val_loss: 2.7468 - val_mean_squared_error: 2.7469\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1853 - mean_squared_error: 1.1853 - val_loss: 2.4038 - val_mean_squared_error: 2.4038\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1414 - mean_squared_error: 1.1414 - val_loss: 2.4798 - val_mean_squared_error: 2.4798\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.0354 - mean_squared_error: 1.0354 - val_loss: 2.6224 - val_mean_squared_error: 2.6224\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.1260 - mean_squared_error: 1.1260 - val_loss: 2.7559 - val_mean_squared_error: 2.7559\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0979 - mean_squared_error: 1.0979 - val_loss: 2.5160 - val_mean_squared_error: 2.5160\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0765 - mean_squared_error: 1.0765 - val_loss: 2.2306 - val_mean_squared_error: 2.2306\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.1135 - mean_squared_error: 1.1135 - val_loss: 2.4810 - val_mean_squared_error: 2.4810\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0546 - mean_squared_error: 1.0546 - val_loss: 2.3302 - val_mean_squared_error: 2.3302\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.9824 - mean_squared_error: 0.9824 - val_loss: 2.1250 - val_mean_squared_error: 2.1250\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.9627 - mean_squared_error: 0.9627 - val_loss: 2.2449 - val_mean_squared_error: 2.2449\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.0564 - mean_squared_error: 1.0564 - val_loss: 2.6057 - val_mean_squared_error: 2.6057\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 2.5537 - val_mean_squared_error: 2.5537\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0426 - mean_squared_error: 1.0426 - val_loss: 2.4166 - val_mean_squared_error: 2.4166\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.0749 - mean_squared_error: 1.0749 - val_loss: 2.4403 - val_mean_squared_error: 2.4403\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1465 - mean_squared_error: 1.1465 - val_loss: 2.3266 - val_mean_squared_error: 2.3266\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1381 - mean_squared_error: 1.1381 - val_loss: 2.4487 - val_mean_squared_error: 2.4487\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.1148 - mean_squared_error: 1.1148 - val_loss: 2.2685 - val_mean_squared_error: 2.2685\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9501 - mean_squared_error: 0.9501 - val_loss: 2.4012 - val_mean_squared_error: 2.4012\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 2.2030 - val_mean_squared_error: 2.2030\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.9416 - mean_squared_error: 0.9416 - val_loss: 2.2382 - val_mean_squared_error: 2.2382\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.9354 - mean_squared_error: 0.9354 - val_loss: 2.3631 - val_mean_squared_error: 2.3631\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9508 - mean_squared_error: 0.9508 - val_loss: 2.2576 - val_mean_squared_error: 2.2576\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0628 - mean_squared_error: 1.0628 - val_loss: 2.5812 - val_mean_squared_error: 2.5812\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.9478 - mean_squared_error: 0.9478 - val_loss: 2.2796 - val_mean_squared_error: 2.2796\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9964 - mean_squared_error: 0.9964 - val_loss: 2.5090 - val_mean_squared_error: 2.5090\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.0527 - mean_squared_error: 1.0527 - val_loss: 2.4610 - val_mean_squared_error: 2.4610\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9604 - mean_squared_error: 0.9604 - val_loss: 2.5280 - val_mean_squared_error: 2.5280\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.9446 - mean_squared_error: 0.9446 - val_loss: 2.2863 - val_mean_squared_error: 2.2863\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 0.9984 - mean_squared_error: 0.9984 - val_loss: 2.2232 - val_mean_squared_error: 2.2232\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.8621 - mean_squared_error: 0.8621 - val_loss: 2.1915 - val_mean_squared_error: 2.1915\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 0.8990 - mean_squared_error: 0.8990 - val_loss: 2.2157 - val_mean_squared_error: 2.2157\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.8700 - mean_squared_error: 0.8700 - val_loss: 2.1891 - val_mean_squared_error: 2.1891\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.9445 - mean_squared_error: 0.9445 - val_loss: 2.3553 - val_mean_squared_error: 2.3553\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 2.7447 - val_mean_squared_error: 2.7447\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.9512 - mean_squared_error: 0.9512 - val_loss: 2.1259 - val_mean_squared_error: 2.1259\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.8616 - mean_squared_error: 0.8616 - val_loss: 2.1555 - val_mean_squared_error: 2.1555\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.9247 - mean_squared_error: 0.9247 - val_loss: 2.2156 - val_mean_squared_error: 2.2156\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9498 - mean_squared_error: 0.9498 - val_loss: 2.3529 - val_mean_squared_error: 2.3529\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9424 - mean_squared_error: 0.9424 - val_loss: 2.2930 - val_mean_squared_error: 2.2930\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.8101 - mean_squared_error: 0.8101 - val_loss: 2.4248 - val_mean_squared_error: 2.4248\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.8882 - mean_squared_error: 0.8882 - val_loss: 2.3728 - val_mean_squared_error: 2.3728\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.9317 - mean_squared_error: 0.9317 - val_loss: 2.5592 - val_mean_squared_error: 2.5592\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.8729 - mean_squared_error: 0.8729 - val_loss: 2.4841 - val_mean_squared_error: 2.4841\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.9211 - mean_squared_error: 0.9211 - val_loss: 2.3825 - val_mean_squared_error: 2.3825\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.9133 - mean_squared_error: 0.9133 - val_loss: 2.2487 - val_mean_squared_error: 2.2487\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.9049 - mean_squared_error: 0.9049 - val_loss: 2.1880 - val_mean_squared_error: 2.1880\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.9042 - mean_squared_error: 0.9042 - val_loss: 2.4900 - val_mean_squared_error: 2.4900\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9154 - mean_squared_error: 0.9154 - val_loss: 2.2577 - val_mean_squared_error: 2.2577\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.8228 - mean_squared_error: 0.8228 - val_loss: 2.3956 - val_mean_squared_error: 2.3956\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.9032 - mean_squared_error: 0.9032 - val_loss: 2.1554 - val_mean_squared_error: 2.1554\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.7802 - mean_squared_error: 0.7802 - val_loss: 2.3113 - val_mean_squared_error: 2.3113\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8511 - mean_squared_error: 0.8511 - val_loss: 2.2460 - val_mean_squared_error: 2.2460\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.8208 - mean_squared_error: 0.8208 - val_loss: 2.2457 - val_mean_squared_error: 2.2457\n",
            "==================================================\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_33 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_34 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_57 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_35 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_58 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_59 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 752.1986 - mean_squared_error: 752.1985 - val_loss: 534.1517 - val_mean_squared_error: 534.1518\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 24.3845 - mean_squared_error: 24.3845 - val_loss: 248.4816 - val_mean_squared_error: 248.4816\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 19.8738 - mean_squared_error: 19.8738 - val_loss: 98.9694 - val_mean_squared_error: 98.9694\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 16.7640 - mean_squared_error: 16.7640 - val_loss: 54.1145 - val_mean_squared_error: 54.1145\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 16.8459 - mean_squared_error: 16.8459 - val_loss: 34.9952 - val_mean_squared_error: 34.9952\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 14.7466 - mean_squared_error: 14.7466 - val_loss: 21.0725 - val_mean_squared_error: 21.0725\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 15.6489 - mean_squared_error: 15.6489 - val_loss: 16.1047 - val_mean_squared_error: 16.1047\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 14.3627 - mean_squared_error: 14.3627 - val_loss: 14.6660 - val_mean_squared_error: 14.6660\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 12.4579 - mean_squared_error: 12.4579 - val_loss: 12.5658 - val_mean_squared_error: 12.5658\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 12.4075 - mean_squared_error: 12.4075 - val_loss: 11.0527 - val_mean_squared_error: 11.0527\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 12.6290 - mean_squared_error: 12.6290 - val_loss: 13.2095 - val_mean_squared_error: 13.2095\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 11.1727 - mean_squared_error: 11.1727 - val_loss: 10.0876 - val_mean_squared_error: 10.0876\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 10.3542 - mean_squared_error: 10.3542 - val_loss: 9.7482 - val_mean_squared_error: 9.7482\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 10.2630 - mean_squared_error: 10.2630 - val_loss: 10.6784 - val_mean_squared_error: 10.6784\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 10.5202 - mean_squared_error: 10.5202 - val_loss: 10.2482 - val_mean_squared_error: 10.2482\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 9.9334 - mean_squared_error: 9.9334 - val_loss: 9.4758 - val_mean_squared_error: 9.4758\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 9.0175 - mean_squared_error: 9.0175 - val_loss: 7.9077 - val_mean_squared_error: 7.9077\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 8.0286 - mean_squared_error: 8.0286 - val_loss: 8.2970 - val_mean_squared_error: 8.2970\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 8.1455 - mean_squared_error: 8.1455 - val_loss: 9.4227 - val_mean_squared_error: 9.4227\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 7.6024 - mean_squared_error: 7.6024 - val_loss: 9.2771 - val_mean_squared_error: 9.2771\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 7.7661 - mean_squared_error: 7.7661 - val_loss: 6.4719 - val_mean_squared_error: 6.4719\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 6.8875 - mean_squared_error: 6.8875 - val_loss: 6.4349 - val_mean_squared_error: 6.4349\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 6.2862 - mean_squared_error: 6.2862 - val_loss: 5.6345 - val_mean_squared_error: 5.6345\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 5.4928 - mean_squared_error: 5.4928 - val_loss: 6.3206 - val_mean_squared_error: 6.3206\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 5.5448 - mean_squared_error: 5.5448 - val_loss: 6.9210 - val_mean_squared_error: 6.9210\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 5.6824 - mean_squared_error: 5.6824 - val_loss: 7.3765 - val_mean_squared_error: 7.3765\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 5.0526 - mean_squared_error: 5.0526 - val_loss: 6.2780 - val_mean_squared_error: 6.2780\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 5.3514 - mean_squared_error: 5.3514 - val_loss: 4.7147 - val_mean_squared_error: 4.7147\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 4.5073 - mean_squared_error: 4.5073 - val_loss: 4.5604 - val_mean_squared_error: 4.5604\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 4.3030 - mean_squared_error: 4.3030 - val_loss: 4.2244 - val_mean_squared_error: 4.2244\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 4.0508 - mean_squared_error: 4.0508 - val_loss: 4.9321 - val_mean_squared_error: 4.9321\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.8754 - mean_squared_error: 3.8754 - val_loss: 3.9640 - val_mean_squared_error: 3.9640\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 4.2685 - mean_squared_error: 4.2685 - val_loss: 3.9723 - val_mean_squared_error: 3.9723\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 3.7145 - mean_squared_error: 3.7145 - val_loss: 4.5816 - val_mean_squared_error: 4.5816\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.6295 - mean_squared_error: 3.6295 - val_loss: 3.8054 - val_mean_squared_error: 3.8054\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 3.6687 - mean_squared_error: 3.6687 - val_loss: 4.9714 - val_mean_squared_error: 4.9714\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 3.3654 - mean_squared_error: 3.3654 - val_loss: 4.4141 - val_mean_squared_error: 4.4141\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 3.5009 - mean_squared_error: 3.5009 - val_loss: 3.5451 - val_mean_squared_error: 3.5451\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 3.2190 - mean_squared_error: 3.2190 - val_loss: 3.4112 - val_mean_squared_error: 3.4112\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.2970 - mean_squared_error: 3.2970 - val_loss: 3.1970 - val_mean_squared_error: 3.1970\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 3.2285 - mean_squared_error: 3.2285 - val_loss: 4.0807 - val_mean_squared_error: 4.0807\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 3.0722 - mean_squared_error: 3.0722 - val_loss: 4.7109 - val_mean_squared_error: 4.7109\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 3.0998 - mean_squared_error: 3.0998 - val_loss: 3.2946 - val_mean_squared_error: 3.2946\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 3.2267 - mean_squared_error: 3.2267 - val_loss: 3.6321 - val_mean_squared_error: 3.6321\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 3.1035 - mean_squared_error: 3.1035 - val_loss: 3.3005 - val_mean_squared_error: 3.3005\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.1403 - mean_squared_error: 3.1403 - val_loss: 3.1860 - val_mean_squared_error: 3.1860\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.8081 - mean_squared_error: 2.8081 - val_loss: 3.1400 - val_mean_squared_error: 3.1400\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 2.6955 - mean_squared_error: 2.6955 - val_loss: 3.2461 - val_mean_squared_error: 3.2461\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.6811 - mean_squared_error: 2.6811 - val_loss: 3.6074 - val_mean_squared_error: 3.6074\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.7350 - mean_squared_error: 2.7350 - val_loss: 2.9631 - val_mean_squared_error: 2.9631\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 2.8626 - mean_squared_error: 2.8626 - val_loss: 3.2306 - val_mean_squared_error: 3.2306\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.6092 - mean_squared_error: 2.6092 - val_loss: 3.5948 - val_mean_squared_error: 3.5948\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.6041 - mean_squared_error: 2.6041 - val_loss: 3.1378 - val_mean_squared_error: 3.1378\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.4501 - mean_squared_error: 2.4501 - val_loss: 3.0456 - val_mean_squared_error: 3.0456\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.6547 - mean_squared_error: 2.6547 - val_loss: 3.4038 - val_mean_squared_error: 3.4038\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.8570 - mean_squared_error: 2.8570 - val_loss: 3.5684 - val_mean_squared_error: 3.5684\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.6893 - mean_squared_error: 2.6893 - val_loss: 4.9947 - val_mean_squared_error: 4.9947\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.6713 - mean_squared_error: 2.6713 - val_loss: 3.9542 - val_mean_squared_error: 3.9542\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.6668 - mean_squared_error: 2.6668 - val_loss: 3.2717 - val_mean_squared_error: 3.2717\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.6563 - mean_squared_error: 2.6563 - val_loss: 3.2141 - val_mean_squared_error: 3.2141\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.3759 - mean_squared_error: 2.3759 - val_loss: 3.0971 - val_mean_squared_error: 3.0971\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.2362 - mean_squared_error: 2.2362 - val_loss: 3.0688 - val_mean_squared_error: 3.0688\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.1922 - mean_squared_error: 2.1922 - val_loss: 2.9677 - val_mean_squared_error: 2.9677\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.3284 - mean_squared_error: 2.3284 - val_loss: 3.2215 - val_mean_squared_error: 3.2215\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.1304 - mean_squared_error: 2.1304 - val_loss: 3.0090 - val_mean_squared_error: 3.0090\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.3140 - mean_squared_error: 2.3140 - val_loss: 4.2960 - val_mean_squared_error: 4.2960\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.2229 - mean_squared_error: 2.2229 - val_loss: 3.0986 - val_mean_squared_error: 3.0986\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.0881 - mean_squared_error: 2.0881 - val_loss: 2.7570 - val_mean_squared_error: 2.7570\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.1357 - mean_squared_error: 2.1357 - val_loss: 2.7784 - val_mean_squared_error: 2.7784\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.1188 - mean_squared_error: 2.1188 - val_loss: 3.0322 - val_mean_squared_error: 3.0322\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.1576 - mean_squared_error: 2.1576 - val_loss: 3.0662 - val_mean_squared_error: 3.0661\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.1658 - mean_squared_error: 2.1658 - val_loss: 3.1087 - val_mean_squared_error: 3.1087\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 2.2649 - mean_squared_error: 2.2649 - val_loss: 2.9215 - val_mean_squared_error: 2.9215\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.2024 - mean_squared_error: 2.2024 - val_loss: 2.7287 - val_mean_squared_error: 2.7287\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.0518 - mean_squared_error: 2.0518 - val_loss: 2.5431 - val_mean_squared_error: 2.5431\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.8179 - mean_squared_error: 1.8179 - val_loss: 2.4742 - val_mean_squared_error: 2.4742\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.8153 - mean_squared_error: 1.8153 - val_loss: 2.8756 - val_mean_squared_error: 2.8756\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.9486 - mean_squared_error: 1.9486 - val_loss: 2.5526 - val_mean_squared_error: 2.5526\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.0740 - mean_squared_error: 2.0740 - val_loss: 3.1856 - val_mean_squared_error: 3.1856\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.9375 - mean_squared_error: 1.9375 - val_loss: 2.7138 - val_mean_squared_error: 2.7138\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.0821 - mean_squared_error: 2.0821 - val_loss: 2.8163 - val_mean_squared_error: 2.8163\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.8090 - mean_squared_error: 1.8090 - val_loss: 2.9758 - val_mean_squared_error: 2.9758\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.3308 - mean_squared_error: 2.3308 - val_loss: 2.9677 - val_mean_squared_error: 2.9677\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.9497 - mean_squared_error: 1.9497 - val_loss: 2.7577 - val_mean_squared_error: 2.7577\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.8596 - mean_squared_error: 1.8596 - val_loss: 2.4212 - val_mean_squared_error: 2.4212\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.7618 - mean_squared_error: 1.7618 - val_loss: 2.6277 - val_mean_squared_error: 2.6277\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.9479 - mean_squared_error: 1.9479 - val_loss: 2.6791 - val_mean_squared_error: 2.6791\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.9175 - mean_squared_error: 1.9175 - val_loss: 2.7412 - val_mean_squared_error: 2.7412\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.8554 - mean_squared_error: 1.8554 - val_loss: 2.7587 - val_mean_squared_error: 2.7587\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.9116 - mean_squared_error: 1.9116 - val_loss: 2.4971 - val_mean_squared_error: 2.4971\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.6563 - mean_squared_error: 1.6563 - val_loss: 2.8362 - val_mean_squared_error: 2.8362\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.7758 - mean_squared_error: 1.7758 - val_loss: 2.8262 - val_mean_squared_error: 2.8262\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.7134 - mean_squared_error: 1.7134 - val_loss: 3.5328 - val_mean_squared_error: 3.5328\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.7982 - mean_squared_error: 1.7982 - val_loss: 2.4426 - val_mean_squared_error: 2.4426\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.7507 - mean_squared_error: 1.7507 - val_loss: 3.0024 - val_mean_squared_error: 3.0024\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.8164 - mean_squared_error: 1.8164 - val_loss: 2.7491 - val_mean_squared_error: 2.7491\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.6892 - mean_squared_error: 1.6892 - val_loss: 3.1048 - val_mean_squared_error: 3.1048\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.8877 - mean_squared_error: 1.8877 - val_loss: 2.4411 - val_mean_squared_error: 2.4411\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.6656 - mean_squared_error: 1.6656 - val_loss: 2.8577 - val_mean_squared_error: 2.8577\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.6569 - mean_squared_error: 1.6569 - val_loss: 2.4723 - val_mean_squared_error: 2.4723\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.6613 - mean_squared_error: 1.6613 - val_loss: 2.4679 - val_mean_squared_error: 2.4679\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.5090 - mean_squared_error: 1.5090 - val_loss: 2.8371 - val_mean_squared_error: 2.8371\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.8839 - mean_squared_error: 1.8839 - val_loss: 2.7366 - val_mean_squared_error: 2.7366\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.6416 - mean_squared_error: 1.6416 - val_loss: 2.3460 - val_mean_squared_error: 2.3460\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.9036 - mean_squared_error: 1.9036 - val_loss: 2.6199 - val_mean_squared_error: 2.6199\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6189 - mean_squared_error: 1.6189 - val_loss: 4.2692 - val_mean_squared_error: 4.2692\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.5758 - mean_squared_error: 1.5758 - val_loss: 2.2797 - val_mean_squared_error: 2.2797\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.5486 - mean_squared_error: 1.5486 - val_loss: 2.4350 - val_mean_squared_error: 2.4350\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.5136 - mean_squared_error: 1.5136 - val_loss: 2.3951 - val_mean_squared_error: 2.3951\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.6169 - mean_squared_error: 1.6169 - val_loss: 2.5829 - val_mean_squared_error: 2.5829\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.5414 - mean_squared_error: 1.5414 - val_loss: 2.6355 - val_mean_squared_error: 2.6355\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.5867 - mean_squared_error: 1.5867 - val_loss: 2.2788 - val_mean_squared_error: 2.2788\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.4014 - mean_squared_error: 1.4014 - val_loss: 2.3455 - val_mean_squared_error: 2.3455\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.4817 - mean_squared_error: 1.4817 - val_loss: 2.3173 - val_mean_squared_error: 2.3173\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.3983 - mean_squared_error: 1.3983 - val_loss: 3.4181 - val_mean_squared_error: 3.4181\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.5110 - mean_squared_error: 1.5110 - val_loss: 2.3870 - val_mean_squared_error: 2.3870\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4351 - mean_squared_error: 1.4351 - val_loss: 2.4293 - val_mean_squared_error: 2.4293\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.6235 - mean_squared_error: 1.6235 - val_loss: 2.4864 - val_mean_squared_error: 2.4864\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.4229 - mean_squared_error: 1.4229 - val_loss: 2.6238 - val_mean_squared_error: 2.6238\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.4823 - mean_squared_error: 1.4823 - val_loss: 2.3457 - val_mean_squared_error: 2.3457\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.3169 - mean_squared_error: 1.3169 - val_loss: 2.2587 - val_mean_squared_error: 2.2587\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.4494 - mean_squared_error: 1.4494 - val_loss: 2.2011 - val_mean_squared_error: 2.2011\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.3293 - mean_squared_error: 1.3293 - val_loss: 2.3077 - val_mean_squared_error: 2.3077\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.3495 - mean_squared_error: 1.3495 - val_loss: 2.5291 - val_mean_squared_error: 2.5291\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.3434 - mean_squared_error: 1.3434 - val_loss: 2.1875 - val_mean_squared_error: 2.1875\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.4021 - mean_squared_error: 1.4021 - val_loss: 2.3828 - val_mean_squared_error: 2.3828\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.2606 - mean_squared_error: 1.2606 - val_loss: 2.1962 - val_mean_squared_error: 2.1962\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.3369 - mean_squared_error: 1.3369 - val_loss: 2.4195 - val_mean_squared_error: 2.4195\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.3881 - mean_squared_error: 1.3881 - val_loss: 2.3533 - val_mean_squared_error: 2.3533\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.4829 - mean_squared_error: 1.4829 - val_loss: 3.0559 - val_mean_squared_error: 3.0559\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.2455 - mean_squared_error: 1.2455 - val_loss: 2.1961 - val_mean_squared_error: 2.1961\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.2759 - mean_squared_error: 1.2759 - val_loss: 2.4746 - val_mean_squared_error: 2.4746\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.3192 - mean_squared_error: 1.3192 - val_loss: 2.5398 - val_mean_squared_error: 2.5398\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.3562 - mean_squared_error: 1.3562 - val_loss: 2.2581 - val_mean_squared_error: 2.2581\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.1850 - mean_squared_error: 1.1850 - val_loss: 2.5764 - val_mean_squared_error: 2.5764\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.2066 - mean_squared_error: 1.2066 - val_loss: 2.6194 - val_mean_squared_error: 2.6194\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.2501 - mean_squared_error: 1.2501 - val_loss: 2.2233 - val_mean_squared_error: 2.2233\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.2946 - mean_squared_error: 1.2946 - val_loss: 2.7840 - val_mean_squared_error: 2.7840\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3935 - mean_squared_error: 1.3935 - val_loss: 2.2108 - val_mean_squared_error: 2.2108\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3284 - mean_squared_error: 1.3284 - val_loss: 2.3022 - val_mean_squared_error: 2.3022\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.2520 - mean_squared_error: 1.2520 - val_loss: 2.3099 - val_mean_squared_error: 2.3099\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.3061 - mean_squared_error: 1.3061 - val_loss: 2.1250 - val_mean_squared_error: 2.1250\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.3170 - mean_squared_error: 1.3170 - val_loss: 2.1705 - val_mean_squared_error: 2.1705\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2321 - mean_squared_error: 1.2321 - val_loss: 2.3150 - val_mean_squared_error: 2.3150\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1603 - mean_squared_error: 1.1603 - val_loss: 2.1484 - val_mean_squared_error: 2.1484\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.1962 - mean_squared_error: 1.1962 - val_loss: 2.3739 - val_mean_squared_error: 2.3739\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.3321 - mean_squared_error: 1.3321 - val_loss: 2.3498 - val_mean_squared_error: 2.3498\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2715 - mean_squared_error: 1.2715 - val_loss: 2.3346 - val_mean_squared_error: 2.3346\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.2357 - mean_squared_error: 1.2357 - val_loss: 2.1247 - val_mean_squared_error: 2.1247\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2748 - mean_squared_error: 1.2748 - val_loss: 2.3861 - val_mean_squared_error: 2.3861\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2475 - mean_squared_error: 1.2475 - val_loss: 2.5283 - val_mean_squared_error: 2.5283\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.1414 - mean_squared_error: 1.1414 - val_loss: 2.2124 - val_mean_squared_error: 2.2124\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2894 - mean_squared_error: 1.2894 - val_loss: 2.4712 - val_mean_squared_error: 2.4712\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.1821 - mean_squared_error: 1.1821 - val_loss: 2.2930 - val_mean_squared_error: 2.2930\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.1155 - mean_squared_error: 1.1155 - val_loss: 2.1339 - val_mean_squared_error: 2.1339\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.1553 - mean_squared_error: 1.1553 - val_loss: 2.2563 - val_mean_squared_error: 2.2563\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2439 - mean_squared_error: 1.2439 - val_loss: 2.1525 - val_mean_squared_error: 2.1525\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1279 - mean_squared_error: 1.1279 - val_loss: 2.2014 - val_mean_squared_error: 2.2014\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.2116 - mean_squared_error: 1.2116 - val_loss: 2.2219 - val_mean_squared_error: 2.2219\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.1376 - mean_squared_error: 1.1376 - val_loss: 2.3575 - val_mean_squared_error: 2.3575\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2018 - mean_squared_error: 1.2018 - val_loss: 2.1707 - val_mean_squared_error: 2.1707\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.1327 - mean_squared_error: 1.1327 - val_loss: 2.1658 - val_mean_squared_error: 2.1658\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.0184 - mean_squared_error: 1.0184 - val_loss: 2.0903 - val_mean_squared_error: 2.0903\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0570 - mean_squared_error: 1.0570 - val_loss: 2.4442 - val_mean_squared_error: 2.4442\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0841 - mean_squared_error: 1.0841 - val_loss: 2.1440 - val_mean_squared_error: 2.1440\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9954 - mean_squared_error: 0.9954 - val_loss: 2.1729 - val_mean_squared_error: 2.1729\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9920 - mean_squared_error: 0.9920 - val_loss: 2.1099 - val_mean_squared_error: 2.1099\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.9963 - mean_squared_error: 0.9963 - val_loss: 2.2615 - val_mean_squared_error: 2.2615\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9983 - mean_squared_error: 0.9983 - val_loss: 2.1167 - val_mean_squared_error: 2.1167\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.1002 - mean_squared_error: 1.1002 - val_loss: 2.4127 - val_mean_squared_error: 2.4127\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0980 - mean_squared_error: 1.0980 - val_loss: 2.3205 - val_mean_squared_error: 2.3205\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1612 - mean_squared_error: 1.1612 - val_loss: 2.4647 - val_mean_squared_error: 2.4647\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1085 - mean_squared_error: 1.1085 - val_loss: 1.9791 - val_mean_squared_error: 1.9791\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9925 - mean_squared_error: 0.9925 - val_loss: 2.1290 - val_mean_squared_error: 2.1290\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.0365 - mean_squared_error: 1.0365 - val_loss: 2.2745 - val_mean_squared_error: 2.2745\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0356 - mean_squared_error: 1.0356 - val_loss: 2.2691 - val_mean_squared_error: 2.2691\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.9765 - val_mean_squared_error: 1.9765\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1015 - mean_squared_error: 1.1015 - val_loss: 2.2427 - val_mean_squared_error: 2.2427\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 2.3609 - val_mean_squared_error: 2.3609\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0315 - mean_squared_error: 1.0315 - val_loss: 2.1489 - val_mean_squared_error: 2.1489\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.1122 - mean_squared_error: 1.1122 - val_loss: 2.2758 - val_mean_squared_error: 2.2758\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0691 - mean_squared_error: 1.0691 - val_loss: 2.1859 - val_mean_squared_error: 2.1859\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.1282 - mean_squared_error: 1.1282 - val_loss: 2.3373 - val_mean_squared_error: 2.3373\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.0598 - mean_squared_error: 1.0598 - val_loss: 2.2976 - val_mean_squared_error: 2.2976\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0981 - mean_squared_error: 1.0981 - val_loss: 2.2423 - val_mean_squared_error: 2.2423\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9486 - mean_squared_error: 0.9486 - val_loss: 2.1178 - val_mean_squared_error: 2.1178\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.9691 - mean_squared_error: 0.9691 - val_loss: 2.2658 - val_mean_squared_error: 2.2658\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9321 - mean_squared_error: 0.9321 - val_loss: 2.1423 - val_mean_squared_error: 2.1423\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.0795 - mean_squared_error: 1.0795 - val_loss: 2.2842 - val_mean_squared_error: 2.2842\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0715 - mean_squared_error: 1.0715 - val_loss: 2.6895 - val_mean_squared_error: 2.6895\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9993 - mean_squared_error: 0.9993 - val_loss: 1.9700 - val_mean_squared_error: 1.9700\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.9279 - mean_squared_error: 0.9279 - val_loss: 2.0840 - val_mean_squared_error: 2.0840\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9980 - mean_squared_error: 0.9980 - val_loss: 2.0046 - val_mean_squared_error: 2.0046\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0268 - mean_squared_error: 1.0268 - val_loss: 2.0250 - val_mean_squared_error: 2.0250\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 2.2118 - val_mean_squared_error: 2.2118\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9665 - mean_squared_error: 0.9665 - val_loss: 2.3356 - val_mean_squared_error: 2.3356\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9104 - mean_squared_error: 0.9104 - val_loss: 2.1622 - val_mean_squared_error: 2.1622\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.9322 - mean_squared_error: 0.9322 - val_loss: 2.1543 - val_mean_squared_error: 2.1543\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.9701 - mean_squared_error: 0.9701 - val_loss: 2.0931 - val_mean_squared_error: 2.0931\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9959 - mean_squared_error: 0.9959 - val_loss: 2.0809 - val_mean_squared_error: 2.0809\n",
            "==================================================\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_36 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_60 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_37 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_38 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_50 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_51 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 744.7664 - mean_squared_error: 744.7665 - val_loss: 498.6510 - val_mean_squared_error: 498.6511\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 31.3930 - mean_squared_error: 31.3930 - val_loss: 188.4672 - val_mean_squared_error: 188.4672\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 20.1494 - mean_squared_error: 20.1494 - val_loss: 72.5403 - val_mean_squared_error: 72.5403\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 17.2957 - mean_squared_error: 17.2957 - val_loss: 34.4835 - val_mean_squared_error: 34.4835\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 18.2430 - mean_squared_error: 18.2430 - val_loss: 29.0839 - val_mean_squared_error: 29.0839\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 15.6686 - mean_squared_error: 15.6686 - val_loss: 30.3222 - val_mean_squared_error: 30.3222\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 17.3686 - mean_squared_error: 17.3686 - val_loss: 18.2780 - val_mean_squared_error: 18.2780\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 13.9815 - mean_squared_error: 13.9815 - val_loss: 11.0656 - val_mean_squared_error: 11.0656\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 13.2488 - mean_squared_error: 13.2488 - val_loss: 10.7433 - val_mean_squared_error: 10.7433\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 12.8396 - mean_squared_error: 12.8396 - val_loss: 11.0404 - val_mean_squared_error: 11.0404\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 11.5661 - mean_squared_error: 11.5661 - val_loss: 13.7729 - val_mean_squared_error: 13.7729\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 11.4579 - mean_squared_error: 11.4579 - val_loss: 13.5753 - val_mean_squared_error: 13.5753\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 10.7504 - mean_squared_error: 10.7504 - val_loss: 10.3545 - val_mean_squared_error: 10.3545\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 10.0759 - mean_squared_error: 10.0759 - val_loss: 13.8030 - val_mean_squared_error: 13.8030\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 10.1458 - mean_squared_error: 10.1458 - val_loss: 10.2259 - val_mean_squared_error: 10.2259\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 8.5863 - mean_squared_error: 8.5863 - val_loss: 10.0732 - val_mean_squared_error: 10.0732\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 8.7602 - mean_squared_error: 8.7602 - val_loss: 9.2241 - val_mean_squared_error: 9.2241\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 8.6665 - mean_squared_error: 8.6665 - val_loss: 8.3891 - val_mean_squared_error: 8.3891\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 7.8400 - mean_squared_error: 7.8400 - val_loss: 7.9285 - val_mean_squared_error: 7.9285\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 7.2093 - mean_squared_error: 7.2093 - val_loss: 7.6086 - val_mean_squared_error: 7.6086\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 7.2693 - mean_squared_error: 7.2693 - val_loss: 7.1022 - val_mean_squared_error: 7.1022\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 7.4667 - mean_squared_error: 7.4667 - val_loss: 6.2520 - val_mean_squared_error: 6.2520\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 6.2485 - mean_squared_error: 6.2485 - val_loss: 6.1972 - val_mean_squared_error: 6.1972\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 5.6106 - mean_squared_error: 5.6106 - val_loss: 5.8409 - val_mean_squared_error: 5.8409\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 5.6618 - mean_squared_error: 5.6618 - val_loss: 5.4778 - val_mean_squared_error: 5.4778\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 5.4279 - mean_squared_error: 5.4279 - val_loss: 5.6356 - val_mean_squared_error: 5.6356\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 5.0637 - mean_squared_error: 5.0637 - val_loss: 4.9591 - val_mean_squared_error: 4.9591\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 4.6501 - mean_squared_error: 4.6501 - val_loss: 5.3719 - val_mean_squared_error: 5.3719\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 4.5210 - mean_squared_error: 4.5210 - val_loss: 4.1210 - val_mean_squared_error: 4.1210\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 4.5525 - mean_squared_error: 4.5525 - val_loss: 4.5281 - val_mean_squared_error: 4.5281\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 4.5493 - mean_squared_error: 4.5493 - val_loss: 4.4851 - val_mean_squared_error: 4.4851\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 4.2779 - mean_squared_error: 4.2779 - val_loss: 3.7843 - val_mean_squared_error: 3.7843\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 4.2088 - mean_squared_error: 4.2088 - val_loss: 4.4585 - val_mean_squared_error: 4.4585\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.7948 - mean_squared_error: 3.7948 - val_loss: 4.3356 - val_mean_squared_error: 4.3356\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.8005 - mean_squared_error: 3.8005 - val_loss: 4.4593 - val_mean_squared_error: 4.4593\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 3.5982 - mean_squared_error: 3.5982 - val_loss: 3.5955 - val_mean_squared_error: 3.5955\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 3.9677 - mean_squared_error: 3.9677 - val_loss: 3.6547 - val_mean_squared_error: 3.6547\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.8666 - mean_squared_error: 3.8666 - val_loss: 3.6495 - val_mean_squared_error: 3.6495\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.6389 - mean_squared_error: 3.6389 - val_loss: 3.3577 - val_mean_squared_error: 3.3577\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 3.8272 - mean_squared_error: 3.8272 - val_loss: 4.9095 - val_mean_squared_error: 4.9095\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.4793 - mean_squared_error: 3.4793 - val_loss: 4.0986 - val_mean_squared_error: 4.0986\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 3.5569 - mean_squared_error: 3.5569 - val_loss: 3.5576 - val_mean_squared_error: 3.5576\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 3.3134 - mean_squared_error: 3.3134 - val_loss: 4.3686 - val_mean_squared_error: 4.3686\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 3.0659 - mean_squared_error: 3.0659 - val_loss: 3.3438 - val_mean_squared_error: 3.3438\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 3.2455 - mean_squared_error: 3.2455 - val_loss: 4.2986 - val_mean_squared_error: 4.2986\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.9685 - mean_squared_error: 2.9685 - val_loss: 3.4461 - val_mean_squared_error: 3.4461\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.0872 - mean_squared_error: 3.0872 - val_loss: 3.2406 - val_mean_squared_error: 3.2406\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.9136 - mean_squared_error: 2.9136 - val_loss: 3.2224 - val_mean_squared_error: 3.2224\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.9970 - mean_squared_error: 2.9970 - val_loss: 3.0736 - val_mean_squared_error: 3.0736\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.0005 - mean_squared_error: 3.0005 - val_loss: 3.1038 - val_mean_squared_error: 3.1038\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.8061 - mean_squared_error: 2.8061 - val_loss: 3.0358 - val_mean_squared_error: 3.0358\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.8333 - mean_squared_error: 2.8333 - val_loss: 3.4064 - val_mean_squared_error: 3.4064\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.8243 - mean_squared_error: 2.8243 - val_loss: 3.8932 - val_mean_squared_error: 3.8932\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.9184 - mean_squared_error: 2.9184 - val_loss: 3.6671 - val_mean_squared_error: 3.6671\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.7147 - mean_squared_error: 2.7147 - val_loss: 2.8932 - val_mean_squared_error: 2.8932\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.7333 - mean_squared_error: 2.7333 - val_loss: 3.9129 - val_mean_squared_error: 3.9129\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.7616 - mean_squared_error: 2.7616 - val_loss: 3.1524 - val_mean_squared_error: 3.1524\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.5217 - mean_squared_error: 2.5217 - val_loss: 2.7445 - val_mean_squared_error: 2.7445\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.4664 - mean_squared_error: 2.4664 - val_loss: 3.6373 - val_mean_squared_error: 3.6373\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.5176 - mean_squared_error: 2.5176 - val_loss: 3.1708 - val_mean_squared_error: 3.1708\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.4913 - mean_squared_error: 2.4913 - val_loss: 3.3340 - val_mean_squared_error: 3.3340\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.4545 - mean_squared_error: 2.4545 - val_loss: 3.1687 - val_mean_squared_error: 3.1687\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 2.5585 - mean_squared_error: 2.5585 - val_loss: 3.7236 - val_mean_squared_error: 3.7236\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 2.6561 - mean_squared_error: 2.6561 - val_loss: 3.1687 - val_mean_squared_error: 3.1687\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.7840 - mean_squared_error: 2.7840 - val_loss: 3.1641 - val_mean_squared_error: 3.1641\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.5260 - mean_squared_error: 2.5260 - val_loss: 2.8832 - val_mean_squared_error: 2.8832\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.4678 - mean_squared_error: 2.4678 - val_loss: 2.8352 - val_mean_squared_error: 2.8352\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.2909 - mean_squared_error: 2.2909 - val_loss: 3.3281 - val_mean_squared_error: 3.3281\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.2477 - mean_squared_error: 2.2477 - val_loss: 2.8469 - val_mean_squared_error: 2.8469\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.2574 - mean_squared_error: 2.2574 - val_loss: 3.6339 - val_mean_squared_error: 3.6339\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.2573 - mean_squared_error: 2.2573 - val_loss: 3.2940 - val_mean_squared_error: 3.2940\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.3617 - mean_squared_error: 2.3617 - val_loss: 2.4790 - val_mean_squared_error: 2.4790\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.4411 - mean_squared_error: 2.4411 - val_loss: 3.1831 - val_mean_squared_error: 3.1831\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.1997 - mean_squared_error: 2.1997 - val_loss: 2.7107 - val_mean_squared_error: 2.7107\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.1457 - mean_squared_error: 2.1457 - val_loss: 2.8729 - val_mean_squared_error: 2.8729\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.0063 - mean_squared_error: 2.0063 - val_loss: 2.8514 - val_mean_squared_error: 2.8514\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.0822 - mean_squared_error: 2.0822 - val_loss: 2.8635 - val_mean_squared_error: 2.8635\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.1237 - mean_squared_error: 2.1237 - val_loss: 2.4750 - val_mean_squared_error: 2.4750\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 2.3085 - mean_squared_error: 2.3085 - val_loss: 2.8874 - val_mean_squared_error: 2.8874\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.0254 - mean_squared_error: 2.0254 - val_loss: 2.8180 - val_mean_squared_error: 2.8180\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.9348 - mean_squared_error: 1.9348 - val_loss: 2.9486 - val_mean_squared_error: 2.9486\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.8511 - mean_squared_error: 1.8511 - val_loss: 2.5530 - val_mean_squared_error: 2.5530\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.8896 - mean_squared_error: 1.8896 - val_loss: 2.5076 - val_mean_squared_error: 2.5076\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.9918 - mean_squared_error: 1.9918 - val_loss: 3.2569 - val_mean_squared_error: 3.2569\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.9767 - mean_squared_error: 1.9767 - val_loss: 2.7913 - val_mean_squared_error: 2.7913\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.8878 - mean_squared_error: 1.8878 - val_loss: 2.3898 - val_mean_squared_error: 2.3898\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.0900 - mean_squared_error: 2.0900 - val_loss: 3.6339 - val_mean_squared_error: 3.6339\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.9939 - mean_squared_error: 1.9939 - val_loss: 2.3909 - val_mean_squared_error: 2.3909\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.8288 - mean_squared_error: 1.8288 - val_loss: 2.7325 - val_mean_squared_error: 2.7325\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.9897 - mean_squared_error: 1.9897 - val_loss: 2.9673 - val_mean_squared_error: 2.9673\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.9780 - mean_squared_error: 1.9780 - val_loss: 2.4469 - val_mean_squared_error: 2.4469\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.9697 - mean_squared_error: 1.9697 - val_loss: 2.9802 - val_mean_squared_error: 2.9802\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.7208 - mean_squared_error: 1.7208 - val_loss: 2.5833 - val_mean_squared_error: 2.5833\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.0056 - mean_squared_error: 2.0056 - val_loss: 2.5204 - val_mean_squared_error: 2.5204\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.7907 - mean_squared_error: 1.7907 - val_loss: 2.4330 - val_mean_squared_error: 2.4330\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.8186 - mean_squared_error: 1.8186 - val_loss: 2.5936 - val_mean_squared_error: 2.5936\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.6979 - mean_squared_error: 1.6979 - val_loss: 2.3131 - val_mean_squared_error: 2.3131\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.7359 - mean_squared_error: 1.7359 - val_loss: 2.5654 - val_mean_squared_error: 2.5654\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.7557 - mean_squared_error: 1.7557 - val_loss: 2.5708 - val_mean_squared_error: 2.5708\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7049 - mean_squared_error: 1.7049 - val_loss: 2.5552 - val_mean_squared_error: 2.5552\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6274 - mean_squared_error: 1.6274 - val_loss: 3.1053 - val_mean_squared_error: 3.1053\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.6094 - mean_squared_error: 1.6094 - val_loss: 2.6774 - val_mean_squared_error: 2.6774\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.6772 - mean_squared_error: 1.6772 - val_loss: 2.5400 - val_mean_squared_error: 2.5400\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.6797 - mean_squared_error: 1.6797 - val_loss: 2.5937 - val_mean_squared_error: 2.5937\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7029 - mean_squared_error: 1.7029 - val_loss: 2.5015 - val_mean_squared_error: 2.5015\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.6431 - mean_squared_error: 1.6431 - val_loss: 2.7047 - val_mean_squared_error: 2.7047\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7337 - mean_squared_error: 1.7337 - val_loss: 2.2543 - val_mean_squared_error: 2.2543\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.7086 - mean_squared_error: 1.7086 - val_loss: 2.2739 - val_mean_squared_error: 2.2739\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5550 - mean_squared_error: 1.5550 - val_loss: 3.0210 - val_mean_squared_error: 3.0210\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.5375 - mean_squared_error: 1.5375 - val_loss: 2.2888 - val_mean_squared_error: 2.2888\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5560 - mean_squared_error: 1.5560 - val_loss: 2.4733 - val_mean_squared_error: 2.4733\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5845 - mean_squared_error: 1.5845 - val_loss: 2.4023 - val_mean_squared_error: 2.4023\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.5845 - mean_squared_error: 1.5845 - val_loss: 3.6543 - val_mean_squared_error: 3.6543\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.7443 - mean_squared_error: 1.7443 - val_loss: 2.2730 - val_mean_squared_error: 2.2730\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.6137 - mean_squared_error: 1.6137 - val_loss: 2.5785 - val_mean_squared_error: 2.5785\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.5617 - mean_squared_error: 1.5617 - val_loss: 2.6495 - val_mean_squared_error: 2.6495\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.6300 - mean_squared_error: 1.6300 - val_loss: 2.5898 - val_mean_squared_error: 2.5898\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.6138 - mean_squared_error: 1.6138 - val_loss: 2.4724 - val_mean_squared_error: 2.4724\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4704 - mean_squared_error: 1.4704 - val_loss: 2.1782 - val_mean_squared_error: 2.1782\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.4452 - mean_squared_error: 1.4452 - val_loss: 2.2634 - val_mean_squared_error: 2.2634\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.4382 - mean_squared_error: 1.4382 - val_loss: 2.4765 - val_mean_squared_error: 2.4765\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.5336 - mean_squared_error: 1.5336 - val_loss: 2.7176 - val_mean_squared_error: 2.7176\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.4525 - mean_squared_error: 1.4525 - val_loss: 2.1976 - val_mean_squared_error: 2.1976\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.4805 - mean_squared_error: 1.4804 - val_loss: 2.3275 - val_mean_squared_error: 2.3275\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4755 - mean_squared_error: 1.4755 - val_loss: 2.2820 - val_mean_squared_error: 2.2820\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5107 - mean_squared_error: 1.5107 - val_loss: 2.5054 - val_mean_squared_error: 2.5054\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.6007 - mean_squared_error: 1.6007 - val_loss: 2.4419 - val_mean_squared_error: 2.4419\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3281 - mean_squared_error: 1.3281 - val_loss: 2.2774 - val_mean_squared_error: 2.2774\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.5225 - mean_squared_error: 1.5225 - val_loss: 2.3579 - val_mean_squared_error: 2.3579\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.3712 - mean_squared_error: 1.3712 - val_loss: 2.4636 - val_mean_squared_error: 2.4636\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.4159 - mean_squared_error: 1.4159 - val_loss: 2.4332 - val_mean_squared_error: 2.4332\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3592 - mean_squared_error: 1.3592 - val_loss: 2.2298 - val_mean_squared_error: 2.2298\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3637 - mean_squared_error: 1.3637 - val_loss: 2.1555 - val_mean_squared_error: 2.1555\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.4614 - mean_squared_error: 1.4614 - val_loss: 2.2743 - val_mean_squared_error: 2.2743\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.3735 - mean_squared_error: 1.3735 - val_loss: 2.3385 - val_mean_squared_error: 2.3385\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.3413 - mean_squared_error: 1.3413 - val_loss: 2.3043 - val_mean_squared_error: 2.3043\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.4111 - mean_squared_error: 1.4111 - val_loss: 2.2122 - val_mean_squared_error: 2.2122\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.3732 - mean_squared_error: 1.3732 - val_loss: 2.2829 - val_mean_squared_error: 2.2829\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.4266 - mean_squared_error: 1.4266 - val_loss: 2.1884 - val_mean_squared_error: 2.1884\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.1911 - mean_squared_error: 1.1911 - val_loss: 2.3118 - val_mean_squared_error: 2.3118\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3604 - mean_squared_error: 1.3604 - val_loss: 2.2564 - val_mean_squared_error: 2.2564\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3844 - mean_squared_error: 1.3844 - val_loss: 2.1640 - val_mean_squared_error: 2.1640\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1843 - mean_squared_error: 1.1843 - val_loss: 2.1473 - val_mean_squared_error: 2.1473\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.3183 - mean_squared_error: 1.3183 - val_loss: 2.3268 - val_mean_squared_error: 2.3268\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.4229 - mean_squared_error: 1.4229 - val_loss: 2.6671 - val_mean_squared_error: 2.6671\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.3663 - mean_squared_error: 1.3663 - val_loss: 2.1952 - val_mean_squared_error: 2.1952\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.3716 - mean_squared_error: 1.3716 - val_loss: 2.1986 - val_mean_squared_error: 2.1986\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.3507 - mean_squared_error: 1.3507 - val_loss: 2.0669 - val_mean_squared_error: 2.0669\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.3093 - mean_squared_error: 1.3093 - val_loss: 2.3107 - val_mean_squared_error: 2.3107\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2419 - mean_squared_error: 1.2419 - val_loss: 2.1713 - val_mean_squared_error: 2.1713\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3208 - mean_squared_error: 1.3208 - val_loss: 2.4019 - val_mean_squared_error: 2.4019\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2385 - mean_squared_error: 1.2385 - val_loss: 2.2552 - val_mean_squared_error: 2.2552\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2226 - mean_squared_error: 1.2226 - val_loss: 2.0960 - val_mean_squared_error: 2.0960\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1517 - mean_squared_error: 1.1517 - val_loss: 2.1489 - val_mean_squared_error: 2.1489\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.2870 - mean_squared_error: 1.2870 - val_loss: 2.0862 - val_mean_squared_error: 2.0862\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.2595 - mean_squared_error: 1.2595 - val_loss: 2.2626 - val_mean_squared_error: 2.2626\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3970 - mean_squared_error: 1.3970 - val_loss: 2.2959 - val_mean_squared_error: 2.2959\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2829 - mean_squared_error: 1.2829 - val_loss: 2.3139 - val_mean_squared_error: 2.3139\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1949 - mean_squared_error: 1.1949 - val_loss: 2.2368 - val_mean_squared_error: 2.2368\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.2412 - mean_squared_error: 1.2412 - val_loss: 2.1772 - val_mean_squared_error: 2.1772\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1304 - mean_squared_error: 1.1304 - val_loss: 2.0791 - val_mean_squared_error: 2.0791\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1819 - mean_squared_error: 1.1819 - val_loss: 1.9861 - val_mean_squared_error: 1.9861\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2681 - mean_squared_error: 1.2681 - val_loss: 2.2493 - val_mean_squared_error: 2.2493\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1077 - mean_squared_error: 1.1077 - val_loss: 2.1157 - val_mean_squared_error: 2.1157\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2278 - mean_squared_error: 1.2278 - val_loss: 2.0587 - val_mean_squared_error: 2.0587\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2580 - mean_squared_error: 1.2580 - val_loss: 2.1662 - val_mean_squared_error: 2.1662\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0257 - mean_squared_error: 1.0257 - val_loss: 2.0818 - val_mean_squared_error: 2.0818\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.0923 - mean_squared_error: 1.0923 - val_loss: 2.0511 - val_mean_squared_error: 2.0511\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.1949 - mean_squared_error: 1.1949 - val_loss: 2.2058 - val_mean_squared_error: 2.2058\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1442 - mean_squared_error: 1.1442 - val_loss: 2.1597 - val_mean_squared_error: 2.1597\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1076 - mean_squared_error: 1.1076 - val_loss: 1.9340 - val_mean_squared_error: 1.9340\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1871 - mean_squared_error: 1.1871 - val_loss: 2.0702 - val_mean_squared_error: 2.0702\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1748 - mean_squared_error: 1.1748 - val_loss: 2.0336 - val_mean_squared_error: 2.0336\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2314 - mean_squared_error: 1.2314 - val_loss: 2.4283 - val_mean_squared_error: 2.4283\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0883 - mean_squared_error: 1.0883 - val_loss: 2.2362 - val_mean_squared_error: 2.2362\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0318 - mean_squared_error: 1.0318 - val_loss: 2.0358 - val_mean_squared_error: 2.0358\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.0320 - mean_squared_error: 1.0320 - val_loss: 2.4448 - val_mean_squared_error: 2.4448\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 2.2951 - val_mean_squared_error: 2.2951\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 2.0049 - val_mean_squared_error: 2.0049\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0465 - mean_squared_error: 1.0465 - val_loss: 1.9664 - val_mean_squared_error: 1.9664\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0363 - mean_squared_error: 1.0363 - val_loss: 2.0912 - val_mean_squared_error: 2.0912\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9784 - mean_squared_error: 0.9784 - val_loss: 2.1574 - val_mean_squared_error: 2.1574\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 2.0406 - val_mean_squared_error: 2.0406\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0380 - mean_squared_error: 1.0380 - val_loss: 2.0789 - val_mean_squared_error: 2.0789\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.0501 - mean_squared_error: 1.0501 - val_loss: 2.1814 - val_mean_squared_error: 2.1814\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.0705 - mean_squared_error: 1.0705 - val_loss: 2.0276 - val_mean_squared_error: 2.0276\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.9434 - mean_squared_error: 0.9434 - val_loss: 2.0416 - val_mean_squared_error: 2.0416\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1239 - mean_squared_error: 1.1239 - val_loss: 1.9965 - val_mean_squared_error: 1.9965\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0584 - mean_squared_error: 1.0584 - val_loss: 1.9297 - val_mean_squared_error: 1.9297\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0387 - mean_squared_error: 1.0387 - val_loss: 2.1367 - val_mean_squared_error: 2.1367\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.0928 - mean_squared_error: 1.0928 - val_loss: 2.2209 - val_mean_squared_error: 2.2209\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.0288 - mean_squared_error: 1.0288 - val_loss: 2.3100 - val_mean_squared_error: 2.3100\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1001 - mean_squared_error: 1.1001 - val_loss: 2.0450 - val_mean_squared_error: 2.0450\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0502 - mean_squared_error: 1.0502 - val_loss: 2.1577 - val_mean_squared_error: 2.1577\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.9957 - mean_squared_error: 0.9957 - val_loss: 2.1361 - val_mean_squared_error: 2.1361\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 2.2196 - val_mean_squared_error: 2.2196\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.0501 - mean_squared_error: 1.0501 - val_loss: 2.0946 - val_mean_squared_error: 2.0946\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.9383 - mean_squared_error: 0.9383 - val_loss: 2.1165 - val_mean_squared_error: 2.1165\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9182 - mean_squared_error: 0.9182 - val_loss: 2.6021 - val_mean_squared_error: 2.6021\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0233 - mean_squared_error: 1.0233 - val_loss: 2.1529 - val_mean_squared_error: 2.1529\n",
            "==================================================\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_39 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_39 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_52 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_40 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_53 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_41 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_54 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_68 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_55 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_69 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 745.6133 - mean_squared_error: 745.6135 - val_loss: 303.8260 - val_mean_squared_error: 303.8260\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 429us/sample - loss: 27.8673 - mean_squared_error: 27.8673 - val_loss: 153.7515 - val_mean_squared_error: 153.7515\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 20.8401 - mean_squared_error: 20.8401 - val_loss: 65.6920 - val_mean_squared_error: 65.6920\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 16.6916 - mean_squared_error: 16.6916 - val_loss: 31.7187 - val_mean_squared_error: 31.7187\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 17.5166 - mean_squared_error: 17.5166 - val_loss: 20.4461 - val_mean_squared_error: 20.4461\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 16.2899 - mean_squared_error: 16.2899 - val_loss: 28.1037 - val_mean_squared_error: 28.1037\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 15.5508 - mean_squared_error: 15.5508 - val_loss: 34.0748 - val_mean_squared_error: 34.0748\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 13.7372 - mean_squared_error: 13.7372 - val_loss: 21.8118 - val_mean_squared_error: 21.8118\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 13.9834 - mean_squared_error: 13.9834 - val_loss: 16.2481 - val_mean_squared_error: 16.2481\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 12.3348 - mean_squared_error: 12.3348 - val_loss: 15.4518 - val_mean_squared_error: 15.4518\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 12.6907 - mean_squared_error: 12.6907 - val_loss: 11.5376 - val_mean_squared_error: 11.5376\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 11.1371 - mean_squared_error: 11.1371 - val_loss: 14.9451 - val_mean_squared_error: 14.9451\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 10.3166 - mean_squared_error: 10.3166 - val_loss: 9.7148 - val_mean_squared_error: 9.7148\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 10.7966 - mean_squared_error: 10.7966 - val_loss: 9.1983 - val_mean_squared_error: 9.1983\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 9.1412 - mean_squared_error: 9.1412 - val_loss: 10.0934 - val_mean_squared_error: 10.0934\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 9.4348 - mean_squared_error: 9.4348 - val_loss: 9.3583 - val_mean_squared_error: 9.3583\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 8.4294 - mean_squared_error: 8.4294 - val_loss: 10.0675 - val_mean_squared_error: 10.0675\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 8.1602 - mean_squared_error: 8.1602 - val_loss: 9.9098 - val_mean_squared_error: 9.9098\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 7.7183 - mean_squared_error: 7.7183 - val_loss: 7.8585 - val_mean_squared_error: 7.8585\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 8.0797 - mean_squared_error: 8.0797 - val_loss: 7.4049 - val_mean_squared_error: 7.4049\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 7.2127 - mean_squared_error: 7.2127 - val_loss: 9.5262 - val_mean_squared_error: 9.5262\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 5.8596 - mean_squared_error: 5.8596 - val_loss: 7.0908 - val_mean_squared_error: 7.0908\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 5.5701 - mean_squared_error: 5.5701 - val_loss: 8.2753 - val_mean_squared_error: 8.2753\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 5.8409 - mean_squared_error: 5.8409 - val_loss: 6.7143 - val_mean_squared_error: 6.7143\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 5.1985 - mean_squared_error: 5.1985 - val_loss: 8.5983 - val_mean_squared_error: 8.5983\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 4.8552 - mean_squared_error: 4.8552 - val_loss: 5.5380 - val_mean_squared_error: 5.5380\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 5.0369 - mean_squared_error: 5.0369 - val_loss: 5.2841 - val_mean_squared_error: 5.2841\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 4.5474 - mean_squared_error: 4.5474 - val_loss: 6.0543 - val_mean_squared_error: 6.0543\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 4.2366 - mean_squared_error: 4.2366 - val_loss: 5.8112 - val_mean_squared_error: 5.8112\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 4.1318 - mean_squared_error: 4.1318 - val_loss: 4.1216 - val_mean_squared_error: 4.1216\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 4.1546 - mean_squared_error: 4.1546 - val_loss: 5.5824 - val_mean_squared_error: 5.5824\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 3.8492 - mean_squared_error: 3.8492 - val_loss: 5.7024 - val_mean_squared_error: 5.7024\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 3.4541 - mean_squared_error: 3.4541 - val_loss: 4.1712 - val_mean_squared_error: 4.1712\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 3.4203 - mean_squared_error: 3.4203 - val_loss: 5.7653 - val_mean_squared_error: 5.7653\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 3.5375 - mean_squared_error: 3.5375 - val_loss: 4.0001 - val_mean_squared_error: 4.0001\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 3.3105 - mean_squared_error: 3.3105 - val_loss: 4.3544 - val_mean_squared_error: 4.3544\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 3.3663 - mean_squared_error: 3.3663 - val_loss: 3.5655 - val_mean_squared_error: 3.5655\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.1105 - mean_squared_error: 3.1105 - val_loss: 3.9568 - val_mean_squared_error: 3.9568\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 3.1144 - mean_squared_error: 3.1144 - val_loss: 3.6330 - val_mean_squared_error: 3.6330\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.9434 - mean_squared_error: 2.9434 - val_loss: 5.2661 - val_mean_squared_error: 5.2661\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.9767 - mean_squared_error: 2.9767 - val_loss: 3.2099 - val_mean_squared_error: 3.2099\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.6067 - mean_squared_error: 2.6067 - val_loss: 3.5518 - val_mean_squared_error: 3.5518\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.8585 - mean_squared_error: 2.8585 - val_loss: 4.1120 - val_mean_squared_error: 4.1120\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.7646 - mean_squared_error: 2.7646 - val_loss: 3.8210 - val_mean_squared_error: 3.8210\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.7175 - mean_squared_error: 2.7175 - val_loss: 3.4013 - val_mean_squared_error: 3.4013\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 2.6004 - mean_squared_error: 2.6004 - val_loss: 3.7921 - val_mean_squared_error: 3.7921\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.8821 - mean_squared_error: 2.8821 - val_loss: 3.9603 - val_mean_squared_error: 3.9603\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.6933 - mean_squared_error: 2.6933 - val_loss: 3.6574 - val_mean_squared_error: 3.6574\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.6365 - mean_squared_error: 2.6365 - val_loss: 3.5027 - val_mean_squared_error: 3.5027\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.5787 - mean_squared_error: 2.5787 - val_loss: 4.0338 - val_mean_squared_error: 4.0338\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.3672 - mean_squared_error: 2.3672 - val_loss: 3.4640 - val_mean_squared_error: 3.4640\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.2316 - mean_squared_error: 2.2316 - val_loss: 3.0240 - val_mean_squared_error: 3.0240\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.2397 - mean_squared_error: 2.2397 - val_loss: 3.8662 - val_mean_squared_error: 3.8662\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.2076 - mean_squared_error: 2.2076 - val_loss: 4.0191 - val_mean_squared_error: 4.0191\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.3432 - mean_squared_error: 2.3432 - val_loss: 2.8821 - val_mean_squared_error: 2.8821\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.2975 - mean_squared_error: 2.2975 - val_loss: 3.1151 - val_mean_squared_error: 3.1151\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.4811 - mean_squared_error: 2.4811 - val_loss: 3.3429 - val_mean_squared_error: 3.3429\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.0555 - mean_squared_error: 2.0555 - val_loss: 4.2722 - val_mean_squared_error: 4.2722\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 2.2696 - mean_squared_error: 2.2696 - val_loss: 2.8939 - val_mean_squared_error: 2.8939\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.0093 - mean_squared_error: 2.0093 - val_loss: 3.4436 - val_mean_squared_error: 3.4436\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 2.0629 - mean_squared_error: 2.0629 - val_loss: 3.2624 - val_mean_squared_error: 3.2624\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.0194 - mean_squared_error: 2.0194 - val_loss: 2.5811 - val_mean_squared_error: 2.5811\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.8057 - mean_squared_error: 1.8057 - val_loss: 2.8072 - val_mean_squared_error: 2.8072\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.9500 - mean_squared_error: 1.9500 - val_loss: 3.2399 - val_mean_squared_error: 3.2399\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.8382 - mean_squared_error: 1.8382 - val_loss: 2.8729 - val_mean_squared_error: 2.8729\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.8863 - mean_squared_error: 1.8863 - val_loss: 3.1060 - val_mean_squared_error: 3.1060\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.7791 - mean_squared_error: 1.7791 - val_loss: 2.8204 - val_mean_squared_error: 2.8204\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.7289 - mean_squared_error: 1.7289 - val_loss: 3.2007 - val_mean_squared_error: 3.2007\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.6494 - mean_squared_error: 1.6494 - val_loss: 2.7110 - val_mean_squared_error: 2.7110\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.8325 - mean_squared_error: 1.8325 - val_loss: 2.6197 - val_mean_squared_error: 2.6197\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.8252 - mean_squared_error: 1.8252 - val_loss: 3.1516 - val_mean_squared_error: 3.1516\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.8135 - mean_squared_error: 1.8135 - val_loss: 2.8175 - val_mean_squared_error: 2.8175\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.6712 - mean_squared_error: 1.6712 - val_loss: 3.0783 - val_mean_squared_error: 3.0783\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.1722 - mean_squared_error: 2.1722 - val_loss: 2.9674 - val_mean_squared_error: 2.9674\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.7360 - mean_squared_error: 1.7360 - val_loss: 3.1685 - val_mean_squared_error: 3.1685\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.6402 - mean_squared_error: 1.6402 - val_loss: 2.6865 - val_mean_squared_error: 2.6865\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.7943 - mean_squared_error: 1.7943 - val_loss: 3.6520 - val_mean_squared_error: 3.6520\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.7131 - mean_squared_error: 1.7131 - val_loss: 3.1272 - val_mean_squared_error: 3.1272\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.6605 - mean_squared_error: 1.6605 - val_loss: 3.7808 - val_mean_squared_error: 3.7808\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.5172 - mean_squared_error: 1.5172 - val_loss: 3.1616 - val_mean_squared_error: 3.1616\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.6389 - mean_squared_error: 1.6389 - val_loss: 2.7515 - val_mean_squared_error: 2.7515\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.7106 - mean_squared_error: 1.7106 - val_loss: 2.9848 - val_mean_squared_error: 2.9848\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.5406 - mean_squared_error: 1.5406 - val_loss: 3.4108 - val_mean_squared_error: 3.4108\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.7183 - mean_squared_error: 1.7183 - val_loss: 3.1289 - val_mean_squared_error: 3.1289\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.5187 - mean_squared_error: 1.5187 - val_loss: 2.7475 - val_mean_squared_error: 2.7475\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.5121 - mean_squared_error: 1.5121 - val_loss: 2.7486 - val_mean_squared_error: 2.7486\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.5240 - mean_squared_error: 1.5240 - val_loss: 4.7895 - val_mean_squared_error: 4.7895\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7989 - mean_squared_error: 1.7989 - val_loss: 2.9316 - val_mean_squared_error: 2.9316\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7458 - mean_squared_error: 1.7458 - val_loss: 3.6794 - val_mean_squared_error: 3.6794\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.6285 - mean_squared_error: 1.6285 - val_loss: 3.0512 - val_mean_squared_error: 3.0512\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.5131 - mean_squared_error: 1.5131 - val_loss: 2.8959 - val_mean_squared_error: 2.8959\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.5737 - mean_squared_error: 1.5737 - val_loss: 2.9068 - val_mean_squared_error: 2.9068\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.7463 - mean_squared_error: 1.7463 - val_loss: 2.8158 - val_mean_squared_error: 2.8158\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.3833 - mean_squared_error: 1.3833 - val_loss: 2.4751 - val_mean_squared_error: 2.4751\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.4725 - mean_squared_error: 1.4725 - val_loss: 2.7711 - val_mean_squared_error: 2.7711\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.5298 - mean_squared_error: 1.5298 - val_loss: 2.6938 - val_mean_squared_error: 2.6938\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.4706 - mean_squared_error: 1.4706 - val_loss: 2.9522 - val_mean_squared_error: 2.9522\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.4295 - mean_squared_error: 1.4295 - val_loss: 2.5477 - val_mean_squared_error: 2.5477\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4450 - mean_squared_error: 1.4450 - val_loss: 2.7101 - val_mean_squared_error: 2.7101\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4297 - mean_squared_error: 1.4297 - val_loss: 2.8083 - val_mean_squared_error: 2.8083\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3473 - mean_squared_error: 1.3473 - val_loss: 2.5506 - val_mean_squared_error: 2.5506\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2573 - mean_squared_error: 1.2573 - val_loss: 2.7891 - val_mean_squared_error: 2.7891\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.4580 - mean_squared_error: 1.4580 - val_loss: 2.6443 - val_mean_squared_error: 2.6443\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.4894 - mean_squared_error: 1.4894 - val_loss: 3.2256 - val_mean_squared_error: 3.2256\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.4837 - mean_squared_error: 1.4837 - val_loss: 3.2296 - val_mean_squared_error: 3.2296\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.5350 - mean_squared_error: 1.5350 - val_loss: 3.3703 - val_mean_squared_error: 3.3703\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3471 - mean_squared_error: 1.3471 - val_loss: 3.0538 - val_mean_squared_error: 3.0538\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.3356 - mean_squared_error: 1.3356 - val_loss: 2.7943 - val_mean_squared_error: 2.7943\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.2876 - mean_squared_error: 1.2876 - val_loss: 2.4374 - val_mean_squared_error: 2.4374\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.3024 - mean_squared_error: 1.3024 - val_loss: 2.6729 - val_mean_squared_error: 2.6729\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.4270 - mean_squared_error: 1.4270 - val_loss: 2.8585 - val_mean_squared_error: 2.8585\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2861 - mean_squared_error: 1.2861 - val_loss: 2.4633 - val_mean_squared_error: 2.4633\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.2813 - mean_squared_error: 1.2813 - val_loss: 2.9591 - val_mean_squared_error: 2.9591\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4085 - mean_squared_error: 1.4085 - val_loss: 2.4398 - val_mean_squared_error: 2.4398\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1953 - mean_squared_error: 1.1953 - val_loss: 2.5410 - val_mean_squared_error: 2.5410\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.2925 - mean_squared_error: 1.2925 - val_loss: 2.6545 - val_mean_squared_error: 2.6545\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1109 - mean_squared_error: 1.1109 - val_loss: 2.4855 - val_mean_squared_error: 2.4855\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.2884 - mean_squared_error: 1.2884 - val_loss: 2.5215 - val_mean_squared_error: 2.5215\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.1541 - mean_squared_error: 1.1541 - val_loss: 2.5572 - val_mean_squared_error: 2.5572\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1528 - mean_squared_error: 1.1528 - val_loss: 2.4909 - val_mean_squared_error: 2.4909\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2254 - mean_squared_error: 1.2254 - val_loss: 2.6764 - val_mean_squared_error: 2.6764\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.3492 - mean_squared_error: 1.3492 - val_loss: 2.4223 - val_mean_squared_error: 2.4223\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1926 - mean_squared_error: 1.1926 - val_loss: 2.6280 - val_mean_squared_error: 2.6280\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1666 - mean_squared_error: 1.1666 - val_loss: 2.4311 - val_mean_squared_error: 2.4311\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1634 - mean_squared_error: 1.1634 - val_loss: 2.6984 - val_mean_squared_error: 2.6984\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.1051 - mean_squared_error: 1.1051 - val_loss: 2.4609 - val_mean_squared_error: 2.4609\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.0881 - mean_squared_error: 1.0881 - val_loss: 2.4505 - val_mean_squared_error: 2.4505\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.2237 - mean_squared_error: 1.2237 - val_loss: 3.2393 - val_mean_squared_error: 3.2393\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2490 - mean_squared_error: 1.2490 - val_loss: 2.5808 - val_mean_squared_error: 2.5808\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2039 - mean_squared_error: 1.2039 - val_loss: 2.6345 - val_mean_squared_error: 2.6345\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2898 - mean_squared_error: 1.2898 - val_loss: 2.5468 - val_mean_squared_error: 2.5468\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.1185 - mean_squared_error: 1.1185 - val_loss: 2.9004 - val_mean_squared_error: 2.9004\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.0369 - mean_squared_error: 1.0369 - val_loss: 2.5447 - val_mean_squared_error: 2.5447\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.0738 - mean_squared_error: 1.0738 - val_loss: 2.5671 - val_mean_squared_error: 2.5671\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.0709 - mean_squared_error: 1.0709 - val_loss: 2.5096 - val_mean_squared_error: 2.5096\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1460 - mean_squared_error: 1.1460 - val_loss: 2.4075 - val_mean_squared_error: 2.4075\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.1030 - mean_squared_error: 1.1030 - val_loss: 2.5298 - val_mean_squared_error: 2.5298\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.2097 - mean_squared_error: 1.2097 - val_loss: 3.0899 - val_mean_squared_error: 3.0899\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.0863 - mean_squared_error: 1.0863 - val_loss: 2.4729 - val_mean_squared_error: 2.4729\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0416 - mean_squared_error: 1.0416 - val_loss: 2.5005 - val_mean_squared_error: 2.5005\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.9876 - mean_squared_error: 0.9876 - val_loss: 3.0387 - val_mean_squared_error: 3.0387\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1113 - mean_squared_error: 1.1113 - val_loss: 2.6465 - val_mean_squared_error: 2.6465\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.0995 - mean_squared_error: 1.0995 - val_loss: 2.7676 - val_mean_squared_error: 2.7676\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0263 - mean_squared_error: 1.0263 - val_loss: 2.3736 - val_mean_squared_error: 2.3736\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0784 - mean_squared_error: 1.0784 - val_loss: 2.3036 - val_mean_squared_error: 2.3036\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1041 - mean_squared_error: 1.1041 - val_loss: 2.9215 - val_mean_squared_error: 2.9215\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0518 - mean_squared_error: 1.0518 - val_loss: 2.3348 - val_mean_squared_error: 2.3348\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 2.9823 - val_mean_squared_error: 2.9823\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.9514 - mean_squared_error: 0.9514 - val_loss: 2.6070 - val_mean_squared_error: 2.6070\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 0.9568 - mean_squared_error: 0.9568 - val_loss: 2.9025 - val_mean_squared_error: 2.9025\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.0293 - mean_squared_error: 1.0293 - val_loss: 2.4389 - val_mean_squared_error: 2.4389\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 2.3475 - val_mean_squared_error: 2.3475\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0532 - mean_squared_error: 1.0532 - val_loss: 2.3192 - val_mean_squared_error: 2.3192\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9850 - mean_squared_error: 0.9850 - val_loss: 2.5433 - val_mean_squared_error: 2.5433\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 2.4193 - val_mean_squared_error: 2.4193\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9586 - mean_squared_error: 0.9586 - val_loss: 2.3195 - val_mean_squared_error: 2.3195\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.9296 - mean_squared_error: 0.9296 - val_loss: 2.4845 - val_mean_squared_error: 2.4845\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 0.9040 - mean_squared_error: 0.9040 - val_loss: 2.4106 - val_mean_squared_error: 2.4106\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 0.9483 - mean_squared_error: 0.9483 - val_loss: 2.3817 - val_mean_squared_error: 2.3817\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9827 - mean_squared_error: 0.9827 - val_loss: 2.4940 - val_mean_squared_error: 2.4940\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.1006 - mean_squared_error: 1.1006 - val_loss: 2.6512 - val_mean_squared_error: 2.6512\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 2.3680 - val_mean_squared_error: 2.3680\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.9214 - mean_squared_error: 0.9214 - val_loss: 2.5511 - val_mean_squared_error: 2.5511\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.9867 - mean_squared_error: 0.9867 - val_loss: 2.3813 - val_mean_squared_error: 2.3813\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 0.9061 - mean_squared_error: 0.9061 - val_loss: 2.5390 - val_mean_squared_error: 2.5390\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.9754 - mean_squared_error: 0.9754 - val_loss: 2.3755 - val_mean_squared_error: 2.3755\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 2.3366 - val_mean_squared_error: 2.3366\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.8575 - mean_squared_error: 0.8575 - val_loss: 2.4064 - val_mean_squared_error: 2.4064\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 0.8485 - mean_squared_error: 0.8485 - val_loss: 2.4859 - val_mean_squared_error: 2.4859\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.8981 - mean_squared_error: 0.8981 - val_loss: 2.1629 - val_mean_squared_error: 2.1629\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 0.8777 - mean_squared_error: 0.8777 - val_loss: 2.8474 - val_mean_squared_error: 2.8474\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 0.8885 - mean_squared_error: 0.8885 - val_loss: 2.3664 - val_mean_squared_error: 2.3664\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.8689 - mean_squared_error: 0.8689 - val_loss: 2.2518 - val_mean_squared_error: 2.2518\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9085 - mean_squared_error: 0.9085 - val_loss: 2.4091 - val_mean_squared_error: 2.4091\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.0262 - mean_squared_error: 1.0262 - val_loss: 2.3562 - val_mean_squared_error: 2.3562\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.9319 - mean_squared_error: 0.9319 - val_loss: 2.2361 - val_mean_squared_error: 2.2361\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.8412 - mean_squared_error: 0.8412 - val_loss: 2.3402 - val_mean_squared_error: 2.3402\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 0.8830 - mean_squared_error: 0.8830 - val_loss: 2.3363 - val_mean_squared_error: 2.3363\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9089 - mean_squared_error: 0.9089 - val_loss: 2.2249 - val_mean_squared_error: 2.2249\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.8658 - mean_squared_error: 0.8658 - val_loss: 2.2872 - val_mean_squared_error: 2.2872\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.9162 - mean_squared_error: 0.9162 - val_loss: 2.2765 - val_mean_squared_error: 2.2765\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.9146 - mean_squared_error: 0.9146 - val_loss: 2.3920 - val_mean_squared_error: 2.3920\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.8279 - mean_squared_error: 0.8279 - val_loss: 2.3482 - val_mean_squared_error: 2.3482\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 0.8879 - mean_squared_error: 0.8879 - val_loss: 2.6417 - val_mean_squared_error: 2.6417\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 0.8857 - mean_squared_error: 0.8857 - val_loss: 2.1424 - val_mean_squared_error: 2.1424\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9053 - mean_squared_error: 0.9053 - val_loss: 2.2735 - val_mean_squared_error: 2.2735\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9171 - mean_squared_error: 0.9171 - val_loss: 2.3231 - val_mean_squared_error: 2.3231\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9193 - mean_squared_error: 0.9193 - val_loss: 2.5461 - val_mean_squared_error: 2.5461\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 0.8264 - mean_squared_error: 0.8264 - val_loss: 2.4633 - val_mean_squared_error: 2.4633\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.8161 - mean_squared_error: 0.8161 - val_loss: 2.3354 - val_mean_squared_error: 2.3354\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.7867 - mean_squared_error: 0.7867 - val_loss: 2.4526 - val_mean_squared_error: 2.4526\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 0.8838 - mean_squared_error: 0.8838 - val_loss: 2.3398 - val_mean_squared_error: 2.3398\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.8209 - mean_squared_error: 0.8209 - val_loss: 2.3276 - val_mean_squared_error: 2.3276\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 0.7827 - mean_squared_error: 0.7827 - val_loss: 2.3655 - val_mean_squared_error: 2.3655\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.7976 - mean_squared_error: 0.7976 - val_loss: 2.4401 - val_mean_squared_error: 2.4401\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.8704 - mean_squared_error: 0.8704 - val_loss: 2.5527 - val_mean_squared_error: 2.5527\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.7674 - mean_squared_error: 0.7674 - val_loss: 2.2928 - val_mean_squared_error: 2.2928\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.7478 - mean_squared_error: 0.7478 - val_loss: 2.3833 - val_mean_squared_error: 2.3833\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 0.8316 - mean_squared_error: 0.8316 - val_loss: 2.4501 - val_mean_squared_error: 2.4501\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.8223 - mean_squared_error: 0.8223 - val_loss: 2.3273 - val_mean_squared_error: 2.3273\n",
            "==================================================\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_42 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_42 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_56 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_43 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_57 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_58 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_73 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_59 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_74 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 1ms/sample - loss: 743.1793 - mean_squared_error: 743.1794 - val_loss: 411.6637 - val_mean_squared_error: 411.6637\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 30.7726 - mean_squared_error: 30.7726 - val_loss: 182.6833 - val_mean_squared_error: 182.6833\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 21.5008 - mean_squared_error: 21.5008 - val_loss: 126.1154 - val_mean_squared_error: 126.1154\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 17.6650 - mean_squared_error: 17.6650 - val_loss: 46.1510 - val_mean_squared_error: 46.1510\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 16.3376 - mean_squared_error: 16.3376 - val_loss: 27.6476 - val_mean_squared_error: 27.6476\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 16.5621 - mean_squared_error: 16.5621 - val_loss: 22.9356 - val_mean_squared_error: 22.9356\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 14.7701 - mean_squared_error: 14.7701 - val_loss: 14.1347 - val_mean_squared_error: 14.1347\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 13.4100 - mean_squared_error: 13.4100 - val_loss: 11.1211 - val_mean_squared_error: 11.1211\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 13.4876 - mean_squared_error: 13.4876 - val_loss: 12.9272 - val_mean_squared_error: 12.9272\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 13.7582 - mean_squared_error: 13.7582 - val_loss: 11.4282 - val_mean_squared_error: 11.4282\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 12.6098 - mean_squared_error: 12.6098 - val_loss: 10.5650 - val_mean_squared_error: 10.5650\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 11.9811 - mean_squared_error: 11.9811 - val_loss: 12.7312 - val_mean_squared_error: 12.7312\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 10.5635 - mean_squared_error: 10.5635 - val_loss: 9.8527 - val_mean_squared_error: 9.8527\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 10.8585 - mean_squared_error: 10.8585 - val_loss: 10.6739 - val_mean_squared_error: 10.6739\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 10.5921 - mean_squared_error: 10.5921 - val_loss: 16.0467 - val_mean_squared_error: 16.0467\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 10.0769 - mean_squared_error: 10.0769 - val_loss: 9.8508 - val_mean_squared_error: 9.8508\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 9.1165 - mean_squared_error: 9.1165 - val_loss: 9.4303 - val_mean_squared_error: 9.4303\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 7.7997 - mean_squared_error: 7.7997 - val_loss: 7.9824 - val_mean_squared_error: 7.9824\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 8.5688 - mean_squared_error: 8.5688 - val_loss: 7.9258 - val_mean_squared_error: 7.9258\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 7.7676 - mean_squared_error: 7.7676 - val_loss: 8.6440 - val_mean_squared_error: 8.6440\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 7.1361 - mean_squared_error: 7.1361 - val_loss: 7.0364 - val_mean_squared_error: 7.0364\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 6.5011 - mean_squared_error: 6.5011 - val_loss: 7.4390 - val_mean_squared_error: 7.4390\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 6.1379 - mean_squared_error: 6.1379 - val_loss: 6.2372 - val_mean_squared_error: 6.2372\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 5.5918 - mean_squared_error: 5.5918 - val_loss: 5.8169 - val_mean_squared_error: 5.8169\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 5.3805 - mean_squared_error: 5.3805 - val_loss: 5.5332 - val_mean_squared_error: 5.5332\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 5.3690 - mean_squared_error: 5.3690 - val_loss: 8.2816 - val_mean_squared_error: 8.2816\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 4.9244 - mean_squared_error: 4.9244 - val_loss: 5.3611 - val_mean_squared_error: 5.3611\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 4.8016 - mean_squared_error: 4.8016 - val_loss: 4.8772 - val_mean_squared_error: 4.8772\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 4.3167 - mean_squared_error: 4.3167 - val_loss: 4.7712 - val_mean_squared_error: 4.7712\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 4.3730 - mean_squared_error: 4.3730 - val_loss: 4.8211 - val_mean_squared_error: 4.8211\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 4.5438 - mean_squared_error: 4.5438 - val_loss: 4.2221 - val_mean_squared_error: 4.2221\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 3.9664 - mean_squared_error: 3.9664 - val_loss: 4.6743 - val_mean_squared_error: 4.6743\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.6629 - mean_squared_error: 3.6629 - val_loss: 4.1294 - val_mean_squared_error: 4.1294\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 3.6327 - mean_squared_error: 3.6327 - val_loss: 4.0934 - val_mean_squared_error: 4.0934\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 3.6399 - mean_squared_error: 3.6399 - val_loss: 4.5390 - val_mean_squared_error: 4.5390\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 3.6622 - mean_squared_error: 3.6622 - val_loss: 3.8313 - val_mean_squared_error: 3.8313\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 3.3605 - mean_squared_error: 3.3605 - val_loss: 4.0505 - val_mean_squared_error: 4.0505\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 3.1278 - mean_squared_error: 3.1278 - val_loss: 3.4002 - val_mean_squared_error: 3.4002\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 3.2628 - mean_squared_error: 3.2628 - val_loss: 4.6368 - val_mean_squared_error: 4.6368\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 3.2076 - mean_squared_error: 3.2076 - val_loss: 3.9250 - val_mean_squared_error: 3.9250\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 3.4058 - mean_squared_error: 3.4058 - val_loss: 3.6158 - val_mean_squared_error: 3.6158\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 3.0812 - mean_squared_error: 3.0812 - val_loss: 3.5619 - val_mean_squared_error: 3.5619\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.9096 - mean_squared_error: 2.9096 - val_loss: 3.5798 - val_mean_squared_error: 3.5798\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.6488 - mean_squared_error: 2.6488 - val_loss: 3.0100 - val_mean_squared_error: 3.0100\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.9331 - mean_squared_error: 2.9331 - val_loss: 3.7271 - val_mean_squared_error: 3.7271\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.9408 - mean_squared_error: 2.9408 - val_loss: 3.2108 - val_mean_squared_error: 3.2108\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.7952 - mean_squared_error: 2.7952 - val_loss: 3.3503 - val_mean_squared_error: 3.3503\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.4621 - mean_squared_error: 2.4621 - val_loss: 3.1263 - val_mean_squared_error: 3.1263\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.6798 - mean_squared_error: 2.6798 - val_loss: 3.5906 - val_mean_squared_error: 3.5906\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.5708 - mean_squared_error: 2.5708 - val_loss: 3.0743 - val_mean_squared_error: 3.0743\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 2.4658 - mean_squared_error: 2.4658 - val_loss: 3.1131 - val_mean_squared_error: 3.1131\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.5495 - mean_squared_error: 2.5495 - val_loss: 3.1338 - val_mean_squared_error: 3.1338\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.3966 - mean_squared_error: 2.3966 - val_loss: 2.9962 - val_mean_squared_error: 2.9962\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.4069 - mean_squared_error: 2.4069 - val_loss: 2.8434 - val_mean_squared_error: 2.8434\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.4481 - mean_squared_error: 2.4481 - val_loss: 4.0425 - val_mean_squared_error: 4.0425\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.2073 - mean_squared_error: 2.2073 - val_loss: 3.3820 - val_mean_squared_error: 3.3820\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.3938 - mean_squared_error: 2.3938 - val_loss: 2.9715 - val_mean_squared_error: 2.9715\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.4014 - mean_squared_error: 2.4014 - val_loss: 3.4581 - val_mean_squared_error: 3.4581\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.4768 - mean_squared_error: 2.4768 - val_loss: 3.3404 - val_mean_squared_error: 3.3404\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.3032 - mean_squared_error: 2.3032 - val_loss: 2.9064 - val_mean_squared_error: 2.9064\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.1181 - mean_squared_error: 2.1181 - val_loss: 3.2509 - val_mean_squared_error: 3.2509\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.0955 - mean_squared_error: 2.0955 - val_loss: 3.2530 - val_mean_squared_error: 3.2530\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.0910 - mean_squared_error: 2.0910 - val_loss: 2.9411 - val_mean_squared_error: 2.9411\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 2.0335 - mean_squared_error: 2.0335 - val_loss: 2.7311 - val_mean_squared_error: 2.7311\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.8403 - mean_squared_error: 1.8403 - val_loss: 2.8389 - val_mean_squared_error: 2.8389\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.9382 - mean_squared_error: 1.9382 - val_loss: 2.6558 - val_mean_squared_error: 2.6558\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.9388 - mean_squared_error: 1.9388 - val_loss: 2.9566 - val_mean_squared_error: 2.9566\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.9198 - mean_squared_error: 1.9198 - val_loss: 2.9705 - val_mean_squared_error: 2.9705\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 2.0065 - mean_squared_error: 2.0065 - val_loss: 2.8616 - val_mean_squared_error: 2.8616\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.8824 - mean_squared_error: 1.8824 - val_loss: 2.6958 - val_mean_squared_error: 2.6958\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.9751 - mean_squared_error: 1.9751 - val_loss: 2.8253 - val_mean_squared_error: 2.8253\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.9699 - mean_squared_error: 1.9699 - val_loss: 3.7658 - val_mean_squared_error: 3.7658\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.9771 - mean_squared_error: 1.9771 - val_loss: 3.3412 - val_mean_squared_error: 3.3412\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.1992 - mean_squared_error: 2.1992 - val_loss: 2.7774 - val_mean_squared_error: 2.7774\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.7563 - mean_squared_error: 1.7563 - val_loss: 2.8693 - val_mean_squared_error: 2.8693\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.7965 - mean_squared_error: 1.7965 - val_loss: 2.6083 - val_mean_squared_error: 2.6083\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.0056 - mean_squared_error: 2.0056 - val_loss: 4.0336 - val_mean_squared_error: 4.0336\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.9595 - mean_squared_error: 1.9595 - val_loss: 3.0917 - val_mean_squared_error: 3.0917\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.9030 - mean_squared_error: 1.9030 - val_loss: 3.0691 - val_mean_squared_error: 3.0691\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.7763 - mean_squared_error: 1.7763 - val_loss: 3.3287 - val_mean_squared_error: 3.3287\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.7053 - mean_squared_error: 1.7053 - val_loss: 2.7387 - val_mean_squared_error: 2.7387\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.7425 - mean_squared_error: 1.7425 - val_loss: 3.0481 - val_mean_squared_error: 3.0481\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.7605 - mean_squared_error: 1.7605 - val_loss: 3.0878 - val_mean_squared_error: 3.0878\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.8717 - mean_squared_error: 1.8717 - val_loss: 2.7271 - val_mean_squared_error: 2.7271\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.8512 - mean_squared_error: 1.8512 - val_loss: 3.2179 - val_mean_squared_error: 3.2179\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.8839 - mean_squared_error: 1.8839 - val_loss: 3.6014 - val_mean_squared_error: 3.6014\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.6234 - mean_squared_error: 1.6234 - val_loss: 3.2575 - val_mean_squared_error: 3.2575\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.7757 - mean_squared_error: 1.7757 - val_loss: 2.6265 - val_mean_squared_error: 2.6265\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.7103 - mean_squared_error: 1.7103 - val_loss: 2.7903 - val_mean_squared_error: 2.7903\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.6392 - mean_squared_error: 1.6392 - val_loss: 3.2799 - val_mean_squared_error: 3.2799\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.7466 - mean_squared_error: 1.7466 - val_loss: 2.7690 - val_mean_squared_error: 2.7690\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.5465 - mean_squared_error: 1.5465 - val_loss: 2.7383 - val_mean_squared_error: 2.7383\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.6128 - mean_squared_error: 1.6128 - val_loss: 2.8318 - val_mean_squared_error: 2.8318\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.7001 - mean_squared_error: 1.7001 - val_loss: 2.6507 - val_mean_squared_error: 2.6507\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4380 - mean_squared_error: 1.4380 - val_loss: 2.7746 - val_mean_squared_error: 2.7746\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.7599 - mean_squared_error: 1.7599 - val_loss: 2.7466 - val_mean_squared_error: 2.7466\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.6809 - mean_squared_error: 1.6809 - val_loss: 2.7988 - val_mean_squared_error: 2.7988\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.5231 - mean_squared_error: 1.5231 - val_loss: 2.6225 - val_mean_squared_error: 2.6225\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5390 - mean_squared_error: 1.5390 - val_loss: 2.7320 - val_mean_squared_error: 2.7320\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.6243 - mean_squared_error: 1.6243 - val_loss: 2.8335 - val_mean_squared_error: 2.8335\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.6067 - mean_squared_error: 1.6067 - val_loss: 2.7014 - val_mean_squared_error: 2.7014\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.5499 - mean_squared_error: 1.5499 - val_loss: 2.6371 - val_mean_squared_error: 2.6371\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.4416 - mean_squared_error: 1.4416 - val_loss: 2.6394 - val_mean_squared_error: 2.6394\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.3842 - mean_squared_error: 1.3842 - val_loss: 2.6173 - val_mean_squared_error: 2.6173\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.5424 - mean_squared_error: 1.5424 - val_loss: 2.8257 - val_mean_squared_error: 2.8257\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.5696 - mean_squared_error: 1.5696 - val_loss: 2.9212 - val_mean_squared_error: 2.9212\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.4325 - mean_squared_error: 1.4325 - val_loss: 2.7598 - val_mean_squared_error: 2.7598\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.3408 - mean_squared_error: 1.3408 - val_loss: 2.6592 - val_mean_squared_error: 2.6592\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.4234 - mean_squared_error: 1.4234 - val_loss: 2.5908 - val_mean_squared_error: 2.5908\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.3726 - mean_squared_error: 1.3726 - val_loss: 2.3100 - val_mean_squared_error: 2.3100\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.5268 - mean_squared_error: 1.5268 - val_loss: 2.7391 - val_mean_squared_error: 2.7391\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5120 - mean_squared_error: 1.5120 - val_loss: 2.4333 - val_mean_squared_error: 2.4333\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.3229 - mean_squared_error: 1.3229 - val_loss: 2.5627 - val_mean_squared_error: 2.5627\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.3861 - mean_squared_error: 1.3861 - val_loss: 2.7265 - val_mean_squared_error: 2.7265\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3503 - mean_squared_error: 1.3503 - val_loss: 2.3467 - val_mean_squared_error: 2.3467\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5341 - mean_squared_error: 1.5341 - val_loss: 2.7507 - val_mean_squared_error: 2.7507\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.2174 - mean_squared_error: 1.2174 - val_loss: 2.3585 - val_mean_squared_error: 2.3585\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.2284 - mean_squared_error: 1.2284 - val_loss: 2.5643 - val_mean_squared_error: 2.5643\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.3173 - mean_squared_error: 1.3173 - val_loss: 2.4127 - val_mean_squared_error: 2.4127\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.5239 - mean_squared_error: 1.5239 - val_loss: 2.8258 - val_mean_squared_error: 2.8258\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3991 - mean_squared_error: 1.3991 - val_loss: 2.5370 - val_mean_squared_error: 2.5370\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.2710 - mean_squared_error: 1.2710 - val_loss: 2.6370 - val_mean_squared_error: 2.6370\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.3357 - mean_squared_error: 1.3357 - val_loss: 2.4036 - val_mean_squared_error: 2.4036\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.1850 - mean_squared_error: 1.1850 - val_loss: 2.3080 - val_mean_squared_error: 2.3080\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.2844 - mean_squared_error: 1.2844 - val_loss: 2.5497 - val_mean_squared_error: 2.5497\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.3077 - mean_squared_error: 1.3077 - val_loss: 2.3727 - val_mean_squared_error: 2.3727\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.2766 - mean_squared_error: 1.2766 - val_loss: 2.3938 - val_mean_squared_error: 2.3938\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.3428 - mean_squared_error: 1.3428 - val_loss: 2.3915 - val_mean_squared_error: 2.3915\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.3314 - mean_squared_error: 1.3314 - val_loss: 2.3449 - val_mean_squared_error: 2.3449\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.2730 - mean_squared_error: 1.2730 - val_loss: 2.3921 - val_mean_squared_error: 2.3921\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.2292 - mean_squared_error: 1.2292 - val_loss: 2.4702 - val_mean_squared_error: 2.4702\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.2500 - mean_squared_error: 1.2500 - val_loss: 2.4635 - val_mean_squared_error: 2.4635\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.2592 - mean_squared_error: 1.2592 - val_loss: 2.2901 - val_mean_squared_error: 2.2901\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.2677 - mean_squared_error: 1.2677 - val_loss: 2.5292 - val_mean_squared_error: 2.5292\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1825 - mean_squared_error: 1.1825 - val_loss: 2.6145 - val_mean_squared_error: 2.6145\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.2642 - mean_squared_error: 1.2642 - val_loss: 2.7472 - val_mean_squared_error: 2.7472\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.3371 - mean_squared_error: 1.3371 - val_loss: 2.3433 - val_mean_squared_error: 2.3433\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.1301 - mean_squared_error: 1.1301 - val_loss: 2.2595 - val_mean_squared_error: 2.2595\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.1060 - mean_squared_error: 1.1060 - val_loss: 2.5805 - val_mean_squared_error: 2.5805\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3300 - mean_squared_error: 1.3300 - val_loss: 2.4703 - val_mean_squared_error: 2.4703\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0991 - mean_squared_error: 1.0991 - val_loss: 2.3172 - val_mean_squared_error: 2.3172\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.2393 - mean_squared_error: 1.2393 - val_loss: 2.3789 - val_mean_squared_error: 2.3789\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.0926 - mean_squared_error: 1.0926 - val_loss: 2.3790 - val_mean_squared_error: 2.3790\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.1138 - mean_squared_error: 1.1138 - val_loss: 2.2285 - val_mean_squared_error: 2.2285\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1540 - mean_squared_error: 1.1540 - val_loss: 2.4760 - val_mean_squared_error: 2.4760\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 2.2075 - val_mean_squared_error: 2.2075\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.9867 - mean_squared_error: 0.9867 - val_loss: 2.1563 - val_mean_squared_error: 2.1563\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.0464 - mean_squared_error: 1.0464 - val_loss: 2.2675 - val_mean_squared_error: 2.2675\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.1406 - mean_squared_error: 1.1406 - val_loss: 2.8604 - val_mean_squared_error: 2.8604\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.0849 - mean_squared_error: 1.0849 - val_loss: 2.4311 - val_mean_squared_error: 2.4311\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.3010 - mean_squared_error: 1.3010 - val_loss: 2.6315 - val_mean_squared_error: 2.6315\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.1262 - mean_squared_error: 1.1262 - val_loss: 2.2936 - val_mean_squared_error: 2.2936\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1548 - mean_squared_error: 1.1548 - val_loss: 2.8016 - val_mean_squared_error: 2.8016\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.2050 - mean_squared_error: 1.2050 - val_loss: 2.1916 - val_mean_squared_error: 2.1916\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0468 - mean_squared_error: 1.0468 - val_loss: 2.4310 - val_mean_squared_error: 2.4310\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.1209 - mean_squared_error: 1.1209 - val_loss: 2.5331 - val_mean_squared_error: 2.5331\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.2376 - mean_squared_error: 1.2376 - val_loss: 2.3343 - val_mean_squared_error: 2.3343\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1616 - mean_squared_error: 1.1616 - val_loss: 2.2910 - val_mean_squared_error: 2.2910\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0762 - mean_squared_error: 1.0762 - val_loss: 2.3514 - val_mean_squared_error: 2.3514\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0311 - mean_squared_error: 1.0311 - val_loss: 2.5703 - val_mean_squared_error: 2.5703\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 0.9724 - mean_squared_error: 0.9724 - val_loss: 2.1681 - val_mean_squared_error: 2.1681\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.0692 - mean_squared_error: 1.0692 - val_loss: 2.3325 - val_mean_squared_error: 2.3325\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1638 - mean_squared_error: 1.1638 - val_loss: 2.2081 - val_mean_squared_error: 2.2081\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.0344 - mean_squared_error: 1.0344 - val_loss: 2.3698 - val_mean_squared_error: 2.3698\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0653 - mean_squared_error: 1.0653 - val_loss: 2.1086 - val_mean_squared_error: 2.1086\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0369 - mean_squared_error: 1.0369 - val_loss: 2.2235 - val_mean_squared_error: 2.2235\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 2.3070 - val_mean_squared_error: 2.3070\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0775 - mean_squared_error: 1.0775 - val_loss: 2.3624 - val_mean_squared_error: 2.3624\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.0813 - mean_squared_error: 1.0813 - val_loss: 2.5639 - val_mean_squared_error: 2.5639\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 2.1615 - val_mean_squared_error: 2.1615\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0290 - mean_squared_error: 1.0290 - val_loss: 2.3684 - val_mean_squared_error: 2.3684\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.9511 - mean_squared_error: 0.9511 - val_loss: 2.3360 - val_mean_squared_error: 2.3360\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9427 - mean_squared_error: 0.9427 - val_loss: 2.2741 - val_mean_squared_error: 2.2741\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.0270 - mean_squared_error: 1.0270 - val_loss: 2.1451 - val_mean_squared_error: 2.1451\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 2.3579 - val_mean_squared_error: 2.3579\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 0.9364 - mean_squared_error: 0.9364 - val_loss: 2.2932 - val_mean_squared_error: 2.2932\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.0271 - mean_squared_error: 1.0271 - val_loss: 2.3476 - val_mean_squared_error: 2.3476\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 0.9349 - mean_squared_error: 0.9349 - val_loss: 2.3855 - val_mean_squared_error: 2.3855\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 0.9712 - mean_squared_error: 0.9712 - val_loss: 2.2780 - val_mean_squared_error: 2.2780\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.8912 - mean_squared_error: 0.8912 - val_loss: 2.1343 - val_mean_squared_error: 2.1343\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.8843 - mean_squared_error: 0.8843 - val_loss: 2.2464 - val_mean_squared_error: 2.2464\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 0.9682 - mean_squared_error: 0.9682 - val_loss: 2.3887 - val_mean_squared_error: 2.3887\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.9269 - mean_squared_error: 0.9269 - val_loss: 2.1364 - val_mean_squared_error: 2.1364\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9473 - mean_squared_error: 0.9473 - val_loss: 2.1062 - val_mean_squared_error: 2.1062\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9564 - mean_squared_error: 0.9564 - val_loss: 2.3744 - val_mean_squared_error: 2.3744\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 0.9813 - mean_squared_error: 0.9813 - val_loss: 2.2241 - val_mean_squared_error: 2.2241\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.9978 - mean_squared_error: 0.9978 - val_loss: 2.2983 - val_mean_squared_error: 2.2983\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 2.5100 - val_mean_squared_error: 2.5100\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 2.2739 - val_mean_squared_error: 2.2739\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 2.2801 - val_mean_squared_error: 2.2801\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.8900 - mean_squared_error: 0.8900 - val_loss: 2.0954 - val_mean_squared_error: 2.0954\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.8586 - mean_squared_error: 0.8586 - val_loss: 2.3252 - val_mean_squared_error: 2.3252\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.8172 - mean_squared_error: 0.8172 - val_loss: 2.2352 - val_mean_squared_error: 2.2352\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 0.8880 - mean_squared_error: 0.8880 - val_loss: 2.3229 - val_mean_squared_error: 2.3229\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.9048 - mean_squared_error: 0.9048 - val_loss: 2.5670 - val_mean_squared_error: 2.5670\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 0.8865 - mean_squared_error: 0.8865 - val_loss: 2.2735 - val_mean_squared_error: 2.2735\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 0.9180 - mean_squared_error: 0.9180 - val_loss: 2.2548 - val_mean_squared_error: 2.2548\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.8912 - mean_squared_error: 0.8912 - val_loss: 2.3753 - val_mean_squared_error: 2.3753\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 0.8958 - mean_squared_error: 0.8958 - val_loss: 2.2707 - val_mean_squared_error: 2.2707\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.8132 - mean_squared_error: 0.8132 - val_loss: 2.1822 - val_mean_squared_error: 2.1822\n",
            "==================================================\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_45 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_75 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_60 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_76 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_61 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_77 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_62 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_78 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_63 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_79 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 753.0562 - mean_squared_error: 753.0562 - val_loss: 250.2811 - val_mean_squared_error: 250.2812\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 26.0089 - mean_squared_error: 26.0089 - val_loss: 159.0666 - val_mean_squared_error: 159.0666\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 23.2541 - mean_squared_error: 23.2541 - val_loss: 87.8484 - val_mean_squared_error: 87.8484\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 20.4509 - mean_squared_error: 20.4509 - val_loss: 57.9091 - val_mean_squared_error: 57.9091\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 18.5957 - mean_squared_error: 18.5957 - val_loss: 43.3002 - val_mean_squared_error: 43.3002\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 15.0023 - mean_squared_error: 15.0023 - val_loss: 23.3918 - val_mean_squared_error: 23.3918\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 14.2971 - mean_squared_error: 14.2971 - val_loss: 11.2723 - val_mean_squared_error: 11.2723\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 13.8887 - mean_squared_error: 13.8887 - val_loss: 15.3285 - val_mean_squared_error: 15.3285\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 13.4735 - mean_squared_error: 13.4735 - val_loss: 10.2951 - val_mean_squared_error: 10.2951\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 11.4351 - mean_squared_error: 11.4351 - val_loss: 10.5614 - val_mean_squared_error: 10.5614\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 11.4157 - mean_squared_error: 11.4157 - val_loss: 10.7054 - val_mean_squared_error: 10.7054\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 11.4259 - mean_squared_error: 11.4259 - val_loss: 10.5276 - val_mean_squared_error: 10.5276\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 10.2537 - mean_squared_error: 10.2537 - val_loss: 9.5482 - val_mean_squared_error: 9.5482\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 11.0130 - mean_squared_error: 11.0130 - val_loss: 10.3718 - val_mean_squared_error: 10.3718\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 9.7294 - mean_squared_error: 9.7294 - val_loss: 10.2092 - val_mean_squared_error: 10.2092\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 9.5422 - mean_squared_error: 9.5422 - val_loss: 8.5970 - val_mean_squared_error: 8.5970\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 8.6297 - mean_squared_error: 8.6297 - val_loss: 8.8894 - val_mean_squared_error: 8.8894\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 8.3929 - mean_squared_error: 8.3929 - val_loss: 7.9015 - val_mean_squared_error: 7.9015\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 8.3007 - mean_squared_error: 8.3007 - val_loss: 8.0452 - val_mean_squared_error: 8.0452\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 7.6472 - mean_squared_error: 7.6472 - val_loss: 6.3099 - val_mean_squared_error: 6.3099\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 6.6874 - mean_squared_error: 6.6874 - val_loss: 8.5467 - val_mean_squared_error: 8.5467\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 5.9739 - mean_squared_error: 5.9739 - val_loss: 8.2092 - val_mean_squared_error: 8.2092\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 5.5713 - mean_squared_error: 5.5713 - val_loss: 5.7632 - val_mean_squared_error: 5.7632\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 5.3579 - mean_squared_error: 5.3579 - val_loss: 5.2074 - val_mean_squared_error: 5.2074\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 5.2532 - mean_squared_error: 5.2532 - val_loss: 8.2307 - val_mean_squared_error: 8.2307\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 5.1055 - mean_squared_error: 5.1055 - val_loss: 5.0655 - val_mean_squared_error: 5.0655\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 4.6763 - mean_squared_error: 4.6763 - val_loss: 4.8017 - val_mean_squared_error: 4.8017\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 5.3340 - mean_squared_error: 5.3340 - val_loss: 5.0530 - val_mean_squared_error: 5.0530\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 4.6261 - mean_squared_error: 4.6261 - val_loss: 4.7111 - val_mean_squared_error: 4.7111\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 4.4422 - mean_squared_error: 4.4422 - val_loss: 4.7063 - val_mean_squared_error: 4.7063\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 4.1595 - mean_squared_error: 4.1595 - val_loss: 4.1052 - val_mean_squared_error: 4.1052\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 4.2677 - mean_squared_error: 4.2677 - val_loss: 4.6181 - val_mean_squared_error: 4.6181\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 3.9908 - mean_squared_error: 3.9908 - val_loss: 4.8473 - val_mean_squared_error: 4.8473\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 3.9810 - mean_squared_error: 3.9810 - val_loss: 3.6960 - val_mean_squared_error: 3.6960\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 4.2928 - mean_squared_error: 4.2928 - val_loss: 3.5762 - val_mean_squared_error: 3.5762\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 3.7905 - mean_squared_error: 3.7905 - val_loss: 3.7584 - val_mean_squared_error: 3.7584\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 3.7207 - mean_squared_error: 3.7207 - val_loss: 4.7404 - val_mean_squared_error: 4.7404\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 3.7188 - mean_squared_error: 3.7188 - val_loss: 3.7877 - val_mean_squared_error: 3.7877\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 3.4081 - mean_squared_error: 3.4081 - val_loss: 3.5337 - val_mean_squared_error: 3.5337\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 3.2681 - mean_squared_error: 3.2681 - val_loss: 3.6266 - val_mean_squared_error: 3.6266\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 3.1823 - mean_squared_error: 3.1823 - val_loss: 3.5721 - val_mean_squared_error: 3.5721\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 3.1914 - mean_squared_error: 3.1914 - val_loss: 4.3131 - val_mean_squared_error: 4.3131\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 3.0698 - mean_squared_error: 3.0698 - val_loss: 4.0227 - val_mean_squared_error: 4.0227\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.9318 - mean_squared_error: 2.9318 - val_loss: 3.3166 - val_mean_squared_error: 3.3166\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.8608 - mean_squared_error: 2.8608 - val_loss: 2.9856 - val_mean_squared_error: 2.9856\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 3.1245 - mean_squared_error: 3.1245 - val_loss: 3.9469 - val_mean_squared_error: 3.9469\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 3.0963 - mean_squared_error: 3.0963 - val_loss: 3.1999 - val_mean_squared_error: 3.1999\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 3.0552 - mean_squared_error: 3.0552 - val_loss: 3.1298 - val_mean_squared_error: 3.1298\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.6983 - mean_squared_error: 2.6983 - val_loss: 3.7537 - val_mean_squared_error: 3.7537\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.5780 - mean_squared_error: 2.5780 - val_loss: 2.9933 - val_mean_squared_error: 2.9933\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.4753 - mean_squared_error: 2.4753 - val_loss: 2.7652 - val_mean_squared_error: 2.7652\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.6374 - mean_squared_error: 2.6374 - val_loss: 3.2536 - val_mean_squared_error: 3.2536\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.5934 - mean_squared_error: 2.5934 - val_loss: 3.0862 - val_mean_squared_error: 3.0862\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 426us/sample - loss: 2.6970 - mean_squared_error: 2.6970 - val_loss: 4.2265 - val_mean_squared_error: 4.2265\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 2.5365 - mean_squared_error: 2.5365 - val_loss: 2.5958 - val_mean_squared_error: 2.5958\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.4086 - mean_squared_error: 2.4086 - val_loss: 3.0449 - val_mean_squared_error: 3.0449\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.4635 - mean_squared_error: 2.4635 - val_loss: 3.0666 - val_mean_squared_error: 3.0666\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.2313 - mean_squared_error: 2.2313 - val_loss: 3.2674 - val_mean_squared_error: 3.2674\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.3394 - mean_squared_error: 2.3394 - val_loss: 2.9134 - val_mean_squared_error: 2.9134\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.4582 - mean_squared_error: 2.4582 - val_loss: 3.0992 - val_mean_squared_error: 3.0992\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.5455 - mean_squared_error: 2.5455 - val_loss: 2.4473 - val_mean_squared_error: 2.4473\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.3015 - mean_squared_error: 2.3015 - val_loss: 3.1394 - val_mean_squared_error: 3.1394\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.2267 - mean_squared_error: 2.2267 - val_loss: 2.9810 - val_mean_squared_error: 2.9810\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.1398 - mean_squared_error: 2.1398 - val_loss: 2.6451 - val_mean_squared_error: 2.6451\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 2.4800 - mean_squared_error: 2.4800 - val_loss: 2.9124 - val_mean_squared_error: 2.9124\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.1483 - mean_squared_error: 2.1483 - val_loss: 3.2527 - val_mean_squared_error: 3.2527\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.2066 - mean_squared_error: 2.2066 - val_loss: 2.9694 - val_mean_squared_error: 2.9694\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.1634 - mean_squared_error: 2.1634 - val_loss: 2.6760 - val_mean_squared_error: 2.6760\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 2.1036 - mean_squared_error: 2.1036 - val_loss: 3.2113 - val_mean_squared_error: 3.2113\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.0705 - mean_squared_error: 2.0705 - val_loss: 2.5099 - val_mean_squared_error: 2.5099\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.2092 - mean_squared_error: 2.2092 - val_loss: 2.9634 - val_mean_squared_error: 2.9634\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.1668 - mean_squared_error: 2.1668 - val_loss: 3.0380 - val_mean_squared_error: 3.0380\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.9623 - mean_squared_error: 1.9623 - val_loss: 2.6299 - val_mean_squared_error: 2.6299\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.9850 - mean_squared_error: 1.9850 - val_loss: 2.5319 - val_mean_squared_error: 2.5319\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.0601 - mean_squared_error: 2.0601 - val_loss: 3.2501 - val_mean_squared_error: 3.2501\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.1172 - mean_squared_error: 2.1172 - val_loss: 2.6181 - val_mean_squared_error: 2.6181\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.9964 - mean_squared_error: 1.9964 - val_loss: 2.6968 - val_mean_squared_error: 2.6968\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.8942 - mean_squared_error: 1.8942 - val_loss: 2.9468 - val_mean_squared_error: 2.9468\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.9665 - mean_squared_error: 1.9665 - val_loss: 2.9503 - val_mean_squared_error: 2.9503\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.8623 - mean_squared_error: 1.8623 - val_loss: 2.8835 - val_mean_squared_error: 2.8835\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.0771 - mean_squared_error: 2.0771 - val_loss: 2.3437 - val_mean_squared_error: 2.3437\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.9082 - mean_squared_error: 1.9082 - val_loss: 3.0660 - val_mean_squared_error: 3.0660\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.7986 - mean_squared_error: 1.7986 - val_loss: 2.9077 - val_mean_squared_error: 2.9077\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.7632 - mean_squared_error: 1.7632 - val_loss: 2.6371 - val_mean_squared_error: 2.6371\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.8367 - mean_squared_error: 1.8367 - val_loss: 2.3572 - val_mean_squared_error: 2.3572\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.8043 - mean_squared_error: 1.8043 - val_loss: 2.5936 - val_mean_squared_error: 2.5936\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.9114 - mean_squared_error: 1.9114 - val_loss: 2.4415 - val_mean_squared_error: 2.4415\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.7531 - mean_squared_error: 1.7531 - val_loss: 2.5498 - val_mean_squared_error: 2.5498\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.7876 - mean_squared_error: 1.7876 - val_loss: 2.3366 - val_mean_squared_error: 2.3366\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.7784 - mean_squared_error: 1.7784 - val_loss: 2.6403 - val_mean_squared_error: 2.6403\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.8427 - mean_squared_error: 1.8427 - val_loss: 2.5063 - val_mean_squared_error: 2.5063\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.9331 - mean_squared_error: 1.9331 - val_loss: 2.7074 - val_mean_squared_error: 2.7074\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.0808 - mean_squared_error: 2.0808 - val_loss: 2.8196 - val_mean_squared_error: 2.8196\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.7893 - mean_squared_error: 1.7893 - val_loss: 2.4076 - val_mean_squared_error: 2.4076\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.6584 - mean_squared_error: 1.6584 - val_loss: 2.5904 - val_mean_squared_error: 2.5904\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.7397 - mean_squared_error: 1.7397 - val_loss: 2.5156 - val_mean_squared_error: 2.5156\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.8046 - mean_squared_error: 1.8046 - val_loss: 2.5374 - val_mean_squared_error: 2.5374\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.5662 - mean_squared_error: 1.5662 - val_loss: 2.2277 - val_mean_squared_error: 2.2277\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.7006 - mean_squared_error: 1.7006 - val_loss: 2.5084 - val_mean_squared_error: 2.5084\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.7823 - mean_squared_error: 1.7823 - val_loss: 2.4548 - val_mean_squared_error: 2.4548\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.7700 - mean_squared_error: 1.7700 - val_loss: 2.7092 - val_mean_squared_error: 2.7092\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.6116 - mean_squared_error: 1.6116 - val_loss: 2.4991 - val_mean_squared_error: 2.4991\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.5326 - mean_squared_error: 1.5326 - val_loss: 2.7656 - val_mean_squared_error: 2.7656\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.5278 - mean_squared_error: 1.5278 - val_loss: 2.2559 - val_mean_squared_error: 2.2559\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.5933 - mean_squared_error: 1.5933 - val_loss: 3.4635 - val_mean_squared_error: 3.4635\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.6319 - mean_squared_error: 1.6319 - val_loss: 2.6492 - val_mean_squared_error: 2.6492\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.5915 - mean_squared_error: 1.5915 - val_loss: 2.1926 - val_mean_squared_error: 2.1926\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.6261 - mean_squared_error: 1.6261 - val_loss: 2.1652 - val_mean_squared_error: 2.1652\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.4626 - mean_squared_error: 1.4626 - val_loss: 2.7071 - val_mean_squared_error: 2.7071\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.5993 - mean_squared_error: 1.5993 - val_loss: 2.7569 - val_mean_squared_error: 2.7569\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.4946 - mean_squared_error: 1.4946 - val_loss: 2.2736 - val_mean_squared_error: 2.2736\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.4976 - mean_squared_error: 1.4976 - val_loss: 2.4347 - val_mean_squared_error: 2.4347\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.5286 - mean_squared_error: 1.5286 - val_loss: 2.4146 - val_mean_squared_error: 2.4146\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.5504 - mean_squared_error: 1.5504 - val_loss: 2.2641 - val_mean_squared_error: 2.2641\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 1.4255 - mean_squared_error: 1.4255 - val_loss: 2.4199 - val_mean_squared_error: 2.4199\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.4024 - mean_squared_error: 1.4024 - val_loss: 2.1516 - val_mean_squared_error: 2.1516\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.3945 - mean_squared_error: 1.3945 - val_loss: 2.4137 - val_mean_squared_error: 2.4137\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.5724 - mean_squared_error: 1.5724 - val_loss: 2.3808 - val_mean_squared_error: 2.3808\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.4060 - mean_squared_error: 1.4060 - val_loss: 2.1345 - val_mean_squared_error: 2.1345\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.3566 - mean_squared_error: 1.3566 - val_loss: 2.3118 - val_mean_squared_error: 2.3118\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.4805 - mean_squared_error: 1.4805 - val_loss: 2.2211 - val_mean_squared_error: 2.2211\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3435 - mean_squared_error: 1.3435 - val_loss: 2.4292 - val_mean_squared_error: 2.4292\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.4961 - mean_squared_error: 1.4961 - val_loss: 2.1314 - val_mean_squared_error: 2.1314\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.3918 - mean_squared_error: 1.3918 - val_loss: 2.0738 - val_mean_squared_error: 2.0738\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.4596 - mean_squared_error: 1.4596 - val_loss: 2.3113 - val_mean_squared_error: 2.3113\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3738 - mean_squared_error: 1.3738 - val_loss: 2.0718 - val_mean_squared_error: 2.0718\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.2775 - mean_squared_error: 1.2775 - val_loss: 2.1011 - val_mean_squared_error: 2.1011\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.3061 - mean_squared_error: 1.3061 - val_loss: 2.3051 - val_mean_squared_error: 2.3051\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3622 - mean_squared_error: 1.3622 - val_loss: 2.3407 - val_mean_squared_error: 2.3407\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 437us/sample - loss: 1.3996 - mean_squared_error: 1.3996 - val_loss: 2.3482 - val_mean_squared_error: 2.3482\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.3554 - mean_squared_error: 1.3554 - val_loss: 2.3414 - val_mean_squared_error: 2.3414\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.2544 - mean_squared_error: 1.2544 - val_loss: 2.2672 - val_mean_squared_error: 2.2672\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.2665 - mean_squared_error: 1.2665 - val_loss: 2.2190 - val_mean_squared_error: 2.2190\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.2998 - mean_squared_error: 1.2998 - val_loss: 2.4626 - val_mean_squared_error: 2.4626\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.2901 - mean_squared_error: 1.2901 - val_loss: 2.5971 - val_mean_squared_error: 2.5971\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.2352 - mean_squared_error: 1.2352 - val_loss: 2.3038 - val_mean_squared_error: 2.3038\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.1953 - mean_squared_error: 1.1953 - val_loss: 2.1566 - val_mean_squared_error: 2.1566\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.2685 - mean_squared_error: 1.2685 - val_loss: 2.4261 - val_mean_squared_error: 2.4261\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.4433 - mean_squared_error: 1.4433 - val_loss: 2.5511 - val_mean_squared_error: 2.5511\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.2827 - mean_squared_error: 1.2827 - val_loss: 2.2710 - val_mean_squared_error: 2.2710\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.1893 - mean_squared_error: 1.1893 - val_loss: 2.2059 - val_mean_squared_error: 2.2059\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.3752 - mean_squared_error: 1.3752 - val_loss: 2.4817 - val_mean_squared_error: 2.4817\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.1852 - mean_squared_error: 1.1852 - val_loss: 2.1824 - val_mean_squared_error: 2.1824\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.2435 - mean_squared_error: 1.2435 - val_loss: 2.2571 - val_mean_squared_error: 2.2571\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.2307 - mean_squared_error: 1.2307 - val_loss: 2.2904 - val_mean_squared_error: 2.2904\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.2651 - mean_squared_error: 1.2651 - val_loss: 2.1194 - val_mean_squared_error: 2.1194\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.1839 - mean_squared_error: 1.1839 - val_loss: 2.4064 - val_mean_squared_error: 2.4064\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.1142 - mean_squared_error: 1.1142 - val_loss: 2.1343 - val_mean_squared_error: 2.1343\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.2381 - mean_squared_error: 1.2381 - val_loss: 2.3711 - val_mean_squared_error: 2.3711\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.2033 - mean_squared_error: 1.2033 - val_loss: 2.1879 - val_mean_squared_error: 2.1879\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.2752 - mean_squared_error: 1.2752 - val_loss: 2.1999 - val_mean_squared_error: 2.1999\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.1783 - mean_squared_error: 1.1783 - val_loss: 2.3195 - val_mean_squared_error: 2.3195\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.1661 - mean_squared_error: 1.1661 - val_loss: 2.1442 - val_mean_squared_error: 2.1442\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.2431 - mean_squared_error: 1.2431 - val_loss: 2.3498 - val_mean_squared_error: 2.3498\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0818 - mean_squared_error: 1.0818 - val_loss: 2.1458 - val_mean_squared_error: 2.1458\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.2012 - mean_squared_error: 1.2012 - val_loss: 2.0415 - val_mean_squared_error: 2.0415\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.2551 - mean_squared_error: 1.2551 - val_loss: 2.0954 - val_mean_squared_error: 2.0954\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1134 - mean_squared_error: 1.1134 - val_loss: 2.0387 - val_mean_squared_error: 2.0387\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.1197 - mean_squared_error: 1.1197 - val_loss: 2.2099 - val_mean_squared_error: 2.2099\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.0515 - mean_squared_error: 1.0515 - val_loss: 2.0813 - val_mean_squared_error: 2.0813\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.1420 - mean_squared_error: 1.1420 - val_loss: 2.4952 - val_mean_squared_error: 2.4952\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1015 - mean_squared_error: 1.1015 - val_loss: 2.2521 - val_mean_squared_error: 2.2521\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.0384 - mean_squared_error: 1.0384 - val_loss: 2.3415 - val_mean_squared_error: 2.3415\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.1777 - mean_squared_error: 1.1777 - val_loss: 2.3495 - val_mean_squared_error: 2.3495\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 2.0559 - val_mean_squared_error: 2.0559\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.0493 - mean_squared_error: 1.0493 - val_loss: 2.2231 - val_mean_squared_error: 2.2231\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.1037 - mean_squared_error: 1.1037 - val_loss: 2.1634 - val_mean_squared_error: 2.1634\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.1058 - mean_squared_error: 1.1058 - val_loss: 2.2906 - val_mean_squared_error: 2.2906\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0844 - mean_squared_error: 1.0844 - val_loss: 2.2644 - val_mean_squared_error: 2.2644\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.1728 - mean_squared_error: 1.1728 - val_loss: 2.3525 - val_mean_squared_error: 2.3525\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.0744 - mean_squared_error: 1.0744 - val_loss: 2.1168 - val_mean_squared_error: 2.1168\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 2.1928 - val_mean_squared_error: 2.1928\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0883 - mean_squared_error: 1.0883 - val_loss: 2.2490 - val_mean_squared_error: 2.2490\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0579 - mean_squared_error: 1.0579 - val_loss: 2.1001 - val_mean_squared_error: 2.1001\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.1175 - mean_squared_error: 1.1175 - val_loss: 2.2695 - val_mean_squared_error: 2.2695\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.0310 - mean_squared_error: 1.0310 - val_loss: 2.0140 - val_mean_squared_error: 2.0140\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0445 - mean_squared_error: 1.0445 - val_loss: 2.2986 - val_mean_squared_error: 2.2986\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.1041 - mean_squared_error: 1.1041 - val_loss: 2.1554 - val_mean_squared_error: 2.1554\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.0976 - mean_squared_error: 1.0977 - val_loss: 2.1259 - val_mean_squared_error: 2.1259\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0975 - mean_squared_error: 1.0975 - val_loss: 2.1448 - val_mean_squared_error: 2.1448\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 2.2203 - val_mean_squared_error: 2.2203\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9202 - mean_squared_error: 0.9202 - val_loss: 2.0054 - val_mean_squared_error: 2.0054\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0016 - mean_squared_error: 1.0016 - val_loss: 2.0989 - val_mean_squared_error: 2.0989\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9475 - mean_squared_error: 0.9475 - val_loss: 1.9835 - val_mean_squared_error: 1.9835\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.9256 - mean_squared_error: 0.9256 - val_loss: 2.1247 - val_mean_squared_error: 2.1247\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.9553 - mean_squared_error: 0.9553 - val_loss: 2.0015 - val_mean_squared_error: 2.0015\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 2.0495 - val_mean_squared_error: 2.0495\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.0277 - mean_squared_error: 1.0277 - val_loss: 2.2133 - val_mean_squared_error: 2.2133\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 2.1871 - val_mean_squared_error: 2.1871\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 0.9955 - mean_squared_error: 0.9955 - val_loss: 2.1908 - val_mean_squared_error: 2.1908\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0347 - mean_squared_error: 1.0347 - val_loss: 2.2908 - val_mean_squared_error: 2.2908\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.9616 - mean_squared_error: 0.9616 - val_loss: 2.0805 - val_mean_squared_error: 2.0805\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 0.9558 - mean_squared_error: 0.9558 - val_loss: 2.0264 - val_mean_squared_error: 2.0264\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9089 - mean_squared_error: 0.9089 - val_loss: 2.1074 - val_mean_squared_error: 2.1074\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 0.9156 - mean_squared_error: 0.9156 - val_loss: 2.1308 - val_mean_squared_error: 2.1308\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.8591 - mean_squared_error: 0.8591 - val_loss: 2.1233 - val_mean_squared_error: 2.1233\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 0.9310 - mean_squared_error: 0.9310 - val_loss: 2.2993 - val_mean_squared_error: 2.2993\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.9521 - mean_squared_error: 0.9521 - val_loss: 2.0933 - val_mean_squared_error: 2.0933\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.9338 - mean_squared_error: 0.9338 - val_loss: 2.0310 - val_mean_squared_error: 2.0310\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.9333 - mean_squared_error: 0.9333 - val_loss: 1.9910 - val_mean_squared_error: 1.9910\n",
            "==================================================\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_48 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_80 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_48 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_64 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_81 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_49 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_65 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_82 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_50 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_66 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_16 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_83 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_67 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_84 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 740.7165 - mean_squared_error: 740.7165 - val_loss: 150.2956 - val_mean_squared_error: 150.2956\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 24.9611 - mean_squared_error: 24.9611 - val_loss: 215.6149 - val_mean_squared_error: 215.6149\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 22.9044 - mean_squared_error: 22.9044 - val_loss: 98.3007 - val_mean_squared_error: 98.3007\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 20.3983 - mean_squared_error: 20.3983 - val_loss: 42.8952 - val_mean_squared_error: 42.8952\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 18.9170 - mean_squared_error: 18.9170 - val_loss: 46.5930 - val_mean_squared_error: 46.5930\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 17.7848 - mean_squared_error: 17.7848 - val_loss: 20.2060 - val_mean_squared_error: 20.2060\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 18.4044 - mean_squared_error: 18.4044 - val_loss: 12.3409 - val_mean_squared_error: 12.3409\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 426us/sample - loss: 14.4934 - mean_squared_error: 14.4934 - val_loss: 17.3885 - val_mean_squared_error: 17.3885\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 15.2179 - mean_squared_error: 15.2179 - val_loss: 11.7173 - val_mean_squared_error: 11.7173\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 14.4597 - mean_squared_error: 14.4597 - val_loss: 13.5235 - val_mean_squared_error: 13.5235\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 13.5792 - mean_squared_error: 13.5792 - val_loss: 10.9821 - val_mean_squared_error: 10.9821\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 13.4026 - mean_squared_error: 13.4026 - val_loss: 13.3499 - val_mean_squared_error: 13.3499\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 11.5951 - mean_squared_error: 11.5951 - val_loss: 9.6882 - val_mean_squared_error: 9.6882\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 11.6850 - mean_squared_error: 11.6850 - val_loss: 12.2045 - val_mean_squared_error: 12.2045\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 10.2340 - mean_squared_error: 10.2340 - val_loss: 10.0193 - val_mean_squared_error: 10.0193\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 11.4431 - mean_squared_error: 11.4431 - val_loss: 11.0818 - val_mean_squared_error: 11.0818\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 11.7257 - mean_squared_error: 11.7257 - val_loss: 12.7232 - val_mean_squared_error: 12.7232\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 9.2755 - mean_squared_error: 9.2755 - val_loss: 8.6970 - val_mean_squared_error: 8.6970\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 8.2508 - mean_squared_error: 8.2508 - val_loss: 9.2268 - val_mean_squared_error: 9.2268\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 8.4637 - mean_squared_error: 8.4637 - val_loss: 7.9339 - val_mean_squared_error: 7.9339\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 7.5296 - mean_squared_error: 7.5296 - val_loss: 7.2865 - val_mean_squared_error: 7.2865\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 6.8334 - mean_squared_error: 6.8334 - val_loss: 14.3107 - val_mean_squared_error: 14.3107\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 6.9663 - mean_squared_error: 6.9663 - val_loss: 13.5646 - val_mean_squared_error: 13.5646\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 6.6484 - mean_squared_error: 6.6484 - val_loss: 11.0407 - val_mean_squared_error: 11.0407\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 6.8678 - mean_squared_error: 6.8678 - val_loss: 5.0086 - val_mean_squared_error: 5.0086\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 5.9208 - mean_squared_error: 5.9208 - val_loss: 6.3669 - val_mean_squared_error: 6.3669\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 5.6869 - mean_squared_error: 5.6869 - val_loss: 5.5930 - val_mean_squared_error: 5.5930\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 5.2893 - mean_squared_error: 5.2893 - val_loss: 5.9945 - val_mean_squared_error: 5.9945\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 5.2279 - mean_squared_error: 5.2279 - val_loss: 5.9377 - val_mean_squared_error: 5.9377\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 4.9471 - mean_squared_error: 4.9471 - val_loss: 4.9048 - val_mean_squared_error: 4.9048\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 4.9557 - mean_squared_error: 4.9557 - val_loss: 4.1596 - val_mean_squared_error: 4.1596\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 4.3408 - mean_squared_error: 4.3408 - val_loss: 5.1557 - val_mean_squared_error: 5.1557\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 4.3376 - mean_squared_error: 4.3376 - val_loss: 5.8924 - val_mean_squared_error: 5.8924\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 4.0815 - mean_squared_error: 4.0815 - val_loss: 4.6473 - val_mean_squared_error: 4.6473\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 3.9964 - mean_squared_error: 3.9964 - val_loss: 3.9483 - val_mean_squared_error: 3.9483\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 4.0974 - mean_squared_error: 4.0974 - val_loss: 4.4632 - val_mean_squared_error: 4.4632\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 3.6902 - mean_squared_error: 3.6902 - val_loss: 4.7745 - val_mean_squared_error: 4.7745\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 3.6078 - mean_squared_error: 3.6078 - val_loss: 4.1745 - val_mean_squared_error: 4.1745\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 3.6816 - mean_squared_error: 3.6816 - val_loss: 4.0517 - val_mean_squared_error: 4.0517\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 3.5029 - mean_squared_error: 3.5029 - val_loss: 4.2578 - val_mean_squared_error: 4.2578\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 3.6161 - mean_squared_error: 3.6161 - val_loss: 3.5813 - val_mean_squared_error: 3.5813\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 3.3790 - mean_squared_error: 3.3790 - val_loss: 3.3067 - val_mean_squared_error: 3.3067\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 3.3160 - mean_squared_error: 3.3160 - val_loss: 3.3401 - val_mean_squared_error: 3.3401\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 3.0615 - mean_squared_error: 3.0615 - val_loss: 3.3698 - val_mean_squared_error: 3.3698\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.9650 - mean_squared_error: 2.9650 - val_loss: 3.2303 - val_mean_squared_error: 3.2303\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 3.1753 - mean_squared_error: 3.1753 - val_loss: 4.3234 - val_mean_squared_error: 4.3234\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 3.1293 - mean_squared_error: 3.1293 - val_loss: 3.4003 - val_mean_squared_error: 3.4003\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 3.1396 - mean_squared_error: 3.1396 - val_loss: 3.0382 - val_mean_squared_error: 3.0382\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 3.0140 - mean_squared_error: 3.0140 - val_loss: 2.8247 - val_mean_squared_error: 2.8247\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.7658 - mean_squared_error: 2.7658 - val_loss: 3.5215 - val_mean_squared_error: 3.5215\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.6616 - mean_squared_error: 2.6616 - val_loss: 3.3789 - val_mean_squared_error: 3.3789\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.8484 - mean_squared_error: 2.8484 - val_loss: 3.3486 - val_mean_squared_error: 3.3486\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.7012 - mean_squared_error: 2.7012 - val_loss: 3.2813 - val_mean_squared_error: 3.2813\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.7864 - mean_squared_error: 2.7864 - val_loss: 3.4003 - val_mean_squared_error: 3.4003\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 2.6714 - mean_squared_error: 2.6714 - val_loss: 4.0587 - val_mean_squared_error: 4.0587\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.7272 - mean_squared_error: 2.7272 - val_loss: 2.8432 - val_mean_squared_error: 2.8432\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 2.4371 - mean_squared_error: 2.4371 - val_loss: 2.9424 - val_mean_squared_error: 2.9424\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.5822 - mean_squared_error: 2.5822 - val_loss: 3.1044 - val_mean_squared_error: 3.1044\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.7367 - mean_squared_error: 2.7367 - val_loss: 3.6863 - val_mean_squared_error: 3.6863\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.4853 - mean_squared_error: 2.4853 - val_loss: 2.8848 - val_mean_squared_error: 2.8848\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 2.4819 - mean_squared_error: 2.4819 - val_loss: 2.8375 - val_mean_squared_error: 2.8375\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.3850 - mean_squared_error: 2.3850 - val_loss: 4.5973 - val_mean_squared_error: 4.5973\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.5087 - mean_squared_error: 2.5087 - val_loss: 3.0785 - val_mean_squared_error: 3.0785\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 2.3293 - mean_squared_error: 2.3293 - val_loss: 2.9698 - val_mean_squared_error: 2.9698\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.3167 - mean_squared_error: 2.3167 - val_loss: 2.6513 - val_mean_squared_error: 2.6513\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.3935 - mean_squared_error: 2.3935 - val_loss: 3.4006 - val_mean_squared_error: 3.4006\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.9109 - mean_squared_error: 2.9109 - val_loss: 3.6884 - val_mean_squared_error: 3.6884\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.4542 - mean_squared_error: 2.4542 - val_loss: 2.8207 - val_mean_squared_error: 2.8207\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.2559 - mean_squared_error: 2.2559 - val_loss: 2.7323 - val_mean_squared_error: 2.7323\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.1925 - mean_squared_error: 2.1925 - val_loss: 3.0122 - val_mean_squared_error: 3.0122\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.3742 - mean_squared_error: 2.3742 - val_loss: 3.4489 - val_mean_squared_error: 3.4489\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 2.2746 - mean_squared_error: 2.2746 - val_loss: 3.0756 - val_mean_squared_error: 3.0756\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.1114 - mean_squared_error: 2.1114 - val_loss: 2.7561 - val_mean_squared_error: 2.7561\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.9945 - mean_squared_error: 1.9945 - val_loss: 2.8292 - val_mean_squared_error: 2.8292\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.2274 - mean_squared_error: 2.2274 - val_loss: 3.1468 - val_mean_squared_error: 3.1468\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.1852 - mean_squared_error: 2.1852 - val_loss: 2.9362 - val_mean_squared_error: 2.9362\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.9834 - mean_squared_error: 1.9834 - val_loss: 2.5934 - val_mean_squared_error: 2.5934\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.8565 - mean_squared_error: 1.8565 - val_loss: 2.7226 - val_mean_squared_error: 2.7226\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.9092 - mean_squared_error: 1.9092 - val_loss: 2.7217 - val_mean_squared_error: 2.7217\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.8650 - mean_squared_error: 1.8650 - val_loss: 2.5629 - val_mean_squared_error: 2.5629\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.7981 - mean_squared_error: 1.7981 - val_loss: 2.7652 - val_mean_squared_error: 2.7652\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.9398 - mean_squared_error: 1.9398 - val_loss: 3.0461 - val_mean_squared_error: 3.0461\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.0470 - mean_squared_error: 2.0470 - val_loss: 2.7057 - val_mean_squared_error: 2.7057\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.8805 - mean_squared_error: 1.8805 - val_loss: 2.7274 - val_mean_squared_error: 2.7274\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.9768 - mean_squared_error: 1.9768 - val_loss: 2.2918 - val_mean_squared_error: 2.2918\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.9369 - mean_squared_error: 1.9369 - val_loss: 2.6423 - val_mean_squared_error: 2.6423\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.7990 - mean_squared_error: 1.7990 - val_loss: 2.4877 - val_mean_squared_error: 2.4877\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.8707 - mean_squared_error: 1.8707 - val_loss: 2.6699 - val_mean_squared_error: 2.6699\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.7579 - mean_squared_error: 1.7579 - val_loss: 2.5768 - val_mean_squared_error: 2.5768\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.8453 - mean_squared_error: 1.8453 - val_loss: 2.6844 - val_mean_squared_error: 2.6844\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.8245 - mean_squared_error: 1.8245 - val_loss: 2.5079 - val_mean_squared_error: 2.5079\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.7898 - mean_squared_error: 1.7898 - val_loss: 2.3629 - val_mean_squared_error: 2.3629\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.8269 - mean_squared_error: 1.8269 - val_loss: 2.3799 - val_mean_squared_error: 2.3799\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.8926 - mean_squared_error: 1.8926 - val_loss: 2.6071 - val_mean_squared_error: 2.6071\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.8237 - mean_squared_error: 1.8237 - val_loss: 2.6074 - val_mean_squared_error: 2.6074\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.7589 - mean_squared_error: 1.7589 - val_loss: 2.4828 - val_mean_squared_error: 2.4828\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.8923 - mean_squared_error: 1.8923 - val_loss: 2.7220 - val_mean_squared_error: 2.7220\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.7086 - mean_squared_error: 1.7086 - val_loss: 2.4830 - val_mean_squared_error: 2.4830\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.7620 - mean_squared_error: 1.7620 - val_loss: 2.7111 - val_mean_squared_error: 2.7111\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.6925 - mean_squared_error: 1.6925 - val_loss: 2.5993 - val_mean_squared_error: 2.5993\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.5820 - mean_squared_error: 1.5820 - val_loss: 2.2946 - val_mean_squared_error: 2.2946\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.6128 - mean_squared_error: 1.6128 - val_loss: 2.3597 - val_mean_squared_error: 2.3597\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7012 - mean_squared_error: 1.7012 - val_loss: 2.5800 - val_mean_squared_error: 2.5800\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.7115 - mean_squared_error: 1.7115 - val_loss: 3.0187 - val_mean_squared_error: 3.0187\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.7032 - mean_squared_error: 1.7032 - val_loss: 2.6865 - val_mean_squared_error: 2.6865\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.6236 - mean_squared_error: 1.6236 - val_loss: 2.4241 - val_mean_squared_error: 2.4241\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.7082 - mean_squared_error: 1.7082 - val_loss: 2.4039 - val_mean_squared_error: 2.4039\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.7076 - mean_squared_error: 1.7076 - val_loss: 2.2967 - val_mean_squared_error: 2.2967\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5776 - mean_squared_error: 1.5776 - val_loss: 2.6205 - val_mean_squared_error: 2.6205\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.5427 - mean_squared_error: 1.5427 - val_loss: 2.1924 - val_mean_squared_error: 2.1924\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.6288 - mean_squared_error: 1.6288 - val_loss: 3.0233 - val_mean_squared_error: 3.0233\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.6488 - mean_squared_error: 1.6488 - val_loss: 2.4856 - val_mean_squared_error: 2.4856\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.5139 - mean_squared_error: 1.5139 - val_loss: 3.1656 - val_mean_squared_error: 3.1656\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.5753 - mean_squared_error: 1.5753 - val_loss: 2.3671 - val_mean_squared_error: 2.3671\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.5718 - mean_squared_error: 1.5718 - val_loss: 2.4057 - val_mean_squared_error: 2.4057\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5262 - mean_squared_error: 1.5262 - val_loss: 2.3915 - val_mean_squared_error: 2.3915\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.5943 - mean_squared_error: 1.5943 - val_loss: 2.3766 - val_mean_squared_error: 2.3766\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.5002 - mean_squared_error: 1.5002 - val_loss: 2.3285 - val_mean_squared_error: 2.3285\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.4817 - mean_squared_error: 1.4817 - val_loss: 2.3610 - val_mean_squared_error: 2.3610\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.4510 - mean_squared_error: 1.4510 - val_loss: 2.5535 - val_mean_squared_error: 2.5535\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5448 - mean_squared_error: 1.5448 - val_loss: 2.3715 - val_mean_squared_error: 2.3715\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.5241 - mean_squared_error: 1.5241 - val_loss: 2.2018 - val_mean_squared_error: 2.2018\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.4802 - mean_squared_error: 1.4802 - val_loss: 2.2154 - val_mean_squared_error: 2.2154\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3660 - mean_squared_error: 1.3660 - val_loss: 2.0900 - val_mean_squared_error: 2.0900\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.3466 - mean_squared_error: 1.3466 - val_loss: 2.3019 - val_mean_squared_error: 2.3019\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.3287 - mean_squared_error: 1.3287 - val_loss: 2.2065 - val_mean_squared_error: 2.2065\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.4125 - mean_squared_error: 1.4125 - val_loss: 2.2722 - val_mean_squared_error: 2.2722\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.4477 - mean_squared_error: 1.4477 - val_loss: 2.3388 - val_mean_squared_error: 2.3388\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.3905 - mean_squared_error: 1.3905 - val_loss: 2.3224 - val_mean_squared_error: 2.3224\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.4414 - mean_squared_error: 1.4414 - val_loss: 2.1277 - val_mean_squared_error: 2.1277\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.4235 - mean_squared_error: 1.4235 - val_loss: 2.2116 - val_mean_squared_error: 2.2116\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3086 - mean_squared_error: 1.3086 - val_loss: 2.2472 - val_mean_squared_error: 2.2472\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.2584 - mean_squared_error: 1.2584 - val_loss: 2.2079 - val_mean_squared_error: 2.2079\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.2241 - mean_squared_error: 1.2241 - val_loss: 2.5467 - val_mean_squared_error: 2.5467\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3383 - mean_squared_error: 1.3383 - val_loss: 2.2485 - val_mean_squared_error: 2.2485\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.3585 - mean_squared_error: 1.3585 - val_loss: 2.3151 - val_mean_squared_error: 2.3151\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.4136 - mean_squared_error: 1.4136 - val_loss: 2.3679 - val_mean_squared_error: 2.3679\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.3233 - mean_squared_error: 1.3233 - val_loss: 2.2102 - val_mean_squared_error: 2.2102\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.2690 - mean_squared_error: 1.2690 - val_loss: 2.2570 - val_mean_squared_error: 2.2570\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.1937 - mean_squared_error: 1.1937 - val_loss: 2.3854 - val_mean_squared_error: 2.3854\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3905 - mean_squared_error: 1.3905 - val_loss: 2.0692 - val_mean_squared_error: 2.0692\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.3653 - mean_squared_error: 1.3653 - val_loss: 2.5321 - val_mean_squared_error: 2.5321\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.3266 - mean_squared_error: 1.3266 - val_loss: 2.0609 - val_mean_squared_error: 2.0609\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.3252 - mean_squared_error: 1.3252 - val_loss: 2.1149 - val_mean_squared_error: 2.1149\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.4196 - mean_squared_error: 1.4196 - val_loss: 2.0704 - val_mean_squared_error: 2.0704\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.2191 - mean_squared_error: 1.2191 - val_loss: 2.1395 - val_mean_squared_error: 2.1395\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.1174 - mean_squared_error: 1.1174 - val_loss: 2.1653 - val_mean_squared_error: 2.1653\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.4348 - mean_squared_error: 1.4348 - val_loss: 2.4979 - val_mean_squared_error: 2.4979\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.1371 - mean_squared_error: 1.1371 - val_loss: 2.0068 - val_mean_squared_error: 2.0068\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.3256 - mean_squared_error: 1.3256 - val_loss: 2.2638 - val_mean_squared_error: 2.2638\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.1861 - mean_squared_error: 1.1861 - val_loss: 2.1621 - val_mean_squared_error: 2.1621\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1277 - mean_squared_error: 1.1277 - val_loss: 2.1324 - val_mean_squared_error: 2.1324\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.2065 - mean_squared_error: 1.2065 - val_loss: 1.9916 - val_mean_squared_error: 1.9916\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.1885 - mean_squared_error: 1.1885 - val_loss: 2.1943 - val_mean_squared_error: 2.1943\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.3001 - mean_squared_error: 1.3001 - val_loss: 2.1388 - val_mean_squared_error: 2.1388\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.2389 - mean_squared_error: 1.2389 - val_loss: 2.4125 - val_mean_squared_error: 2.4125\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1189 - mean_squared_error: 1.1189 - val_loss: 2.0630 - val_mean_squared_error: 2.0630\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1281 - mean_squared_error: 1.1281 - val_loss: 2.1922 - val_mean_squared_error: 2.1922\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.1950 - mean_squared_error: 1.1950 - val_loss: 2.1875 - val_mean_squared_error: 2.1875\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.1191 - mean_squared_error: 1.1191 - val_loss: 2.1299 - val_mean_squared_error: 2.1299\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.1466 - mean_squared_error: 1.1466 - val_loss: 2.3212 - val_mean_squared_error: 2.3212\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1844 - mean_squared_error: 1.1844 - val_loss: 2.0311 - val_mean_squared_error: 2.0311\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.1630 - mean_squared_error: 1.1630 - val_loss: 2.1826 - val_mean_squared_error: 2.1826\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.0775 - mean_squared_error: 1.0775 - val_loss: 2.0646 - val_mean_squared_error: 2.0646\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.1792 - mean_squared_error: 1.1792 - val_loss: 2.1648 - val_mean_squared_error: 2.1648\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.0661 - mean_squared_error: 1.0661 - val_loss: 2.0615 - val_mean_squared_error: 2.0615\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.0992 - mean_squared_error: 1.0992 - val_loss: 2.0679 - val_mean_squared_error: 2.0679\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 428us/sample - loss: 1.1462 - mean_squared_error: 1.1462 - val_loss: 2.1947 - val_mean_squared_error: 2.1947\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.1175 - mean_squared_error: 1.1175 - val_loss: 2.0002 - val_mean_squared_error: 2.0002\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1431 - mean_squared_error: 1.1431 - val_loss: 1.9368 - val_mean_squared_error: 1.9368\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.1243 - mean_squared_error: 1.1243 - val_loss: 2.2280 - val_mean_squared_error: 2.2280\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0360 - mean_squared_error: 1.0360 - val_loss: 1.8933 - val_mean_squared_error: 1.8933\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0364 - mean_squared_error: 1.0364 - val_loss: 2.1237 - val_mean_squared_error: 2.1237\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1034 - mean_squared_error: 1.1034 - val_loss: 2.3500 - val_mean_squared_error: 2.3500\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.1687 - mean_squared_error: 1.1687 - val_loss: 2.2222 - val_mean_squared_error: 2.2222\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.0741 - mean_squared_error: 1.0741 - val_loss: 2.1083 - val_mean_squared_error: 2.1083\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.0801 - mean_squared_error: 1.0801 - val_loss: 2.0477 - val_mean_squared_error: 2.0477\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.1544 - mean_squared_error: 1.1544 - val_loss: 1.8880 - val_mean_squared_error: 1.8880\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1007 - mean_squared_error: 1.1007 - val_loss: 2.3233 - val_mean_squared_error: 2.3233\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 2.0148 - val_mean_squared_error: 2.0148\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.9631 - val_mean_squared_error: 1.9631\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.0317 - mean_squared_error: 1.0317 - val_loss: 1.9363 - val_mean_squared_error: 1.9363\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.9102 - mean_squared_error: 0.9102 - val_loss: 2.0905 - val_mean_squared_error: 2.0905\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 0.9126 - mean_squared_error: 0.9126 - val_loss: 1.9862 - val_mean_squared_error: 1.9862\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0282 - mean_squared_error: 1.0282 - val_loss: 1.9758 - val_mean_squared_error: 1.9758\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.0312 - mean_squared_error: 1.0312 - val_loss: 2.1548 - val_mean_squared_error: 2.1548\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.9864 - mean_squared_error: 0.9864 - val_loss: 2.0231 - val_mean_squared_error: 2.0231\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0512 - mean_squared_error: 1.0512 - val_loss: 2.6688 - val_mean_squared_error: 2.6688\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0929 - mean_squared_error: 1.0929 - val_loss: 2.3375 - val_mean_squared_error: 2.3375\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.0883 - mean_squared_error: 1.0883 - val_loss: 2.1146 - val_mean_squared_error: 2.1146\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.9917 - mean_squared_error: 0.9917 - val_loss: 2.0380 - val_mean_squared_error: 2.0380\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 2.2400 - val_mean_squared_error: 2.2400\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9581 - mean_squared_error: 0.9581 - val_loss: 1.8816 - val_mean_squared_error: 1.8816\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9297 - mean_squared_error: 0.9297 - val_loss: 1.9738 - val_mean_squared_error: 1.9738\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9709 - mean_squared_error: 0.9709 - val_loss: 1.8909 - val_mean_squared_error: 1.8909\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.9072 - mean_squared_error: 0.9072 - val_loss: 2.1748 - val_mean_squared_error: 2.1748\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9587 - mean_squared_error: 0.9587 - val_loss: 2.0196 - val_mean_squared_error: 2.0196\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 0.9247 - mean_squared_error: 0.9247 - val_loss: 1.9865 - val_mean_squared_error: 1.9865\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.9612 - mean_squared_error: 0.9612 - val_loss: 2.1130 - val_mean_squared_error: 2.1130\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.9485 - mean_squared_error: 0.9485 - val_loss: 1.9281 - val_mean_squared_error: 1.9281\n",
            "==================================================\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_51 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_85 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_51 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_68 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_86 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_52 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_69 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_87 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_53 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_70 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_17 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_88 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_71 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_89 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 430.9327 - mean_squared_error: 430.9329 - val_loss: 1058.7855 - val_mean_squared_error: 1058.7854\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 25.5230 - mean_squared_error: 25.5230 - val_loss: 105.4128 - val_mean_squared_error: 105.4128\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 19.9697 - mean_squared_error: 19.9697 - val_loss: 80.4349 - val_mean_squared_error: 80.4349\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 18.7678 - mean_squared_error: 18.7678 - val_loss: 49.5850 - val_mean_squared_error: 49.5850\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 16.6285 - mean_squared_error: 16.6285 - val_loss: 26.0924 - val_mean_squared_error: 26.0924\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 15.0649 - mean_squared_error: 15.0649 - val_loss: 16.9502 - val_mean_squared_error: 16.9501\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 16.1270 - mean_squared_error: 16.1270 - val_loss: 16.0407 - val_mean_squared_error: 16.0407\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 15.4224 - mean_squared_error: 15.4224 - val_loss: 14.1706 - val_mean_squared_error: 14.1706\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 13.8504 - mean_squared_error: 13.8504 - val_loss: 19.8908 - val_mean_squared_error: 19.8908\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 13.0052 - mean_squared_error: 13.0052 - val_loss: 9.9893 - val_mean_squared_error: 9.9893\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 12.9992 - mean_squared_error: 12.9992 - val_loss: 13.8475 - val_mean_squared_error: 13.8475\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 12.2460 - mean_squared_error: 12.2460 - val_loss: 12.3287 - val_mean_squared_error: 12.3287\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 12.1221 - mean_squared_error: 12.1221 - val_loss: 10.9210 - val_mean_squared_error: 10.9210\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 9.9575 - mean_squared_error: 9.9575 - val_loss: 8.6426 - val_mean_squared_error: 8.6426\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 9.7992 - mean_squared_error: 9.7992 - val_loss: 8.5889 - val_mean_squared_error: 8.5889\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 8.9510 - mean_squared_error: 8.9510 - val_loss: 8.5484 - val_mean_squared_error: 8.5484\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 8.3906 - mean_squared_error: 8.3906 - val_loss: 8.1375 - val_mean_squared_error: 8.1375\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 7.6946 - mean_squared_error: 7.6946 - val_loss: 8.2614 - val_mean_squared_error: 8.2614\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 6.8369 - mean_squared_error: 6.8369 - val_loss: 7.7165 - val_mean_squared_error: 7.7165\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 6.5977 - mean_squared_error: 6.5977 - val_loss: 7.3291 - val_mean_squared_error: 7.3291\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 6.0316 - mean_squared_error: 6.0316 - val_loss: 7.5579 - val_mean_squared_error: 7.5579\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 5.6708 - mean_squared_error: 5.6708 - val_loss: 5.9580 - val_mean_squared_error: 5.9580\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 5.5774 - mean_squared_error: 5.5774 - val_loss: 5.5308 - val_mean_squared_error: 5.5308\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 5.0368 - mean_squared_error: 5.0368 - val_loss: 4.7958 - val_mean_squared_error: 4.7958\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 5.0783 - mean_squared_error: 5.0783 - val_loss: 5.1052 - val_mean_squared_error: 5.1052\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 4.3893 - mean_squared_error: 4.3893 - val_loss: 4.8496 - val_mean_squared_error: 4.8496\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 4.1720 - mean_squared_error: 4.1720 - val_loss: 5.2794 - val_mean_squared_error: 5.2794\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.9461 - mean_squared_error: 3.9461 - val_loss: 4.3288 - val_mean_squared_error: 4.3288\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.8434 - mean_squared_error: 3.8434 - val_loss: 5.0517 - val_mean_squared_error: 5.0517\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.6688 - mean_squared_error: 3.6688 - val_loss: 4.3756 - val_mean_squared_error: 4.3756\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.6963 - mean_squared_error: 3.6963 - val_loss: 4.3694 - val_mean_squared_error: 4.3694\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.1690 - mean_squared_error: 3.1690 - val_loss: 3.8068 - val_mean_squared_error: 3.8068\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 3.3061 - mean_squared_error: 3.3061 - val_loss: 3.8989 - val_mean_squared_error: 3.8989\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 3.0379 - mean_squared_error: 3.0379 - val_loss: 4.0061 - val_mean_squared_error: 4.0061\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.9653 - mean_squared_error: 2.9653 - val_loss: 3.5251 - val_mean_squared_error: 3.5251\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 3.0329 - mean_squared_error: 3.0329 - val_loss: 3.7840 - val_mean_squared_error: 3.7840\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.7493 - mean_squared_error: 2.7493 - val_loss: 4.7237 - val_mean_squared_error: 4.7237\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.7386 - mean_squared_error: 2.7386 - val_loss: 4.8819 - val_mean_squared_error: 4.8819\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.6631 - mean_squared_error: 2.6631 - val_loss: 4.0154 - val_mean_squared_error: 4.0154\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.6855 - mean_squared_error: 2.6855 - val_loss: 4.0935 - val_mean_squared_error: 4.0935\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.9296 - mean_squared_error: 2.9296 - val_loss: 3.7986 - val_mean_squared_error: 3.7986\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.4960 - mean_squared_error: 2.4960 - val_loss: 3.3139 - val_mean_squared_error: 3.3139\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.3519 - mean_squared_error: 2.3519 - val_loss: 3.2999 - val_mean_squared_error: 3.2999\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.5713 - mean_squared_error: 2.5713 - val_loss: 3.8741 - val_mean_squared_error: 3.8741\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.4620 - mean_squared_error: 2.4620 - val_loss: 3.1572 - val_mean_squared_error: 3.1572\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.2132 - mean_squared_error: 2.2132 - val_loss: 3.2936 - val_mean_squared_error: 3.2936\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.0484 - mean_squared_error: 2.0484 - val_loss: 3.8837 - val_mean_squared_error: 3.8837\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.2078 - mean_squared_error: 2.2078 - val_loss: 3.3985 - val_mean_squared_error: 3.3985\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.3053 - mean_squared_error: 2.3053 - val_loss: 3.1700 - val_mean_squared_error: 3.1700\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 2.0829 - mean_squared_error: 2.0829 - val_loss: 3.1448 - val_mean_squared_error: 3.1448\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.0146 - mean_squared_error: 2.0146 - val_loss: 3.4179 - val_mean_squared_error: 3.4179\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.9241 - mean_squared_error: 1.9241 - val_loss: 3.0612 - val_mean_squared_error: 3.0612\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.9214 - mean_squared_error: 1.9214 - val_loss: 3.4187 - val_mean_squared_error: 3.4187\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.1229 - mean_squared_error: 2.1229 - val_loss: 3.4827 - val_mean_squared_error: 3.4827\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.1329 - mean_squared_error: 2.1329 - val_loss: 3.1352 - val_mean_squared_error: 3.1352\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.8467 - mean_squared_error: 1.8467 - val_loss: 3.4026 - val_mean_squared_error: 3.4026\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.8214 - mean_squared_error: 1.8214 - val_loss: 3.1037 - val_mean_squared_error: 3.1037\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.9067 - mean_squared_error: 1.9067 - val_loss: 3.0615 - val_mean_squared_error: 3.0615\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.7898 - mean_squared_error: 1.7898 - val_loss: 2.9600 - val_mean_squared_error: 2.9600\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.7326 - mean_squared_error: 1.7326 - val_loss: 3.0294 - val_mean_squared_error: 3.0294\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.6937 - mean_squared_error: 1.6937 - val_loss: 3.2704 - val_mean_squared_error: 3.2704\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.9196 - mean_squared_error: 1.9196 - val_loss: 4.0891 - val_mean_squared_error: 4.0891\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.8364 - mean_squared_error: 1.8364 - val_loss: 3.3865 - val_mean_squared_error: 3.3865\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.8120 - mean_squared_error: 1.8120 - val_loss: 3.1010 - val_mean_squared_error: 3.1010\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.6315 - mean_squared_error: 1.6315 - val_loss: 3.5935 - val_mean_squared_error: 3.5935\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.7684 - mean_squared_error: 1.7684 - val_loss: 2.9733 - val_mean_squared_error: 2.9733\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.4643 - mean_squared_error: 1.4643 - val_loss: 3.1366 - val_mean_squared_error: 3.1366\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.7212 - mean_squared_error: 1.7212 - val_loss: 3.0150 - val_mean_squared_error: 3.0150\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.7504 - mean_squared_error: 1.7504 - val_loss: 3.2548 - val_mean_squared_error: 3.2548\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.7773 - mean_squared_error: 1.7773 - val_loss: 3.0299 - val_mean_squared_error: 3.0299\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.7301 - mean_squared_error: 1.7301 - val_loss: 3.0273 - val_mean_squared_error: 3.0273\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.6204 - mean_squared_error: 1.6204 - val_loss: 3.0540 - val_mean_squared_error: 3.0540\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.4885 - mean_squared_error: 1.4885 - val_loss: 3.1243 - val_mean_squared_error: 3.1243\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.7571 - mean_squared_error: 1.7571 - val_loss: 2.8481 - val_mean_squared_error: 2.8481\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.6439 - mean_squared_error: 1.6439 - val_loss: 3.5215 - val_mean_squared_error: 3.5215\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.4518 - mean_squared_error: 1.4518 - val_loss: 2.6663 - val_mean_squared_error: 2.6663\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.4180 - mean_squared_error: 1.4180 - val_loss: 2.6267 - val_mean_squared_error: 2.6267\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.5382 - mean_squared_error: 1.5382 - val_loss: 3.0352 - val_mean_squared_error: 3.0352\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.6743 - mean_squared_error: 1.6743 - val_loss: 2.8364 - val_mean_squared_error: 2.8364\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.4876 - mean_squared_error: 1.4876 - val_loss: 2.4852 - val_mean_squared_error: 2.4852\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.2879 - mean_squared_error: 1.2879 - val_loss: 2.6516 - val_mean_squared_error: 2.6516\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4759 - mean_squared_error: 1.4758 - val_loss: 2.7963 - val_mean_squared_error: 2.7963\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.3752 - mean_squared_error: 1.3752 - val_loss: 3.0305 - val_mean_squared_error: 3.0305\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5129 - mean_squared_error: 1.5129 - val_loss: 2.6531 - val_mean_squared_error: 2.6531\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3783 - mean_squared_error: 1.3783 - val_loss: 2.5613 - val_mean_squared_error: 2.5613\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3767 - mean_squared_error: 1.3767 - val_loss: 2.6297 - val_mean_squared_error: 2.6297\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.2971 - mean_squared_error: 1.2971 - val_loss: 3.1369 - val_mean_squared_error: 3.1369\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.4976 - mean_squared_error: 1.4976 - val_loss: 3.0843 - val_mean_squared_error: 3.0843\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.3416 - mean_squared_error: 1.3416 - val_loss: 2.6873 - val_mean_squared_error: 2.6873\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2076 - mean_squared_error: 1.2076 - val_loss: 2.5766 - val_mean_squared_error: 2.5766\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.4169 - mean_squared_error: 1.4169 - val_loss: 3.0155 - val_mean_squared_error: 3.0155\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3111 - mean_squared_error: 1.3111 - val_loss: 2.4734 - val_mean_squared_error: 2.4734\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.0557 - mean_squared_error: 1.0557 - val_loss: 2.7785 - val_mean_squared_error: 2.7785\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.1825 - mean_squared_error: 1.1825 - val_loss: 2.6491 - val_mean_squared_error: 2.6491\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.2398 - mean_squared_error: 1.2398 - val_loss: 3.0080 - val_mean_squared_error: 3.0080\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.3310 - mean_squared_error: 1.3310 - val_loss: 2.9531 - val_mean_squared_error: 2.9531\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2508 - mean_squared_error: 1.2508 - val_loss: 2.4429 - val_mean_squared_error: 2.4429\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.1768 - mean_squared_error: 1.1768 - val_loss: 2.8996 - val_mean_squared_error: 2.8996\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.1630 - mean_squared_error: 1.1630 - val_loss: 3.0198 - val_mean_squared_error: 3.0198\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.6011 - mean_squared_error: 1.6011 - val_loss: 2.9528 - val_mean_squared_error: 2.9528\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.0584 - mean_squared_error: 1.0584 - val_loss: 2.5421 - val_mean_squared_error: 2.5421\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1494 - mean_squared_error: 1.1494 - val_loss: 3.0451 - val_mean_squared_error: 3.0451\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.2504 - mean_squared_error: 1.2504 - val_loss: 2.7452 - val_mean_squared_error: 2.7452\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.1600 - mean_squared_error: 1.1600 - val_loss: 2.4928 - val_mean_squared_error: 2.4928\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.0151 - mean_squared_error: 1.0151 - val_loss: 2.5069 - val_mean_squared_error: 2.5069\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.1597 - mean_squared_error: 1.1597 - val_loss: 2.7526 - val_mean_squared_error: 2.7526\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2175 - mean_squared_error: 1.2175 - val_loss: 2.4394 - val_mean_squared_error: 2.4394\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.1406 - mean_squared_error: 1.1406 - val_loss: 2.7160 - val_mean_squared_error: 2.7160\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.1403 - mean_squared_error: 1.1403 - val_loss: 2.9003 - val_mean_squared_error: 2.9003\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1091 - mean_squared_error: 1.1091 - val_loss: 2.7654 - val_mean_squared_error: 2.7654\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.1319 - mean_squared_error: 1.1319 - val_loss: 2.7540 - val_mean_squared_error: 2.7540\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.2249 - mean_squared_error: 1.2249 - val_loss: 2.4269 - val_mean_squared_error: 2.4269\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.1565 - mean_squared_error: 1.1565 - val_loss: 2.7291 - val_mean_squared_error: 2.7291\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.3250 - mean_squared_error: 1.3250 - val_loss: 2.7059 - val_mean_squared_error: 2.7059\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.0259 - mean_squared_error: 1.0259 - val_loss: 2.8190 - val_mean_squared_error: 2.8190\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.1352 - mean_squared_error: 1.1352 - val_loss: 2.5601 - val_mean_squared_error: 2.5601\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0281 - mean_squared_error: 1.0281 - val_loss: 2.5447 - val_mean_squared_error: 2.5447\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1579 - mean_squared_error: 1.1579 - val_loss: 2.4240 - val_mean_squared_error: 2.4240\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0946 - mean_squared_error: 1.0946 - val_loss: 3.6818 - val_mean_squared_error: 3.6818\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0955 - mean_squared_error: 1.0955 - val_loss: 3.0463 - val_mean_squared_error: 3.0463\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.2350 - mean_squared_error: 1.2350 - val_loss: 2.6644 - val_mean_squared_error: 2.6644\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 1.1172 - mean_squared_error: 1.1172 - val_loss: 2.4889 - val_mean_squared_error: 2.4889\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.0350 - mean_squared_error: 1.0350 - val_loss: 3.1611 - val_mean_squared_error: 3.1611\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.0562 - mean_squared_error: 1.0562 - val_loss: 2.6938 - val_mean_squared_error: 2.6938\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.9801 - mean_squared_error: 0.9801 - val_loss: 2.5245 - val_mean_squared_error: 2.5245\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1384 - mean_squared_error: 1.1384 - val_loss: 2.6256 - val_mean_squared_error: 2.6256\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.1384 - mean_squared_error: 1.1384 - val_loss: 2.9761 - val_mean_squared_error: 2.9761\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.1546 - mean_squared_error: 1.1546 - val_loss: 2.7967 - val_mean_squared_error: 2.7967\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0812 - mean_squared_error: 1.0812 - val_loss: 2.5133 - val_mean_squared_error: 2.5133\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.0356 - mean_squared_error: 1.0356 - val_loss: 2.3721 - val_mean_squared_error: 2.3721\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.8914 - mean_squared_error: 0.8914 - val_loss: 2.5451 - val_mean_squared_error: 2.5451\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 0.9463 - mean_squared_error: 0.9463 - val_loss: 2.4335 - val_mean_squared_error: 2.4335\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9679 - mean_squared_error: 0.9679 - val_loss: 2.4543 - val_mean_squared_error: 2.4543\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.0876 - mean_squared_error: 1.0876 - val_loss: 2.5147 - val_mean_squared_error: 2.5147\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.9411 - mean_squared_error: 0.9411 - val_loss: 2.3907 - val_mean_squared_error: 2.3907\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 0.8256 - mean_squared_error: 0.8256 - val_loss: 2.4775 - val_mean_squared_error: 2.4775\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.0344 - mean_squared_error: 1.0344 - val_loss: 2.4763 - val_mean_squared_error: 2.4763\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.9723 - mean_squared_error: 0.9723 - val_loss: 2.4177 - val_mean_squared_error: 2.4177\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0963 - mean_squared_error: 1.0963 - val_loss: 2.8690 - val_mean_squared_error: 2.8690\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9336 - mean_squared_error: 0.9336 - val_loss: 2.5603 - val_mean_squared_error: 2.5603\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9853 - mean_squared_error: 0.9853 - val_loss: 2.5237 - val_mean_squared_error: 2.5237\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9562 - mean_squared_error: 0.9562 - val_loss: 2.7671 - val_mean_squared_error: 2.7671\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9595 - mean_squared_error: 0.9595 - val_loss: 2.3691 - val_mean_squared_error: 2.3691\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.8906 - mean_squared_error: 0.8906 - val_loss: 2.3879 - val_mean_squared_error: 2.3879\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9314 - mean_squared_error: 0.9314 - val_loss: 2.6580 - val_mean_squared_error: 2.6580\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0020 - mean_squared_error: 1.0020 - val_loss: 2.3490 - val_mean_squared_error: 2.3490\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.8170 - mean_squared_error: 0.8170 - val_loss: 2.3071 - val_mean_squared_error: 2.3071\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.9801 - mean_squared_error: 0.9801 - val_loss: 2.4158 - val_mean_squared_error: 2.4158\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.8906 - mean_squared_error: 0.8906 - val_loss: 2.3794 - val_mean_squared_error: 2.3794\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9209 - mean_squared_error: 0.9209 - val_loss: 2.5152 - val_mean_squared_error: 2.5152\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.9044 - mean_squared_error: 0.9044 - val_loss: 2.4093 - val_mean_squared_error: 2.4093\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.8066 - mean_squared_error: 0.8066 - val_loss: 2.4161 - val_mean_squared_error: 2.4161\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.8238 - mean_squared_error: 0.8238 - val_loss: 2.6522 - val_mean_squared_error: 2.6522\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.9501 - mean_squared_error: 0.9501 - val_loss: 2.3801 - val_mean_squared_error: 2.3801\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.8700 - mean_squared_error: 0.8700 - val_loss: 2.2605 - val_mean_squared_error: 2.2605\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 0.7260 - mean_squared_error: 0.7260 - val_loss: 2.3172 - val_mean_squared_error: 2.3172\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.7686 - mean_squared_error: 0.7686 - val_loss: 2.6266 - val_mean_squared_error: 2.6266\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.9787 - mean_squared_error: 0.9787 - val_loss: 2.2892 - val_mean_squared_error: 2.2892\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.8068 - mean_squared_error: 0.8068 - val_loss: 2.2288 - val_mean_squared_error: 2.2288\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.7934 - mean_squared_error: 0.7934 - val_loss: 2.2922 - val_mean_squared_error: 2.2922\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.8947 - mean_squared_error: 0.8947 - val_loss: 2.4290 - val_mean_squared_error: 2.4290\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7982 - mean_squared_error: 0.7982 - val_loss: 2.2783 - val_mean_squared_error: 2.2783\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8816 - mean_squared_error: 0.8816 - val_loss: 2.6282 - val_mean_squared_error: 2.6282\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.8392 - mean_squared_error: 0.8392 - val_loss: 2.4651 - val_mean_squared_error: 2.4651\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.8584 - mean_squared_error: 0.8584 - val_loss: 2.2567 - val_mean_squared_error: 2.2567\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8446 - mean_squared_error: 0.8446 - val_loss: 2.2793 - val_mean_squared_error: 2.2793\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.7515 - mean_squared_error: 0.7515 - val_loss: 2.4248 - val_mean_squared_error: 2.4248\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.8238 - mean_squared_error: 0.8238 - val_loss: 2.3614 - val_mean_squared_error: 2.3614\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.7985 - mean_squared_error: 0.7985 - val_loss: 2.3099 - val_mean_squared_error: 2.3099\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8280 - mean_squared_error: 0.8280 - val_loss: 2.2292 - val_mean_squared_error: 2.2292\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.7472 - mean_squared_error: 0.7472 - val_loss: 2.4743 - val_mean_squared_error: 2.4743\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8071 - mean_squared_error: 0.8071 - val_loss: 2.5587 - val_mean_squared_error: 2.5587\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 2.3405 - val_mean_squared_error: 2.3405\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.7568 - mean_squared_error: 0.7568 - val_loss: 2.5943 - val_mean_squared_error: 2.5943\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8112 - mean_squared_error: 0.8112 - val_loss: 2.3315 - val_mean_squared_error: 2.3315\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8643 - mean_squared_error: 0.8643 - val_loss: 2.3898 - val_mean_squared_error: 2.3898\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9597 - mean_squared_error: 0.9597 - val_loss: 2.8807 - val_mean_squared_error: 2.8807\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8931 - mean_squared_error: 0.8931 - val_loss: 2.3473 - val_mean_squared_error: 2.3473\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.7320 - mean_squared_error: 0.7320 - val_loss: 2.4731 - val_mean_squared_error: 2.4731\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.7046 - mean_squared_error: 0.7046 - val_loss: 2.3760 - val_mean_squared_error: 2.3760\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.8400 - mean_squared_error: 0.8400 - val_loss: 2.2512 - val_mean_squared_error: 2.2512\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.7660 - mean_squared_error: 0.7660 - val_loss: 2.2651 - val_mean_squared_error: 2.2651\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7495 - mean_squared_error: 0.7495 - val_loss: 2.5073 - val_mean_squared_error: 2.5073\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8171 - mean_squared_error: 0.8171 - val_loss: 2.2126 - val_mean_squared_error: 2.2126\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.7384 - mean_squared_error: 0.7384 - val_loss: 2.2209 - val_mean_squared_error: 2.2209\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.7888 - mean_squared_error: 0.7888 - val_loss: 2.1842 - val_mean_squared_error: 2.1842\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.7686 - mean_squared_error: 0.7686 - val_loss: 2.1745 - val_mean_squared_error: 2.1745\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.7141 - mean_squared_error: 0.7141 - val_loss: 2.1811 - val_mean_squared_error: 2.1811\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 2.3404 - val_mean_squared_error: 2.3404\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.7655 - mean_squared_error: 0.7655 - val_loss: 2.4917 - val_mean_squared_error: 2.4917\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7968 - mean_squared_error: 0.7968 - val_loss: 2.1101 - val_mean_squared_error: 2.1101\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.7525 - mean_squared_error: 0.7525 - val_loss: 2.3355 - val_mean_squared_error: 2.3355\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.6856 - mean_squared_error: 0.6856 - val_loss: 2.2347 - val_mean_squared_error: 2.2347\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8177 - mean_squared_error: 0.8177 - val_loss: 2.3541 - val_mean_squared_error: 2.3541\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.6416 - mean_squared_error: 0.6416 - val_loss: 2.5078 - val_mean_squared_error: 2.5078\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.7029 - mean_squared_error: 0.7029 - val_loss: 2.2412 - val_mean_squared_error: 2.2412\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.6523 - mean_squared_error: 0.6523 - val_loss: 2.4389 - val_mean_squared_error: 2.4389\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.6782 - mean_squared_error: 0.6782 - val_loss: 2.2628 - val_mean_squared_error: 2.2628\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.7141 - mean_squared_error: 0.7141 - val_loss: 2.3942 - val_mean_squared_error: 2.3942\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.6717 - mean_squared_error: 0.6717 - val_loss: 2.1435 - val_mean_squared_error: 2.1435\n",
            "==================================================\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_54 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_90 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_54 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_72 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_91 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_55 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_73 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_92 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_56 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_74 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_18 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_93 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_75 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_54 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_94 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_55 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 428.6786 - mean_squared_error: 428.6786 - val_loss: 615.6566 - val_mean_squared_error: 615.6566\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 23.9186 - mean_squared_error: 23.9187 - val_loss: 163.0092 - val_mean_squared_error: 163.0092\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 19.3289 - mean_squared_error: 19.3289 - val_loss: 73.1411 - val_mean_squared_error: 73.1411\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 19.7389 - mean_squared_error: 19.7389 - val_loss: 52.8824 - val_mean_squared_error: 52.8824\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 17.7648 - mean_squared_error: 17.7648 - val_loss: 26.2673 - val_mean_squared_error: 26.2673\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 15.1859 - mean_squared_error: 15.1859 - val_loss: 18.0131 - val_mean_squared_error: 18.0131\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 14.3092 - mean_squared_error: 14.3092 - val_loss: 19.5632 - val_mean_squared_error: 19.5632\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 14.1019 - mean_squared_error: 14.1019 - val_loss: 11.4687 - val_mean_squared_error: 11.4687\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 12.2902 - mean_squared_error: 12.2902 - val_loss: 13.7582 - val_mean_squared_error: 13.7582\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 11.5285 - mean_squared_error: 11.5285 - val_loss: 10.1523 - val_mean_squared_error: 10.1523\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 12.2988 - mean_squared_error: 12.2988 - val_loss: 9.8216 - val_mean_squared_error: 9.8216\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 10.7700 - mean_squared_error: 10.7700 - val_loss: 10.7992 - val_mean_squared_error: 10.7992\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 9.8836 - mean_squared_error: 9.8836 - val_loss: 10.1370 - val_mean_squared_error: 10.1370\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 9.3488 - mean_squared_error: 9.3488 - val_loss: 8.3176 - val_mean_squared_error: 8.3176\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 8.7290 - mean_squared_error: 8.7290 - val_loss: 8.1832 - val_mean_squared_error: 8.1832\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 7.9337 - mean_squared_error: 7.9337 - val_loss: 7.8077 - val_mean_squared_error: 7.8077\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 7.8395 - mean_squared_error: 7.8395 - val_loss: 7.5739 - val_mean_squared_error: 7.5739\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 6.5681 - mean_squared_error: 6.5681 - val_loss: 7.9082 - val_mean_squared_error: 7.9082\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 6.3193 - mean_squared_error: 6.3193 - val_loss: 10.6534 - val_mean_squared_error: 10.6534\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 5.6246 - mean_squared_error: 5.6246 - val_loss: 5.3889 - val_mean_squared_error: 5.3889\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 5.3301 - mean_squared_error: 5.3301 - val_loss: 5.1867 - val_mean_squared_error: 5.1867\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 4.9720 - mean_squared_error: 4.9720 - val_loss: 4.9478 - val_mean_squared_error: 4.9478\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 4.8464 - mean_squared_error: 4.8464 - val_loss: 5.3046 - val_mean_squared_error: 5.3046\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 4.2994 - mean_squared_error: 4.2994 - val_loss: 4.2816 - val_mean_squared_error: 4.2816\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 4.0811 - mean_squared_error: 4.0811 - val_loss: 4.4112 - val_mean_squared_error: 4.4112\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 4.0538 - mean_squared_error: 4.0538 - val_loss: 4.1257 - val_mean_squared_error: 4.1257\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.9973 - mean_squared_error: 3.9973 - val_loss: 4.3025 - val_mean_squared_error: 4.3025\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.9488 - mean_squared_error: 3.9488 - val_loss: 3.8052 - val_mean_squared_error: 3.8052\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.6075 - mean_squared_error: 3.6075 - val_loss: 3.8159 - val_mean_squared_error: 3.8159\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.4281 - mean_squared_error: 3.4281 - val_loss: 4.2163 - val_mean_squared_error: 4.2163\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.3315 - mean_squared_error: 3.3315 - val_loss: 3.7677 - val_mean_squared_error: 3.7677\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.1338 - mean_squared_error: 3.1338 - val_loss: 3.4373 - val_mean_squared_error: 3.4373\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 3.1819 - mean_squared_error: 3.1819 - val_loss: 3.7768 - val_mean_squared_error: 3.7768\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.9134 - mean_squared_error: 2.9134 - val_loss: 2.9456 - val_mean_squared_error: 2.9456\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.8028 - mean_squared_error: 2.8028 - val_loss: 3.2790 - val_mean_squared_error: 3.2790\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.8758 - mean_squared_error: 2.8758 - val_loss: 3.2155 - val_mean_squared_error: 3.2155\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.5845 - mean_squared_error: 2.5845 - val_loss: 3.6549 - val_mean_squared_error: 3.6549\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.1454 - mean_squared_error: 3.1454 - val_loss: 3.5465 - val_mean_squared_error: 3.5465\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.5500 - mean_squared_error: 2.5500 - val_loss: 3.4028 - val_mean_squared_error: 3.4028\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.4621 - mean_squared_error: 2.4621 - val_loss: 3.8440 - val_mean_squared_error: 3.8440\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.5305 - mean_squared_error: 2.5305 - val_loss: 2.8687 - val_mean_squared_error: 2.8687\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.3129 - mean_squared_error: 2.3129 - val_loss: 3.0525 - val_mean_squared_error: 3.0525\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.4819 - mean_squared_error: 2.4819 - val_loss: 3.2793 - val_mean_squared_error: 3.2793\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.4445 - mean_squared_error: 2.4445 - val_loss: 3.4022 - val_mean_squared_error: 3.4022\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.4921 - mean_squared_error: 2.4921 - val_loss: 3.3525 - val_mean_squared_error: 3.3525\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.4824 - mean_squared_error: 2.4824 - val_loss: 2.8059 - val_mean_squared_error: 2.8059\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.1404 - mean_squared_error: 2.1404 - val_loss: 2.6886 - val_mean_squared_error: 2.6886\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.2036 - mean_squared_error: 2.2036 - val_loss: 2.6302 - val_mean_squared_error: 2.6302\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.2964 - mean_squared_error: 2.2964 - val_loss: 2.9426 - val_mean_squared_error: 2.9426\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.0216 - mean_squared_error: 2.0216 - val_loss: 3.1963 - val_mean_squared_error: 3.1963\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.9983 - mean_squared_error: 1.9983 - val_loss: 2.6013 - val_mean_squared_error: 2.6013\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.9124 - mean_squared_error: 1.9124 - val_loss: 2.7638 - val_mean_squared_error: 2.7638\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.1879 - mean_squared_error: 2.1879 - val_loss: 2.6550 - val_mean_squared_error: 2.6550\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.9262 - mean_squared_error: 1.9262 - val_loss: 2.8754 - val_mean_squared_error: 2.8754\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.9771 - mean_squared_error: 1.9771 - val_loss: 3.0161 - val_mean_squared_error: 3.0161\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.0849 - mean_squared_error: 2.0849 - val_loss: 3.4743 - val_mean_squared_error: 3.4743\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.9990 - mean_squared_error: 1.9990 - val_loss: 3.3620 - val_mean_squared_error: 3.3620\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.1521 - mean_squared_error: 2.1521 - val_loss: 3.0371 - val_mean_squared_error: 3.0371\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.1012 - mean_squared_error: 2.1012 - val_loss: 2.6538 - val_mean_squared_error: 2.6538\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.7883 - mean_squared_error: 1.7883 - val_loss: 3.1449 - val_mean_squared_error: 3.1449\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.7283 - mean_squared_error: 1.7283 - val_loss: 2.7009 - val_mean_squared_error: 2.7009\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.7121 - mean_squared_error: 1.7121 - val_loss: 2.3392 - val_mean_squared_error: 2.3392\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.8096 - mean_squared_error: 1.8096 - val_loss: 2.5806 - val_mean_squared_error: 2.5806\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.6941 - mean_squared_error: 1.6941 - val_loss: 2.6885 - val_mean_squared_error: 2.6885\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.6848 - mean_squared_error: 1.6848 - val_loss: 2.7032 - val_mean_squared_error: 2.7032\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.7045 - mean_squared_error: 1.7045 - val_loss: 2.5585 - val_mean_squared_error: 2.5585\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.7466 - mean_squared_error: 1.7466 - val_loss: 2.7773 - val_mean_squared_error: 2.7773\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.7958 - mean_squared_error: 1.7958 - val_loss: 2.6197 - val_mean_squared_error: 2.6197\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.7448 - mean_squared_error: 1.7448 - val_loss: 2.5110 - val_mean_squared_error: 2.5110\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.9094 - mean_squared_error: 1.9094 - val_loss: 2.8845 - val_mean_squared_error: 2.8845\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.6569 - mean_squared_error: 1.6569 - val_loss: 2.5355 - val_mean_squared_error: 2.5355\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.5588 - mean_squared_error: 1.5588 - val_loss: 2.8197 - val_mean_squared_error: 2.8197\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.6138 - mean_squared_error: 1.6138 - val_loss: 2.5728 - val_mean_squared_error: 2.5728\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7056 - mean_squared_error: 1.7056 - val_loss: 2.8890 - val_mean_squared_error: 2.8890\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.7974 - mean_squared_error: 1.7974 - val_loss: 2.7150 - val_mean_squared_error: 2.7150\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.6240 - mean_squared_error: 1.6240 - val_loss: 2.6954 - val_mean_squared_error: 2.6954\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.5664 - mean_squared_error: 1.5664 - val_loss: 2.4739 - val_mean_squared_error: 2.4739\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.5822 - mean_squared_error: 1.5822 - val_loss: 2.7296 - val_mean_squared_error: 2.7296\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6083 - mean_squared_error: 1.6083 - val_loss: 2.8219 - val_mean_squared_error: 2.8219\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.5422 - mean_squared_error: 1.5422 - val_loss: 2.6241 - val_mean_squared_error: 2.6241\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.3149 - mean_squared_error: 1.3149 - val_loss: 2.3266 - val_mean_squared_error: 2.3266\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.4650 - mean_squared_error: 1.4650 - val_loss: 2.3567 - val_mean_squared_error: 2.3567\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.5084 - mean_squared_error: 1.5084 - val_loss: 2.5849 - val_mean_squared_error: 2.5849\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.6194 - mean_squared_error: 1.6194 - val_loss: 2.7590 - val_mean_squared_error: 2.7590\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.3794 - mean_squared_error: 1.3794 - val_loss: 2.9521 - val_mean_squared_error: 2.9521\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.5677 - mean_squared_error: 1.5677 - val_loss: 3.0100 - val_mean_squared_error: 3.0100\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.3820 - mean_squared_error: 1.3820 - val_loss: 2.2904 - val_mean_squared_error: 2.2904\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.5455 - mean_squared_error: 1.5455 - val_loss: 2.8284 - val_mean_squared_error: 2.8284\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.3920 - mean_squared_error: 1.3920 - val_loss: 2.6326 - val_mean_squared_error: 2.6326\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4498 - mean_squared_error: 1.4498 - val_loss: 2.5823 - val_mean_squared_error: 2.5823\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.3936 - mean_squared_error: 1.3936 - val_loss: 2.4863 - val_mean_squared_error: 2.4863\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.3283 - mean_squared_error: 1.3283 - val_loss: 2.4734 - val_mean_squared_error: 2.4734\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2934 - mean_squared_error: 1.2934 - val_loss: 2.2214 - val_mean_squared_error: 2.2214\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.3919 - mean_squared_error: 1.3919 - val_loss: 2.9126 - val_mean_squared_error: 2.9126\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.2437 - mean_squared_error: 1.2437 - val_loss: 2.3224 - val_mean_squared_error: 2.3224\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3420 - mean_squared_error: 1.3420 - val_loss: 2.3729 - val_mean_squared_error: 2.3729\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.2321 - mean_squared_error: 1.2321 - val_loss: 2.3310 - val_mean_squared_error: 2.3310\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1721 - mean_squared_error: 1.1721 - val_loss: 2.3392 - val_mean_squared_error: 2.3392\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.2786 - mean_squared_error: 1.2786 - val_loss: 2.3787 - val_mean_squared_error: 2.3787\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3087 - mean_squared_error: 1.3087 - val_loss: 2.4017 - val_mean_squared_error: 2.4017\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3934 - mean_squared_error: 1.3934 - val_loss: 2.4495 - val_mean_squared_error: 2.4495\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.3462 - mean_squared_error: 1.3462 - val_loss: 2.3560 - val_mean_squared_error: 2.3560\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2430 - mean_squared_error: 1.2430 - val_loss: 2.4449 - val_mean_squared_error: 2.4449\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.3659 - mean_squared_error: 1.3659 - val_loss: 2.3172 - val_mean_squared_error: 2.3172\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4333 - mean_squared_error: 1.4333 - val_loss: 2.6012 - val_mean_squared_error: 2.6012\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2574 - mean_squared_error: 1.2574 - val_loss: 2.3555 - val_mean_squared_error: 2.3555\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3175 - mean_squared_error: 1.3175 - val_loss: 2.3279 - val_mean_squared_error: 2.3279\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.3031 - mean_squared_error: 1.3031 - val_loss: 2.3120 - val_mean_squared_error: 2.3120\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.2710 - mean_squared_error: 1.2710 - val_loss: 2.4093 - val_mean_squared_error: 2.4093\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.2235 - mean_squared_error: 1.2235 - val_loss: 2.3998 - val_mean_squared_error: 2.3998\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1920 - mean_squared_error: 1.1920 - val_loss: 2.4827 - val_mean_squared_error: 2.4827\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3387 - mean_squared_error: 1.3387 - val_loss: 2.5139 - val_mean_squared_error: 2.5139\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2489 - mean_squared_error: 1.2489 - val_loss: 2.3785 - val_mean_squared_error: 2.3785\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.1985 - mean_squared_error: 1.1985 - val_loss: 2.3240 - val_mean_squared_error: 2.3240\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.2551 - mean_squared_error: 1.2551 - val_loss: 2.6304 - val_mean_squared_error: 2.6304\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2189 - mean_squared_error: 1.2189 - val_loss: 2.6674 - val_mean_squared_error: 2.6674\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.1358 - mean_squared_error: 1.1358 - val_loss: 2.3443 - val_mean_squared_error: 2.3443\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.1254 - mean_squared_error: 1.1254 - val_loss: 2.3837 - val_mean_squared_error: 2.3837\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.3945 - mean_squared_error: 1.3945 - val_loss: 2.4857 - val_mean_squared_error: 2.4857\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.1870 - mean_squared_error: 1.1870 - val_loss: 2.8425 - val_mean_squared_error: 2.8425\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1504 - mean_squared_error: 1.1504 - val_loss: 2.2698 - val_mean_squared_error: 2.2698\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.0453 - mean_squared_error: 1.0453 - val_loss: 2.3898 - val_mean_squared_error: 2.3898\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.0985 - mean_squared_error: 1.0985 - val_loss: 2.2262 - val_mean_squared_error: 2.2262\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.0476 - mean_squared_error: 1.0476 - val_loss: 2.1989 - val_mean_squared_error: 2.1989\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.1029 - mean_squared_error: 1.1029 - val_loss: 2.1765 - val_mean_squared_error: 2.1765\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.1655 - mean_squared_error: 1.1655 - val_loss: 2.1942 - val_mean_squared_error: 2.1942\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1885 - mean_squared_error: 1.1885 - val_loss: 2.2190 - val_mean_squared_error: 2.2190\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.9861 - mean_squared_error: 0.9861 - val_loss: 2.1038 - val_mean_squared_error: 2.1038\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.9923 - mean_squared_error: 0.9923 - val_loss: 2.1027 - val_mean_squared_error: 2.1027\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.1114 - mean_squared_error: 1.1114 - val_loss: 2.3167 - val_mean_squared_error: 2.3167\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.1083 - mean_squared_error: 1.1083 - val_loss: 2.1891 - val_mean_squared_error: 2.1891\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0358 - mean_squared_error: 1.0358 - val_loss: 2.1378 - val_mean_squared_error: 2.1378\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0570 - mean_squared_error: 1.0570 - val_loss: 2.0212 - val_mean_squared_error: 2.0212\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.0000 - mean_squared_error: 1.0000 - val_loss: 2.2540 - val_mean_squared_error: 2.2540\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0625 - mean_squared_error: 1.0625 - val_loss: 2.2083 - val_mean_squared_error: 2.2083\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9492 - mean_squared_error: 0.9492 - val_loss: 2.2381 - val_mean_squared_error: 2.2381\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8952 - mean_squared_error: 0.8952 - val_loss: 2.0250 - val_mean_squared_error: 2.0250\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0798 - mean_squared_error: 1.0798 - val_loss: 2.0326 - val_mean_squared_error: 2.0326\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.9278 - mean_squared_error: 0.9278 - val_loss: 2.5856 - val_mean_squared_error: 2.5856\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0559 - mean_squared_error: 1.0559 - val_loss: 2.4170 - val_mean_squared_error: 2.4170\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.3637 - mean_squared_error: 1.3637 - val_loss: 2.5935 - val_mean_squared_error: 2.5935\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0374 - mean_squared_error: 1.0374 - val_loss: 2.2053 - val_mean_squared_error: 2.2053\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.9725 - mean_squared_error: 0.9725 - val_loss: 2.6579 - val_mean_squared_error: 2.6579\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.1627 - mean_squared_error: 1.1627 - val_loss: 2.2827 - val_mean_squared_error: 2.2827\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0309 - mean_squared_error: 1.0309 - val_loss: 2.2186 - val_mean_squared_error: 2.2186\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.8957 - mean_squared_error: 0.8957 - val_loss: 2.0731 - val_mean_squared_error: 2.0731\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 2.4350 - val_mean_squared_error: 2.4350\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.1762 - mean_squared_error: 1.1762 - val_loss: 2.2261 - val_mean_squared_error: 2.2261\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9955 - mean_squared_error: 0.9955 - val_loss: 2.2821 - val_mean_squared_error: 2.2821\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.0268 - mean_squared_error: 1.0268 - val_loss: 3.5035 - val_mean_squared_error: 3.5035\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0651 - mean_squared_error: 1.0651 - val_loss: 2.0200 - val_mean_squared_error: 2.0200\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.8722 - mean_squared_error: 0.8722 - val_loss: 2.2049 - val_mean_squared_error: 2.2049\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0547 - mean_squared_error: 1.0547 - val_loss: 2.0890 - val_mean_squared_error: 2.0890\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1247 - mean_squared_error: 1.1247 - val_loss: 1.9508 - val_mean_squared_error: 1.9508\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.8458 - mean_squared_error: 0.8458 - val_loss: 1.9658 - val_mean_squared_error: 1.9658\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.8194 - mean_squared_error: 0.8194 - val_loss: 2.1903 - val_mean_squared_error: 2.1903\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.0038 - mean_squared_error: 1.0038 - val_loss: 2.1744 - val_mean_squared_error: 2.1744\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.8187 - mean_squared_error: 0.8187 - val_loss: 1.9096 - val_mean_squared_error: 1.9096\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9063 - mean_squared_error: 0.9063 - val_loss: 2.1571 - val_mean_squared_error: 2.1571\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9368 - mean_squared_error: 0.9368 - val_loss: 2.1649 - val_mean_squared_error: 2.1649\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.7867 - mean_squared_error: 0.7867 - val_loss: 2.0148 - val_mean_squared_error: 2.0148\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9976 - mean_squared_error: 0.9976 - val_loss: 2.1018 - val_mean_squared_error: 2.1018\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.9570 - mean_squared_error: 0.9570 - val_loss: 2.3011 - val_mean_squared_error: 2.3011\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8370 - mean_squared_error: 0.8370 - val_loss: 2.0218 - val_mean_squared_error: 2.0218\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.9221 - mean_squared_error: 0.9221 - val_loss: 2.2492 - val_mean_squared_error: 2.2492\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8152 - mean_squared_error: 0.8152 - val_loss: 1.9188 - val_mean_squared_error: 1.9188\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.8574 - mean_squared_error: 0.8574 - val_loss: 1.9493 - val_mean_squared_error: 1.9493\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7971 - mean_squared_error: 0.7971 - val_loss: 2.2570 - val_mean_squared_error: 2.2570\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.8692 - mean_squared_error: 0.8692 - val_loss: 2.1362 - val_mean_squared_error: 2.1362\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9270 - mean_squared_error: 0.9270 - val_loss: 2.3584 - val_mean_squared_error: 2.3584\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.8560 - mean_squared_error: 0.8560 - val_loss: 2.1884 - val_mean_squared_error: 2.1884\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.9070 - mean_squared_error: 0.9070 - val_loss: 2.0495 - val_mean_squared_error: 2.0495\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9336 - mean_squared_error: 0.9336 - val_loss: 2.2069 - val_mean_squared_error: 2.2069\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 0.8127 - mean_squared_error: 0.8127 - val_loss: 2.2172 - val_mean_squared_error: 2.2172\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.8229 - mean_squared_error: 0.8229 - val_loss: 1.9901 - val_mean_squared_error: 1.9901\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.8411 - mean_squared_error: 0.8411 - val_loss: 2.2582 - val_mean_squared_error: 2.2582\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.7668 - mean_squared_error: 0.7668 - val_loss: 2.0985 - val_mean_squared_error: 2.0985\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.7228 - mean_squared_error: 0.7228 - val_loss: 2.0415 - val_mean_squared_error: 2.0415\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.7843 - mean_squared_error: 0.7843 - val_loss: 2.1484 - val_mean_squared_error: 2.1484\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8579 - mean_squared_error: 0.8579 - val_loss: 2.0915 - val_mean_squared_error: 2.0915\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.7658 - mean_squared_error: 0.7658 - val_loss: 1.9949 - val_mean_squared_error: 1.9949\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.7809 - mean_squared_error: 0.7809 - val_loss: 1.9587 - val_mean_squared_error: 1.9587\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.7708 - mean_squared_error: 0.7708 - val_loss: 2.1720 - val_mean_squared_error: 2.1720\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8331 - mean_squared_error: 0.8331 - val_loss: 2.3951 - val_mean_squared_error: 2.3951\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8634 - mean_squared_error: 0.8634 - val_loss: 1.8582 - val_mean_squared_error: 1.8582\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.8005 - mean_squared_error: 0.8005 - val_loss: 2.0356 - val_mean_squared_error: 2.0356\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.7554 - mean_squared_error: 0.7554 - val_loss: 2.3553 - val_mean_squared_error: 2.3553\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.7712 - mean_squared_error: 0.7712 - val_loss: 2.0958 - val_mean_squared_error: 2.0958\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.7537 - mean_squared_error: 0.7537 - val_loss: 1.9287 - val_mean_squared_error: 1.9287\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.7013 - mean_squared_error: 0.7013 - val_loss: 2.2033 - val_mean_squared_error: 2.2033\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8372 - mean_squared_error: 0.8372 - val_loss: 2.0894 - val_mean_squared_error: 2.0894\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7680 - mean_squared_error: 0.7680 - val_loss: 1.9875 - val_mean_squared_error: 1.9875\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.7731 - mean_squared_error: 0.7731 - val_loss: 2.1904 - val_mean_squared_error: 2.1904\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.7750 - mean_squared_error: 0.7750 - val_loss: 2.2533 - val_mean_squared_error: 2.2533\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.8032 - mean_squared_error: 0.8032 - val_loss: 1.8684 - val_mean_squared_error: 1.8684\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.7611 - mean_squared_error: 0.7611 - val_loss: 2.1432 - val_mean_squared_error: 2.1432\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8087 - mean_squared_error: 0.8087 - val_loss: 2.2644 - val_mean_squared_error: 2.2644\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8524 - mean_squared_error: 0.8524 - val_loss: 1.9689 - val_mean_squared_error: 1.9689\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7537 - mean_squared_error: 0.7537 - val_loss: 2.0600 - val_mean_squared_error: 2.0600\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.6896 - mean_squared_error: 0.6896 - val_loss: 2.1662 - val_mean_squared_error: 2.1662\n",
            "==================================================\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_57 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_95 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_57 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_76 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_58 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_96 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_58 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_77 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_97 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_59 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_78 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_98 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_79 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_57 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_99 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_58 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_59 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 426.6153 - mean_squared_error: 426.6153 - val_loss: 1143.9678 - val_mean_squared_error: 1143.9679\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 21.8722 - mean_squared_error: 21.8722 - val_loss: 144.6354 - val_mean_squared_error: 144.6354\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 20.2001 - mean_squared_error: 20.2001 - val_loss: 98.3358 - val_mean_squared_error: 98.3358\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 22.3840 - mean_squared_error: 22.3840 - val_loss: 54.5365 - val_mean_squared_error: 54.5365\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 18.6919 - mean_squared_error: 18.6919 - val_loss: 32.9817 - val_mean_squared_error: 32.9817\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 15.4608 - mean_squared_error: 15.4608 - val_loss: 20.0282 - val_mean_squared_error: 20.0282\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 14.6066 - mean_squared_error: 14.6066 - val_loss: 12.6181 - val_mean_squared_error: 12.6181\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 13.8483 - mean_squared_error: 13.8483 - val_loss: 11.7142 - val_mean_squared_error: 11.7142\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 13.7807 - mean_squared_error: 13.7807 - val_loss: 10.3840 - val_mean_squared_error: 10.3840\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 14.6598 - mean_squared_error: 14.6598 - val_loss: 16.4228 - val_mean_squared_error: 16.4228\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 12.2580 - mean_squared_error: 12.2580 - val_loss: 11.2432 - val_mean_squared_error: 11.2432\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 10.7118 - mean_squared_error: 10.7118 - val_loss: 10.2248 - val_mean_squared_error: 10.2248\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 10.7972 - mean_squared_error: 10.7972 - val_loss: 10.9756 - val_mean_squared_error: 10.9756\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 10.4075 - mean_squared_error: 10.4075 - val_loss: 9.8774 - val_mean_squared_error: 9.8774\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 9.7284 - mean_squared_error: 9.7284 - val_loss: 8.7340 - val_mean_squared_error: 8.7340\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 9.4383 - mean_squared_error: 9.4383 - val_loss: 8.4860 - val_mean_squared_error: 8.4860\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 8.5536 - mean_squared_error: 8.5536 - val_loss: 7.5165 - val_mean_squared_error: 7.5165\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 8.1669 - mean_squared_error: 8.1669 - val_loss: 7.8729 - val_mean_squared_error: 7.8729\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 7.2447 - mean_squared_error: 7.2447 - val_loss: 6.8231 - val_mean_squared_error: 6.8231\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 6.9085 - mean_squared_error: 6.9085 - val_loss: 7.4406 - val_mean_squared_error: 7.4406\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 6.7554 - mean_squared_error: 6.7554 - val_loss: 6.0907 - val_mean_squared_error: 6.0907\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 5.7660 - mean_squared_error: 5.7660 - val_loss: 5.1195 - val_mean_squared_error: 5.1195\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 5.4672 - mean_squared_error: 5.4672 - val_loss: 6.3296 - val_mean_squared_error: 6.3296\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 5.2651 - mean_squared_error: 5.2652 - val_loss: 6.1872 - val_mean_squared_error: 6.1872\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 5.0133 - mean_squared_error: 5.0133 - val_loss: 6.0183 - val_mean_squared_error: 6.0183\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 4.7613 - mean_squared_error: 4.7613 - val_loss: 7.8782 - val_mean_squared_error: 7.8782\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 5.0873 - mean_squared_error: 5.0873 - val_loss: 5.3673 - val_mean_squared_error: 5.3673\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 4.3690 - mean_squared_error: 4.3690 - val_loss: 4.4812 - val_mean_squared_error: 4.4812\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 4.2342 - mean_squared_error: 4.2342 - val_loss: 4.4697 - val_mean_squared_error: 4.4697\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 3.8447 - mean_squared_error: 3.8447 - val_loss: 3.9087 - val_mean_squared_error: 3.9087\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.8826 - mean_squared_error: 3.8826 - val_loss: 4.2703 - val_mean_squared_error: 4.2703\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.9179 - mean_squared_error: 3.9179 - val_loss: 4.6891 - val_mean_squared_error: 4.6891\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 3.5019 - mean_squared_error: 3.5019 - val_loss: 4.0235 - val_mean_squared_error: 4.0235\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 3.4258 - mean_squared_error: 3.4258 - val_loss: 3.9588 - val_mean_squared_error: 3.9588\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 3.3158 - mean_squared_error: 3.3158 - val_loss: 3.4133 - val_mean_squared_error: 3.4133\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 3.1781 - mean_squared_error: 3.1781 - val_loss: 3.2850 - val_mean_squared_error: 3.2850\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 3.1277 - mean_squared_error: 3.1277 - val_loss: 3.3950 - val_mean_squared_error: 3.3950\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.9160 - mean_squared_error: 2.9160 - val_loss: 3.4031 - val_mean_squared_error: 3.4031\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 2.8458 - mean_squared_error: 2.8458 - val_loss: 3.1154 - val_mean_squared_error: 3.1154\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.8309 - mean_squared_error: 2.8309 - val_loss: 3.5739 - val_mean_squared_error: 3.5739\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.9902 - mean_squared_error: 2.9902 - val_loss: 3.4316 - val_mean_squared_error: 3.4316\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 2.8561 - mean_squared_error: 2.8561 - val_loss: 3.7055 - val_mean_squared_error: 3.7055\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.9818 - mean_squared_error: 2.9818 - val_loss: 3.0011 - val_mean_squared_error: 3.0011\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.6614 - mean_squared_error: 2.6614 - val_loss: 3.1576 - val_mean_squared_error: 3.1576\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.5085 - mean_squared_error: 2.5085 - val_loss: 3.0235 - val_mean_squared_error: 3.0235\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.7180 - mean_squared_error: 2.7180 - val_loss: 3.0542 - val_mean_squared_error: 3.0542\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.6552 - mean_squared_error: 2.6552 - val_loss: 3.9185 - val_mean_squared_error: 3.9185\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 2.6002 - mean_squared_error: 2.6002 - val_loss: 3.4970 - val_mean_squared_error: 3.4970\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.4975 - mean_squared_error: 2.4975 - val_loss: 3.1776 - val_mean_squared_error: 3.1776\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.4238 - mean_squared_error: 2.4238 - val_loss: 3.0948 - val_mean_squared_error: 3.0948\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 2.4679 - mean_squared_error: 2.4679 - val_loss: 2.9817 - val_mean_squared_error: 2.9817\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.4183 - mean_squared_error: 2.4183 - val_loss: 3.0782 - val_mean_squared_error: 3.0782\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.2719 - mean_squared_error: 2.2719 - val_loss: 2.7525 - val_mean_squared_error: 2.7525\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.2524 - mean_squared_error: 2.2524 - val_loss: 2.7415 - val_mean_squared_error: 2.7415\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.2813 - mean_squared_error: 2.2813 - val_loss: 2.7609 - val_mean_squared_error: 2.7609\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.1660 - mean_squared_error: 2.1660 - val_loss: 2.7617 - val_mean_squared_error: 2.7617\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 2.2474 - mean_squared_error: 2.2474 - val_loss: 2.9912 - val_mean_squared_error: 2.9912\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.1505 - mean_squared_error: 2.1505 - val_loss: 3.1561 - val_mean_squared_error: 3.1561\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.1804 - mean_squared_error: 2.1804 - val_loss: 2.7873 - val_mean_squared_error: 2.7873\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.9887 - mean_squared_error: 1.9887 - val_loss: 2.7868 - val_mean_squared_error: 2.7868\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.0957 - mean_squared_error: 2.0957 - val_loss: 3.1874 - val_mean_squared_error: 3.1874\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.1519 - mean_squared_error: 2.1519 - val_loss: 3.5275 - val_mean_squared_error: 3.5275\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.8900 - mean_squared_error: 1.8900 - val_loss: 2.5971 - val_mean_squared_error: 2.5971\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.9267 - mean_squared_error: 1.9267 - val_loss: 2.9004 - val_mean_squared_error: 2.9004\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.8904 - mean_squared_error: 1.8904 - val_loss: 2.5404 - val_mean_squared_error: 2.5404\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.9214 - mean_squared_error: 1.9214 - val_loss: 3.3238 - val_mean_squared_error: 3.3238\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.1177 - mean_squared_error: 2.1177 - val_loss: 2.4281 - val_mean_squared_error: 2.4281\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.8718 - mean_squared_error: 1.8718 - val_loss: 2.5634 - val_mean_squared_error: 2.5634\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 2.0192 - mean_squared_error: 2.0192 - val_loss: 2.6452 - val_mean_squared_error: 2.6452\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.8566 - mean_squared_error: 1.8566 - val_loss: 2.8116 - val_mean_squared_error: 2.8116\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.8990 - mean_squared_error: 1.8990 - val_loss: 3.1665 - val_mean_squared_error: 3.1665\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.8871 - mean_squared_error: 1.8871 - val_loss: 3.2921 - val_mean_squared_error: 3.2921\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.8846 - mean_squared_error: 1.8846 - val_loss: 2.4567 - val_mean_squared_error: 2.4567\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.7840 - mean_squared_error: 1.7840 - val_loss: 2.4715 - val_mean_squared_error: 2.4715\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.7733 - mean_squared_error: 1.7733 - val_loss: 2.4820 - val_mean_squared_error: 2.4820\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.6763 - mean_squared_error: 1.6763 - val_loss: 2.3985 - val_mean_squared_error: 2.3985\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.6507 - mean_squared_error: 1.6507 - val_loss: 2.7780 - val_mean_squared_error: 2.7780\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.5483 - mean_squared_error: 1.5483 - val_loss: 2.2758 - val_mean_squared_error: 2.2758\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.9409 - mean_squared_error: 1.9409 - val_loss: 2.5229 - val_mean_squared_error: 2.5229\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.6283 - mean_squared_error: 1.6283 - val_loss: 2.5616 - val_mean_squared_error: 2.5616\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4453 - mean_squared_error: 1.4453 - val_loss: 2.4821 - val_mean_squared_error: 2.4821\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6782 - mean_squared_error: 1.6782 - val_loss: 2.4475 - val_mean_squared_error: 2.4475\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.4957 - mean_squared_error: 1.4957 - val_loss: 2.3695 - val_mean_squared_error: 2.3695\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.5474 - mean_squared_error: 1.5474 - val_loss: 2.6531 - val_mean_squared_error: 2.6531\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.5988 - mean_squared_error: 1.5988 - val_loss: 2.6179 - val_mean_squared_error: 2.6179\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.6614 - mean_squared_error: 1.6614 - val_loss: 2.6222 - val_mean_squared_error: 2.6222\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.5070 - mean_squared_error: 1.5070 - val_loss: 2.3446 - val_mean_squared_error: 2.3446\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.4505 - mean_squared_error: 1.4505 - val_loss: 2.3496 - val_mean_squared_error: 2.3496\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.4129 - mean_squared_error: 1.4129 - val_loss: 2.8425 - val_mean_squared_error: 2.8425\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.4697 - mean_squared_error: 1.4697 - val_loss: 2.3166 - val_mean_squared_error: 2.3166\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.4935 - mean_squared_error: 1.4935 - val_loss: 2.7592 - val_mean_squared_error: 2.7592\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.4395 - mean_squared_error: 1.4395 - val_loss: 2.3179 - val_mean_squared_error: 2.3179\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.3955 - mean_squared_error: 1.3955 - val_loss: 2.1801 - val_mean_squared_error: 2.1801\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3529 - mean_squared_error: 1.3529 - val_loss: 2.7483 - val_mean_squared_error: 2.7483\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4067 - mean_squared_error: 1.4067 - val_loss: 2.5365 - val_mean_squared_error: 2.5365\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.4256 - mean_squared_error: 1.4256 - val_loss: 2.1893 - val_mean_squared_error: 2.1893\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.3680 - mean_squared_error: 1.3680 - val_loss: 2.2493 - val_mean_squared_error: 2.2493\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.3601 - mean_squared_error: 1.3601 - val_loss: 2.3587 - val_mean_squared_error: 2.3587\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.2880 - mean_squared_error: 1.2880 - val_loss: 2.3550 - val_mean_squared_error: 2.3550\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.5029 - mean_squared_error: 1.5029 - val_loss: 2.5827 - val_mean_squared_error: 2.5827\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.3426 - mean_squared_error: 1.3426 - val_loss: 2.4242 - val_mean_squared_error: 2.4242\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.2882 - mean_squared_error: 1.2882 - val_loss: 2.3157 - val_mean_squared_error: 2.3157\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.3570 - mean_squared_error: 1.3570 - val_loss: 2.4012 - val_mean_squared_error: 2.4012\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.3714 - mean_squared_error: 1.3714 - val_loss: 2.3300 - val_mean_squared_error: 2.3300\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.4116 - mean_squared_error: 1.4116 - val_loss: 2.5236 - val_mean_squared_error: 2.5236\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.3879 - mean_squared_error: 1.3879 - val_loss: 3.1167 - val_mean_squared_error: 3.1167\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.4988 - mean_squared_error: 1.4988 - val_loss: 2.2303 - val_mean_squared_error: 2.2303\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.2030 - mean_squared_error: 1.2030 - val_loss: 2.4486 - val_mean_squared_error: 2.4486\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.3012 - mean_squared_error: 1.3012 - val_loss: 2.2890 - val_mean_squared_error: 2.2890\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.2922 - mean_squared_error: 1.2922 - val_loss: 2.4395 - val_mean_squared_error: 2.4395\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.2031 - mean_squared_error: 1.2031 - val_loss: 2.4925 - val_mean_squared_error: 2.4925\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.4507 - mean_squared_error: 1.4507 - val_loss: 2.2449 - val_mean_squared_error: 2.2449\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 1.2812 - mean_squared_error: 1.2812 - val_loss: 2.4850 - val_mean_squared_error: 2.4850\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.2290 - mean_squared_error: 1.2290 - val_loss: 2.2379 - val_mean_squared_error: 2.2379\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.2805 - mean_squared_error: 1.2805 - val_loss: 2.6597 - val_mean_squared_error: 2.6597\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.1993 - mean_squared_error: 1.1993 - val_loss: 2.2184 - val_mean_squared_error: 2.2184\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.2152 - mean_squared_error: 1.2152 - val_loss: 2.2219 - val_mean_squared_error: 2.2219\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1723 - mean_squared_error: 1.1723 - val_loss: 2.1882 - val_mean_squared_error: 2.1882\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.1156 - mean_squared_error: 1.1156 - val_loss: 2.2872 - val_mean_squared_error: 2.2872\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.1776 - mean_squared_error: 1.1776 - val_loss: 2.1816 - val_mean_squared_error: 2.1816\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.2312 - mean_squared_error: 1.2312 - val_loss: 2.2313 - val_mean_squared_error: 2.2313\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.0888 - mean_squared_error: 1.0888 - val_loss: 2.2382 - val_mean_squared_error: 2.2382\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1368 - mean_squared_error: 1.1368 - val_loss: 2.0726 - val_mean_squared_error: 2.0726\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2422 - mean_squared_error: 1.2422 - val_loss: 2.2001 - val_mean_squared_error: 2.2001\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.2733 - mean_squared_error: 1.2733 - val_loss: 2.0745 - val_mean_squared_error: 2.0745\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.1050 - mean_squared_error: 1.1050 - val_loss: 2.2787 - val_mean_squared_error: 2.2787\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.1230 - mean_squared_error: 1.1230 - val_loss: 2.1821 - val_mean_squared_error: 2.1821\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0704 - mean_squared_error: 1.0704 - val_loss: 2.6542 - val_mean_squared_error: 2.6542\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2849 - mean_squared_error: 1.2849 - val_loss: 2.1816 - val_mean_squared_error: 2.1816\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1985 - mean_squared_error: 1.1985 - val_loss: 2.4453 - val_mean_squared_error: 2.4453\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.1040 - mean_squared_error: 1.1040 - val_loss: 2.2107 - val_mean_squared_error: 2.2107\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0418 - mean_squared_error: 1.0418 - val_loss: 1.9651 - val_mean_squared_error: 1.9651\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.0862 - mean_squared_error: 1.0862 - val_loss: 2.1148 - val_mean_squared_error: 2.1148\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.1372 - mean_squared_error: 1.1372 - val_loss: 2.3680 - val_mean_squared_error: 2.3680\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0511 - mean_squared_error: 1.0511 - val_loss: 2.1998 - val_mean_squared_error: 2.1998\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.9699 - mean_squared_error: 0.9699 - val_loss: 1.9581 - val_mean_squared_error: 1.9581\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.0293 - mean_squared_error: 1.0293 - val_loss: 2.0708 - val_mean_squared_error: 2.0708\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8700 - mean_squared_error: 0.8700 - val_loss: 2.1299 - val_mean_squared_error: 2.1299\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.9779 - mean_squared_error: 0.9779 - val_loss: 2.3039 - val_mean_squared_error: 2.3039\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0488 - mean_squared_error: 1.0488 - val_loss: 2.1797 - val_mean_squared_error: 2.1797\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9997 - mean_squared_error: 0.9997 - val_loss: 2.2536 - val_mean_squared_error: 2.2536\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.1242 - mean_squared_error: 1.1242 - val_loss: 2.2580 - val_mean_squared_error: 2.2580\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 2.1507 - val_mean_squared_error: 2.1507\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 0.9204 - mean_squared_error: 0.9204 - val_loss: 1.9990 - val_mean_squared_error: 1.9990\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.1138 - mean_squared_error: 1.1138 - val_loss: 2.4531 - val_mean_squared_error: 2.4531\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.0591 - mean_squared_error: 1.0591 - val_loss: 1.9582 - val_mean_squared_error: 1.9582\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.1667 - mean_squared_error: 1.1667 - val_loss: 2.5621 - val_mean_squared_error: 2.5621\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.1700 - mean_squared_error: 1.1700 - val_loss: 2.2410 - val_mean_squared_error: 2.2410\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.1350 - mean_squared_error: 1.1350 - val_loss: 2.2178 - val_mean_squared_error: 2.2178\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 0.9873 - mean_squared_error: 0.9873 - val_loss: 2.1039 - val_mean_squared_error: 2.1039\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.9166 - mean_squared_error: 0.9166 - val_loss: 2.2730 - val_mean_squared_error: 2.2730\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.0226 - mean_squared_error: 1.0226 - val_loss: 1.9791 - val_mean_squared_error: 1.9791\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.9697 - val_mean_squared_error: 1.9697\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 0.8879 - mean_squared_error: 0.8879 - val_loss: 2.3632 - val_mean_squared_error: 2.3632\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.0314 - mean_squared_error: 1.0314 - val_loss: 2.1536 - val_mean_squared_error: 2.1536\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 0.8438 - mean_squared_error: 0.8438 - val_loss: 2.1338 - val_mean_squared_error: 2.1338\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.9538 - mean_squared_error: 0.9538 - val_loss: 2.0675 - val_mean_squared_error: 2.0675\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.0188 - mean_squared_error: 1.0188 - val_loss: 2.3784 - val_mean_squared_error: 2.3784\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.9702 - mean_squared_error: 0.9702 - val_loss: 2.0329 - val_mean_squared_error: 2.0329\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.8893 - mean_squared_error: 0.8893 - val_loss: 1.9328 - val_mean_squared_error: 1.9328\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.9034 - mean_squared_error: 0.9034 - val_loss: 1.9747 - val_mean_squared_error: 1.9747\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8704 - mean_squared_error: 0.8704 - val_loss: 1.9527 - val_mean_squared_error: 1.9527\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8064 - mean_squared_error: 0.8064 - val_loss: 2.1770 - val_mean_squared_error: 2.1770\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 2.2321 - val_mean_squared_error: 2.2321\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9028 - mean_squared_error: 0.9028 - val_loss: 2.2342 - val_mean_squared_error: 2.2342\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 2.2147 - val_mean_squared_error: 2.2147\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.0268 - mean_squared_error: 1.0268 - val_loss: 2.2305 - val_mean_squared_error: 2.2305\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.8919 - mean_squared_error: 0.8919 - val_loss: 2.2604 - val_mean_squared_error: 2.2604\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9118 - mean_squared_error: 0.9118 - val_loss: 2.0153 - val_mean_squared_error: 2.0153\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.9123 - mean_squared_error: 0.9123 - val_loss: 2.3711 - val_mean_squared_error: 2.3711\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0287 - mean_squared_error: 1.0287 - val_loss: 2.4483 - val_mean_squared_error: 2.4483\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.8870 - mean_squared_error: 0.8870 - val_loss: 2.1776 - val_mean_squared_error: 2.1776\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.8777 - mean_squared_error: 0.8777 - val_loss: 2.0088 - val_mean_squared_error: 2.0088\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.8244 - mean_squared_error: 0.8244 - val_loss: 2.2370 - val_mean_squared_error: 2.2370\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.9402 - mean_squared_error: 0.9402 - val_loss: 2.0504 - val_mean_squared_error: 2.0504\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 0.8858 - mean_squared_error: 0.8858 - val_loss: 1.9646 - val_mean_squared_error: 1.9646\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8285 - mean_squared_error: 0.8285 - val_loss: 2.0114 - val_mean_squared_error: 2.0114\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.9137 - mean_squared_error: 0.9137 - val_loss: 2.4070 - val_mean_squared_error: 2.4070\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.8648 - mean_squared_error: 0.8648 - val_loss: 2.2980 - val_mean_squared_error: 2.2980\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.7853 - mean_squared_error: 0.7853 - val_loss: 2.0156 - val_mean_squared_error: 2.0156\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8987 - mean_squared_error: 0.8987 - val_loss: 2.1708 - val_mean_squared_error: 2.1708\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.8328 - mean_squared_error: 0.8328 - val_loss: 2.0002 - val_mean_squared_error: 2.0002\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8220 - mean_squared_error: 0.8220 - val_loss: 1.9817 - val_mean_squared_error: 1.9817\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.8702 - mean_squared_error: 0.8702 - val_loss: 2.2412 - val_mean_squared_error: 2.2412\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.8469 - mean_squared_error: 0.8469 - val_loss: 1.9596 - val_mean_squared_error: 1.9596\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.7713 - mean_squared_error: 0.7713 - val_loss: 2.0808 - val_mean_squared_error: 2.0808\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9653 - mean_squared_error: 0.9653 - val_loss: 2.1548 - val_mean_squared_error: 2.1548\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.8994 - mean_squared_error: 0.8994 - val_loss: 1.9598 - val_mean_squared_error: 1.9598\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8961 - mean_squared_error: 0.8961 - val_loss: 1.9136 - val_mean_squared_error: 1.9136\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.9077 - mean_squared_error: 0.9077 - val_loss: 1.9929 - val_mean_squared_error: 1.9929\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 2.2810 - val_mean_squared_error: 2.2810\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.7825 - mean_squared_error: 0.7825 - val_loss: 2.1755 - val_mean_squared_error: 2.1755\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9636 - mean_squared_error: 0.9636 - val_loss: 1.9043 - val_mean_squared_error: 1.9043\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 2.1135 - val_mean_squared_error: 2.1135\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7479 - mean_squared_error: 0.7479 - val_loss: 1.9204 - val_mean_squared_error: 1.9204\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.7815 - mean_squared_error: 0.7815 - val_loss: 2.0875 - val_mean_squared_error: 2.0875\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.8370 - mean_squared_error: 0.8370 - val_loss: 2.1161 - val_mean_squared_error: 2.1161\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8889 - mean_squared_error: 0.8889 - val_loss: 2.1548 - val_mean_squared_error: 2.1548\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 0.7852 - mean_squared_error: 0.7852 - val_loss: 1.9479 - val_mean_squared_error: 1.9479\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8600 - mean_squared_error: 0.8600 - val_loss: 1.9711 - val_mean_squared_error: 1.9711\n",
            "==================================================\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_60 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_100 (Bat (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_60 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_80 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_61 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_101 (Bat (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_61 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_81 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_62 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_102 (Bat (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_62 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_82 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_20 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_103 (Bat (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_83 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_104 (Bat (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_62 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 434.5872 - mean_squared_error: 434.5872 - val_loss: 155.4282 - val_mean_squared_error: 155.4282\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 21.9364 - mean_squared_error: 21.9364 - val_loss: 137.7350 - val_mean_squared_error: 137.7350\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 19.9353 - mean_squared_error: 19.9353 - val_loss: 72.5046 - val_mean_squared_error: 72.5046\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 19.3327 - mean_squared_error: 19.3327 - val_loss: 39.3669 - val_mean_squared_error: 39.3669\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 21.3071 - mean_squared_error: 21.3071 - val_loss: 35.5651 - val_mean_squared_error: 35.5651\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 13.8328 - mean_squared_error: 13.8328 - val_loss: 35.0230 - val_mean_squared_error: 35.0230\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 14.4416 - mean_squared_error: 14.4416 - val_loss: 14.1730 - val_mean_squared_error: 14.1730\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 13.0539 - mean_squared_error: 13.0539 - val_loss: 12.8951 - val_mean_squared_error: 12.8951\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 13.2892 - mean_squared_error: 13.2892 - val_loss: 10.5067 - val_mean_squared_error: 10.5068\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 11.8025 - mean_squared_error: 11.8025 - val_loss: 11.7583 - val_mean_squared_error: 11.7583\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 11.7017 - mean_squared_error: 11.7017 - val_loss: 10.4534 - val_mean_squared_error: 10.4534\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 12.1203 - mean_squared_error: 12.1203 - val_loss: 9.6361 - val_mean_squared_error: 9.6361\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 10.0101 - mean_squared_error: 10.0101 - val_loss: 8.9598 - val_mean_squared_error: 8.9598\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 9.4068 - mean_squared_error: 9.4068 - val_loss: 8.8583 - val_mean_squared_error: 8.8583\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 9.1770 - mean_squared_error: 9.1770 - val_loss: 10.5378 - val_mean_squared_error: 10.5378\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 8.8956 - mean_squared_error: 8.8956 - val_loss: 7.8999 - val_mean_squared_error: 7.8999\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 7.7453 - mean_squared_error: 7.7453 - val_loss: 10.1128 - val_mean_squared_error: 10.1128\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 7.6154 - mean_squared_error: 7.6154 - val_loss: 8.0096 - val_mean_squared_error: 8.0096\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 7.8546 - mean_squared_error: 7.8546 - val_loss: 6.5890 - val_mean_squared_error: 6.5890\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 6.7369 - mean_squared_error: 6.7369 - val_loss: 7.1536 - val_mean_squared_error: 7.1536\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 5.9822 - mean_squared_error: 5.9822 - val_loss: 6.3987 - val_mean_squared_error: 6.3987\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 6.1123 - mean_squared_error: 6.1123 - val_loss: 5.2549 - val_mean_squared_error: 5.2549\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 5.7408 - mean_squared_error: 5.7408 - val_loss: 5.6197 - val_mean_squared_error: 5.6197\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 5.2223 - mean_squared_error: 5.2223 - val_loss: 6.2201 - val_mean_squared_error: 6.2201\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 4.8635 - mean_squared_error: 4.8635 - val_loss: 4.4686 - val_mean_squared_error: 4.4686\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 4.6578 - mean_squared_error: 4.6578 - val_loss: 5.3812 - val_mean_squared_error: 5.3812\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 4.4877 - mean_squared_error: 4.4877 - val_loss: 5.7714 - val_mean_squared_error: 5.7714\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 4.2148 - mean_squared_error: 4.2148 - val_loss: 4.0059 - val_mean_squared_error: 4.0059\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 4.2785 - mean_squared_error: 4.2785 - val_loss: 4.2265 - val_mean_squared_error: 4.2265\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.9831 - mean_squared_error: 3.9831 - val_loss: 5.1458 - val_mean_squared_error: 5.1458\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 4.1138 - mean_squared_error: 4.1138 - val_loss: 4.2280 - val_mean_squared_error: 4.2280\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 3.8976 - mean_squared_error: 3.8976 - val_loss: 3.5897 - val_mean_squared_error: 3.5897\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.6373 - mean_squared_error: 3.6373 - val_loss: 3.4507 - val_mean_squared_error: 3.4507\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.4846 - mean_squared_error: 3.4846 - val_loss: 4.4398 - val_mean_squared_error: 4.4398\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 3.5614 - mean_squared_error: 3.5614 - val_loss: 3.4444 - val_mean_squared_error: 3.4444\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.5186 - mean_squared_error: 3.5186 - val_loss: 3.1938 - val_mean_squared_error: 3.1938\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.5497 - mean_squared_error: 3.5497 - val_loss: 4.4018 - val_mean_squared_error: 4.4018\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.3185 - mean_squared_error: 3.3185 - val_loss: 3.5503 - val_mean_squared_error: 3.5503\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 3.1255 - mean_squared_error: 3.1255 - val_loss: 4.0442 - val_mean_squared_error: 4.0442\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.9737 - mean_squared_error: 2.9737 - val_loss: 3.6310 - val_mean_squared_error: 3.6310\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 3.0693 - mean_squared_error: 3.0693 - val_loss: 3.2468 - val_mean_squared_error: 3.2468\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.9542 - mean_squared_error: 2.9542 - val_loss: 3.0880 - val_mean_squared_error: 3.0880\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.8049 - mean_squared_error: 2.8049 - val_loss: 3.5473 - val_mean_squared_error: 3.5473\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.6493 - mean_squared_error: 2.6493 - val_loss: 3.1291 - val_mean_squared_error: 3.1291\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.7417 - mean_squared_error: 2.7417 - val_loss: 2.7860 - val_mean_squared_error: 2.7860\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.6465 - mean_squared_error: 2.6465 - val_loss: 3.2320 - val_mean_squared_error: 3.2320\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 2.6953 - mean_squared_error: 2.6953 - val_loss: 3.3308 - val_mean_squared_error: 3.3308\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.6147 - mean_squared_error: 2.6147 - val_loss: 3.0830 - val_mean_squared_error: 3.0830\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.6176 - mean_squared_error: 2.6176 - val_loss: 2.7876 - val_mean_squared_error: 2.7876\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.6865 - mean_squared_error: 2.6865 - val_loss: 2.8747 - val_mean_squared_error: 2.8747\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.4482 - mean_squared_error: 2.4482 - val_loss: 3.0898 - val_mean_squared_error: 3.0898\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.5204 - mean_squared_error: 2.5204 - val_loss: 3.0433 - val_mean_squared_error: 3.0433\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.4757 - mean_squared_error: 2.4757 - val_loss: 2.9001 - val_mean_squared_error: 2.9001\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.3879 - mean_squared_error: 2.3879 - val_loss: 2.6680 - val_mean_squared_error: 2.6680\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.4322 - mean_squared_error: 2.4322 - val_loss: 3.9038 - val_mean_squared_error: 3.9039\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.4150 - mean_squared_error: 2.4150 - val_loss: 3.0664 - val_mean_squared_error: 3.0664\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.3407 - mean_squared_error: 2.3407 - val_loss: 2.9161 - val_mean_squared_error: 2.9161\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.3655 - mean_squared_error: 2.3655 - val_loss: 3.4004 - val_mean_squared_error: 3.4004\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.0986 - mean_squared_error: 2.0986 - val_loss: 2.7760 - val_mean_squared_error: 2.7760\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.2515 - mean_squared_error: 2.2515 - val_loss: 2.7088 - val_mean_squared_error: 2.7088\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 2.0459 - mean_squared_error: 2.0459 - val_loss: 2.7441 - val_mean_squared_error: 2.7441\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.0735 - mean_squared_error: 2.0735 - val_loss: 2.7865 - val_mean_squared_error: 2.7865\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.1954 - mean_squared_error: 2.1954 - val_loss: 2.7692 - val_mean_squared_error: 2.7692\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.0901 - mean_squared_error: 2.0901 - val_loss: 2.5707 - val_mean_squared_error: 2.5707\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.0014 - mean_squared_error: 2.0014 - val_loss: 2.6938 - val_mean_squared_error: 2.6938\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.0705 - mean_squared_error: 2.0705 - val_loss: 2.9593 - val_mean_squared_error: 2.9593\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.9658 - mean_squared_error: 1.9658 - val_loss: 2.5505 - val_mean_squared_error: 2.5505\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.9122 - mean_squared_error: 1.9122 - val_loss: 2.6709 - val_mean_squared_error: 2.6709\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.7726 - mean_squared_error: 1.7726 - val_loss: 2.4264 - val_mean_squared_error: 2.4264\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.9370 - mean_squared_error: 1.9370 - val_loss: 2.5171 - val_mean_squared_error: 2.5171\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.7608 - mean_squared_error: 1.7608 - val_loss: 2.4133 - val_mean_squared_error: 2.4133\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.9518 - mean_squared_error: 1.9518 - val_loss: 2.7262 - val_mean_squared_error: 2.7262\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.9564 - mean_squared_error: 1.9564 - val_loss: 3.5060 - val_mean_squared_error: 3.5060\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.0039 - mean_squared_error: 2.0039 - val_loss: 2.7842 - val_mean_squared_error: 2.7842\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.8509 - mean_squared_error: 1.8509 - val_loss: 2.3729 - val_mean_squared_error: 2.3729\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.7537 - mean_squared_error: 1.7537 - val_loss: 2.4811 - val_mean_squared_error: 2.4811\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.7879 - mean_squared_error: 1.7879 - val_loss: 2.4117 - val_mean_squared_error: 2.4117\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.6894 - mean_squared_error: 1.6894 - val_loss: 2.3036 - val_mean_squared_error: 2.3036\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7063 - mean_squared_error: 1.7063 - val_loss: 2.4750 - val_mean_squared_error: 2.4750\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6163 - mean_squared_error: 1.6163 - val_loss: 2.3922 - val_mean_squared_error: 2.3922\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.6341 - mean_squared_error: 1.6341 - val_loss: 2.8513 - val_mean_squared_error: 2.8513\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.7469 - mean_squared_error: 1.7469 - val_loss: 2.7258 - val_mean_squared_error: 2.7258\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.8312 - mean_squared_error: 1.8312 - val_loss: 2.6049 - val_mean_squared_error: 2.6049\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.7276 - mean_squared_error: 1.7276 - val_loss: 2.5769 - val_mean_squared_error: 2.5769\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.5717 - mean_squared_error: 1.5717 - val_loss: 2.6970 - val_mean_squared_error: 2.6970\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.7738 - mean_squared_error: 1.7738 - val_loss: 3.0548 - val_mean_squared_error: 3.0548\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.6817 - mean_squared_error: 1.6817 - val_loss: 2.3990 - val_mean_squared_error: 2.3990\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.6338 - mean_squared_error: 1.6338 - val_loss: 2.7463 - val_mean_squared_error: 2.7463\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6488 - mean_squared_error: 1.6488 - val_loss: 2.4876 - val_mean_squared_error: 2.4876\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7226 - mean_squared_error: 1.7226 - val_loss: 2.8844 - val_mean_squared_error: 2.8844\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.6248 - mean_squared_error: 1.6248 - val_loss: 2.6661 - val_mean_squared_error: 2.6661\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.5504 - mean_squared_error: 1.5504 - val_loss: 2.1839 - val_mean_squared_error: 2.1839\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.5007 - mean_squared_error: 1.5007 - val_loss: 2.5135 - val_mean_squared_error: 2.5135\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.6562 - mean_squared_error: 1.6562 - val_loss: 2.0781 - val_mean_squared_error: 2.0781\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5526 - mean_squared_error: 1.5526 - val_loss: 2.2705 - val_mean_squared_error: 2.2705\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.5397 - mean_squared_error: 1.5397 - val_loss: 2.1859 - val_mean_squared_error: 2.1859\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4487 - mean_squared_error: 1.4487 - val_loss: 2.6510 - val_mean_squared_error: 2.6510\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.4984 - mean_squared_error: 1.4984 - val_loss: 2.2123 - val_mean_squared_error: 2.2123\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.4339 - mean_squared_error: 1.4339 - val_loss: 2.5116 - val_mean_squared_error: 2.5116\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.4135 - mean_squared_error: 1.4135 - val_loss: 2.4382 - val_mean_squared_error: 2.4382\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.3738 - mean_squared_error: 1.3738 - val_loss: 2.1617 - val_mean_squared_error: 2.1617\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.3231 - mean_squared_error: 1.3231 - val_loss: 2.2347 - val_mean_squared_error: 2.2347\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.4309 - mean_squared_error: 1.4309 - val_loss: 2.4155 - val_mean_squared_error: 2.4155\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.3073 - mean_squared_error: 1.3073 - val_loss: 2.2290 - val_mean_squared_error: 2.2290\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.5492 - mean_squared_error: 1.5492 - val_loss: 2.2101 - val_mean_squared_error: 2.2101\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.5553 - mean_squared_error: 1.5553 - val_loss: 2.4382 - val_mean_squared_error: 2.4382\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2990 - mean_squared_error: 1.2990 - val_loss: 2.3674 - val_mean_squared_error: 2.3674\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.4890 - mean_squared_error: 1.4890 - val_loss: 2.0886 - val_mean_squared_error: 2.0886\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2432 - mean_squared_error: 1.2432 - val_loss: 2.0731 - val_mean_squared_error: 2.0731\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2121 - mean_squared_error: 1.2121 - val_loss: 2.0970 - val_mean_squared_error: 2.0970\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3086 - mean_squared_error: 1.3086 - val_loss: 2.1430 - val_mean_squared_error: 2.1430\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2405 - mean_squared_error: 1.2405 - val_loss: 2.3338 - val_mean_squared_error: 2.3338\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2569 - mean_squared_error: 1.2569 - val_loss: 2.6190 - val_mean_squared_error: 2.6190\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.4052 - mean_squared_error: 1.4052 - val_loss: 2.3346 - val_mean_squared_error: 2.3346\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.3051 - mean_squared_error: 1.3051 - val_loss: 2.2047 - val_mean_squared_error: 2.2047\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.3941 - mean_squared_error: 1.3941 - val_loss: 2.3710 - val_mean_squared_error: 2.3710\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.3398 - mean_squared_error: 1.3398 - val_loss: 2.2236 - val_mean_squared_error: 2.2236\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.3316 - mean_squared_error: 1.3316 - val_loss: 2.3353 - val_mean_squared_error: 2.3353\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3196 - mean_squared_error: 1.3196 - val_loss: 2.0881 - val_mean_squared_error: 2.0881\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1876 - mean_squared_error: 1.1876 - val_loss: 1.9427 - val_mean_squared_error: 1.9427\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1887 - mean_squared_error: 1.1887 - val_loss: 2.3655 - val_mean_squared_error: 2.3655\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1981 - mean_squared_error: 1.1981 - val_loss: 2.2501 - val_mean_squared_error: 2.2501\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.2577 - mean_squared_error: 1.2577 - val_loss: 2.1970 - val_mean_squared_error: 2.1970\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.1821 - mean_squared_error: 1.1821 - val_loss: 2.2366 - val_mean_squared_error: 2.2366\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1863 - mean_squared_error: 1.1863 - val_loss: 2.1768 - val_mean_squared_error: 2.1768\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1824 - mean_squared_error: 1.1824 - val_loss: 2.1951 - val_mean_squared_error: 2.1951\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2608 - mean_squared_error: 1.2608 - val_loss: 2.5986 - val_mean_squared_error: 2.5986\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2521 - mean_squared_error: 1.2521 - val_loss: 2.2589 - val_mean_squared_error: 2.2589\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.1073 - mean_squared_error: 1.1073 - val_loss: 2.0868 - val_mean_squared_error: 2.0868\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1105 - mean_squared_error: 1.1105 - val_loss: 2.3951 - val_mean_squared_error: 2.3951\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.0395 - mean_squared_error: 1.0395 - val_loss: 2.2324 - val_mean_squared_error: 2.2324\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.0701 - mean_squared_error: 1.0701 - val_loss: 1.9771 - val_mean_squared_error: 1.9771\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0862 - mean_squared_error: 1.0862 - val_loss: 2.3613 - val_mean_squared_error: 2.3613\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2991 - mean_squared_error: 1.2991 - val_loss: 2.2021 - val_mean_squared_error: 2.2021\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2605 - mean_squared_error: 1.2605 - val_loss: 2.1777 - val_mean_squared_error: 2.1777\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1996 - mean_squared_error: 1.1996 - val_loss: 1.9763 - val_mean_squared_error: 1.9763\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.1852 - mean_squared_error: 1.1852 - val_loss: 2.0522 - val_mean_squared_error: 2.0522\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2108 - mean_squared_error: 1.2108 - val_loss: 2.0079 - val_mean_squared_error: 2.0079\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.1044 - mean_squared_error: 1.1044 - val_loss: 2.6387 - val_mean_squared_error: 2.6387\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3096 - mean_squared_error: 1.3096 - val_loss: 2.1446 - val_mean_squared_error: 2.1446\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1147 - mean_squared_error: 1.1147 - val_loss: 2.0674 - val_mean_squared_error: 2.0674\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0550 - mean_squared_error: 1.0550 - val_loss: 2.0669 - val_mean_squared_error: 2.0669\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.0552 - mean_squared_error: 1.0552 - val_loss: 2.1682 - val_mean_squared_error: 2.1682\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9663 - mean_squared_error: 0.9663 - val_loss: 1.8727 - val_mean_squared_error: 1.8727\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.0017 - mean_squared_error: 1.0017 - val_loss: 2.2134 - val_mean_squared_error: 2.2134\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.0003 - mean_squared_error: 1.0003 - val_loss: 2.2407 - val_mean_squared_error: 2.2407\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.1245 - mean_squared_error: 1.1245 - val_loss: 2.3113 - val_mean_squared_error: 2.3113\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0798 - mean_squared_error: 1.0798 - val_loss: 2.0999 - val_mean_squared_error: 2.0999\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9733 - mean_squared_error: 0.9733 - val_loss: 1.9330 - val_mean_squared_error: 1.9330\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 2.2879 - val_mean_squared_error: 2.2879\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9600 - mean_squared_error: 0.9600 - val_loss: 2.2402 - val_mean_squared_error: 2.2402\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.9941 - mean_squared_error: 0.9941 - val_loss: 2.2128 - val_mean_squared_error: 2.2128\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9862 - mean_squared_error: 0.9862 - val_loss: 2.1124 - val_mean_squared_error: 2.1124\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.2063 - mean_squared_error: 1.2063 - val_loss: 2.0755 - val_mean_squared_error: 2.0755\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9315 - mean_squared_error: 0.9315 - val_loss: 2.0733 - val_mean_squared_error: 2.0733\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9786 - mean_squared_error: 0.9786 - val_loss: 2.2865 - val_mean_squared_error: 2.2865\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.0705 - mean_squared_error: 1.0705 - val_loss: 2.0851 - val_mean_squared_error: 2.0851\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9685 - mean_squared_error: 0.9685 - val_loss: 2.0335 - val_mean_squared_error: 2.0335\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.8827 - mean_squared_error: 0.8827 - val_loss: 1.9128 - val_mean_squared_error: 1.9128\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0260 - mean_squared_error: 1.0260 - val_loss: 2.1086 - val_mean_squared_error: 2.1086\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.8621 - mean_squared_error: 0.8621 - val_loss: 1.9814 - val_mean_squared_error: 1.9814\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.9427 - mean_squared_error: 0.9427 - val_loss: 2.0451 - val_mean_squared_error: 2.0451\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.9964 - mean_squared_error: 0.9964 - val_loss: 2.2534 - val_mean_squared_error: 2.2534\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0407 - mean_squared_error: 1.0407 - val_loss: 2.2777 - val_mean_squared_error: 2.2777\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 2.1738 - val_mean_squared_error: 2.1738\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0265 - mean_squared_error: 1.0265 - val_loss: 2.0932 - val_mean_squared_error: 2.0932\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9607 - mean_squared_error: 0.9607 - val_loss: 2.0359 - val_mean_squared_error: 2.0359\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.9149 - mean_squared_error: 0.9149 - val_loss: 2.0026 - val_mean_squared_error: 2.0026\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9984 - mean_squared_error: 0.9984 - val_loss: 2.1271 - val_mean_squared_error: 2.1271\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.9189 - mean_squared_error: 0.9189 - val_loss: 1.9008 - val_mean_squared_error: 1.9008\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.9301 - mean_squared_error: 0.9301 - val_loss: 1.9584 - val_mean_squared_error: 1.9584\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 0.8751 - mean_squared_error: 0.8751 - val_loss: 2.3009 - val_mean_squared_error: 2.3009\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.0570 - mean_squared_error: 1.0570 - val_loss: 2.0713 - val_mean_squared_error: 2.0713\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9547 - mean_squared_error: 0.9547 - val_loss: 2.0468 - val_mean_squared_error: 2.0468\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8819 - mean_squared_error: 0.8819 - val_loss: 2.0435 - val_mean_squared_error: 2.0435\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 0.9473 - mean_squared_error: 0.9473 - val_loss: 2.1747 - val_mean_squared_error: 2.1747\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8880 - mean_squared_error: 0.8880 - val_loss: 2.0068 - val_mean_squared_error: 2.0068\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0223 - mean_squared_error: 1.0223 - val_loss: 2.4768 - val_mean_squared_error: 2.4768\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.9740 - mean_squared_error: 0.9740 - val_loss: 2.1267 - val_mean_squared_error: 2.1267\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.9069 - mean_squared_error: 0.9069 - val_loss: 2.0401 - val_mean_squared_error: 2.0401\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8396 - mean_squared_error: 0.8396 - val_loss: 2.0219 - val_mean_squared_error: 2.0219\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.8969 - mean_squared_error: 0.8969 - val_loss: 2.1786 - val_mean_squared_error: 2.1786\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0035 - mean_squared_error: 1.0035 - val_loss: 2.1757 - val_mean_squared_error: 2.1757\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 0.8584 - mean_squared_error: 0.8584 - val_loss: 2.1698 - val_mean_squared_error: 2.1698\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.8908 - mean_squared_error: 0.8908 - val_loss: 2.7449 - val_mean_squared_error: 2.7449\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9143 - mean_squared_error: 0.9143 - val_loss: 2.1799 - val_mean_squared_error: 2.1799\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8164 - mean_squared_error: 0.8164 - val_loss: 2.2452 - val_mean_squared_error: 2.2452\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8787 - mean_squared_error: 0.8787 - val_loss: 1.9836 - val_mean_squared_error: 1.9836\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.8571 - mean_squared_error: 0.8571 - val_loss: 2.0335 - val_mean_squared_error: 2.0335\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.0387 - mean_squared_error: 1.0387 - val_loss: 2.0702 - val_mean_squared_error: 2.0702\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.8133 - mean_squared_error: 0.8133 - val_loss: 2.1649 - val_mean_squared_error: 2.1649\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8213 - mean_squared_error: 0.8213 - val_loss: 2.1420 - val_mean_squared_error: 2.1420\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8568 - mean_squared_error: 0.8568 - val_loss: 2.1754 - val_mean_squared_error: 2.1754\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 0.8018 - mean_squared_error: 0.8018 - val_loss: 2.0211 - val_mean_squared_error: 2.0211\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.7538 - mean_squared_error: 0.7538 - val_loss: 2.0559 - val_mean_squared_error: 2.0559\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7710 - mean_squared_error: 0.7710 - val_loss: 2.1141 - val_mean_squared_error: 2.1141\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8615 - mean_squared_error: 0.8615 - val_loss: 2.0452 - val_mean_squared_error: 2.0452\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.8626 - mean_squared_error: 0.8626 - val_loss: 2.0608 - val_mean_squared_error: 2.0608\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8264 - mean_squared_error: 0.8264 - val_loss: 1.8905 - val_mean_squared_error: 1.8905\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7948 - mean_squared_error: 0.7948 - val_loss: 2.0180 - val_mean_squared_error: 2.0180\n",
            "==================================================\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_63 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_105 (Bat (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_63 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_84 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_64 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_106 (Bat (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_64 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_85 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_65 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_107 (Bat (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_65 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_86 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_21 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_108 (Bat (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_87 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_109 (Bat (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_64 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_65 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 435.7556 - mean_squared_error: 435.7555 - val_loss: 674.6690 - val_mean_squared_error: 674.6690\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 439us/sample - loss: 26.5904 - mean_squared_error: 26.5904 - val_loss: 204.3783 - val_mean_squared_error: 204.3783\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 19.1073 - mean_squared_error: 19.1073 - val_loss: 95.7703 - val_mean_squared_error: 95.7703\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 17.9118 - mean_squared_error: 17.9118 - val_loss: 39.8561 - val_mean_squared_error: 39.8561\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 16.8827 - mean_squared_error: 16.8827 - val_loss: 25.7815 - val_mean_squared_error: 25.7815\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 15.5188 - mean_squared_error: 15.5188 - val_loss: 27.9373 - val_mean_squared_error: 27.9373\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 13.3579 - mean_squared_error: 13.3579 - val_loss: 14.9442 - val_mean_squared_error: 14.9442\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 13.5001 - mean_squared_error: 13.5001 - val_loss: 16.9691 - val_mean_squared_error: 16.9691\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 14.3462 - mean_squared_error: 14.3462 - val_loss: 18.7997 - val_mean_squared_error: 18.7997\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 14.6973 - mean_squared_error: 14.6973 - val_loss: 14.3484 - val_mean_squared_error: 14.3484\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 11.8980 - mean_squared_error: 11.8980 - val_loss: 10.3229 - val_mean_squared_error: 10.3229\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 12.0574 - mean_squared_error: 12.0574 - val_loss: 11.7732 - val_mean_squared_error: 11.7732\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 11.3080 - mean_squared_error: 11.3080 - val_loss: 9.8323 - val_mean_squared_error: 9.8323\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 10.5594 - mean_squared_error: 10.5594 - val_loss: 9.7571 - val_mean_squared_error: 9.7571\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 8.8900 - mean_squared_error: 8.8900 - val_loss: 9.7912 - val_mean_squared_error: 9.7912\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 8.6572 - mean_squared_error: 8.6572 - val_loss: 8.6413 - val_mean_squared_error: 8.6413\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 8.6855 - mean_squared_error: 8.6855 - val_loss: 7.9110 - val_mean_squared_error: 7.9110\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 7.4352 - mean_squared_error: 7.4352 - val_loss: 7.9442 - val_mean_squared_error: 7.9442\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 6.8161 - mean_squared_error: 6.8161 - val_loss: 7.6068 - val_mean_squared_error: 7.6068\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 6.8647 - mean_squared_error: 6.8647 - val_loss: 6.4209 - val_mean_squared_error: 6.4209\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 6.3429 - mean_squared_error: 6.3429 - val_loss: 5.8201 - val_mean_squared_error: 5.8201\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 5.2819 - mean_squared_error: 5.2819 - val_loss: 6.1824 - val_mean_squared_error: 6.1824\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 4.9959 - mean_squared_error: 4.9959 - val_loss: 5.0843 - val_mean_squared_error: 5.0843\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 4.9905 - mean_squared_error: 4.9905 - val_loss: 5.2240 - val_mean_squared_error: 5.2240\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 4.8624 - mean_squared_error: 4.8624 - val_loss: 5.5956 - val_mean_squared_error: 5.5956\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 4.4330 - mean_squared_error: 4.4330 - val_loss: 4.6978 - val_mean_squared_error: 4.6978\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 3.9655 - mean_squared_error: 3.9655 - val_loss: 5.0325 - val_mean_squared_error: 5.0325\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 3.6837 - mean_squared_error: 3.6837 - val_loss: 4.4543 - val_mean_squared_error: 4.4543\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 3.3944 - mean_squared_error: 3.3944 - val_loss: 3.8487 - val_mean_squared_error: 3.8487\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 3.3157 - mean_squared_error: 3.3157 - val_loss: 4.3652 - val_mean_squared_error: 4.3652\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 3.4040 - mean_squared_error: 3.4040 - val_loss: 5.1666 - val_mean_squared_error: 5.1666\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 3.2595 - mean_squared_error: 3.2595 - val_loss: 3.7293 - val_mean_squared_error: 3.7293\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.9664 - mean_squared_error: 2.9664 - val_loss: 3.6528 - val_mean_squared_error: 3.6528\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.0709 - mean_squared_error: 3.0709 - val_loss: 3.5480 - val_mean_squared_error: 3.5480\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.7163 - mean_squared_error: 2.7163 - val_loss: 3.7479 - val_mean_squared_error: 3.7479\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.8272 - mean_squared_error: 2.8272 - val_loss: 3.6607 - val_mean_squared_error: 3.6607\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.6091 - mean_squared_error: 2.6091 - val_loss: 3.7936 - val_mean_squared_error: 3.7936\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.6853 - mean_squared_error: 2.6853 - val_loss: 3.4707 - val_mean_squared_error: 3.4707\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.7513 - mean_squared_error: 2.7513 - val_loss: 4.4152 - val_mean_squared_error: 4.4152\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.5796 - mean_squared_error: 2.5796 - val_loss: 3.0548 - val_mean_squared_error: 3.0548\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.4735 - mean_squared_error: 2.4735 - val_loss: 3.4478 - val_mean_squared_error: 3.4478\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.5249 - mean_squared_error: 2.5249 - val_loss: 3.4580 - val_mean_squared_error: 3.4580\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.4469 - mean_squared_error: 2.4469 - val_loss: 3.6063 - val_mean_squared_error: 3.6063\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.2310 - mean_squared_error: 2.2310 - val_loss: 4.4772 - val_mean_squared_error: 4.4772\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.1487 - mean_squared_error: 2.1487 - val_loss: 3.4000 - val_mean_squared_error: 3.4000\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.1280 - mean_squared_error: 2.1280 - val_loss: 3.1678 - val_mean_squared_error: 3.1678\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.1403 - mean_squared_error: 2.1403 - val_loss: 3.3705 - val_mean_squared_error: 3.3705\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.0572 - mean_squared_error: 2.0572 - val_loss: 2.9468 - val_mean_squared_error: 2.9468\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.0762 - mean_squared_error: 2.0762 - val_loss: 3.7105 - val_mean_squared_error: 3.7105\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.0138 - mean_squared_error: 2.0138 - val_loss: 3.1421 - val_mean_squared_error: 3.1421\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 2.0929 - mean_squared_error: 2.0929 - val_loss: 3.7048 - val_mean_squared_error: 3.7048\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.0072 - mean_squared_error: 2.0072 - val_loss: 3.2157 - val_mean_squared_error: 3.2157\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.0184 - mean_squared_error: 2.0184 - val_loss: 3.3607 - val_mean_squared_error: 3.3607\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.7560 - mean_squared_error: 1.7560 - val_loss: 4.1636 - val_mean_squared_error: 4.1636\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 2.0833 - mean_squared_error: 2.0833 - val_loss: 3.2000 - val_mean_squared_error: 3.2000\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.8807 - mean_squared_error: 1.8807 - val_loss: 2.6727 - val_mean_squared_error: 2.6727\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.8125 - mean_squared_error: 1.8125 - val_loss: 2.7581 - val_mean_squared_error: 2.7581\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.8328 - mean_squared_error: 1.8328 - val_loss: 2.9527 - val_mean_squared_error: 2.9527\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.8211 - mean_squared_error: 1.8211 - val_loss: 2.8116 - val_mean_squared_error: 2.8116\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.7198 - mean_squared_error: 1.7198 - val_loss: 2.9306 - val_mean_squared_error: 2.9306\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.6428 - mean_squared_error: 1.6428 - val_loss: 2.9298 - val_mean_squared_error: 2.9298\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.5921 - mean_squared_error: 1.5921 - val_loss: 2.8755 - val_mean_squared_error: 2.8755\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.7720 - mean_squared_error: 1.7720 - val_loss: 2.8054 - val_mean_squared_error: 2.8054\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.8431 - mean_squared_error: 1.8431 - val_loss: 2.6573 - val_mean_squared_error: 2.6573\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.7207 - mean_squared_error: 1.7207 - val_loss: 2.8840 - val_mean_squared_error: 2.8840\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.6049 - mean_squared_error: 1.6049 - val_loss: 2.4149 - val_mean_squared_error: 2.4149\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.6197 - mean_squared_error: 1.6197 - val_loss: 3.1083 - val_mean_squared_error: 3.1083\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.6858 - mean_squared_error: 1.6858 - val_loss: 2.8697 - val_mean_squared_error: 2.8697\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.4687 - mean_squared_error: 1.4687 - val_loss: 2.8688 - val_mean_squared_error: 2.8688\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.5913 - mean_squared_error: 1.5913 - val_loss: 2.9017 - val_mean_squared_error: 2.9017\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7102 - mean_squared_error: 1.7102 - val_loss: 2.5575 - val_mean_squared_error: 2.5575\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.5142 - mean_squared_error: 1.5142 - val_loss: 2.9989 - val_mean_squared_error: 2.9989\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.5247 - mean_squared_error: 1.5247 - val_loss: 2.4198 - val_mean_squared_error: 2.4198\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.3199 - mean_squared_error: 1.3199 - val_loss: 2.6846 - val_mean_squared_error: 2.6846\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.4017 - mean_squared_error: 1.4017 - val_loss: 2.8106 - val_mean_squared_error: 2.8106\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.4254 - mean_squared_error: 1.4254 - val_loss: 2.6860 - val_mean_squared_error: 2.6860\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.4742 - mean_squared_error: 1.4742 - val_loss: 2.6523 - val_mean_squared_error: 2.6523\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.3484 - mean_squared_error: 1.3484 - val_loss: 2.6842 - val_mean_squared_error: 2.6842\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4781 - mean_squared_error: 1.4781 - val_loss: 3.5319 - val_mean_squared_error: 3.5319\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.5322 - mean_squared_error: 1.5322 - val_loss: 2.3910 - val_mean_squared_error: 2.3910\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.4503 - mean_squared_error: 1.4503 - val_loss: 3.0645 - val_mean_squared_error: 3.0645\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.4264 - mean_squared_error: 1.4264 - val_loss: 3.0044 - val_mean_squared_error: 3.0044\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.3597 - mean_squared_error: 1.3597 - val_loss: 2.4515 - val_mean_squared_error: 2.4515\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.3376 - mean_squared_error: 1.3376 - val_loss: 3.3169 - val_mean_squared_error: 3.3169\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3922 - mean_squared_error: 1.3922 - val_loss: 2.7167 - val_mean_squared_error: 2.7167\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.2781 - mean_squared_error: 1.2781 - val_loss: 2.6139 - val_mean_squared_error: 2.6139\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.3505 - mean_squared_error: 1.3505 - val_loss: 2.5966 - val_mean_squared_error: 2.5966\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.3639 - mean_squared_error: 1.3639 - val_loss: 2.6802 - val_mean_squared_error: 2.6802\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.4347 - mean_squared_error: 1.4347 - val_loss: 2.6315 - val_mean_squared_error: 2.6315\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.3544 - mean_squared_error: 1.3544 - val_loss: 2.4947 - val_mean_squared_error: 2.4947\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.3093 - mean_squared_error: 1.3093 - val_loss: 2.8110 - val_mean_squared_error: 2.8110\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.4915 - mean_squared_error: 1.4915 - val_loss: 2.4658 - val_mean_squared_error: 2.4658\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.4327 - mean_squared_error: 1.4327 - val_loss: 2.3154 - val_mean_squared_error: 2.3154\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.2560 - mean_squared_error: 1.2560 - val_loss: 2.7920 - val_mean_squared_error: 2.7920\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1190 - mean_squared_error: 1.1190 - val_loss: 2.3222 - val_mean_squared_error: 2.3222\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.2032 - mean_squared_error: 1.2032 - val_loss: 2.6625 - val_mean_squared_error: 2.6625\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.2208 - mean_squared_error: 1.2208 - val_loss: 2.3966 - val_mean_squared_error: 2.3966\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.2222 - mean_squared_error: 1.2222 - val_loss: 2.3331 - val_mean_squared_error: 2.3331\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0859 - mean_squared_error: 1.0859 - val_loss: 2.5125 - val_mean_squared_error: 2.5125\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1633 - mean_squared_error: 1.1633 - val_loss: 2.5684 - val_mean_squared_error: 2.5684\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.2334 - mean_squared_error: 1.2334 - val_loss: 2.2422 - val_mean_squared_error: 2.2422\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.1346 - mean_squared_error: 1.1346 - val_loss: 2.4419 - val_mean_squared_error: 2.4419\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1633 - mean_squared_error: 1.1633 - val_loss: 2.1887 - val_mean_squared_error: 2.1887\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.1656 - mean_squared_error: 1.1656 - val_loss: 2.5879 - val_mean_squared_error: 2.5879\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.0957 - mean_squared_error: 1.0957 - val_loss: 3.5082 - val_mean_squared_error: 3.5082\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.2690 - mean_squared_error: 1.2690 - val_loss: 2.3086 - val_mean_squared_error: 2.3086\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1406 - mean_squared_error: 1.1406 - val_loss: 2.3962 - val_mean_squared_error: 2.3962\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.1084 - mean_squared_error: 1.1084 - val_loss: 2.3895 - val_mean_squared_error: 2.3895\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.1511 - mean_squared_error: 1.1511 - val_loss: 2.2177 - val_mean_squared_error: 2.2177\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.0717 - mean_squared_error: 1.0717 - val_loss: 2.7471 - val_mean_squared_error: 2.7471\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.1091 - mean_squared_error: 1.1091 - val_loss: 2.3507 - val_mean_squared_error: 2.3507\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 0.9784 - mean_squared_error: 0.9784 - val_loss: 2.2903 - val_mean_squared_error: 2.2903\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.0808 - mean_squared_error: 1.0808 - val_loss: 2.6100 - val_mean_squared_error: 2.6100\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0637 - mean_squared_error: 1.0637 - val_loss: 2.6113 - val_mean_squared_error: 2.6113\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.0867 - mean_squared_error: 1.0867 - val_loss: 2.4107 - val_mean_squared_error: 2.4107\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 2.0899 - val_mean_squared_error: 2.0899\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 0.9673 - mean_squared_error: 0.9673 - val_loss: 2.5644 - val_mean_squared_error: 2.5644\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.1671 - mean_squared_error: 1.1671 - val_loss: 2.4289 - val_mean_squared_error: 2.4289\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 0.9959 - mean_squared_error: 0.9959 - val_loss: 2.1977 - val_mean_squared_error: 2.1977\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0470 - mean_squared_error: 1.0470 - val_loss: 2.4880 - val_mean_squared_error: 2.4880\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0732 - mean_squared_error: 1.0732 - val_loss: 2.9755 - val_mean_squared_error: 2.9755\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.2139 - mean_squared_error: 1.2139 - val_loss: 3.3457 - val_mean_squared_error: 3.3457\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 2.0993 - val_mean_squared_error: 2.0993\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.9132 - mean_squared_error: 0.9132 - val_loss: 2.1583 - val_mean_squared_error: 2.1583\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0383 - mean_squared_error: 1.0383 - val_loss: 2.1874 - val_mean_squared_error: 2.1874\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 0.9994 - mean_squared_error: 0.9994 - val_loss: 2.1375 - val_mean_squared_error: 2.1375\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.1730 - mean_squared_error: 1.1730 - val_loss: 2.4620 - val_mean_squared_error: 2.4620\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.9482 - mean_squared_error: 0.9482 - val_loss: 2.3542 - val_mean_squared_error: 2.3542\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.8507 - mean_squared_error: 0.8507 - val_loss: 2.0933 - val_mean_squared_error: 2.0933\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.9484 - mean_squared_error: 0.9484 - val_loss: 2.3103 - val_mean_squared_error: 2.3103\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 0.9250 - mean_squared_error: 0.9250 - val_loss: 2.3162 - val_mean_squared_error: 2.3162\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 2.2414 - val_mean_squared_error: 2.2414\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 0.9800 - mean_squared_error: 0.9800 - val_loss: 2.5735 - val_mean_squared_error: 2.5735\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.9981 - mean_squared_error: 0.9981 - val_loss: 2.3759 - val_mean_squared_error: 2.3759\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 0.9147 - mean_squared_error: 0.9147 - val_loss: 2.2888 - val_mean_squared_error: 2.2888\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 0.8323 - mean_squared_error: 0.8323 - val_loss: 2.4423 - val_mean_squared_error: 2.4423\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.8993 - mean_squared_error: 0.8993 - val_loss: 2.4489 - val_mean_squared_error: 2.4489\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9619 - mean_squared_error: 0.9619 - val_loss: 2.6546 - val_mean_squared_error: 2.6546\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.0290 - mean_squared_error: 1.0290 - val_loss: 2.2706 - val_mean_squared_error: 2.2706\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.8992 - mean_squared_error: 0.8992 - val_loss: 2.3039 - val_mean_squared_error: 2.3039\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.8661 - mean_squared_error: 0.8661 - val_loss: 2.1321 - val_mean_squared_error: 2.1321\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.9729 - mean_squared_error: 0.9729 - val_loss: 2.2907 - val_mean_squared_error: 2.2907\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 0.9910 - mean_squared_error: 0.9910 - val_loss: 2.4985 - val_mean_squared_error: 2.4985\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 2.5932 - val_mean_squared_error: 2.5932\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 0.8574 - mean_squared_error: 0.8574 - val_loss: 2.5828 - val_mean_squared_error: 2.5828\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 0.9428 - mean_squared_error: 0.9428 - val_loss: 2.6775 - val_mean_squared_error: 2.6775\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.9432 - mean_squared_error: 0.9432 - val_loss: 2.5991 - val_mean_squared_error: 2.5991\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9177 - mean_squared_error: 0.9177 - val_loss: 2.7753 - val_mean_squared_error: 2.7753\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.8323 - mean_squared_error: 0.8323 - val_loss: 2.5028 - val_mean_squared_error: 2.5028\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 0.8873 - mean_squared_error: 0.8873 - val_loss: 2.4787 - val_mean_squared_error: 2.4787\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.7827 - mean_squared_error: 0.7827 - val_loss: 2.2422 - val_mean_squared_error: 2.2422\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 0.8333 - mean_squared_error: 0.8333 - val_loss: 2.2000 - val_mean_squared_error: 2.2000\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 0.8409 - mean_squared_error: 0.8409 - val_loss: 2.5370 - val_mean_squared_error: 2.5370\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 0.7965 - mean_squared_error: 0.7965 - val_loss: 2.4531 - val_mean_squared_error: 2.4531\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.7409 - mean_squared_error: 0.7409 - val_loss: 2.2329 - val_mean_squared_error: 2.2329\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.7188 - mean_squared_error: 0.7188 - val_loss: 2.0519 - val_mean_squared_error: 2.0519\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 0.7904 - mean_squared_error: 0.7904 - val_loss: 2.1181 - val_mean_squared_error: 2.1181\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.9303 - mean_squared_error: 0.9303 - val_loss: 2.2279 - val_mean_squared_error: 2.2279\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.7770 - mean_squared_error: 0.7770 - val_loss: 2.0755 - val_mean_squared_error: 2.0755\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.7959 - mean_squared_error: 0.7959 - val_loss: 2.3336 - val_mean_squared_error: 2.3336\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 0.8705 - mean_squared_error: 0.8705 - val_loss: 2.2232 - val_mean_squared_error: 2.2232\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.7791 - mean_squared_error: 0.7791 - val_loss: 2.2116 - val_mean_squared_error: 2.2116\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.8022 - mean_squared_error: 0.8022 - val_loss: 2.1405 - val_mean_squared_error: 2.1405\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.7834 - mean_squared_error: 0.7834 - val_loss: 2.3223 - val_mean_squared_error: 2.3223\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9678 - mean_squared_error: 0.9678 - val_loss: 2.4984 - val_mean_squared_error: 2.4984\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.8494 - mean_squared_error: 0.8494 - val_loss: 2.2902 - val_mean_squared_error: 2.2902\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 0.9043 - mean_squared_error: 0.9043 - val_loss: 2.4261 - val_mean_squared_error: 2.4261\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 0.9483 - mean_squared_error: 0.9483 - val_loss: 2.2800 - val_mean_squared_error: 2.2800\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 0.8368 - mean_squared_error: 0.8368 - val_loss: 2.0677 - val_mean_squared_error: 2.0677\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.8337 - mean_squared_error: 0.8337 - val_loss: 2.4368 - val_mean_squared_error: 2.4368\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.6943 - mean_squared_error: 0.6943 - val_loss: 2.1778 - val_mean_squared_error: 2.1778\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.7139 - mean_squared_error: 0.7139 - val_loss: 2.1064 - val_mean_squared_error: 2.1064\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.6949 - mean_squared_error: 0.6949 - val_loss: 2.3017 - val_mean_squared_error: 2.3017\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.7788 - mean_squared_error: 0.7788 - val_loss: 2.1838 - val_mean_squared_error: 2.1838\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.7578 - mean_squared_error: 0.7578 - val_loss: 2.0792 - val_mean_squared_error: 2.0792\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9362 - mean_squared_error: 0.9362 - val_loss: 2.1004 - val_mean_squared_error: 2.1004\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 0.6999 - mean_squared_error: 0.6999 - val_loss: 2.2307 - val_mean_squared_error: 2.2307\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.8478 - mean_squared_error: 0.8478 - val_loss: 2.3371 - val_mean_squared_error: 2.3371\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 0.8501 - mean_squared_error: 0.8501 - val_loss: 2.6596 - val_mean_squared_error: 2.6596\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.7791 - mean_squared_error: 0.7791 - val_loss: 2.2654 - val_mean_squared_error: 2.2654\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.8083 - mean_squared_error: 0.8083 - val_loss: 2.4180 - val_mean_squared_error: 2.4180\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.6768 - mean_squared_error: 0.6768 - val_loss: 2.1567 - val_mean_squared_error: 2.1567\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 0.6465 - mean_squared_error: 0.6465 - val_loss: 2.0315 - val_mean_squared_error: 2.0315\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 0.7483 - mean_squared_error: 0.7483 - val_loss: 2.3864 - val_mean_squared_error: 2.3864\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.7393 - mean_squared_error: 0.7393 - val_loss: 1.9850 - val_mean_squared_error: 1.9850\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.7174 - mean_squared_error: 0.7174 - val_loss: 2.4405 - val_mean_squared_error: 2.4405\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.7497 - mean_squared_error: 0.7497 - val_loss: 2.1534 - val_mean_squared_error: 2.1534\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.7603 - mean_squared_error: 0.7603 - val_loss: 2.0284 - val_mean_squared_error: 2.0284\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.6232 - mean_squared_error: 0.6232 - val_loss: 2.3256 - val_mean_squared_error: 2.3256\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 0.6614 - mean_squared_error: 0.6614 - val_loss: 2.0648 - val_mean_squared_error: 2.0648\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.7596 - mean_squared_error: 0.7596 - val_loss: 2.2070 - val_mean_squared_error: 2.2070\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.8482 - mean_squared_error: 0.8482 - val_loss: 2.2242 - val_mean_squared_error: 2.2242\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.7857 - mean_squared_error: 0.7857 - val_loss: 2.4788 - val_mean_squared_error: 2.4788\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.7754 - mean_squared_error: 0.7754 - val_loss: 2.1644 - val_mean_squared_error: 2.1644\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 0.7190 - mean_squared_error: 0.7190 - val_loss: 2.0403 - val_mean_squared_error: 2.0403\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.7210 - mean_squared_error: 0.7210 - val_loss: 2.1472 - val_mean_squared_error: 2.1472\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.6601 - mean_squared_error: 0.6601 - val_loss: 2.1760 - val_mean_squared_error: 2.1760\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 0.7613 - mean_squared_error: 0.7613 - val_loss: 2.0458 - val_mean_squared_error: 2.0458\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.6954 - mean_squared_error: 0.6954 - val_loss: 2.2517 - val_mean_squared_error: 2.2517\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.8059 - mean_squared_error: 0.8059 - val_loss: 2.5002 - val_mean_squared_error: 2.5002\n",
            "==================================================\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_66 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_110 (Bat (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_66 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_88 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_67 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_111 (Bat (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_67 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_89 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_68 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_112 (Bat (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_68 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_90 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_22 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_113 (Bat (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_91 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_66 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_114 (Bat (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_67 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_68 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 4s 2ms/sample - loss: 437.5176 - mean_squared_error: 437.5175 - val_loss: 3749.0243 - val_mean_squared_error: 3749.0244\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 491us/sample - loss: 19.9134 - mean_squared_error: 19.9134 - val_loss: 82.0867 - val_mean_squared_error: 82.0867\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 17.1657 - mean_squared_error: 17.1657 - val_loss: 89.1242 - val_mean_squared_error: 89.1242\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 16.3735 - mean_squared_error: 16.3735 - val_loss: 69.6415 - val_mean_squared_error: 69.6415\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 14.9973 - mean_squared_error: 14.9973 - val_loss: 35.8690 - val_mean_squared_error: 35.8690\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 14.5025 - mean_squared_error: 14.5025 - val_loss: 18.9886 - val_mean_squared_error: 18.9886\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 11.8687 - mean_squared_error: 11.8687 - val_loss: 14.2482 - val_mean_squared_error: 14.2482\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 12.5166 - mean_squared_error: 12.5166 - val_loss: 14.4081 - val_mean_squared_error: 14.4081\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 11.8452 - mean_squared_error: 11.8452 - val_loss: 12.5020 - val_mean_squared_error: 12.5020\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 10.7142 - mean_squared_error: 10.7142 - val_loss: 9.7094 - val_mean_squared_error: 9.7094\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 10.5313 - mean_squared_error: 10.5313 - val_loss: 12.5790 - val_mean_squared_error: 12.5790\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 10.6849 - mean_squared_error: 10.6849 - val_loss: 13.1979 - val_mean_squared_error: 13.1979\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 10.8407 - mean_squared_error: 10.8407 - val_loss: 9.0066 - val_mean_squared_error: 9.0066\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 9.4029 - mean_squared_error: 9.4029 - val_loss: 10.6420 - val_mean_squared_error: 10.6420\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 8.4578 - mean_squared_error: 8.4578 - val_loss: 8.6541 - val_mean_squared_error: 8.6541\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 8.1547 - mean_squared_error: 8.1547 - val_loss: 7.7704 - val_mean_squared_error: 7.7704\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 7.7339 - mean_squared_error: 7.7339 - val_loss: 9.0822 - val_mean_squared_error: 9.0822\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 6.9422 - mean_squared_error: 6.9422 - val_loss: 6.3010 - val_mean_squared_error: 6.3010\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 7.1553 - mean_squared_error: 7.1553 - val_loss: 7.0474 - val_mean_squared_error: 7.0474\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 6.2846 - mean_squared_error: 6.2846 - val_loss: 6.1113 - val_mean_squared_error: 6.1113\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 5.8167 - mean_squared_error: 5.8167 - val_loss: 5.7456 - val_mean_squared_error: 5.7456\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 6.1267 - mean_squared_error: 6.1267 - val_loss: 7.1977 - val_mean_squared_error: 7.1977\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 5.2921 - mean_squared_error: 5.2921 - val_loss: 5.6627 - val_mean_squared_error: 5.6627\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 5.1150 - mean_squared_error: 5.1150 - val_loss: 5.1804 - val_mean_squared_error: 5.1804\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 4.6331 - mean_squared_error: 4.6331 - val_loss: 5.6799 - val_mean_squared_error: 5.6799\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 4.5856 - mean_squared_error: 4.5856 - val_loss: 4.3600 - val_mean_squared_error: 4.3600\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 4.2585 - mean_squared_error: 4.2585 - val_loss: 4.3248 - val_mean_squared_error: 4.3248\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 4.1508 - mean_squared_error: 4.1508 - val_loss: 4.3417 - val_mean_squared_error: 4.3417\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 3.9610 - mean_squared_error: 3.9610 - val_loss: 4.1994 - val_mean_squared_error: 4.1994\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 4.1334 - mean_squared_error: 4.1334 - val_loss: 4.4825 - val_mean_squared_error: 4.4825\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 3.5403 - mean_squared_error: 3.5403 - val_loss: 3.9024 - val_mean_squared_error: 3.9024\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 3.3467 - mean_squared_error: 3.3467 - val_loss: 4.6181 - val_mean_squared_error: 4.6181\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 3.4031 - mean_squared_error: 3.4031 - val_loss: 4.1581 - val_mean_squared_error: 4.1581\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 3.4111 - mean_squared_error: 3.4111 - val_loss: 3.6121 - val_mean_squared_error: 3.6121\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 3.1426 - mean_squared_error: 3.1426 - val_loss: 3.7487 - val_mean_squared_error: 3.7487\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 3.2908 - mean_squared_error: 3.2908 - val_loss: 3.8874 - val_mean_squared_error: 3.8874\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 3.2430 - mean_squared_error: 3.2430 - val_loss: 3.6498 - val_mean_squared_error: 3.6498\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 3.0179 - mean_squared_error: 3.0179 - val_loss: 3.5699 - val_mean_squared_error: 3.5699\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 2.9155 - mean_squared_error: 2.9155 - val_loss: 3.4397 - val_mean_squared_error: 3.4397\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 3.0342 - mean_squared_error: 3.0342 - val_loss: 3.3243 - val_mean_squared_error: 3.3243\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 2.6486 - mean_squared_error: 2.6486 - val_loss: 3.5206 - val_mean_squared_error: 3.5206\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 3.0353 - mean_squared_error: 3.0353 - val_loss: 4.0437 - val_mean_squared_error: 4.0437\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 2.6449 - mean_squared_error: 2.6449 - val_loss: 3.3960 - val_mean_squared_error: 3.3960\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 2.8796 - mean_squared_error: 2.8796 - val_loss: 3.4062 - val_mean_squared_error: 3.4062\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 2.6043 - mean_squared_error: 2.6043 - val_loss: 4.7661 - val_mean_squared_error: 4.7661\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 426us/sample - loss: 2.5709 - mean_squared_error: 2.5709 - val_loss: 4.0813 - val_mean_squared_error: 4.0813\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 2.4381 - mean_squared_error: 2.4381 - val_loss: 3.5271 - val_mean_squared_error: 3.5271\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 2.3359 - mean_squared_error: 2.3359 - val_loss: 2.8184 - val_mean_squared_error: 2.8184\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 2.4735 - mean_squared_error: 2.4735 - val_loss: 2.7784 - val_mean_squared_error: 2.7784\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 2.2612 - mean_squared_error: 2.2612 - val_loss: 3.6450 - val_mean_squared_error: 3.6450\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 2.3348 - mean_squared_error: 2.3348 - val_loss: 2.8833 - val_mean_squared_error: 2.8833\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 2.1075 - mean_squared_error: 2.1075 - val_loss: 3.1200 - val_mean_squared_error: 3.1200\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 2.3357 - mean_squared_error: 2.3357 - val_loss: 3.2217 - val_mean_squared_error: 3.2217\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 432us/sample - loss: 2.1326 - mean_squared_error: 2.1326 - val_loss: 3.0175 - val_mean_squared_error: 3.0175\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 426us/sample - loss: 2.0958 - mean_squared_error: 2.0958 - val_loss: 3.2534 - val_mean_squared_error: 3.2534\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 2.1727 - mean_squared_error: 2.1727 - val_loss: 3.0403 - val_mean_squared_error: 3.0403\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.1171 - mean_squared_error: 2.1171 - val_loss: 2.8516 - val_mean_squared_error: 2.8516\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 2.1235 - mean_squared_error: 2.1235 - val_loss: 3.2705 - val_mean_squared_error: 3.2705\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 2.0348 - mean_squared_error: 2.0348 - val_loss: 2.9533 - val_mean_squared_error: 2.9533\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.9282 - mean_squared_error: 1.9282 - val_loss: 2.9460 - val_mean_squared_error: 2.9460\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 2.1196 - mean_squared_error: 2.1196 - val_loss: 3.0348 - val_mean_squared_error: 3.0348\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.8834 - mean_squared_error: 1.8834 - val_loss: 2.8115 - val_mean_squared_error: 2.8115\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.8942 - mean_squared_error: 1.8942 - val_loss: 3.0035 - val_mean_squared_error: 3.0035\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.8040 - mean_squared_error: 1.8040 - val_loss: 2.9067 - val_mean_squared_error: 2.9067\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.8541 - mean_squared_error: 1.8541 - val_loss: 2.5103 - val_mean_squared_error: 2.5103\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.6635 - mean_squared_error: 1.6635 - val_loss: 2.8426 - val_mean_squared_error: 2.8426\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.7573 - mean_squared_error: 1.7573 - val_loss: 2.4006 - val_mean_squared_error: 2.4006\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.7437 - mean_squared_error: 1.7437 - val_loss: 2.7636 - val_mean_squared_error: 2.7636\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.7369 - mean_squared_error: 1.7369 - val_loss: 2.4621 - val_mean_squared_error: 2.4621\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.8362 - mean_squared_error: 1.8362 - val_loss: 2.9583 - val_mean_squared_error: 2.9583\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 427us/sample - loss: 1.7807 - mean_squared_error: 1.7807 - val_loss: 2.8566 - val_mean_squared_error: 2.8566\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.5186 - mean_squared_error: 1.5186 - val_loss: 2.5347 - val_mean_squared_error: 2.5347\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.7171 - mean_squared_error: 1.7171 - val_loss: 3.1934 - val_mean_squared_error: 3.1934\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.8133 - mean_squared_error: 1.8133 - val_loss: 2.7679 - val_mean_squared_error: 2.7679\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.6645 - mean_squared_error: 1.6645 - val_loss: 2.6690 - val_mean_squared_error: 2.6690\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.6718 - mean_squared_error: 1.6718 - val_loss: 2.8350 - val_mean_squared_error: 2.8350\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.6227 - mean_squared_error: 1.6227 - val_loss: 2.9620 - val_mean_squared_error: 2.9620\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.6037 - mean_squared_error: 1.6037 - val_loss: 2.8314 - val_mean_squared_error: 2.8314\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.5530 - mean_squared_error: 1.5530 - val_loss: 2.3550 - val_mean_squared_error: 2.3550\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.5271 - mean_squared_error: 1.5271 - val_loss: 2.5968 - val_mean_squared_error: 2.5968\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.5294 - mean_squared_error: 1.5294 - val_loss: 2.9645 - val_mean_squared_error: 2.9645\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.5631 - mean_squared_error: 1.5631 - val_loss: 2.4934 - val_mean_squared_error: 2.4934\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.4769 - mean_squared_error: 1.4769 - val_loss: 2.3847 - val_mean_squared_error: 2.3847\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.5068 - mean_squared_error: 1.5068 - val_loss: 2.4044 - val_mean_squared_error: 2.4044\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.5322 - mean_squared_error: 1.5322 - val_loss: 2.5844 - val_mean_squared_error: 2.5844\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.4226 - mean_squared_error: 1.4226 - val_loss: 2.4944 - val_mean_squared_error: 2.4944\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.5916 - mean_squared_error: 1.5916 - val_loss: 2.4259 - val_mean_squared_error: 2.4259\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.5568 - mean_squared_error: 1.5568 - val_loss: 2.6723 - val_mean_squared_error: 2.6723\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 433us/sample - loss: 1.4771 - mean_squared_error: 1.4771 - val_loss: 2.2924 - val_mean_squared_error: 2.2924\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.3228 - mean_squared_error: 1.3228 - val_loss: 2.7723 - val_mean_squared_error: 2.7723\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.3852 - mean_squared_error: 1.3852 - val_loss: 2.6242 - val_mean_squared_error: 2.6242\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.4892 - mean_squared_error: 1.4892 - val_loss: 2.2853 - val_mean_squared_error: 2.2853\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.5114 - mean_squared_error: 1.5114 - val_loss: 2.7461 - val_mean_squared_error: 2.7461\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.2731 - mean_squared_error: 1.2731 - val_loss: 2.3242 - val_mean_squared_error: 2.3242\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.2847 - mean_squared_error: 1.2847 - val_loss: 3.1307 - val_mean_squared_error: 3.1307\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.4692 - mean_squared_error: 1.4692 - val_loss: 2.4418 - val_mean_squared_error: 2.4418\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.3869 - mean_squared_error: 1.3869 - val_loss: 2.6604 - val_mean_squared_error: 2.6604\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.2960 - mean_squared_error: 1.2960 - val_loss: 2.2149 - val_mean_squared_error: 2.2149\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.3992 - mean_squared_error: 1.3992 - val_loss: 2.3365 - val_mean_squared_error: 2.3365\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.4745 - mean_squared_error: 1.4745 - val_loss: 2.3143 - val_mean_squared_error: 2.3143\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.4461 - mean_squared_error: 1.4461 - val_loss: 2.8584 - val_mean_squared_error: 2.8584\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 426us/sample - loss: 1.3747 - mean_squared_error: 1.3747 - val_loss: 2.1127 - val_mean_squared_error: 2.1127\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.1474 - mean_squared_error: 1.1474 - val_loss: 2.3362 - val_mean_squared_error: 2.3362\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.5437 - mean_squared_error: 1.5437 - val_loss: 2.4539 - val_mean_squared_error: 2.4539\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.2308 - mean_squared_error: 1.2308 - val_loss: 2.6070 - val_mean_squared_error: 2.6070\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.3101 - mean_squared_error: 1.3101 - val_loss: 2.2182 - val_mean_squared_error: 2.2182\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 1.2008 - mean_squared_error: 1.2008 - val_loss: 2.0061 - val_mean_squared_error: 2.0061\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.1399 - mean_squared_error: 1.1399 - val_loss: 2.3828 - val_mean_squared_error: 2.3828\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.2794 - mean_squared_error: 1.2794 - val_loss: 2.9447 - val_mean_squared_error: 2.9447\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.1716 - mean_squared_error: 1.1716 - val_loss: 2.7478 - val_mean_squared_error: 2.7478\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 432us/sample - loss: 1.3308 - mean_squared_error: 1.3308 - val_loss: 2.4110 - val_mean_squared_error: 2.4110\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.0872 - mean_squared_error: 1.0872 - val_loss: 2.2959 - val_mean_squared_error: 2.2959\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.3168 - mean_squared_error: 1.3168 - val_loss: 2.3126 - val_mean_squared_error: 2.3126\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 1.1592 - mean_squared_error: 1.1592 - val_loss: 2.1980 - val_mean_squared_error: 2.1980\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.3326 - mean_squared_error: 1.3326 - val_loss: 2.1960 - val_mean_squared_error: 2.1960\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.1945 - mean_squared_error: 1.1945 - val_loss: 2.3429 - val_mean_squared_error: 2.3429\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.0473 - mean_squared_error: 1.0473 - val_loss: 2.2856 - val_mean_squared_error: 2.2856\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.0780 - mean_squared_error: 1.0780 - val_loss: 2.1208 - val_mean_squared_error: 2.1208\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.0727 - mean_squared_error: 1.0727 - val_loss: 2.1256 - val_mean_squared_error: 2.1256\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.0246 - mean_squared_error: 1.0246 - val_loss: 2.0856 - val_mean_squared_error: 2.0856\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.2010 - mean_squared_error: 1.2010 - val_loss: 2.0179 - val_mean_squared_error: 2.0179\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.0854 - mean_squared_error: 1.0854 - val_loss: 2.2039 - val_mean_squared_error: 2.2039\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.0786 - mean_squared_error: 1.0786 - val_loss: 2.2607 - val_mean_squared_error: 2.2607\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 2.3839 - val_mean_squared_error: 2.3839\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.0841 - mean_squared_error: 1.0841 - val_loss: 2.3506 - val_mean_squared_error: 2.3506\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.1119 - mean_squared_error: 1.1119 - val_loss: 2.1834 - val_mean_squared_error: 2.1834\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 0.9728 - mean_squared_error: 0.9728 - val_loss: 2.1246 - val_mean_squared_error: 2.1246\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.2406 - mean_squared_error: 1.2406 - val_loss: 2.2786 - val_mean_squared_error: 2.2786\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.1242 - mean_squared_error: 1.1242 - val_loss: 2.2454 - val_mean_squared_error: 2.2454\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.1333 - mean_squared_error: 1.1333 - val_loss: 2.7552 - val_mean_squared_error: 2.7552\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.0687 - mean_squared_error: 1.0687 - val_loss: 2.2505 - val_mean_squared_error: 2.2505\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.9922 - mean_squared_error: 0.9922 - val_loss: 2.1218 - val_mean_squared_error: 2.1218\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 0.9910 - mean_squared_error: 0.9910 - val_loss: 2.1222 - val_mean_squared_error: 2.1222\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 0.9855 - mean_squared_error: 0.9855 - val_loss: 2.3845 - val_mean_squared_error: 2.3845\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.0266 - mean_squared_error: 1.0266 - val_loss: 2.1853 - val_mean_squared_error: 2.1853\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.0925 - mean_squared_error: 1.0925 - val_loss: 2.2356 - val_mean_squared_error: 2.2356\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 430us/sample - loss: 1.1664 - mean_squared_error: 1.1664 - val_loss: 2.1801 - val_mean_squared_error: 2.1801\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.0350 - mean_squared_error: 1.0350 - val_loss: 2.3915 - val_mean_squared_error: 2.3915\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.0791 - mean_squared_error: 1.0791 - val_loss: 2.1319 - val_mean_squared_error: 2.1319\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.1720 - mean_squared_error: 1.1720 - val_loss: 2.3416 - val_mean_squared_error: 2.3416\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.0906 - mean_squared_error: 1.0906 - val_loss: 2.2780 - val_mean_squared_error: 2.2780\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.0373 - mean_squared_error: 1.0373 - val_loss: 2.2494 - val_mean_squared_error: 2.2494\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 428us/sample - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 2.0082 - val_mean_squared_error: 2.0082\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.9717 - mean_squared_error: 0.9717 - val_loss: 2.4839 - val_mean_squared_error: 2.4839\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 0.9399 - mean_squared_error: 0.9399 - val_loss: 2.3310 - val_mean_squared_error: 2.3310\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.0037 - mean_squared_error: 1.0037 - val_loss: 2.3127 - val_mean_squared_error: 2.3127\n",
            "Epoch 147/200\n",
            " 608/1819 [=========>....................] - ETA: 0s - loss: 1.1153 - mean_squared_error: 1.1153Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOwRMEPveUor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_vgg_model(start_filter, d, step, bias):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model \n",
        "    '''\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "    \n",
        "    # Input layer is our grayscale image that is 96 pixels by 96 pixels\n",
        "    cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    \n",
        "    # Add our first convolution layers which is two back-to-back conv with 3x3 kernel and same padding\n",
        "    # Add depth with filters\n",
        "    # Our output from these convolutions will be (96,96,start_filter)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (48,48,32)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add some random dropout\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    \n",
        "    # Add our second convolution layers which is three back-to-back conv with 3x3 kernel and same padding\n",
        "    # Double filter depth - output layer will be (48,48,start_filter*2)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "    \n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (24,24,start_filter*2)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add some random dropout\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    \n",
        "    # Add our third convolution layers which is three back-to-back conv with 3x3 kernel and same padding\n",
        "    # Double filter depth again - output layer will be (24,24,start_filter*4)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "    \n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (12,12,start_filter*4)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add some random dropout\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+2*step))\n",
        "    \n",
        "    # Add our fourth and final convolution layers which is three back-to-back conv with 3x3 kernel and same padding\n",
        "    # Double filter depth again - output layer will be (12,12,start_filter*8)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        \n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (6,6,start_filter*8)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Flatten and transition to fully connected layers\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(500))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(500))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(30))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLx43YZAeU0F",
        "colab_type": "code",
        "outputId": "380eff5c-33ac-4532-d165-1771932c05bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Redefine optimizer list to just focus on adam and sgd\n",
        "opt_list = {'adam':adam}\n",
        "\n",
        "# Use an early stopping callback and our timing callback\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                              patience=100, mode='auto')\n",
        "time_callback = TimeHistory()\n",
        "\n",
        "# Initialize a new data frame to hold our output data\n",
        "cnn_vgg_lr_df = pd.DataFrame()\n",
        "\n",
        "# Start filter list\n",
        "start_filters = [12, 16]\n",
        "\n",
        "# Flag for using or not using bias term\n",
        "biases = [False]\n",
        "\n",
        "# Create a list of initial dropout values and steps to increase\n",
        "dropouts = [(0.0,0.02)]\n",
        "\n",
        "\n",
        "for lr_factor in [5, 10]:\n",
        "  for opt_name, opt in opt_list.items():\n",
        "      for start_filter in start_filters:\n",
        "          for bias in biases:\n",
        "              for d in dropouts:\n",
        "                  adam = optimizers.Adam(lr=0.001*lr_factor, beta_1=0.9, beta_2=0.999)\n",
        "                  model = create_vgg_model(start_filter, d[0], d[1], bias)\n",
        "                  model.compile(\n",
        "                        optimizer=opt,\n",
        "                        loss='mean_squared_error',\n",
        "                        metrics=['mean_squared_error'])\n",
        "                  history = model.fit(\n",
        "                      X.astype(np.float32), y.astype(np.float32),\n",
        "                      epochs=200,\n",
        "                      validation_split=0.15, callbacks=[time_callback, early_stop])\n",
        "                  times = time_callback.times\n",
        "\n",
        "                  # Convert to dataframe\n",
        "                  hist = pd.DataFrame(history.history)\n",
        "                  hist['epoch'] = history.epoch\n",
        "                  hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "                  hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "                  hist['times'] = times\n",
        "                  hist['starting_filter'] = start_filter\n",
        "                  hist['layers'] = 4\n",
        "                  hist['pooling'] = 'yes'\n",
        "                  hist['fc_layer'] = 500\n",
        "                  hist['activation'] = 'relu'\n",
        "                  hist['optimizer'] = opt_name\n",
        "                  hist['lrate'] = opt.get_config()['learning_rate']\n",
        "                  hist['dropout_initial'] = d[0]\n",
        "                  hist['dropout_step'] = d[1]\n",
        "                  hist['batch_norm'] = 1\n",
        "                  hist['bias'] = int(bias)\n",
        "                  hist['arch'] = 'vgg'\n",
        "\n",
        "                  # Keep concatenating to dataframe\n",
        "                  cnn_vgg_lr_df = pd.concat([cnn_vgg_lr_df,hist])\n",
        "\n",
        "                  # Re-pickle after every model to retain progress\n",
        "                  cnn_vgg_lr_df.to_pickle(drive_path + \"OutputData/cnn_vgg_lr_df.pkl\")\n",
        "\n",
        "                  # Save models.\n",
        "                  filename = \"cnn_vgg_lr_model_{}_d{}_s{}_sf{}\".format(opt_name, d[0], d[1], start_filter)\n",
        "                  model.save(drive_path + \"Models/\"+filename+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0722 10:39:28.058912 140495902832512 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 96, 96, 12)        108       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 96, 96, 12)        1296      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 96, 96, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 48, 48, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 48, 48, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 48, 48, 24)        2592      \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 48, 48, 24)        5184      \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 48, 48, 24)        5184      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 24, 24, 48)        10368     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 48)        20736     \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 24, 24, 48)        20736     \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 24, 24, 48)        20736     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 48)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 12, 12, 96)        41472     \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 12, 12, 96)        82944     \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 12, 12, 96)        82944     \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 12, 12, 96)        82944     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 96)        384       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 96)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3456)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               1728500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 2,375,994\n",
            "Trainable params: 2,373,634\n",
            "Non-trainable params: 2,360\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 5s 3ms/sample - loss: 781.7488 - mean_squared_error: 781.7488 - val_loss: 213.4187 - val_mean_squared_error: 213.4187\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 654us/sample - loss: 33.0116 - mean_squared_error: 33.0116 - val_loss: 323.8731 - val_mean_squared_error: 323.8730\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 655us/sample - loss: 21.4419 - mean_squared_error: 21.4419 - val_loss: 46.9439 - val_mean_squared_error: 46.9439\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 657us/sample - loss: 20.4273 - mean_squared_error: 20.4273 - val_loss: 17.3434 - val_mean_squared_error: 17.3434\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 20.3017 - mean_squared_error: 20.3017 - val_loss: 16.5353 - val_mean_squared_error: 16.5353\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 663us/sample - loss: 19.7075 - mean_squared_error: 19.7075 - val_loss: 13.1390 - val_mean_squared_error: 13.1390\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 16.7445 - mean_squared_error: 16.7445 - val_loss: 141.4397 - val_mean_squared_error: 141.4397\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 17.4659 - mean_squared_error: 17.4659 - val_loss: 18.8964 - val_mean_squared_error: 18.8965\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 15.2210 - mean_squared_error: 15.2210 - val_loss: 27.0519 - val_mean_squared_error: 27.0519\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 14.5126 - mean_squared_error: 14.5126 - val_loss: 10.8032 - val_mean_squared_error: 10.8032\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 12.8241 - mean_squared_error: 12.8241 - val_loss: 12.9085 - val_mean_squared_error: 12.9085\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 13.3964 - mean_squared_error: 13.3964 - val_loss: 11.7909 - val_mean_squared_error: 11.7909\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 14.4514 - mean_squared_error: 14.4514 - val_loss: 520.1386 - val_mean_squared_error: 520.1386\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 15.5599 - mean_squared_error: 15.5599 - val_loss: 25.7744 - val_mean_squared_error: 25.7744\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 14.1538 - mean_squared_error: 14.1538 - val_loss: 11.9240 - val_mean_squared_error: 11.9240\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 657us/sample - loss: 12.6293 - mean_squared_error: 12.6293 - val_loss: 20.3850 - val_mean_squared_error: 20.3850\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 657us/sample - loss: 12.0446 - mean_squared_error: 12.0446 - val_loss: 9.8792 - val_mean_squared_error: 9.8792\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 12.0523 - mean_squared_error: 12.0523 - val_loss: 9.6812 - val_mean_squared_error: 9.6812\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 660us/sample - loss: 11.3910 - mean_squared_error: 11.3910 - val_loss: 10.0952 - val_mean_squared_error: 10.0952\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 11.1157 - mean_squared_error: 11.1157 - val_loss: 10.7322 - val_mean_squared_error: 10.7322\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 676us/sample - loss: 11.8150 - mean_squared_error: 11.8150 - val_loss: 13.6929 - val_mean_squared_error: 13.6929\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 683us/sample - loss: 11.3459 - mean_squared_error: 11.3459 - val_loss: 11.1519 - val_mean_squared_error: 11.1519\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 678us/sample - loss: 11.5824 - mean_squared_error: 11.5824 - val_loss: 9.7110 - val_mean_squared_error: 9.7110\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 11.5645 - mean_squared_error: 11.5645 - val_loss: 11.7049 - val_mean_squared_error: 11.7049\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 11.2882 - mean_squared_error: 11.2882 - val_loss: 10.7311 - val_mean_squared_error: 10.7311\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 11.1574 - mean_squared_error: 11.1574 - val_loss: 10.6219 - val_mean_squared_error: 10.6219\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 11.4434 - mean_squared_error: 11.4434 - val_loss: 9.6728 - val_mean_squared_error: 9.6728\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 676us/sample - loss: 10.5530 - mean_squared_error: 10.5530 - val_loss: 10.1547 - val_mean_squared_error: 10.1547\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 674us/sample - loss: 10.3133 - mean_squared_error: 10.3133 - val_loss: 10.2442 - val_mean_squared_error: 10.2442\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 677us/sample - loss: 11.0315 - mean_squared_error: 11.0315 - val_loss: 10.7732 - val_mean_squared_error: 10.7732\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 10.5117 - mean_squared_error: 10.5117 - val_loss: 11.2396 - val_mean_squared_error: 11.2396\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 10.3841 - mean_squared_error: 10.3841 - val_loss: 10.3234 - val_mean_squared_error: 10.3234\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 10.2838 - mean_squared_error: 10.2838 - val_loss: 11.0230 - val_mean_squared_error: 11.0230\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 10.2839 - mean_squared_error: 10.2839 - val_loss: 10.2624 - val_mean_squared_error: 10.2624\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 10.1579 - mean_squared_error: 10.1579 - val_loss: 9.7536 - val_mean_squared_error: 9.7536\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 9.8685 - mean_squared_error: 9.8685 - val_loss: 9.7529 - val_mean_squared_error: 9.7529\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 10.0016 - mean_squared_error: 10.0016 - val_loss: 11.7652 - val_mean_squared_error: 11.7652\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 9.5580 - mean_squared_error: 9.5580 - val_loss: 9.4250 - val_mean_squared_error: 9.4250\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 9.4760 - mean_squared_error: 9.4760 - val_loss: 9.6632 - val_mean_squared_error: 9.6632\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 9.9115 - mean_squared_error: 9.9115 - val_loss: 10.5957 - val_mean_squared_error: 10.5957\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 9.5592 - mean_squared_error: 9.5592 - val_loss: 9.9106 - val_mean_squared_error: 9.9106\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 9.5861 - mean_squared_error: 9.5861 - val_loss: 13.6348 - val_mean_squared_error: 13.6348\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 9.4420 - mean_squared_error: 9.4420 - val_loss: 9.3828 - val_mean_squared_error: 9.3828\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 9.1607 - mean_squared_error: 9.1607 - val_loss: 9.0999 - val_mean_squared_error: 9.0999\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 9.3557 - mean_squared_error: 9.3557 - val_loss: 10.6301 - val_mean_squared_error: 10.6301\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 8.9450 - mean_squared_error: 8.9450 - val_loss: 12.4846 - val_mean_squared_error: 12.4846\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 8.4611 - mean_squared_error: 8.4611 - val_loss: 10.2994 - val_mean_squared_error: 10.2994\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 8.4522 - mean_squared_error: 8.4522 - val_loss: 9.5915 - val_mean_squared_error: 9.5915\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 8.1157 - mean_squared_error: 8.1157 - val_loss: 8.0825 - val_mean_squared_error: 8.0825\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 8.0416 - mean_squared_error: 8.0416 - val_loss: 8.3795 - val_mean_squared_error: 8.3795\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 7.6043 - mean_squared_error: 7.6043 - val_loss: 8.1233 - val_mean_squared_error: 8.1233\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 7.6726 - mean_squared_error: 7.6726 - val_loss: 10.6667 - val_mean_squared_error: 10.6667\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 7.3175 - mean_squared_error: 7.3175 - val_loss: 7.9211 - val_mean_squared_error: 7.9211\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 7.4542 - mean_squared_error: 7.4542 - val_loss: 7.1623 - val_mean_squared_error: 7.1623\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 6.8886 - mean_squared_error: 6.8886 - val_loss: 7.3377 - val_mean_squared_error: 7.3377\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 6.6909 - mean_squared_error: 6.6909 - val_loss: 7.0870 - val_mean_squared_error: 7.0870\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 6.3353 - mean_squared_error: 6.3353 - val_loss: 7.1091 - val_mean_squared_error: 7.1091\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 6.2692 - mean_squared_error: 6.2692 - val_loss: 7.3138 - val_mean_squared_error: 7.3138\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 6.0298 - mean_squared_error: 6.0298 - val_loss: 6.0455 - val_mean_squared_error: 6.0455\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 676us/sample - loss: 5.7625 - mean_squared_error: 5.7625 - val_loss: 7.5024 - val_mean_squared_error: 7.5024\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 5.3185 - mean_squared_error: 5.3185 - val_loss: 5.8191 - val_mean_squared_error: 5.8191\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 5.0421 - mean_squared_error: 5.0421 - val_loss: 6.5544 - val_mean_squared_error: 6.5544\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 4.7464 - mean_squared_error: 4.7464 - val_loss: 5.2862 - val_mean_squared_error: 5.2862\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 4.6785 - mean_squared_error: 4.6785 - val_loss: 6.6575 - val_mean_squared_error: 6.6575\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 4.5185 - mean_squared_error: 4.5185 - val_loss: 5.0154 - val_mean_squared_error: 5.0154\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 4.3239 - mean_squared_error: 4.3239 - val_loss: 4.8475 - val_mean_squared_error: 4.8475\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 674us/sample - loss: 4.1968 - mean_squared_error: 4.1968 - val_loss: 5.6522 - val_mean_squared_error: 5.6522\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 4.2169 - mean_squared_error: 4.2169 - val_loss: 4.5850 - val_mean_squared_error: 4.5850\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 4.0747 - mean_squared_error: 4.0747 - val_loss: 5.2850 - val_mean_squared_error: 5.2850\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 4.2512 - mean_squared_error: 4.2512 - val_loss: 5.5351 - val_mean_squared_error: 5.5351\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 3.8330 - mean_squared_error: 3.8330 - val_loss: 4.4725 - val_mean_squared_error: 4.4725\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 3.5864 - mean_squared_error: 3.5864 - val_loss: 4.4639 - val_mean_squared_error: 4.4639\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 3.5063 - mean_squared_error: 3.5063 - val_loss: 4.0759 - val_mean_squared_error: 4.0759\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 3.6088 - mean_squared_error: 3.6088 - val_loss: 4.9594 - val_mean_squared_error: 4.9594\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 3.5377 - mean_squared_error: 3.5377 - val_loss: 5.1274 - val_mean_squared_error: 5.1274\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 661us/sample - loss: 3.3203 - mean_squared_error: 3.3203 - val_loss: 4.6053 - val_mean_squared_error: 4.6053\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 3.3047 - mean_squared_error: 3.3047 - val_loss: 3.9096 - val_mean_squared_error: 3.9096\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 3.2879 - mean_squared_error: 3.2879 - val_loss: 3.8551 - val_mean_squared_error: 3.8551\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.8942 - mean_squared_error: 2.8942 - val_loss: 3.5435 - val_mean_squared_error: 3.5435\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 3.1107 - mean_squared_error: 3.1107 - val_loss: 4.3281 - val_mean_squared_error: 4.3281\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 2.9916 - mean_squared_error: 2.9916 - val_loss: 3.5635 - val_mean_squared_error: 3.5635\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 664us/sample - loss: 2.7491 - mean_squared_error: 2.7491 - val_loss: 3.9909 - val_mean_squared_error: 3.9909\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 2.8301 - mean_squared_error: 2.8301 - val_loss: 3.9517 - val_mean_squared_error: 3.9517\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.7259 - mean_squared_error: 2.7259 - val_loss: 3.6731 - val_mean_squared_error: 3.6731\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.7306 - mean_squared_error: 2.7306 - val_loss: 4.0923 - val_mean_squared_error: 4.0923\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 664us/sample - loss: 2.7359 - mean_squared_error: 2.7359 - val_loss: 3.7006 - val_mean_squared_error: 3.7006\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 661us/sample - loss: 2.6547 - mean_squared_error: 2.6547 - val_loss: 3.5455 - val_mean_squared_error: 3.5455\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 2.6200 - mean_squared_error: 2.6200 - val_loss: 3.9449 - val_mean_squared_error: 3.9449\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 2.5168 - mean_squared_error: 2.5168 - val_loss: 3.5784 - val_mean_squared_error: 3.5784\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 658us/sample - loss: 2.7307 - mean_squared_error: 2.7307 - val_loss: 3.6002 - val_mean_squared_error: 3.6002\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 660us/sample - loss: 2.6080 - mean_squared_error: 2.6080 - val_loss: 3.6788 - val_mean_squared_error: 3.6788\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.3676 - mean_squared_error: 2.3676 - val_loss: 3.5532 - val_mean_squared_error: 3.5532\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.2929 - mean_squared_error: 2.2929 - val_loss: 3.5577 - val_mean_squared_error: 3.5577\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 2.3902 - mean_squared_error: 2.3902 - val_loss: 3.2699 - val_mean_squared_error: 3.2699\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.3489 - mean_squared_error: 2.3489 - val_loss: 4.4773 - val_mean_squared_error: 4.4773\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 2.1969 - mean_squared_error: 2.1969 - val_loss: 3.5717 - val_mean_squared_error: 3.5717\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 663us/sample - loss: 2.2736 - mean_squared_error: 2.2736 - val_loss: 3.4035 - val_mean_squared_error: 3.4035\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 2.1394 - mean_squared_error: 2.1394 - val_loss: 3.2530 - val_mean_squared_error: 3.2530\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.1236 - mean_squared_error: 2.1236 - val_loss: 3.5594 - val_mean_squared_error: 3.5594\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.1007 - mean_squared_error: 2.1007 - val_loss: 3.3337 - val_mean_squared_error: 3.3337\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.1726 - mean_squared_error: 2.1726 - val_loss: 3.4282 - val_mean_squared_error: 3.4282\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 2.1330 - mean_squared_error: 2.1330 - val_loss: 3.3860 - val_mean_squared_error: 3.3860\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.9995 - mean_squared_error: 1.9995 - val_loss: 3.1529 - val_mean_squared_error: 3.1529\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.9982 - mean_squared_error: 1.9982 - val_loss: 3.1133 - val_mean_squared_error: 3.1133\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.0883 - mean_squared_error: 2.0883 - val_loss: 3.2291 - val_mean_squared_error: 3.2291\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.0609 - mean_squared_error: 2.0609 - val_loss: 3.0584 - val_mean_squared_error: 3.0584\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.8807 - mean_squared_error: 1.8807 - val_loss: 3.1473 - val_mean_squared_error: 3.1473\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.9055 - mean_squared_error: 1.9055 - val_loss: 2.8346 - val_mean_squared_error: 2.8346\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 1.7902 - mean_squared_error: 1.7902 - val_loss: 3.4267 - val_mean_squared_error: 3.4267\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 676us/sample - loss: 1.8433 - mean_squared_error: 1.8433 - val_loss: 3.4882 - val_mean_squared_error: 3.4882\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.0085 - mean_squared_error: 2.0085 - val_loss: 3.3058 - val_mean_squared_error: 3.3058\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.8577 - mean_squared_error: 1.8577 - val_loss: 3.1193 - val_mean_squared_error: 3.1193\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.9528 - mean_squared_error: 1.9528 - val_loss: 3.1749 - val_mean_squared_error: 3.1749\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.8028 - mean_squared_error: 1.8028 - val_loss: 3.2943 - val_mean_squared_error: 3.2943\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.7760 - mean_squared_error: 1.7760 - val_loss: 3.2595 - val_mean_squared_error: 3.2595\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.7441 - mean_squared_error: 1.7441 - val_loss: 3.0925 - val_mean_squared_error: 3.0925\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.8537 - mean_squared_error: 1.8537 - val_loss: 3.0208 - val_mean_squared_error: 3.0208\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.6527 - mean_squared_error: 1.6527 - val_loss: 2.9037 - val_mean_squared_error: 2.9037\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.6069 - mean_squared_error: 1.6069 - val_loss: 2.8961 - val_mean_squared_error: 2.8961\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 1.7075 - mean_squared_error: 1.7075 - val_loss: 2.8007 - val_mean_squared_error: 2.8007\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 1.7059 - mean_squared_error: 1.7059 - val_loss: 2.8529 - val_mean_squared_error: 2.8529\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.6784 - mean_squared_error: 1.6784 - val_loss: 3.0307 - val_mean_squared_error: 3.0307\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.6287 - mean_squared_error: 1.6287 - val_loss: 2.9501 - val_mean_squared_error: 2.9501\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5836 - mean_squared_error: 1.5836 - val_loss: 2.8622 - val_mean_squared_error: 2.8622\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.5142 - mean_squared_error: 1.5142 - val_loss: 3.0432 - val_mean_squared_error: 3.0432\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.4984 - mean_squared_error: 1.4984 - val_loss: 2.9539 - val_mean_squared_error: 2.9539\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.5808 - mean_squared_error: 1.5808 - val_loss: 2.8908 - val_mean_squared_error: 2.8908\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.4875 - mean_squared_error: 1.4875 - val_loss: 3.2125 - val_mean_squared_error: 3.2125\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5390 - mean_squared_error: 1.5390 - val_loss: 2.8865 - val_mean_squared_error: 2.8865\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5137 - mean_squared_error: 1.5137 - val_loss: 3.2376 - val_mean_squared_error: 3.2376\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5833 - mean_squared_error: 1.5833 - val_loss: 3.0944 - val_mean_squared_error: 3.0944\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.4758 - mean_squared_error: 1.4758 - val_loss: 3.3393 - val_mean_squared_error: 3.3393\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.4668 - mean_squared_error: 1.4668 - val_loss: 3.2321 - val_mean_squared_error: 3.2321\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.4128 - mean_squared_error: 1.4128 - val_loss: 2.6181 - val_mean_squared_error: 2.6181\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 1.4526 - mean_squared_error: 1.4526 - val_loss: 2.7075 - val_mean_squared_error: 2.7075\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.4075 - mean_squared_error: 1.4075 - val_loss: 2.6173 - val_mean_squared_error: 2.6173\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.3516 - mean_squared_error: 1.3516 - val_loss: 2.6175 - val_mean_squared_error: 2.6175\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.3323 - mean_squared_error: 1.3323 - val_loss: 2.6932 - val_mean_squared_error: 2.6932\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.3929 - mean_squared_error: 1.3929 - val_loss: 2.6788 - val_mean_squared_error: 2.6788\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 1.3954 - mean_squared_error: 1.3954 - val_loss: 2.7899 - val_mean_squared_error: 2.7899\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2841 - mean_squared_error: 1.2841 - val_loss: 2.8571 - val_mean_squared_error: 2.8571\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.4039 - mean_squared_error: 1.4039 - val_loss: 2.5903 - val_mean_squared_error: 2.5903\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.5930 - mean_squared_error: 1.5930 - val_loss: 3.0941 - val_mean_squared_error: 3.0941\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.3589 - mean_squared_error: 1.3589 - val_loss: 2.7713 - val_mean_squared_error: 2.7713\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.3106 - mean_squared_error: 1.3106 - val_loss: 2.8425 - val_mean_squared_error: 2.8425\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.3236 - mean_squared_error: 1.3236 - val_loss: 2.5247 - val_mean_squared_error: 2.5247\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.3223 - mean_squared_error: 1.3223 - val_loss: 2.7986 - val_mean_squared_error: 2.7986\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.3095 - mean_squared_error: 1.3095 - val_loss: 3.3503 - val_mean_squared_error: 3.3503\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2898 - mean_squared_error: 1.2898 - val_loss: 2.7936 - val_mean_squared_error: 2.7936\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.3018 - mean_squared_error: 1.3018 - val_loss: 2.7062 - val_mean_squared_error: 2.7062\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 661us/sample - loss: 1.2810 - mean_squared_error: 1.2810 - val_loss: 2.9768 - val_mean_squared_error: 2.9768\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.2980 - mean_squared_error: 1.2980 - val_loss: 2.4653 - val_mean_squared_error: 2.4653\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.3378 - mean_squared_error: 1.3378 - val_loss: 2.7352 - val_mean_squared_error: 2.7352\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2834 - mean_squared_error: 1.2834 - val_loss: 2.8166 - val_mean_squared_error: 2.8166\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2247 - mean_squared_error: 1.2247 - val_loss: 3.3077 - val_mean_squared_error: 3.3077\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.2764 - mean_squared_error: 1.2764 - val_loss: 2.6752 - val_mean_squared_error: 2.6752\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 1.1730 - mean_squared_error: 1.1730 - val_loss: 2.5317 - val_mean_squared_error: 2.5317\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 658us/sample - loss: 1.2203 - mean_squared_error: 1.2203 - val_loss: 2.6737 - val_mean_squared_error: 2.6737\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.1753 - mean_squared_error: 1.1753 - val_loss: 2.5449 - val_mean_squared_error: 2.5449\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.4003 - mean_squared_error: 1.4003 - val_loss: 2.5955 - val_mean_squared_error: 2.5955\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.2116 - mean_squared_error: 1.2116 - val_loss: 2.7069 - val_mean_squared_error: 2.7069\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 658us/sample - loss: 1.3028 - mean_squared_error: 1.3028 - val_loss: 2.6700 - val_mean_squared_error: 2.6700\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.0947 - mean_squared_error: 1.0947 - val_loss: 2.6218 - val_mean_squared_error: 2.6218\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1501 - mean_squared_error: 1.1501 - val_loss: 2.7646 - val_mean_squared_error: 2.7646\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.2130 - mean_squared_error: 1.2130 - val_loss: 2.5896 - val_mean_squared_error: 2.5896\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 1.1233 - mean_squared_error: 1.1233 - val_loss: 2.7454 - val_mean_squared_error: 2.7454\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 658us/sample - loss: 1.2586 - mean_squared_error: 1.2586 - val_loss: 2.6570 - val_mean_squared_error: 2.6570\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.1925 - mean_squared_error: 1.1925 - val_loss: 2.8514 - val_mean_squared_error: 2.8514\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.1625 - mean_squared_error: 1.1625 - val_loss: 2.5137 - val_mean_squared_error: 2.5137\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.2470 - mean_squared_error: 1.2470 - val_loss: 2.5911 - val_mean_squared_error: 2.5911\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1259 - mean_squared_error: 1.1259 - val_loss: 2.4710 - val_mean_squared_error: 2.4710\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 1.2117 - mean_squared_error: 1.2117 - val_loss: 2.7863 - val_mean_squared_error: 2.7863\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 663us/sample - loss: 1.1724 - mean_squared_error: 1.1724 - val_loss: 2.7623 - val_mean_squared_error: 2.7623\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0330 - mean_squared_error: 1.0330 - val_loss: 2.6495 - val_mean_squared_error: 2.6495\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.1306 - mean_squared_error: 1.1306 - val_loss: 2.7890 - val_mean_squared_error: 2.7890\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.0980 - mean_squared_error: 1.0980 - val_loss: 2.4605 - val_mean_squared_error: 2.4605\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.0215 - mean_squared_error: 1.0215 - val_loss: 2.5733 - val_mean_squared_error: 2.5733\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.0710 - mean_squared_error: 1.0710 - val_loss: 2.4814 - val_mean_squared_error: 2.4814\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.1621 - mean_squared_error: 1.1621 - val_loss: 2.8667 - val_mean_squared_error: 2.8667\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.1974 - mean_squared_error: 1.1974 - val_loss: 2.5795 - val_mean_squared_error: 2.5795\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.1507 - mean_squared_error: 1.1507 - val_loss: 2.5841 - val_mean_squared_error: 2.5841\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.0835 - mean_squared_error: 1.0835 - val_loss: 2.7142 - val_mean_squared_error: 2.7142\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.0939 - mean_squared_error: 1.0939 - val_loss: 2.6409 - val_mean_squared_error: 2.6409\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0306 - mean_squared_error: 1.0306 - val_loss: 2.6295 - val_mean_squared_error: 2.6295\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0605 - mean_squared_error: 1.0605 - val_loss: 2.4907 - val_mean_squared_error: 2.4907\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 0.9623 - mean_squared_error: 0.9623 - val_loss: 2.4525 - val_mean_squared_error: 2.4525\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 0.9777 - mean_squared_error: 0.9777 - val_loss: 2.5021 - val_mean_squared_error: 2.5021\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 0.9957 - mean_squared_error: 0.9957 - val_loss: 16.5056 - val_mean_squared_error: 16.5056\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 1.0914 - mean_squared_error: 1.0914 - val_loss: 2.5721 - val_mean_squared_error: 2.5721\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 662us/sample - loss: 0.9815 - mean_squared_error: 0.9815 - val_loss: 2.2492 - val_mean_squared_error: 2.2492\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 0.9615 - mean_squared_error: 0.9615 - val_loss: 2.5312 - val_mean_squared_error: 2.5312\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 1.0336 - mean_squared_error: 1.0336 - val_loss: 2.7734 - val_mean_squared_error: 2.7734\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.0494 - mean_squared_error: 1.0494 - val_loss: 2.4974 - val_mean_squared_error: 2.4974\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 0.9679 - mean_squared_error: 0.9679 - val_loss: 2.7406 - val_mean_squared_error: 2.7406\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 0.9539 - mean_squared_error: 0.9539 - val_loss: 2.3828 - val_mean_squared_error: 2.3828\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 0.9334 - mean_squared_error: 0.9334 - val_loss: 2.6093 - val_mean_squared_error: 2.6093\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 0.9557 - mean_squared_error: 0.9557 - val_loss: 2.3734 - val_mean_squared_error: 2.3734\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 1.0251 - mean_squared_error: 1.0251 - val_loss: 2.4811 - val_mean_squared_error: 2.4811\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0644 - mean_squared_error: 1.0644 - val_loss: 2.4911 - val_mean_squared_error: 2.4911\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.0329 - mean_squared_error: 1.0329 - val_loss: 2.8734 - val_mean_squared_error: 2.8734\n",
            "==================================================\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 96, 96, 16)        144       \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 96, 96, 16)        2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 96, 96, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 48, 48, 32)        4608      \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 24, 24, 64)        18432     \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 12, 12, 128)       73728     \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 500)               2304500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,245,598\n",
            "Trainable params: 3,243,118\n",
            "Non-trainable params: 2,480\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 283.0856 - mean_squared_error: 283.0856 - val_loss: 6778.1706 - val_mean_squared_error: 6778.1704\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 2s 826us/sample - loss: 25.7418 - mean_squared_error: 25.7418 - val_loss: 190.2086 - val_mean_squared_error: 190.2086\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 21.4531 - mean_squared_error: 21.4531 - val_loss: 75.9990 - val_mean_squared_error: 75.9990\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 22.6889 - mean_squared_error: 22.6889 - val_loss: 53.2356 - val_mean_squared_error: 53.2356\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 17.1887 - mean_squared_error: 17.1887 - val_loss: 19.3702 - val_mean_squared_error: 19.3702\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 15.1843 - mean_squared_error: 15.1843 - val_loss: 12.2080 - val_mean_squared_error: 12.2080\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 16.1694 - mean_squared_error: 16.1694 - val_loss: 10.8145 - val_mean_squared_error: 10.8145\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 803us/sample - loss: 13.9313 - mean_squared_error: 13.9313 - val_loss: 10.2722 - val_mean_squared_error: 10.2722\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 14.0369 - mean_squared_error: 14.0369 - val_loss: 10.7588 - val_mean_squared_error: 10.7588\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 14.2582 - mean_squared_error: 14.2582 - val_loss: 11.1919 - val_mean_squared_error: 11.1919\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 13.3297 - mean_squared_error: 13.3297 - val_loss: 11.2106 - val_mean_squared_error: 11.2106\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 804us/sample - loss: 12.5237 - mean_squared_error: 12.5237 - val_loss: 10.8805 - val_mean_squared_error: 10.8805\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 12.7321 - mean_squared_error: 12.7321 - val_loss: 10.4583 - val_mean_squared_error: 10.4583\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 12.7837 - mean_squared_error: 12.7837 - val_loss: 10.4101 - val_mean_squared_error: 10.4101\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 12.1136 - mean_squared_error: 12.1136 - val_loss: 9.7048 - val_mean_squared_error: 9.7048\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 11.7687 - mean_squared_error: 11.7687 - val_loss: 11.9355 - val_mean_squared_error: 11.9355\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 11.6543 - mean_squared_error: 11.6543 - val_loss: 9.5608 - val_mean_squared_error: 9.5608\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 11.2427 - mean_squared_error: 11.2427 - val_loss: 9.4907 - val_mean_squared_error: 9.4907\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 10.9441 - mean_squared_error: 10.9441 - val_loss: 10.0137 - val_mean_squared_error: 10.0137\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 10.7553 - mean_squared_error: 10.7553 - val_loss: 10.7888 - val_mean_squared_error: 10.7888\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 10.6610 - mean_squared_error: 10.6610 - val_loss: 9.5028 - val_mean_squared_error: 9.5028\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 10.2864 - mean_squared_error: 10.2864 - val_loss: 9.4255 - val_mean_squared_error: 9.4255\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 10.2228 - mean_squared_error: 10.2228 - val_loss: 9.0920 - val_mean_squared_error: 9.0920\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 9.8343 - mean_squared_error: 9.8343 - val_loss: 9.3493 - val_mean_squared_error: 9.3493\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 9.7122 - mean_squared_error: 9.7122 - val_loss: 10.4172 - val_mean_squared_error: 10.4172\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 9.6133 - mean_squared_error: 9.6133 - val_loss: 9.4148 - val_mean_squared_error: 9.4148\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 10.2105 - mean_squared_error: 10.2105 - val_loss: 8.9641 - val_mean_squared_error: 8.9641\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 9.6545 - mean_squared_error: 9.6545 - val_loss: 9.2787 - val_mean_squared_error: 9.2787\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 9.3773 - mean_squared_error: 9.3773 - val_loss: 8.8413 - val_mean_squared_error: 8.8413\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 9.2718 - mean_squared_error: 9.2718 - val_loss: 12.4210 - val_mean_squared_error: 12.4209\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 9.3429 - mean_squared_error: 9.3429 - val_loss: 8.7193 - val_mean_squared_error: 8.7193\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 8.9501 - mean_squared_error: 8.9501 - val_loss: 8.9896 - val_mean_squared_error: 8.9896\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 8.8090 - mean_squared_error: 8.8090 - val_loss: 9.1230 - val_mean_squared_error: 9.1230\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 8.7551 - mean_squared_error: 8.7551 - val_loss: 9.0520 - val_mean_squared_error: 9.0520\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 8.6072 - mean_squared_error: 8.6072 - val_loss: 8.7011 - val_mean_squared_error: 8.7011\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 8.3269 - mean_squared_error: 8.3268 - val_loss: 26.9002 - val_mean_squared_error: 26.9002\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 8.2517 - mean_squared_error: 8.2517 - val_loss: 9.3184 - val_mean_squared_error: 9.3184\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 8.0717 - mean_squared_error: 8.0717 - val_loss: 9.2732 - val_mean_squared_error: 9.2732\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 8.0484 - mean_squared_error: 8.0484 - val_loss: 8.8530 - val_mean_squared_error: 8.8530\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 7.9560 - mean_squared_error: 7.9560 - val_loss: 81.2449 - val_mean_squared_error: 81.2449\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 7.8158 - mean_squared_error: 7.8158 - val_loss: 8.8695 - val_mean_squared_error: 8.8695\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 7.6252 - mean_squared_error: 7.6252 - val_loss: 7.8840 - val_mean_squared_error: 7.8840\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 7.3747 - mean_squared_error: 7.3747 - val_loss: 485.1601 - val_mean_squared_error: 485.1600\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 7.3291 - mean_squared_error: 7.3291 - val_loss: 978.8440 - val_mean_squared_error: 978.8439\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 7.2692 - mean_squared_error: 7.2692 - val_loss: 7.9133 - val_mean_squared_error: 7.9133\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 805us/sample - loss: 7.0582 - mean_squared_error: 7.0582 - val_loss: 7.9514 - val_mean_squared_error: 7.9514\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 6.7937 - mean_squared_error: 6.7937 - val_loss: 7.7165 - val_mean_squared_error: 7.7165\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 6.7274 - mean_squared_error: 6.7274 - val_loss: 7.6869 - val_mean_squared_error: 7.6869\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 6.7357 - mean_squared_error: 6.7357 - val_loss: 7.0314 - val_mean_squared_error: 7.0314\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 6.1675 - mean_squared_error: 6.1675 - val_loss: 7.1733 - val_mean_squared_error: 7.1733\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 6.2564 - mean_squared_error: 6.2564 - val_loss: 7.5155 - val_mean_squared_error: 7.5155\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 5.9459 - mean_squared_error: 5.9459 - val_loss: 7.3448 - val_mean_squared_error: 7.3448\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 5.9539 - mean_squared_error: 5.9539 - val_loss: 7.4376 - val_mean_squared_error: 7.4376\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 5.6518 - mean_squared_error: 5.6518 - val_loss: 6.9594 - val_mean_squared_error: 6.9594\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 5.5051 - mean_squared_error: 5.5051 - val_loss: 2871.5798 - val_mean_squared_error: 2871.5798\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 5.2735 - mean_squared_error: 5.2735 - val_loss: 8.7249 - val_mean_squared_error: 8.7249\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 5.0612 - mean_squared_error: 5.0612 - val_loss: 6.8785 - val_mean_squared_error: 6.8785\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 4.9676 - mean_squared_error: 4.9676 - val_loss: 5.7489 - val_mean_squared_error: 5.7489\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 4.6296 - mean_squared_error: 4.6296 - val_loss: 10.9758 - val_mean_squared_error: 10.9758\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 4.6037 - mean_squared_error: 4.6037 - val_loss: 5.7868 - val_mean_squared_error: 5.7868\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 4.4795 - mean_squared_error: 4.4795 - val_loss: 5.4703 - val_mean_squared_error: 5.4703\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 4.3290 - mean_squared_error: 4.3290 - val_loss: 28.1657 - val_mean_squared_error: 28.1657\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 4.1575 - mean_squared_error: 4.1575 - val_loss: 182.0652 - val_mean_squared_error: 182.0652\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 4.0377 - mean_squared_error: 4.0377 - val_loss: 5.0841 - val_mean_squared_error: 5.0841\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 3.9422 - mean_squared_error: 3.9422 - val_loss: 6.0375 - val_mean_squared_error: 6.0375\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 805us/sample - loss: 3.9375 - mean_squared_error: 3.9375 - val_loss: 5.1940 - val_mean_squared_error: 5.1940\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 3.7475 - mean_squared_error: 3.7475 - val_loss: 5.3226 - val_mean_squared_error: 5.3226\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 806us/sample - loss: 3.7494 - mean_squared_error: 3.7494 - val_loss: 5.0314 - val_mean_squared_error: 5.0314\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 3.5258 - mean_squared_error: 3.5258 - val_loss: 5.1432 - val_mean_squared_error: 5.1432\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 3.3675 - mean_squared_error: 3.3675 - val_loss: 4.6394 - val_mean_squared_error: 4.6394\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 3.3322 - mean_squared_error: 3.3322 - val_loss: 4.9493 - val_mean_squared_error: 4.9493\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 3.3040 - mean_squared_error: 3.3040 - val_loss: 4.7530 - val_mean_squared_error: 4.7530\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 3.1435 - mean_squared_error: 3.1435 - val_loss: 4.3810 - val_mean_squared_error: 4.3810\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 3.1149 - mean_squared_error: 3.1149 - val_loss: 4.8144 - val_mean_squared_error: 4.8144\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 3.2862 - mean_squared_error: 3.2862 - val_loss: 4.5134 - val_mean_squared_error: 4.5134\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 3.1180 - mean_squared_error: 3.1180 - val_loss: 40272.0061 - val_mean_squared_error: 40272.0078\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 2.9768 - mean_squared_error: 2.9768 - val_loss: 16755.9229 - val_mean_squared_error: 16755.9219\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 2.8702 - mean_squared_error: 2.8702 - val_loss: 4.1928 - val_mean_squared_error: 4.1928\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 804us/sample - loss: 2.8014 - mean_squared_error: 2.8014 - val_loss: 193.8874 - val_mean_squared_error: 193.8875\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 2.7207 - mean_squared_error: 2.7207 - val_loss: 12678.5881 - val_mean_squared_error: 12678.5879\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 2.7297 - mean_squared_error: 2.7297 - val_loss: 4.3595 - val_mean_squared_error: 4.3595\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 2.6599 - mean_squared_error: 2.6599 - val_loss: 49.6183 - val_mean_squared_error: 49.6183\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 2.6371 - mean_squared_error: 2.6371 - val_loss: 5239.4426 - val_mean_squared_error: 5239.4424\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 2.6952 - mean_squared_error: 2.6952 - val_loss: 29508.5145 - val_mean_squared_error: 29508.5117\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 2.4789 - mean_squared_error: 2.4789 - val_loss: 4.0677 - val_mean_squared_error: 4.0677\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 805us/sample - loss: 2.5136 - mean_squared_error: 2.5136 - val_loss: 3.8773 - val_mean_squared_error: 3.8773\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 2.3767 - mean_squared_error: 2.3767 - val_loss: 42355.9956 - val_mean_squared_error: 42355.9922\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 2.5694 - mean_squared_error: 2.5694 - val_loss: 3.9865 - val_mean_squared_error: 3.9865\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 804us/sample - loss: 2.2514 - mean_squared_error: 2.2514 - val_loss: 4.1423 - val_mean_squared_error: 4.1423\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 2.2853 - mean_squared_error: 2.2853 - val_loss: 3.6669 - val_mean_squared_error: 3.6669\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 2.3193 - mean_squared_error: 2.3193 - val_loss: 3.7815 - val_mean_squared_error: 3.7815\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 2.1275 - mean_squared_error: 2.1275 - val_loss: 10369.5801 - val_mean_squared_error: 10369.5801\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 2.1623 - mean_squared_error: 2.1623 - val_loss: 3.9116 - val_mean_squared_error: 3.9116\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 2.1581 - mean_squared_error: 2.1581 - val_loss: 17982.1677 - val_mean_squared_error: 17982.1680\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 2.0833 - mean_squared_error: 2.0833 - val_loss: 7.6578 - val_mean_squared_error: 7.6578\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 3.1498 - mean_squared_error: 3.1498 - val_loss: 8.5762 - val_mean_squared_error: 8.5762\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 2.2130 - mean_squared_error: 2.2130 - val_loss: 4.0532 - val_mean_squared_error: 4.0532\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 2.1079 - mean_squared_error: 2.1079 - val_loss: 3.9206 - val_mean_squared_error: 3.9206\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 2.0022 - mean_squared_error: 2.0022 - val_loss: 3.6469 - val_mean_squared_error: 3.6469\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 2.0477 - mean_squared_error: 2.0477 - val_loss: 3.6387 - val_mean_squared_error: 3.6387\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 806us/sample - loss: 2.0465 - mean_squared_error: 2.0465 - val_loss: 3.6914 - val_mean_squared_error: 3.6914\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.9952 - mean_squared_error: 1.9952 - val_loss: 3.7544 - val_mean_squared_error: 3.7544\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.8688 - mean_squared_error: 1.8688 - val_loss: 3.3838 - val_mean_squared_error: 3.3838\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 1.8893 - mean_squared_error: 1.8893 - val_loss: 6322.8100 - val_mean_squared_error: 6322.8101\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 1.7807 - mean_squared_error: 1.7807 - val_loss: 3.3714 - val_mean_squared_error: 3.3714\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 805us/sample - loss: 1.8176 - mean_squared_error: 1.8176 - val_loss: 3.7909 - val_mean_squared_error: 3.7909\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.8755 - mean_squared_error: 1.8755 - val_loss: 3.5771 - val_mean_squared_error: 3.5771\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 1.7374 - mean_squared_error: 1.7374 - val_loss: 3.3112 - val_mean_squared_error: 3.3112\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 1.7798 - mean_squared_error: 1.7798 - val_loss: 3.4745 - val_mean_squared_error: 3.4745\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.8301 - mean_squared_error: 1.8301 - val_loss: 3.6345 - val_mean_squared_error: 3.6345\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.8124 - mean_squared_error: 1.8124 - val_loss: 3.7147 - val_mean_squared_error: 3.7147\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.8126 - mean_squared_error: 1.8126 - val_loss: 3.7235 - val_mean_squared_error: 3.7235\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 1.6952 - mean_squared_error: 1.6952 - val_loss: 3.3468 - val_mean_squared_error: 3.3468\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.7736 - mean_squared_error: 1.7736 - val_loss: 63370.4893 - val_mean_squared_error: 63370.4844\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.7456 - mean_squared_error: 1.7456 - val_loss: 3.5555 - val_mean_squared_error: 3.5555\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.5327 - mean_squared_error: 1.5327 - val_loss: 3.5579 - val_mean_squared_error: 3.5579\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 1.6633 - mean_squared_error: 1.6633 - val_loss: 31878.9279 - val_mean_squared_error: 31878.9277\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.6060 - mean_squared_error: 1.6060 - val_loss: 1308.5057 - val_mean_squared_error: 1308.5055\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.5293 - mean_squared_error: 1.5293 - val_loss: 3.2251 - val_mean_squared_error: 3.2251\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 806us/sample - loss: 1.5359 - mean_squared_error: 1.5359 - val_loss: 3.3853 - val_mean_squared_error: 3.3853\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.6021 - mean_squared_error: 1.6021 - val_loss: 3.1906 - val_mean_squared_error: 3.1906\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.6218 - mean_squared_error: 1.6218 - val_loss: 3.3666 - val_mean_squared_error: 3.3666\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.5537 - mean_squared_error: 1.5537 - val_loss: 3.2930 - val_mean_squared_error: 3.2930\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.5147 - mean_squared_error: 1.5147 - val_loss: 189.5401 - val_mean_squared_error: 189.5401\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.5102 - mean_squared_error: 1.5102 - val_loss: 3.5612 - val_mean_squared_error: 3.5612\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.6404 - mean_squared_error: 1.6404 - val_loss: 93.3893 - val_mean_squared_error: 93.3893\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.6809 - mean_squared_error: 1.6809 - val_loss: 3.3630 - val_mean_squared_error: 3.3630\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.4270 - mean_squared_error: 1.4270 - val_loss: 3.2582 - val_mean_squared_error: 3.2582\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.4828 - mean_squared_error: 1.4828 - val_loss: 3.0475 - val_mean_squared_error: 3.0475\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.4277 - mean_squared_error: 1.4277 - val_loss: 3.2704 - val_mean_squared_error: 3.2704\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.4535 - mean_squared_error: 1.4535 - val_loss: 3.3623 - val_mean_squared_error: 3.3623\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.4467 - mean_squared_error: 1.4467 - val_loss: 3.2592 - val_mean_squared_error: 3.2592\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.4261 - mean_squared_error: 1.4261 - val_loss: 23.0925 - val_mean_squared_error: 23.0925\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.4164 - mean_squared_error: 1.4164 - val_loss: 3.6868 - val_mean_squared_error: 3.6868\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.3747 - mean_squared_error: 1.3747 - val_loss: 3.0768 - val_mean_squared_error: 3.0768\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.2822 - mean_squared_error: 1.2822 - val_loss: 3.3127 - val_mean_squared_error: 3.3127\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 800us/sample - loss: 1.4179 - mean_squared_error: 1.4179 - val_loss: 3.3088 - val_mean_squared_error: 3.3088\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.3185 - mean_squared_error: 1.3185 - val_loss: 2.9790 - val_mean_squared_error: 2.9790\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.3469 - mean_squared_error: 1.3469 - val_loss: 3.1244 - val_mean_squared_error: 3.1244\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 806us/sample - loss: 1.3125 - mean_squared_error: 1.3125 - val_loss: 3.1596 - val_mean_squared_error: 3.1596\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.3027 - mean_squared_error: 1.3027 - val_loss: 3.1042 - val_mean_squared_error: 3.1042\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.4066 - mean_squared_error: 1.4066 - val_loss: 3.4124 - val_mean_squared_error: 3.4124\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.3539 - mean_squared_error: 1.3539 - val_loss: 3.4650 - val_mean_squared_error: 3.4650\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.3601 - mean_squared_error: 1.3601 - val_loss: 3.2816 - val_mean_squared_error: 3.2816\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.1958 - mean_squared_error: 1.1958 - val_loss: 3.1614 - val_mean_squared_error: 3.1614\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.1845 - mean_squared_error: 1.1845 - val_loss: 2.8858 - val_mean_squared_error: 2.8858\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.2907 - mean_squared_error: 1.2907 - val_loss: 3.2707 - val_mean_squared_error: 3.2707\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 804us/sample - loss: 1.1965 - mean_squared_error: 1.1965 - val_loss: 3.1280 - val_mean_squared_error: 3.1280\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.2913 - mean_squared_error: 1.2913 - val_loss: 2.9614 - val_mean_squared_error: 2.9614\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.2817 - mean_squared_error: 1.2817 - val_loss: 3.4856 - val_mean_squared_error: 3.4856\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.3095 - mean_squared_error: 1.3095 - val_loss: 3.4866 - val_mean_squared_error: 3.4866\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.2890 - mean_squared_error: 1.2890 - val_loss: 3.1878 - val_mean_squared_error: 3.1878\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.1995 - mean_squared_error: 1.1995 - val_loss: 2.9959 - val_mean_squared_error: 2.9959\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.2233 - mean_squared_error: 1.2233 - val_loss: 3.0650 - val_mean_squared_error: 3.0650\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1787 - mean_squared_error: 1.1787 - val_loss: 3.0646 - val_mean_squared_error: 3.0646\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.2594 - mean_squared_error: 1.2594 - val_loss: 3.0179 - val_mean_squared_error: 3.0179\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.1424 - mean_squared_error: 1.1424 - val_loss: 3.1534 - val_mean_squared_error: 3.1534\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1096 - mean_squared_error: 1.1096 - val_loss: 3.4802 - val_mean_squared_error: 3.4802\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 802us/sample - loss: 1.2354 - mean_squared_error: 1.2354 - val_loss: 3.0880 - val_mean_squared_error: 3.0880\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 1.2006 - mean_squared_error: 1.2006 - val_loss: 3.1025 - val_mean_squared_error: 3.1025\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.0859 - mean_squared_error: 1.0859 - val_loss: 3.0455 - val_mean_squared_error: 3.0455\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1751 - mean_squared_error: 1.1751 - val_loss: 23866.1084 - val_mean_squared_error: 23866.1074\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.0797 - mean_squared_error: 1.0797 - val_loss: 3.2037 - val_mean_squared_error: 3.2037\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 1.1085 - mean_squared_error: 1.1085 - val_loss: 3.0217 - val_mean_squared_error: 3.0217\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.1737 - mean_squared_error: 1.1737 - val_loss: 3.3149 - val_mean_squared_error: 3.3149\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1195 - mean_squared_error: 1.1195 - val_loss: 3.4969 - val_mean_squared_error: 3.4969\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 3.0631 - val_mean_squared_error: 3.0631\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.1136 - mean_squared_error: 1.1136 - val_loss: 35476.6929 - val_mean_squared_error: 35476.6953\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.1708 - mean_squared_error: 1.1708 - val_loss: 36637.4344 - val_mean_squared_error: 36637.4375\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.0966 - mean_squared_error: 1.0966 - val_loss: 26816.9741 - val_mean_squared_error: 26816.9785\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 802us/sample - loss: 1.0882 - mean_squared_error: 1.0882 - val_loss: 10741.5252 - val_mean_squared_error: 10741.5254\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.0596 - mean_squared_error: 1.0596 - val_loss: 871.0453 - val_mean_squared_error: 871.0453\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.1238 - mean_squared_error: 1.1238 - val_loss: 47610.7632 - val_mean_squared_error: 47610.7656\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 1.0222 - mean_squared_error: 1.0222 - val_loss: 2168.6409 - val_mean_squared_error: 2168.6411\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.0480 - mean_squared_error: 1.0480 - val_loss: 3.0590 - val_mean_squared_error: 3.0590\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.0419 - mean_squared_error: 1.0419 - val_loss: 1526.0547 - val_mean_squared_error: 1526.0546\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.1167 - mean_squared_error: 1.1167 - val_loss: 74284.3815 - val_mean_squared_error: 74284.3828\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 1.0451 - mean_squared_error: 1.0451 - val_loss: 653.7344 - val_mean_squared_error: 653.7343\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 1.0269 - mean_squared_error: 1.0269 - val_loss: 803.2656 - val_mean_squared_error: 803.2655\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1005 - mean_squared_error: 1.1005 - val_loss: 8632.4789 - val_mean_squared_error: 8632.4785\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.0656 - mean_squared_error: 1.0656 - val_loss: 39107.0405 - val_mean_squared_error: 39107.0391\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 0.9551 - mean_squared_error: 0.9551 - val_loss: 109455.5919 - val_mean_squared_error: 109455.5859\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.0466 - mean_squared_error: 1.0466 - val_loss: 157.0315 - val_mean_squared_error: 157.0315\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.0861 - mean_squared_error: 1.0861 - val_loss: 45.6992 - val_mean_squared_error: 45.6992\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.0820 - mean_squared_error: 1.0820 - val_loss: 69309.5230 - val_mean_squared_error: 69309.5234\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.1094 - mean_squared_error: 1.1094 - val_loss: 374.2673 - val_mean_squared_error: 374.2673\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 0.9435 - mean_squared_error: 0.9435 - val_loss: 13.2202 - val_mean_squared_error: 13.2202\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.0778 - mean_squared_error: 1.0778 - val_loss: 77229.9601 - val_mean_squared_error: 77229.9609\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 804us/sample - loss: 0.9833 - mean_squared_error: 0.9833 - val_loss: 14.4667 - val_mean_squared_error: 14.4667\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 805us/sample - loss: 1.0565 - mean_squared_error: 1.0565 - val_loss: 2.9156 - val_mean_squared_error: 2.9156\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 0.9660 - mean_squared_error: 0.9660 - val_loss: 194.7321 - val_mean_squared_error: 194.7321\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 0.9015 - mean_squared_error: 0.9015 - val_loss: 47831.3784 - val_mean_squared_error: 47831.3789\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 0.9231 - mean_squared_error: 0.9231 - val_loss: 243.2538 - val_mean_squared_error: 243.2538\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 3.0351 - val_mean_squared_error: 3.0351\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 806us/sample - loss: 0.8930 - mean_squared_error: 0.8930 - val_loss: 3.2066 - val_mean_squared_error: 3.2066\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 0.9374 - mean_squared_error: 0.9374 - val_loss: 16631.5550 - val_mean_squared_error: 16631.5527\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 0.9476 - mean_squared_error: 0.9476 - val_loss: 136685.4978 - val_mean_squared_error: 136685.5000\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 0.9906 - mean_squared_error: 0.9906 - val_loss: 3.0103 - val_mean_squared_error: 3.0103\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 0.9564 - mean_squared_error: 0.9564 - val_loss: 2.8796 - val_mean_squared_error: 2.8796\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 0.9408 - mean_squared_error: 0.9408 - val_loss: 3.9437 - val_mean_squared_error: 3.9437\n",
            "==================================================\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_26 (Conv2D)           (None, 96, 96, 12)        108       \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 96, 96, 12)        1296      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 96, 96, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 48, 48, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 48, 48, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 48, 48, 24)        2592      \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 48, 48, 24)        5184      \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 48, 48, 24)        5184      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 48, 48, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 24, 24, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 24, 24, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 24, 24, 48)        10368     \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 24, 24, 48)        20736     \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 24, 24, 48)        20736     \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 24, 24, 48)        20736     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 24, 24, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 12, 12, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 12, 12, 48)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 12, 12, 96)        41472     \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 12, 12, 96)        82944     \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 12, 12, 96)        82944     \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 12, 12, 96)        82944     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 12, 12, 96)        384       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 6, 6, 96)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 3456)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 500)               1728500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 2,375,994\n",
            "Trainable params: 2,373,634\n",
            "Non-trainable params: 2,360\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 295.3867 - mean_squared_error: 295.3867 - val_loss: 9236.9352 - val_mean_squared_error: 9236.9355\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 34.3450 - mean_squared_error: 34.3450 - val_loss: 307.6920 - val_mean_squared_error: 307.6920\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 22.6853 - mean_squared_error: 22.6853 - val_loss: 60.3483 - val_mean_squared_error: 60.3483\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 22.9730 - mean_squared_error: 22.9730 - val_loss: 41.9219 - val_mean_squared_error: 41.9219\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 21.3694 - mean_squared_error: 21.3694 - val_loss: 18.3245 - val_mean_squared_error: 18.3245\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 698us/sample - loss: 17.5316 - mean_squared_error: 17.5316 - val_loss: 14.5705 - val_mean_squared_error: 14.5705\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 14.8686 - mean_squared_error: 14.8686 - val_loss: 11.3434 - val_mean_squared_error: 11.3434\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 15.5418 - mean_squared_error: 15.5418 - val_loss: 10.4975 - val_mean_squared_error: 10.4975\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 14.9424 - mean_squared_error: 14.9424 - val_loss: 16.9278 - val_mean_squared_error: 16.9278\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 14.0260 - mean_squared_error: 14.0260 - val_loss: 11.4719 - val_mean_squared_error: 11.4719\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 13.5565 - mean_squared_error: 13.5565 - val_loss: 10.1585 - val_mean_squared_error: 10.1585\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 13.3641 - mean_squared_error: 13.3641 - val_loss: 10.8497 - val_mean_squared_error: 10.8497\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 12.5227 - mean_squared_error: 12.5227 - val_loss: 10.4622 - val_mean_squared_error: 10.4622\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 12.4065 - mean_squared_error: 12.4065 - val_loss: 10.4789 - val_mean_squared_error: 10.4789\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 11.6474 - mean_squared_error: 11.6474 - val_loss: 9.7779 - val_mean_squared_error: 9.7779\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 674us/sample - loss: 11.5767 - mean_squared_error: 11.5767 - val_loss: 10.5764 - val_mean_squared_error: 10.5764\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 11.7368 - mean_squared_error: 11.7368 - val_loss: 10.4414 - val_mean_squared_error: 10.4414\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 11.1159 - mean_squared_error: 11.1159 - val_loss: 10.0077 - val_mean_squared_error: 10.0077\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 10.8677 - mean_squared_error: 10.8677 - val_loss: 10.0170 - val_mean_squared_error: 10.0170\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 10.6900 - mean_squared_error: 10.6900 - val_loss: 9.8660 - val_mean_squared_error: 9.8660\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 10.4956 - mean_squared_error: 10.4956 - val_loss: 10.1080 - val_mean_squared_error: 10.1080\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 676us/sample - loss: 10.4803 - mean_squared_error: 10.4803 - val_loss: 9.4664 - val_mean_squared_error: 9.4664\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 10.1762 - mean_squared_error: 10.1762 - val_loss: 9.6752 - val_mean_squared_error: 9.6753\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 10.3740 - mean_squared_error: 10.3740 - val_loss: 9.8362 - val_mean_squared_error: 9.8362\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 10.3226 - mean_squared_error: 10.3226 - val_loss: 9.6971 - val_mean_squared_error: 9.6971\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 9.9601 - mean_squared_error: 9.9601 - val_loss: 9.5303 - val_mean_squared_error: 9.5303\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 9.7363 - mean_squared_error: 9.7363 - val_loss: 9.9116 - val_mean_squared_error: 9.9116\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 9.7315 - mean_squared_error: 9.7315 - val_loss: 9.7130 - val_mean_squared_error: 9.7130\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 9.6488 - mean_squared_error: 9.6488 - val_loss: 9.3109 - val_mean_squared_error: 9.3109\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 9.3817 - mean_squared_error: 9.3817 - val_loss: 9.2722 - val_mean_squared_error: 9.2722\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 9.2516 - mean_squared_error: 9.2516 - val_loss: 9.2699 - val_mean_squared_error: 9.2699\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 9.2528 - mean_squared_error: 9.2528 - val_loss: 9.4771 - val_mean_squared_error: 9.4771\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 9.4395 - mean_squared_error: 9.4395 - val_loss: 9.8675 - val_mean_squared_error: 9.8675\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 9.2201 - mean_squared_error: 9.2201 - val_loss: 9.4991 - val_mean_squared_error: 9.4991\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 8.9449 - mean_squared_error: 8.9449 - val_loss: 8.7681 - val_mean_squared_error: 8.7681\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 8.7441 - mean_squared_error: 8.7441 - val_loss: 8.7866 - val_mean_squared_error: 8.7866\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 8.5029 - mean_squared_error: 8.5029 - val_loss: 9.9076 - val_mean_squared_error: 9.9076\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 8.6304 - mean_squared_error: 8.6304 - val_loss: 9.7449 - val_mean_squared_error: 9.7449\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 8.4540 - mean_squared_error: 8.4540 - val_loss: 9.0485 - val_mean_squared_error: 9.0485\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 8.2488 - mean_squared_error: 8.2488 - val_loss: 8.6751 - val_mean_squared_error: 8.6751\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 8.1513 - mean_squared_error: 8.1513 - val_loss: 8.9309 - val_mean_squared_error: 8.9309\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 7.9815 - mean_squared_error: 7.9815 - val_loss: 8.9913 - val_mean_squared_error: 8.9913\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 7.8932 - mean_squared_error: 7.8932 - val_loss: 8.6362 - val_mean_squared_error: 8.6362\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 7.7659 - mean_squared_error: 7.7659 - val_loss: 8.1110 - val_mean_squared_error: 8.1110\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 7.8150 - mean_squared_error: 7.8150 - val_loss: 8.3800 - val_mean_squared_error: 8.3800\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 7.7551 - mean_squared_error: 7.7551 - val_loss: 7.9684 - val_mean_squared_error: 7.9684\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 7.7872 - mean_squared_error: 7.7872 - val_loss: 8.3191 - val_mean_squared_error: 8.3191\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 7.3031 - mean_squared_error: 7.3031 - val_loss: 8.7178 - val_mean_squared_error: 8.7178\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 7.1055 - mean_squared_error: 7.1055 - val_loss: 7.8495 - val_mean_squared_error: 7.8495\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 7.1257 - mean_squared_error: 7.1257 - val_loss: 8.1509 - val_mean_squared_error: 8.1509\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 6.9093 - mean_squared_error: 6.9093 - val_loss: 234.1343 - val_mean_squared_error: 234.1343\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 6.6858 - mean_squared_error: 6.6858 - val_loss: 8.2309 - val_mean_squared_error: 8.2309\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 6.6403 - mean_squared_error: 6.6403 - val_loss: 7.7484 - val_mean_squared_error: 7.7484\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 6.3503 - mean_squared_error: 6.3503 - val_loss: 8.0442 - val_mean_squared_error: 8.0442\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 6.2079 - mean_squared_error: 6.2079 - val_loss: 7.4301 - val_mean_squared_error: 7.4301\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 5.9782 - mean_squared_error: 5.9782 - val_loss: 15.7927 - val_mean_squared_error: 15.7927\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 5.7692 - mean_squared_error: 5.7692 - val_loss: 19.1691 - val_mean_squared_error: 19.1691\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 5.6887 - mean_squared_error: 5.6887 - val_loss: 7.2128 - val_mean_squared_error: 7.2128\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 5.4483 - mean_squared_error: 5.4483 - val_loss: 6.5582 - val_mean_squared_error: 6.5582\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 5.2268 - mean_squared_error: 5.2268 - val_loss: 5.7805 - val_mean_squared_error: 5.7805\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 5.0578 - mean_squared_error: 5.0578 - val_loss: 5.9147 - val_mean_squared_error: 5.9147\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 4.8533 - mean_squared_error: 4.8533 - val_loss: 6.2439 - val_mean_squared_error: 6.2439\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 4.6874 - mean_squared_error: 4.6874 - val_loss: 6.1701 - val_mean_squared_error: 6.1701\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 4.5953 - mean_squared_error: 4.5953 - val_loss: 5.3672 - val_mean_squared_error: 5.3672\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 679us/sample - loss: 4.5002 - mean_squared_error: 4.5002 - val_loss: 4.9741 - val_mean_squared_error: 4.9741\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 4.2151 - mean_squared_error: 4.2151 - val_loss: 5.1882 - val_mean_squared_error: 5.1882\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 4.0288 - mean_squared_error: 4.0288 - val_loss: 5.2679 - val_mean_squared_error: 5.2679\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 4.0107 - mean_squared_error: 4.0107 - val_loss: 5.0223 - val_mean_squared_error: 5.0223\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 3.9471 - mean_squared_error: 3.9471 - val_loss: 4.8451 - val_mean_squared_error: 4.8451\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 3.9406 - mean_squared_error: 3.9406 - val_loss: 4.9823 - val_mean_squared_error: 4.9823\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 678us/sample - loss: 3.9192 - mean_squared_error: 3.9192 - val_loss: 4.7936 - val_mean_squared_error: 4.7936\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 3.6442 - mean_squared_error: 3.6442 - val_loss: 5.1146 - val_mean_squared_error: 5.1146\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 3.5257 - mean_squared_error: 3.5257 - val_loss: 4.6329 - val_mean_squared_error: 4.6329\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 3.4635 - mean_squared_error: 3.4635 - val_loss: 4.8771 - val_mean_squared_error: 4.8771\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 3.3157 - mean_squared_error: 3.3157 - val_loss: 4.5077 - val_mean_squared_error: 4.5077\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 3.3428 - mean_squared_error: 3.3428 - val_loss: 4.8979 - val_mean_squared_error: 4.8979\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 3.5848 - mean_squared_error: 3.5848 - val_loss: 4.5981 - val_mean_squared_error: 4.5981\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 3.2131 - mean_squared_error: 3.2131 - val_loss: 4.3424 - val_mean_squared_error: 4.3424\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 3.1149 - mean_squared_error: 3.1149 - val_loss: 6.1179 - val_mean_squared_error: 6.1179\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 3.1213 - mean_squared_error: 3.1213 - val_loss: 4.4562 - val_mean_squared_error: 4.4562\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 2.8890 - mean_squared_error: 2.8890 - val_loss: 4.2072 - val_mean_squared_error: 4.2072\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.9170 - mean_squared_error: 2.9170 - val_loss: 7.2147 - val_mean_squared_error: 7.2147\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 2.7476 - mean_squared_error: 2.7476 - val_loss: 4.3545 - val_mean_squared_error: 4.3545\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 2.7857 - mean_squared_error: 2.7857 - val_loss: 4.3531 - val_mean_squared_error: 4.3531\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.7746 - mean_squared_error: 2.7746 - val_loss: 4.3065 - val_mean_squared_error: 4.3065\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.7317 - mean_squared_error: 2.7317 - val_loss: 4.2460 - val_mean_squared_error: 4.2460\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 2.6136 - mean_squared_error: 2.6136 - val_loss: 4.1005 - val_mean_squared_error: 4.1005\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.5347 - mean_squared_error: 2.5347 - val_loss: 4.3891 - val_mean_squared_error: 4.3891\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 2.5078 - mean_squared_error: 2.5078 - val_loss: 4.3924 - val_mean_squared_error: 4.3924\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.6462 - mean_squared_error: 2.6462 - val_loss: 4.1673 - val_mean_squared_error: 4.1673\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 2.5379 - mean_squared_error: 2.5379 - val_loss: 4.0192 - val_mean_squared_error: 4.0192\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.4444 - mean_squared_error: 2.4444 - val_loss: 4.5141 - val_mean_squared_error: 4.5141\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 2.4897 - mean_squared_error: 2.4897 - val_loss: 3.9326 - val_mean_squared_error: 3.9326\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 2.3469 - mean_squared_error: 2.3469 - val_loss: 4.3309 - val_mean_squared_error: 4.3309\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.2185 - mean_squared_error: 2.2185 - val_loss: 4.0538 - val_mean_squared_error: 4.0538\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.2107 - mean_squared_error: 2.2107 - val_loss: 3.8282 - val_mean_squared_error: 3.8282\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 2.1780 - mean_squared_error: 2.1780 - val_loss: 3.7901 - val_mean_squared_error: 3.7901\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 2.2037 - mean_squared_error: 2.2037 - val_loss: 3.9106 - val_mean_squared_error: 3.9106\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 2.2573 - mean_squared_error: 2.2573 - val_loss: 4.0447 - val_mean_squared_error: 4.0447\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 2.2284 - mean_squared_error: 2.2284 - val_loss: 4.0855 - val_mean_squared_error: 4.0855\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.1440 - mean_squared_error: 2.1440 - val_loss: 4.0782 - val_mean_squared_error: 4.0782\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 2.0369 - mean_squared_error: 2.0369 - val_loss: 81.6499 - val_mean_squared_error: 81.6499\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 1.9784 - mean_squared_error: 1.9784 - val_loss: 4.0817 - val_mean_squared_error: 4.0817\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 664us/sample - loss: 1.9802 - mean_squared_error: 1.9802 - val_loss: 65.9302 - val_mean_squared_error: 65.9302\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 2.0484 - mean_squared_error: 2.0484 - val_loss: 7808.8561 - val_mean_squared_error: 7808.8560\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.0533 - mean_squared_error: 2.0533 - val_loss: 441.2715 - val_mean_squared_error: 441.2714\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.8967 - mean_squared_error: 1.8967 - val_loss: 4.0916 - val_mean_squared_error: 4.0916\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.8581 - mean_squared_error: 1.8581 - val_loss: 3.8710 - val_mean_squared_error: 3.8710\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.9011 - mean_squared_error: 1.9011 - val_loss: 3.7950 - val_mean_squared_error: 3.7950\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.8702 - mean_squared_error: 1.8702 - val_loss: 3.8003 - val_mean_squared_error: 3.8003\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.8868 - mean_squared_error: 1.8868 - val_loss: 4.1000 - val_mean_squared_error: 4.1000\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.8666 - mean_squared_error: 1.8666 - val_loss: 4.0004 - val_mean_squared_error: 4.0004\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.9491 - mean_squared_error: 1.9491 - val_loss: 3.8031 - val_mean_squared_error: 3.8031\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 674us/sample - loss: 1.8564 - mean_squared_error: 1.8564 - val_loss: 4.0019 - val_mean_squared_error: 4.0019\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.8291 - mean_squared_error: 1.8291 - val_loss: 3.8625 - val_mean_squared_error: 3.8625\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.8748 - mean_squared_error: 1.8748 - val_loss: 3.8756 - val_mean_squared_error: 3.8756\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.8084 - mean_squared_error: 1.8084 - val_loss: 3.7936 - val_mean_squared_error: 3.7936\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.8669 - mean_squared_error: 1.8669 - val_loss: 2633.0763 - val_mean_squared_error: 2633.0762\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.7266 - mean_squared_error: 1.7266 - val_loss: 3.8110 - val_mean_squared_error: 3.8110\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.6579 - mean_squared_error: 1.6579 - val_loss: 3.5687 - val_mean_squared_error: 3.5687\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.6801 - mean_squared_error: 1.6801 - val_loss: 3.6569 - val_mean_squared_error: 3.6569\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.5550 - mean_squared_error: 1.5550 - val_loss: 3.6782 - val_mean_squared_error: 3.6782\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 1.5991 - mean_squared_error: 1.5991 - val_loss: 3.7250 - val_mean_squared_error: 3.7250\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 674us/sample - loss: 1.6562 - mean_squared_error: 1.6562 - val_loss: 3.7960 - val_mean_squared_error: 3.7960\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.6527 - mean_squared_error: 1.6527 - val_loss: 3.7688 - val_mean_squared_error: 3.7688\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.6494 - mean_squared_error: 1.6494 - val_loss: 3.5124 - val_mean_squared_error: 3.5124\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.6948 - mean_squared_error: 1.6948 - val_loss: 3.6836 - val_mean_squared_error: 3.6836\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.7256 - mean_squared_error: 1.7256 - val_loss: 86.6729 - val_mean_squared_error: 86.6729\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5743 - mean_squared_error: 1.5743 - val_loss: 3.6062 - val_mean_squared_error: 3.6062\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.6584 - mean_squared_error: 1.6584 - val_loss: 3.5668 - val_mean_squared_error: 3.5668\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.6191 - mean_squared_error: 1.6191 - val_loss: 3.7927 - val_mean_squared_error: 3.7927\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.5214 - mean_squared_error: 1.5214 - val_loss: 3.7203 - val_mean_squared_error: 3.7203\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.6150 - mean_squared_error: 1.6150 - val_loss: 3.5951 - val_mean_squared_error: 3.5951\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5064 - mean_squared_error: 1.5064 - val_loss: 3.7012 - val_mean_squared_error: 3.7012\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.4827 - mean_squared_error: 1.4827 - val_loss: 9.7069 - val_mean_squared_error: 9.7069\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.4519 - mean_squared_error: 1.4519 - val_loss: 3.7172 - val_mean_squared_error: 3.7172\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.4206 - mean_squared_error: 1.4206 - val_loss: 3.5244 - val_mean_squared_error: 3.5244\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5409 - mean_squared_error: 1.5409 - val_loss: 3.3099 - val_mean_squared_error: 3.3099\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.3786 - mean_squared_error: 1.3786 - val_loss: 3.8577 - val_mean_squared_error: 3.8577\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.4273 - mean_squared_error: 1.4273 - val_loss: 3.6116 - val_mean_squared_error: 3.6116\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 1.3729 - mean_squared_error: 1.3729 - val_loss: 3.7245 - val_mean_squared_error: 3.7245\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.4169 - mean_squared_error: 1.4169 - val_loss: 2717.3588 - val_mean_squared_error: 2717.3591\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.3880 - mean_squared_error: 1.3880 - val_loss: 3.4216 - val_mean_squared_error: 3.4216\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2965 - mean_squared_error: 1.2965 - val_loss: 1753.7319 - val_mean_squared_error: 1753.7317\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.3783 - mean_squared_error: 1.3783 - val_loss: 141.2584 - val_mean_squared_error: 141.2584\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.3444 - mean_squared_error: 1.3444 - val_loss: 3.3636 - val_mean_squared_error: 3.3636\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.2446 - mean_squared_error: 1.2446 - val_loss: 3.3369 - val_mean_squared_error: 3.3369\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.3954 - mean_squared_error: 1.3954 - val_loss: 3.4686 - val_mean_squared_error: 3.4686\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.3657 - mean_squared_error: 1.3657 - val_loss: 3.5965 - val_mean_squared_error: 3.5965\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2750 - mean_squared_error: 1.2750 - val_loss: 3.7833 - val_mean_squared_error: 3.7833\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.3049 - mean_squared_error: 1.3049 - val_loss: 3.2883 - val_mean_squared_error: 3.2883\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.2864 - mean_squared_error: 1.2864 - val_loss: 3.4148 - val_mean_squared_error: 3.4148\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2661 - mean_squared_error: 1.2661 - val_loss: 3.3693 - val_mean_squared_error: 3.3693\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.2629 - mean_squared_error: 1.2629 - val_loss: 3.3451 - val_mean_squared_error: 3.3451\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.2083 - mean_squared_error: 1.2083 - val_loss: 3.4906 - val_mean_squared_error: 3.4906\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.1774 - mean_squared_error: 1.1774 - val_loss: 3.2732 - val_mean_squared_error: 3.2732\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.2880 - mean_squared_error: 1.2880 - val_loss: 3.4267 - val_mean_squared_error: 3.4267\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.2194 - mean_squared_error: 1.2194 - val_loss: 3.7043 - val_mean_squared_error: 3.7043\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1976 - mean_squared_error: 1.1976 - val_loss: 3.4881 - val_mean_squared_error: 3.4881\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.2316 - mean_squared_error: 1.2316 - val_loss: 3.4473 - val_mean_squared_error: 3.4473\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.2108 - mean_squared_error: 1.2108 - val_loss: 3.5971 - val_mean_squared_error: 3.5971\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.1832 - mean_squared_error: 1.1832 - val_loss: 3.3377 - val_mean_squared_error: 3.3377\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2426 - mean_squared_error: 1.2426 - val_loss: 3.4465 - val_mean_squared_error: 3.4465\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.1531 - mean_squared_error: 1.1531 - val_loss: 3.4300 - val_mean_squared_error: 3.4300\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.1578 - mean_squared_error: 1.1578 - val_loss: 3.3806 - val_mean_squared_error: 3.3806\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.1616 - mean_squared_error: 1.1616 - val_loss: 3.5081 - val_mean_squared_error: 3.5081\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.1591 - mean_squared_error: 1.1591 - val_loss: 3.2522 - val_mean_squared_error: 3.2522\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1844 - mean_squared_error: 1.1844 - val_loss: 3.4088 - val_mean_squared_error: 3.4088\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.1224 - mean_squared_error: 1.1224 - val_loss: 3.3923 - val_mean_squared_error: 3.3923\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1540 - mean_squared_error: 1.1540 - val_loss: 3.2753 - val_mean_squared_error: 3.2753\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1488 - mean_squared_error: 1.1488 - val_loss: 3.4675 - val_mean_squared_error: 3.4675\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 1.0757 - mean_squared_error: 1.0757 - val_loss: 3.3525 - val_mean_squared_error: 3.3525\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.1425 - mean_squared_error: 1.1425 - val_loss: 3.2989 - val_mean_squared_error: 3.2989\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0608 - mean_squared_error: 1.0608 - val_loss: 3.4753 - val_mean_squared_error: 3.4753\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 1.1655 - mean_squared_error: 1.1655 - val_loss: 3.2971 - val_mean_squared_error: 3.2971\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 3.3095 - val_mean_squared_error: 3.3095\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.0244 - mean_squared_error: 1.0244 - val_loss: 3.2989 - val_mean_squared_error: 3.2989\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.0577 - mean_squared_error: 1.0577 - val_loss: 3.3215 - val_mean_squared_error: 3.3215\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.1047 - mean_squared_error: 1.1047 - val_loss: 63.3033 - val_mean_squared_error: 63.3033\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 1.1211 - mean_squared_error: 1.1211 - val_loss: 3.3903 - val_mean_squared_error: 3.3903\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 679us/sample - loss: 1.0809 - mean_squared_error: 1.0809 - val_loss: 3.3136 - val_mean_squared_error: 3.3136\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.1437 - mean_squared_error: 1.1437 - val_loss: 3.3838 - val_mean_squared_error: 3.3838\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.0242 - mean_squared_error: 1.0242 - val_loss: 3.1915 - val_mean_squared_error: 3.1915\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1158 - mean_squared_error: 1.1158 - val_loss: 3.5232 - val_mean_squared_error: 3.5232\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.0650 - mean_squared_error: 1.0650 - val_loss: 3.2551 - val_mean_squared_error: 3.2551\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.1304 - mean_squared_error: 1.1304 - val_loss: 4.1868 - val_mean_squared_error: 4.1868\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 0.9985 - mean_squared_error: 0.9985 - val_loss: 3.1764 - val_mean_squared_error: 3.1764\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 0.9713 - mean_squared_error: 0.9713 - val_loss: 3.3239 - val_mean_squared_error: 3.3239\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 1.0584 - mean_squared_error: 1.0584 - val_loss: 3.2343 - val_mean_squared_error: 3.2343\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.0020 - mean_squared_error: 1.0020 - val_loss: 3.2821 - val_mean_squared_error: 3.2821\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 3.2058 - val_mean_squared_error: 3.2058\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0511 - mean_squared_error: 1.0511 - val_loss: 3.1859 - val_mean_squared_error: 3.1859\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.0539 - mean_squared_error: 1.0539 - val_loss: 3.3138 - val_mean_squared_error: 3.3138\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0323 - mean_squared_error: 1.0323 - val_loss: 3.3585 - val_mean_squared_error: 3.3585\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 0.9931 - mean_squared_error: 0.9931 - val_loss: 3.3982 - val_mean_squared_error: 3.3982\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 0.9934 - mean_squared_error: 0.9934 - val_loss: 3.6045 - val_mean_squared_error: 3.6045\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.0941 - mean_squared_error: 1.0941 - val_loss: 3.3856 - val_mean_squared_error: 3.3856\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 0.9587 - mean_squared_error: 0.9587 - val_loss: 3.1559 - val_mean_squared_error: 3.1559\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.0729 - mean_squared_error: 1.0729 - val_loss: 3.5053 - val_mean_squared_error: 3.5053\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.0862 - mean_squared_error: 1.0862 - val_loss: 3.4565 - val_mean_squared_error: 3.4565\n",
            "==================================================\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_39 (Conv2D)           (None, 96, 96, 16)        144       \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 96, 96, 16)        2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 96, 96, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 48, 48, 32)        4608      \n",
            "_________________________________________________________________\n",
            "conv2d_42 (Conv2D)           (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 24, 24, 64)        18432     \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 12, 12, 128)       73728     \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 500)               2304500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,245,598\n",
            "Trainable params: 3,243,118\n",
            "Non-trainable params: 2,480\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 1ms/sample - loss: 273.8910 - mean_squared_error: 273.8911 - val_loss: 13333.6746 - val_mean_squared_error: 13333.6748\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 24.4881 - mean_squared_error: 24.4881 - val_loss: 109.3339 - val_mean_squared_error: 109.3339\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 2s 826us/sample - loss: 16.6651 - mean_squared_error: 16.6651 - val_loss: 60.4410 - val_mean_squared_error: 60.4410\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 2s 836us/sample - loss: 17.0565 - mean_squared_error: 17.0565 - val_loss: 33.8199 - val_mean_squared_error: 33.8199\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 15.1106 - mean_squared_error: 15.1106 - val_loss: 22.3857 - val_mean_squared_error: 22.3857\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 14.1056 - mean_squared_error: 14.1056 - val_loss: 23.4611 - val_mean_squared_error: 23.4611\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 15.3339 - mean_squared_error: 15.3339 - val_loss: 14.3752 - val_mean_squared_error: 14.3752\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 13.6443 - mean_squared_error: 13.6443 - val_loss: 13.4047 - val_mean_squared_error: 13.4047\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 12.8736 - mean_squared_error: 12.8736 - val_loss: 14.8578 - val_mean_squared_error: 14.8578\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 12.6316 - mean_squared_error: 12.6316 - val_loss: 11.7870 - val_mean_squared_error: 11.7870\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 12.4650 - mean_squared_error: 12.4650 - val_loss: 11.8320 - val_mean_squared_error: 11.8320\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 12.4836 - mean_squared_error: 12.4836 - val_loss: 13.8243 - val_mean_squared_error: 13.8243\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 11.8246 - mean_squared_error: 11.8246 - val_loss: 14.0607 - val_mean_squared_error: 14.0606\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 11.5956 - mean_squared_error: 11.5956 - val_loss: 11.9650 - val_mean_squared_error: 11.9650\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 11.6570 - mean_squared_error: 11.6570 - val_loss: 10.5140 - val_mean_squared_error: 10.5140\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 11.6803 - mean_squared_error: 11.6803 - val_loss: 9.8737 - val_mean_squared_error: 9.8737\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 11.3569 - mean_squared_error: 11.3569 - val_loss: 10.3258 - val_mean_squared_error: 10.3258\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 11.3322 - mean_squared_error: 11.3322 - val_loss: 10.3475 - val_mean_squared_error: 10.3475\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 2s 825us/sample - loss: 11.1314 - mean_squared_error: 11.1314 - val_loss: 11.4749 - val_mean_squared_error: 11.4749\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 10.7471 - mean_squared_error: 10.7471 - val_loss: 10.4630 - val_mean_squared_error: 10.4630\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 2s 830us/sample - loss: 11.2733 - mean_squared_error: 11.2733 - val_loss: 10.1211 - val_mean_squared_error: 10.1211\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 10.7494 - mean_squared_error: 10.7494 - val_loss: 10.4536 - val_mean_squared_error: 10.4536\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 10.2762 - mean_squared_error: 10.2762 - val_loss: 9.6162 - val_mean_squared_error: 9.6162\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 11.2734 - mean_squared_error: 11.2734 - val_loss: 11.8124 - val_mean_squared_error: 11.8124\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 10.4387 - mean_squared_error: 10.4387 - val_loss: 10.6317 - val_mean_squared_error: 10.6317\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 10.3559 - mean_squared_error: 10.3559 - val_loss: 9.6675 - val_mean_squared_error: 9.6675\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 10.0784 - mean_squared_error: 10.0784 - val_loss: 9.5796 - val_mean_squared_error: 9.5796\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 10.0547 - mean_squared_error: 10.0547 - val_loss: 9.6371 - val_mean_squared_error: 9.6371\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 10.0353 - mean_squared_error: 10.0353 - val_loss: 10.0584 - val_mean_squared_error: 10.0584\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 9.6667 - mean_squared_error: 9.6667 - val_loss: 9.5338 - val_mean_squared_error: 9.5338\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 2s 827us/sample - loss: 9.8177 - mean_squared_error: 9.8177 - val_loss: 9.6866 - val_mean_squared_error: 9.6866\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 2s 829us/sample - loss: 9.9144 - mean_squared_error: 9.9144 - val_loss: 9.7697 - val_mean_squared_error: 9.7697\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 9.5503 - mean_squared_error: 9.5503 - val_loss: 11.0688 - val_mean_squared_error: 11.0688\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 9.2082 - mean_squared_error: 9.2082 - val_loss: 9.3016 - val_mean_squared_error: 9.3016\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 9.2054 - mean_squared_error: 9.2054 - val_loss: 8.8322 - val_mean_squared_error: 8.8322\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 9.1511 - mean_squared_error: 9.1511 - val_loss: 9.3421 - val_mean_squared_error: 9.3421\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 8.8838 - mean_squared_error: 8.8838 - val_loss: 9.5361 - val_mean_squared_error: 9.5361\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 8.8126 - mean_squared_error: 8.8126 - val_loss: 11.6237 - val_mean_squared_error: 11.6237\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 8.6780 - mean_squared_error: 8.6780 - val_loss: 8.5768 - val_mean_squared_error: 8.5768\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 8.8054 - mean_squared_error: 8.8054 - val_loss: 11.8476 - val_mean_squared_error: 11.8476\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 8.3949 - mean_squared_error: 8.3949 - val_loss: 22.4104 - val_mean_squared_error: 22.4104\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 8.2722 - mean_squared_error: 8.2722 - val_loss: 12.2989 - val_mean_squared_error: 12.2989\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 8.0750 - mean_squared_error: 8.0750 - val_loss: 9.7273 - val_mean_squared_error: 9.7273\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 8.3334 - mean_squared_error: 8.3334 - val_loss: 9.5266 - val_mean_squared_error: 9.5266\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 7.8824 - mean_squared_error: 7.8824 - val_loss: 11.5723 - val_mean_squared_error: 11.5723\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 7.7350 - mean_squared_error: 7.7350 - val_loss: 8.5104 - val_mean_squared_error: 8.5104\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 7.7732 - mean_squared_error: 7.7732 - val_loss: 8.4390 - val_mean_squared_error: 8.4390\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 7.5493 - mean_squared_error: 7.5493 - val_loss: 9.8222 - val_mean_squared_error: 9.8222\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 7.2115 - mean_squared_error: 7.2115 - val_loss: 8.4051 - val_mean_squared_error: 8.4051\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 7.1819 - mean_squared_error: 7.1819 - val_loss: 19.3905 - val_mean_squared_error: 19.3905\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 8.1758 - mean_squared_error: 8.1758 - val_loss: 15.4524 - val_mean_squared_error: 15.4524\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 7.3415 - mean_squared_error: 7.3415 - val_loss: 8.2965 - val_mean_squared_error: 8.2965\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 7.0305 - mean_squared_error: 7.0305 - val_loss: 8.4319 - val_mean_squared_error: 8.4319\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 7.0299 - mean_squared_error: 7.0299 - val_loss: 7.8114 - val_mean_squared_error: 7.8114\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 6.4924 - mean_squared_error: 6.4924 - val_loss: 7.3388 - val_mean_squared_error: 7.3388\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 6.5927 - mean_squared_error: 6.5927 - val_loss: 7.4257 - val_mean_squared_error: 7.4257\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 6.4847 - mean_squared_error: 6.4847 - val_loss: 6.8693 - val_mean_squared_error: 6.8693\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 6.2490 - mean_squared_error: 6.2490 - val_loss: 7.2853 - val_mean_squared_error: 7.2853\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 6.0613 - mean_squared_error: 6.0613 - val_loss: 6.4082 - val_mean_squared_error: 6.4082\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 5.6935 - mean_squared_error: 5.6935 - val_loss: 6.7505 - val_mean_squared_error: 6.7505\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 5.6614 - mean_squared_error: 5.6614 - val_loss: 6.6185 - val_mean_squared_error: 6.6185\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 5.4064 - mean_squared_error: 5.4064 - val_loss: 5.9116 - val_mean_squared_error: 5.9116\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 5.1849 - mean_squared_error: 5.1849 - val_loss: 5.8456 - val_mean_squared_error: 5.8456\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 5.1730 - mean_squared_error: 5.1730 - val_loss: 6.6560 - val_mean_squared_error: 6.6560\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 4.9872 - mean_squared_error: 4.9872 - val_loss: 6.2057 - val_mean_squared_error: 6.2057\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 4.7201 - mean_squared_error: 4.7201 - val_loss: 6.0045 - val_mean_squared_error: 6.0045\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 4.6621 - mean_squared_error: 4.6621 - val_loss: 7.2491 - val_mean_squared_error: 7.2491\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 4.4081 - mean_squared_error: 4.4081 - val_loss: 4.9894 - val_mean_squared_error: 4.9894\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 4.4326 - mean_squared_error: 4.4326 - val_loss: 5.0959 - val_mean_squared_error: 5.0959\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 4.2986 - mean_squared_error: 4.2986 - val_loss: 6.6536 - val_mean_squared_error: 6.6536\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 4.0634 - mean_squared_error: 4.0634 - val_loss: 11.5008 - val_mean_squared_error: 11.5008\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 7.6980 - mean_squared_error: 7.6980 - val_loss: 10917.2391 - val_mean_squared_error: 10917.2393\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 6.7963 - mean_squared_error: 6.7963 - val_loss: 197.6106 - val_mean_squared_error: 197.6106\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 5.9647 - mean_squared_error: 5.9647 - val_loss: 10.9257 - val_mean_squared_error: 10.9257\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 5.1556 - mean_squared_error: 5.1556 - val_loss: 6.4147 - val_mean_squared_error: 6.4147\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 4.8592 - mean_squared_error: 4.8591 - val_loss: 7.2277 - val_mean_squared_error: 7.2277\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 4.6324 - mean_squared_error: 4.6324 - val_loss: 6.1691 - val_mean_squared_error: 6.1691\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 4.1926 - mean_squared_error: 4.1926 - val_loss: 5.2707 - val_mean_squared_error: 5.2707\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 4.0394 - mean_squared_error: 4.0394 - val_loss: 5.6335 - val_mean_squared_error: 5.6335\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 3.9595 - mean_squared_error: 3.9595 - val_loss: 4.9833 - val_mean_squared_error: 4.9833\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 3.9636 - mean_squared_error: 3.9636 - val_loss: 5.5737 - val_mean_squared_error: 5.5737\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 2s 825us/sample - loss: 3.8397 - mean_squared_error: 3.8397 - val_loss: 4.7955 - val_mean_squared_error: 4.7955\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 3.5402 - mean_squared_error: 3.5402 - val_loss: 4.8618 - val_mean_squared_error: 4.8618\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 3.6123 - mean_squared_error: 3.6123 - val_loss: 4.3778 - val_mean_squared_error: 4.3778\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 3.4475 - mean_squared_error: 3.4475 - val_loss: 4.6123 - val_mean_squared_error: 4.6123\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 3.4831 - mean_squared_error: 3.4831 - val_loss: 5.3407 - val_mean_squared_error: 5.3407\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 3.3289 - mean_squared_error: 3.3289 - val_loss: 4.3764 - val_mean_squared_error: 4.3764\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 3.1760 - mean_squared_error: 3.1760 - val_loss: 4.8212 - val_mean_squared_error: 4.8212\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 3.2209 - mean_squared_error: 3.2209 - val_loss: 4.5773 - val_mean_squared_error: 4.5773\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 3.1871 - mean_squared_error: 3.1871 - val_loss: 4.5220 - val_mean_squared_error: 4.5220\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 3.1591 - mean_squared_error: 3.1591 - val_loss: 4.2387 - val_mean_squared_error: 4.2387\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 2.9948 - mean_squared_error: 2.9948 - val_loss: 4.3131 - val_mean_squared_error: 4.3131\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 2s 832us/sample - loss: 3.0435 - mean_squared_error: 3.0435 - val_loss: 4.6640 - val_mean_squared_error: 4.6640\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 2s 832us/sample - loss: 2.9250 - mean_squared_error: 2.9250 - val_loss: 4.5259 - val_mean_squared_error: 4.5259\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 2.9304 - mean_squared_error: 2.9304 - val_loss: 4.6000 - val_mean_squared_error: 4.6000\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 2s 825us/sample - loss: 2.7874 - mean_squared_error: 2.7874 - val_loss: 4.1387 - val_mean_squared_error: 4.1387\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 2.7502 - mean_squared_error: 2.7502 - val_loss: 4.1759 - val_mean_squared_error: 4.1759\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 2.6403 - mean_squared_error: 2.6403 - val_loss: 4.3508 - val_mean_squared_error: 4.3508\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 2.6599 - mean_squared_error: 2.6599 - val_loss: 4.1684 - val_mean_squared_error: 4.1684\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 2s 825us/sample - loss: 2.6152 - mean_squared_error: 2.6152 - val_loss: 3.8895 - val_mean_squared_error: 3.8895\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 2.5333 - mean_squared_error: 2.5333 - val_loss: 4.3108 - val_mean_squared_error: 4.3108\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 2.6745 - mean_squared_error: 2.6745 - val_loss: 4.2967 - val_mean_squared_error: 4.2967\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 2.4837 - mean_squared_error: 2.4837 - val_loss: 4.2289 - val_mean_squared_error: 4.2289\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 2.3564 - mean_squared_error: 2.3564 - val_loss: 4.1657 - val_mean_squared_error: 4.1657\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 2.3907 - mean_squared_error: 2.3907 - val_loss: 4.0339 - val_mean_squared_error: 4.0339\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 2.3327 - mean_squared_error: 2.3327 - val_loss: 3.9424 - val_mean_squared_error: 3.9424\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 2s 825us/sample - loss: 2.3506 - mean_squared_error: 2.3506 - val_loss: 4.2217 - val_mean_squared_error: 4.2217\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 2.3082 - mean_squared_error: 2.3082 - val_loss: 3.6749 - val_mean_squared_error: 3.6749\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 2.2588 - mean_squared_error: 2.2588 - val_loss: 4.1502 - val_mean_squared_error: 4.1502\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 2.2909 - mean_squared_error: 2.2909 - val_loss: 4.0312 - val_mean_squared_error: 4.0312\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 2.1897 - mean_squared_error: 2.1897 - val_loss: 3.8155 - val_mean_squared_error: 3.8155\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 2.1337 - mean_squared_error: 2.1337 - val_loss: 3.8442 - val_mean_squared_error: 3.8442\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 2s 827us/sample - loss: 2.1198 - mean_squared_error: 2.1198 - val_loss: 3.6436 - val_mean_squared_error: 3.6436\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 2.1128 - mean_squared_error: 2.1128 - val_loss: 3.6758 - val_mean_squared_error: 3.6758\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 2s 826us/sample - loss: 2.0894 - mean_squared_error: 2.0894 - val_loss: 4.6254 - val_mean_squared_error: 4.6254\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 2s 826us/sample - loss: 2.1500 - mean_squared_error: 2.1500 - val_loss: 3.9511 - val_mean_squared_error: 3.9511\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 2.0744 - mean_squared_error: 2.0744 - val_loss: 3.7353 - val_mean_squared_error: 3.7353\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.9879 - mean_squared_error: 1.9879 - val_loss: 3.9884 - val_mean_squared_error: 3.9884\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 1.9354 - mean_squared_error: 1.9354 - val_loss: 3.7784 - val_mean_squared_error: 3.7784\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.9549 - mean_squared_error: 1.9549 - val_loss: 3.7659 - val_mean_squared_error: 3.7659\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.9644 - mean_squared_error: 1.9644 - val_loss: 3.6961 - val_mean_squared_error: 3.6961\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.8480 - mean_squared_error: 1.8480 - val_loss: 3.6988 - val_mean_squared_error: 3.6988\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 2.0589 - mean_squared_error: 2.0589 - val_loss: 3.7776 - val_mean_squared_error: 3.7776\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.8892 - mean_squared_error: 1.8892 - val_loss: 3.6362 - val_mean_squared_error: 3.6362\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.8942 - mean_squared_error: 1.8942 - val_loss: 3.7585 - val_mean_squared_error: 3.7585\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.7542 - mean_squared_error: 1.7542 - val_loss: 3.5024 - val_mean_squared_error: 3.5024\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.7620 - mean_squared_error: 1.7620 - val_loss: 3.4828 - val_mean_squared_error: 3.4828\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.7448 - mean_squared_error: 1.7448 - val_loss: 3.8156 - val_mean_squared_error: 3.8156\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 2s 825us/sample - loss: 1.7020 - mean_squared_error: 1.7020 - val_loss: 3.6079 - val_mean_squared_error: 3.6079\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 1.7542 - mean_squared_error: 1.7542 - val_loss: 3.7148 - val_mean_squared_error: 3.7148\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.8044 - mean_squared_error: 1.8044 - val_loss: 3.4887 - val_mean_squared_error: 3.4887\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.7336 - mean_squared_error: 1.7336 - val_loss: 3.5980 - val_mean_squared_error: 3.5980\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.6432 - mean_squared_error: 1.6432 - val_loss: 3.6076 - val_mean_squared_error: 3.6076\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 1.6076 - mean_squared_error: 1.6076 - val_loss: 3.5458 - val_mean_squared_error: 3.5458\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.7363 - mean_squared_error: 1.7363 - val_loss: 3.4503 - val_mean_squared_error: 3.4503\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.7249 - mean_squared_error: 1.7249 - val_loss: 3.4889 - val_mean_squared_error: 3.4889\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.6305 - mean_squared_error: 1.6305 - val_loss: 3.4702 - val_mean_squared_error: 3.4702\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.6334 - mean_squared_error: 1.6334 - val_loss: 3.4289 - val_mean_squared_error: 3.4289\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.6024 - mean_squared_error: 1.6024 - val_loss: 3.2870 - val_mean_squared_error: 3.2870\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.7017 - mean_squared_error: 1.7017 - val_loss: 3.4329 - val_mean_squared_error: 3.4329\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.5557 - mean_squared_error: 1.5557 - val_loss: 3.3226 - val_mean_squared_error: 3.3226\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.5974 - mean_squared_error: 1.5974 - val_loss: 3.2645 - val_mean_squared_error: 3.2645\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.5944 - mean_squared_error: 1.5944 - val_loss: 3.6490 - val_mean_squared_error: 3.6490\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 1.5206 - mean_squared_error: 1.5206 - val_loss: 3.5198 - val_mean_squared_error: 3.5198\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.5171 - mean_squared_error: 1.5171 - val_loss: 3.6909 - val_mean_squared_error: 3.6909\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.5203 - mean_squared_error: 1.5203 - val_loss: 3.5700 - val_mean_squared_error: 3.5700\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.4154 - mean_squared_error: 1.4154 - val_loss: 3.3190 - val_mean_squared_error: 3.3190\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.4630 - mean_squared_error: 1.4630 - val_loss: 3.6245 - val_mean_squared_error: 3.6245\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.5264 - mean_squared_error: 1.5264 - val_loss: 3.4491 - val_mean_squared_error: 3.4491\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 1.4918 - mean_squared_error: 1.4918 - val_loss: 3.9935 - val_mean_squared_error: 3.9935\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.4411 - mean_squared_error: 1.4411 - val_loss: 3.3297 - val_mean_squared_error: 3.3297\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.4579 - mean_squared_error: 1.4579 - val_loss: 3.4507 - val_mean_squared_error: 3.4507\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.5740 - mean_squared_error: 1.5740 - val_loss: 3.4145 - val_mean_squared_error: 3.4145\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.4781 - mean_squared_error: 1.4781 - val_loss: 3.4283 - val_mean_squared_error: 3.4283\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.4463 - mean_squared_error: 1.4463 - val_loss: 3.5097 - val_mean_squared_error: 3.5097\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.5053 - mean_squared_error: 1.5053 - val_loss: 4.3172 - val_mean_squared_error: 4.3172\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 1.4624 - mean_squared_error: 1.4624 - val_loss: 3.6864 - val_mean_squared_error: 3.6864\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.4063 - mean_squared_error: 1.4063 - val_loss: 3.2873 - val_mean_squared_error: 3.2873\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.3429 - mean_squared_error: 1.3429 - val_loss: 3.0990 - val_mean_squared_error: 3.0990\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.4060 - mean_squared_error: 1.4060 - val_loss: 3.3785 - val_mean_squared_error: 3.3785\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.3134 - mean_squared_error: 1.3134 - val_loss: 3.3915 - val_mean_squared_error: 3.3915\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.3209 - mean_squared_error: 1.3209 - val_loss: 3.4536 - val_mean_squared_error: 3.4536\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 1.3964 - mean_squared_error: 1.3964 - val_loss: 79758179283.5190 - val_mean_squared_error: 79758180352.0000\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 1.5134 - mean_squared_error: 1.5134 - val_loss: 13.8210 - val_mean_squared_error: 13.8210\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.6741 - mean_squared_error: 1.6741 - val_loss: 7.0179 - val_mean_squared_error: 7.0179\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.5340 - mean_squared_error: 1.5340 - val_loss: 4.4947 - val_mean_squared_error: 4.4947\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 1.4784 - mean_squared_error: 1.4784 - val_loss: 3.6940 - val_mean_squared_error: 3.6940\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.3580 - mean_squared_error: 1.3580 - val_loss: 3.5127 - val_mean_squared_error: 3.5127\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.4098 - mean_squared_error: 1.4098 - val_loss: 3.3660 - val_mean_squared_error: 3.3660\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.3083 - mean_squared_error: 1.3083 - val_loss: 3.6279 - val_mean_squared_error: 3.6279\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.2671 - mean_squared_error: 1.2671 - val_loss: 3.5515 - val_mean_squared_error: 3.5515\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.2957 - mean_squared_error: 1.2957 - val_loss: 3.3659 - val_mean_squared_error: 3.3659\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.3123 - mean_squared_error: 1.3123 - val_loss: 3.3649 - val_mean_squared_error: 3.3649\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.3105 - mean_squared_error: 1.3105 - val_loss: 3.1632 - val_mean_squared_error: 3.1632\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 1.2903 - mean_squared_error: 1.2903 - val_loss: 3.2489 - val_mean_squared_error: 3.2489\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.2793 - mean_squared_error: 1.2793 - val_loss: 3.1540 - val_mean_squared_error: 3.1540\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.2290 - mean_squared_error: 1.2290 - val_loss: 3.3380 - val_mean_squared_error: 3.3380\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.2655 - mean_squared_error: 1.2655 - val_loss: 3.5578 - val_mean_squared_error: 3.5578\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.3512 - mean_squared_error: 1.3512 - val_loss: 3.3107 - val_mean_squared_error: 3.3107\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.2257 - mean_squared_error: 1.2257 - val_loss: 3.1597 - val_mean_squared_error: 3.1597\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.1889 - mean_squared_error: 1.1889 - val_loss: 3.1543 - val_mean_squared_error: 3.1543\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.2376 - mean_squared_error: 1.2376 - val_loss: 3.3840 - val_mean_squared_error: 3.3840\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 1.1475 - mean_squared_error: 1.1475 - val_loss: 3.2270 - val_mean_squared_error: 3.2270\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.2298 - mean_squared_error: 1.2298 - val_loss: 3.4888 - val_mean_squared_error: 3.4888\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.3637 - mean_squared_error: 1.3637 - val_loss: 3.2380 - val_mean_squared_error: 3.2380\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1865 - mean_squared_error: 1.1865 - val_loss: 3.3730 - val_mean_squared_error: 3.3730\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 1.1385 - mean_squared_error: 1.1385 - val_loss: 3.5759 - val_mean_squared_error: 3.5759\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.1464 - mean_squared_error: 1.1464 - val_loss: 3.9510 - val_mean_squared_error: 3.9510\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.1407 - mean_squared_error: 1.1407 - val_loss: 3.1385 - val_mean_squared_error: 3.1385\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.2263 - mean_squared_error: 1.2263 - val_loss: 3.3294 - val_mean_squared_error: 3.3294\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.2090 - mean_squared_error: 1.2090 - val_loss: 3.2951 - val_mean_squared_error: 3.2951\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.1855 - mean_squared_error: 1.1855 - val_loss: 3.1169 - val_mean_squared_error: 3.1169\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 1.1130 - mean_squared_error: 1.1130 - val_loss: 3.1755 - val_mean_squared_error: 3.1755\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1263 - mean_squared_error: 1.1263 - val_loss: 3.2059 - val_mean_squared_error: 3.2059\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.0466 - mean_squared_error: 1.0466 - val_loss: 3.4569 - val_mean_squared_error: 3.4569\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 2s 827us/sample - loss: 1.1067 - mean_squared_error: 1.1067 - val_loss: 3.3188 - val_mean_squared_error: 3.3188\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.1936 - mean_squared_error: 1.1936 - val_loss: 3.3846 - val_mean_squared_error: 3.3846\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.0955 - mean_squared_error: 1.0955 - val_loss: 2.9744 - val_mean_squared_error: 2.9744\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.0458 - mean_squared_error: 1.0458 - val_loss: 3.2041 - val_mean_squared_error: 3.2041\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.0928 - mean_squared_error: 1.0928 - val_loss: 3.2250 - val_mean_squared_error: 3.2250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSsk092Pw83H",
        "colab_type": "text"
      },
      "source": [
        "## Augmentation of Training Data\n",
        "\n",
        "As discussed in the overview of our dataset, many of our training images are not currently being used due to the lack of labels for some keypoints. This unfortunately leaves us with only ~2100 images to split between training and development sets. In order to attempt to make a more robust training set that will hopefully generalize better to the test data, we will augment our images by simply flipping them across the columns (flip across the y-axis). This flip was implemented in the DataExploration.ipynb notebook at the [team's GitHub repo](https://github.com/tomgoter/w207_finalproject). Essentially flipping the pixel data is easy enough. We simply reverse the columns for all of the x-coordinate keypoints (i.e., those that end with \"_x\"). The only tricky part in this is that when we flip an image the labels for keypoints that are oriented by left/right directions are now reversed. So we need to go through the keypoint column names and relabel our columns. This amount of manipulation of our training data can be error prone, so as a quality assurance check the image below (and several others like it) was generated from the original and flipped datasets. All of the keypoints are identified by the blue dots. The original image is on the right, and the flipped image is on the left. One can see that the keypoints have been mirrored appropriately. After creating a flipped dataset we merged it with the original training set and shuffled the data. This dataset (i.e. pandas dataframe) was then pickled for easy porting to Google Drive. At this point, we are ready to use the expanded dataset.\n",
        "\n",
        "Great! We have doubled the size of our data from which we can train\n",
        "\n",
        "![alt text](https://github.com/tomgoter/w207_finalproject/raw/master/Images/flipped.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z91PoSp-QgVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the augmented dataframe from the pickle file\n",
        "df_nostache_nonan_w_flip = pd.read_pickle(drive_path + \"df_nostache_nonan_w_flip.pkl\")\n",
        "\n",
        "# Grab the last column - that is our image data for X matrix\n",
        "flipped_X = df_nostache_nonan_w_flip.iloc[:, -1]\n",
        "\n",
        "# Convert from a series of arrays to an NDarray\n",
        "flipped_X = np.array([x.reshape(96,96,1) for x in flipped_X])\n",
        "\n",
        "# Grab the keypoints and stick into our y-variable\n",
        "flipped_y = np.array(df_nostache_nonan_w_flip.iloc[:,:-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdeY38Znxhl4",
        "colab_type": "code",
        "outputId": "2eb1a756-a3a7-47be-f294-b0d58d94ceab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# We have doubled the size of our training/development data\n",
        "flipped_X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4280, 96, 96, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5DT58Q-x3Zo",
        "colab_type": "text"
      },
      "source": [
        "### Revisit old sensitivities with new data\n",
        "\n",
        "Now that we have more training data to play with, let's run some old sensitivities again.  We start by running with 12 and 16 starting filter depth, two learning rate factors and two dropout algorithms.\n",
        "\n",
        "The image below shows the boost in performance we get through this data augmentation. Unfortunately, as expected, it also comes with a significant run time penalty as we have doubled our training set size (specifially for starting filter depth of 12 and dropout rate of 0 initially increasing by 0.02 each layer).\n",
        "\n",
        "![alt text](https://github.com/tomgoter/w207_finalproject/raw/master/Images/flipped_performance.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDZSGXOxx3hY",
        "colab_type": "code",
        "outputId": "83e87693-7e33-4999-fcce-5aa62d1645a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Use an early stopping callback and our timing callback\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                              patience=100, mode='auto')\n",
        "time_callback = TimeHistory()\n",
        "\n",
        "# Initialize a new data frame to hold our output data\n",
        "cnn_flipped_df = pd.DataFrame()\n",
        "\n",
        "# Start filter list\n",
        "start_filters = [12, 16]\n",
        "\n",
        "# Create a list of initial dropout values and steps to increase\n",
        "dropouts = [(0.00,0.00), (0.00,0.02)]\n",
        "\n",
        "# Run a parametric study\n",
        "for lr_factor in [10, 15]: \n",
        "    for start_filter in start_filters:\n",
        "        for d in dropouts:\n",
        "          \n",
        "            # Create the model with the specified parameters\n",
        "            model = create_bn_cnn_model(start_filter, d[0], d[1])\n",
        "            \n",
        "            # Use the adam optimizer with the default learning rate\n",
        "            adam = optimizers.Adam(lr=0.001*lr_factor, beta_1=0.9, beta_2=0.999)\n",
        "            \n",
        "            # Compile our model\n",
        "            model.compile(\n",
        "                  optimizer=adam,\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_squared_error'])\n",
        "            \n",
        "            # Save the output of the model - also implement both the timining and early stop\n",
        "            # callbacks\n",
        "            history = model.fit(\n",
        "                flipped_X.astype(np.float32), flipped_y.astype(np.float32),\n",
        "                epochs=200,\n",
        "                validation_split=0.15, callbacks=[time_callback, early_stop])\n",
        "            \n",
        "            # Snag the times\n",
        "            times = time_callback.times\n",
        "\n",
        "            # Convert model output data to dataframe\n",
        "            hist = pd.DataFrame(history.history)\n",
        "            hist['epoch'] = history.epoch\n",
        "            hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "            hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "            hist['times'] = times\n",
        "            \n",
        "            # Add model specific metadata to differentiate between models\n",
        "            hist['starting_filter'] = start_filter\n",
        "            hist['layers'] = 3\n",
        "            hist['pooling'] = 'yes'\n",
        "            hist['fc_layer1'] = 500\n",
        "            hist['fc_layer2'] = 500\n",
        "            hist['activation'] = 'relu'\n",
        "            hist['optimizer'] = 'adam'\n",
        "            hist['lrate'] = adam.get_config()['learning_rate']\n",
        "            hist['dropout_initial'] = d[0]\n",
        "            hist['dropout_step'] = d[1]\n",
        "            hist['batch_norm'] = 1\n",
        "            hist['bias'] = 0\n",
        "            hist['stride'] = 1\n",
        "            hist['flipped'] = 1.0\n",
        "\n",
        "            # Keep concatenating to dataframe\n",
        "            cnn_flipped_df = pd.concat([cnn_flipped_df,hist])\n",
        "\n",
        "            # Re-pickle after every model to retain progress\n",
        "            cnn_flipped_df.to_pickle(drive_path+\"OutputData/cnn_flipped_df2.pkl\")\n",
        "\n",
        "            # Save models.\n",
        "            filename = \"cnn_flipped2_{}_d{}_s{}_sf{}_lrfactor{}_flipped_100\".format('adam', d[0], d[1], start_filter, lr_factor)\n",
        "            model.save(drive_path+\"Models/\"+filename+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_30 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_32 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 718us/sample - loss: 223.3142 - mean_squared_error: 223.3142 - val_loss: 151.4509 - val_mean_squared_error: 151.4509\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 20.5596 - mean_squared_error: 20.5596 - val_loss: 43.3977 - val_mean_squared_error: 43.3977\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 16.9446 - mean_squared_error: 16.9446 - val_loss: 21.4646 - val_mean_squared_error: 21.4646\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 15.1629 - mean_squared_error: 15.1629 - val_loss: 23.7014 - val_mean_squared_error: 23.7014\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 13.8136 - mean_squared_error: 13.8136 - val_loss: 13.9691 - val_mean_squared_error: 13.9691\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 14.2498 - mean_squared_error: 14.2498 - val_loss: 11.8729 - val_mean_squared_error: 11.8729\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 11.4777 - mean_squared_error: 11.4777 - val_loss: 9.4305 - val_mean_squared_error: 9.4305\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 9.9448 - mean_squared_error: 9.9448 - val_loss: 8.8529 - val_mean_squared_error: 8.8529\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 8.1301 - mean_squared_error: 8.1301 - val_loss: 6.7840 - val_mean_squared_error: 6.7840\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 6.8384 - mean_squared_error: 6.8384 - val_loss: 8.9403 - val_mean_squared_error: 8.9403\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 6.0532 - mean_squared_error: 6.0532 - val_loss: 7.0981 - val_mean_squared_error: 7.0981\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 6.0849 - mean_squared_error: 6.0849 - val_loss: 6.0997 - val_mean_squared_error: 6.0997\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 5.3542 - mean_squared_error: 5.3542 - val_loss: 7.3873 - val_mean_squared_error: 7.3873\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 4.5127 - mean_squared_error: 4.5127 - val_loss: 4.9114 - val_mean_squared_error: 4.9114\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 4.1213 - mean_squared_error: 4.1213 - val_loss: 4.3341 - val_mean_squared_error: 4.3341\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 3.7479 - mean_squared_error: 3.7479 - val_loss: 4.2510 - val_mean_squared_error: 4.2510\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 3.4966 - mean_squared_error: 3.4966 - val_loss: 3.5520 - val_mean_squared_error: 3.5520\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 3.5349 - mean_squared_error: 3.5349 - val_loss: 3.6550 - val_mean_squared_error: 3.6550\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 3.1214 - mean_squared_error: 3.1214 - val_loss: 4.1181 - val_mean_squared_error: 4.1181\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 2.9157 - mean_squared_error: 2.9157 - val_loss: 3.3085 - val_mean_squared_error: 3.3085\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 2.7540 - mean_squared_error: 2.7540 - val_loss: 3.1140 - val_mean_squared_error: 3.1140\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 2.5722 - mean_squared_error: 2.5722 - val_loss: 3.2011 - val_mean_squared_error: 3.2011\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 2.6377 - mean_squared_error: 2.6377 - val_loss: 3.6096 - val_mean_squared_error: 3.6096\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 2.6453 - mean_squared_error: 2.6453 - val_loss: 3.2134 - val_mean_squared_error: 3.2134\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 2.4185 - mean_squared_error: 2.4185 - val_loss: 2.7516 - val_mean_squared_error: 2.7516\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 2.2970 - mean_squared_error: 2.2970 - val_loss: 3.3797 - val_mean_squared_error: 3.3797\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 2.3633 - mean_squared_error: 2.3633 - val_loss: 3.0593 - val_mean_squared_error: 3.0593\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 2.3117 - mean_squared_error: 2.3117 - val_loss: 2.8924 - val_mean_squared_error: 2.8924\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 2.2194 - mean_squared_error: 2.2194 - val_loss: 3.6893 - val_mean_squared_error: 3.6893\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 2.0648 - mean_squared_error: 2.0648 - val_loss: 2.6712 - val_mean_squared_error: 2.6712\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 2.0753 - mean_squared_error: 2.0753 - val_loss: 2.9479 - val_mean_squared_error: 2.9479\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.9289 - mean_squared_error: 1.9289 - val_loss: 2.8194 - val_mean_squared_error: 2.8194\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 2.0931 - mean_squared_error: 2.0931 - val_loss: 3.9831 - val_mean_squared_error: 3.9831\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 2.0360 - mean_squared_error: 2.0360 - val_loss: 2.6401 - val_mean_squared_error: 2.6401\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.9018 - mean_squared_error: 1.9018 - val_loss: 2.7092 - val_mean_squared_error: 2.7092\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.7849 - mean_squared_error: 1.7849 - val_loss: 2.5822 - val_mean_squared_error: 2.5822\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.7435 - mean_squared_error: 1.7435 - val_loss: 2.6659 - val_mean_squared_error: 2.6659\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.7805 - mean_squared_error: 1.7805 - val_loss: 3.0314 - val_mean_squared_error: 3.0314\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 367us/sample - loss: 1.9482 - mean_squared_error: 1.9482 - val_loss: 3.3140 - val_mean_squared_error: 3.3140\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.9136 - mean_squared_error: 1.9136 - val_loss: 2.4409 - val_mean_squared_error: 2.4409\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.6602 - mean_squared_error: 1.6602 - val_loss: 2.3167 - val_mean_squared_error: 2.3167\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.6723 - mean_squared_error: 1.6723 - val_loss: 2.4204 - val_mean_squared_error: 2.4204\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.6155 - mean_squared_error: 1.6155 - val_loss: 2.5112 - val_mean_squared_error: 2.5112\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.6777 - mean_squared_error: 1.6777 - val_loss: 2.6747 - val_mean_squared_error: 2.6747\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.6113 - mean_squared_error: 1.6113 - val_loss: 2.3743 - val_mean_squared_error: 2.3743\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.5410 - mean_squared_error: 1.5410 - val_loss: 2.5118 - val_mean_squared_error: 2.5118\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.4501 - mean_squared_error: 1.4501 - val_loss: 2.7500 - val_mean_squared_error: 2.7500\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.5445 - mean_squared_error: 1.5445 - val_loss: 2.7402 - val_mean_squared_error: 2.7402\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.4764 - mean_squared_error: 1.4764 - val_loss: 2.4106 - val_mean_squared_error: 2.4106\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.6240 - mean_squared_error: 1.6240 - val_loss: 2.6251 - val_mean_squared_error: 2.6251\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.4339 - mean_squared_error: 1.4339 - val_loss: 2.3960 - val_mean_squared_error: 2.3960\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.5710 - mean_squared_error: 1.5710 - val_loss: 2.4993 - val_mean_squared_error: 2.4993\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.3914 - mean_squared_error: 1.3914 - val_loss: 2.3304 - val_mean_squared_error: 2.3304\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.3163 - mean_squared_error: 1.3163 - val_loss: 2.2393 - val_mean_squared_error: 2.2393\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.4256 - mean_squared_error: 1.4256 - val_loss: 2.5220 - val_mean_squared_error: 2.5220\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.3658 - mean_squared_error: 1.3658 - val_loss: 2.0791 - val_mean_squared_error: 2.0791\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.2934 - mean_squared_error: 1.2934 - val_loss: 2.7063 - val_mean_squared_error: 2.7063\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 1.3094 - mean_squared_error: 1.3094 - val_loss: 2.5179 - val_mean_squared_error: 2.5179\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.2699 - mean_squared_error: 1.2699 - val_loss: 2.2488 - val_mean_squared_error: 2.2488\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.2101 - mean_squared_error: 1.2101 - val_loss: 2.2026 - val_mean_squared_error: 2.2026\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.2877 - mean_squared_error: 1.2877 - val_loss: 2.4205 - val_mean_squared_error: 2.4205\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.2132 - mean_squared_error: 1.2132 - val_loss: 2.4084 - val_mean_squared_error: 2.4084\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.3299 - mean_squared_error: 1.3299 - val_loss: 2.4909 - val_mean_squared_error: 2.4909\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.1113 - mean_squared_error: 1.1113 - val_loss: 2.4012 - val_mean_squared_error: 2.4012\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.3266 - mean_squared_error: 1.3266 - val_loss: 2.1011 - val_mean_squared_error: 2.1011\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.1735 - mean_squared_error: 1.1735 - val_loss: 2.6774 - val_mean_squared_error: 2.6774\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.1599 - mean_squared_error: 1.1599 - val_loss: 2.1230 - val_mean_squared_error: 2.1230\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.1990 - mean_squared_error: 1.1990 - val_loss: 2.2819 - val_mean_squared_error: 2.2819\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.0939 - mean_squared_error: 1.0939 - val_loss: 2.3117 - val_mean_squared_error: 2.3117\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.1494 - mean_squared_error: 1.1494 - val_loss: 2.7000 - val_mean_squared_error: 2.7000\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.1534 - mean_squared_error: 1.1534 - val_loss: 2.2981 - val_mean_squared_error: 2.2981\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.0702 - mean_squared_error: 1.0702 - val_loss: 2.3494 - val_mean_squared_error: 2.3494\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.0750 - mean_squared_error: 1.0750 - val_loss: 2.2360 - val_mean_squared_error: 2.2360\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.0713 - mean_squared_error: 1.0713 - val_loss: 2.1147 - val_mean_squared_error: 2.1147\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.0295 - mean_squared_error: 1.0295 - val_loss: 2.3198 - val_mean_squared_error: 2.3198\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.1420 - mean_squared_error: 1.1420 - val_loss: 2.0474 - val_mean_squared_error: 2.0474\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9440 - mean_squared_error: 0.9440 - val_loss: 2.2433 - val_mean_squared_error: 2.2433\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.9625 - mean_squared_error: 0.9625 - val_loss: 1.9975 - val_mean_squared_error: 1.9975\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.9879 - mean_squared_error: 0.9879 - val_loss: 2.3657 - val_mean_squared_error: 2.3657\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.0710 - mean_squared_error: 1.0710 - val_loss: 2.2988 - val_mean_squared_error: 2.2988\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.1285 - mean_squared_error: 1.1285 - val_loss: 2.4436 - val_mean_squared_error: 2.4436\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 2.0747 - val_mean_squared_error: 2.0747\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.9401 - mean_squared_error: 0.9401 - val_loss: 2.2439 - val_mean_squared_error: 2.2439\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.0704 - mean_squared_error: 1.0704 - val_loss: 2.3139 - val_mean_squared_error: 2.3139\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.9510 - mean_squared_error: 0.9510 - val_loss: 2.2904 - val_mean_squared_error: 2.2904\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.0977 - mean_squared_error: 1.0977 - val_loss: 2.1401 - val_mean_squared_error: 2.1401\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.9336 - mean_squared_error: 0.9336 - val_loss: 2.6698 - val_mean_squared_error: 2.6698\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.9041 - mean_squared_error: 0.9041 - val_loss: 2.1297 - val_mean_squared_error: 2.1297\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.9144 - mean_squared_error: 0.9144 - val_loss: 2.1403 - val_mean_squared_error: 2.1403\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.9380 - mean_squared_error: 0.9380 - val_loss: 2.3060 - val_mean_squared_error: 2.3060\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.9936 - mean_squared_error: 0.9936 - val_loss: 2.0913 - val_mean_squared_error: 2.0913\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9313 - mean_squared_error: 0.9313 - val_loss: 2.3471 - val_mean_squared_error: 2.3471\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.9841 - mean_squared_error: 0.9841 - val_loss: 2.6081 - val_mean_squared_error: 2.6081\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.8000 - mean_squared_error: 0.8000 - val_loss: 1.9621 - val_mean_squared_error: 1.9621\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.9531 - mean_squared_error: 0.9531 - val_loss: 2.1049 - val_mean_squared_error: 2.1049\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9180 - mean_squared_error: 0.9180 - val_loss: 2.1533 - val_mean_squared_error: 2.1533\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.9288 - mean_squared_error: 0.9288 - val_loss: 2.1362 - val_mean_squared_error: 2.1362\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9304 - mean_squared_error: 0.9304 - val_loss: 2.5241 - val_mean_squared_error: 2.5241\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.8880 - mean_squared_error: 0.8880 - val_loss: 2.1001 - val_mean_squared_error: 2.1001\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.8759 - mean_squared_error: 0.8759 - val_loss: 2.3414 - val_mean_squared_error: 2.3414\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.8775 - mean_squared_error: 0.8775 - val_loss: 1.9096 - val_mean_squared_error: 1.9096\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.8010 - mean_squared_error: 0.8010 - val_loss: 2.0138 - val_mean_squared_error: 2.0138\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8591 - mean_squared_error: 0.8591 - val_loss: 2.2449 - val_mean_squared_error: 2.2449\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.8426 - mean_squared_error: 0.8426 - val_loss: 1.9604 - val_mean_squared_error: 1.9604\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.8256 - mean_squared_error: 0.8256 - val_loss: 2.1749 - val_mean_squared_error: 2.1749\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.8663 - mean_squared_error: 0.8663 - val_loss: 2.1019 - val_mean_squared_error: 2.1019\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.7866 - mean_squared_error: 0.7866 - val_loss: 1.9520 - val_mean_squared_error: 1.9520\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.7261 - mean_squared_error: 0.7261 - val_loss: 2.2257 - val_mean_squared_error: 2.2257\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.8033 - mean_squared_error: 0.8033 - val_loss: 1.9886 - val_mean_squared_error: 1.9886\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7307 - mean_squared_error: 0.7307 - val_loss: 1.9284 - val_mean_squared_error: 1.9284\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8307 - mean_squared_error: 0.8307 - val_loss: 1.8873 - val_mean_squared_error: 1.8873\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.8227 - mean_squared_error: 0.8227 - val_loss: 2.1702 - val_mean_squared_error: 2.1702\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.8067 - mean_squared_error: 0.8067 - val_loss: 2.1121 - val_mean_squared_error: 2.1121\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7889 - mean_squared_error: 0.7889 - val_loss: 2.0214 - val_mean_squared_error: 2.0214\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.7891 - mean_squared_error: 0.7891 - val_loss: 2.0418 - val_mean_squared_error: 2.0418\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.8913 - mean_squared_error: 0.8913 - val_loss: 2.0427 - val_mean_squared_error: 2.0427\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8588 - mean_squared_error: 0.8588 - val_loss: 1.9744 - val_mean_squared_error: 1.9744\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.7547 - mean_squared_error: 0.7547 - val_loss: 2.1174 - val_mean_squared_error: 2.1174\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.8103 - mean_squared_error: 0.8103 - val_loss: 1.8900 - val_mean_squared_error: 1.8900\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.6924 - mean_squared_error: 0.6924 - val_loss: 2.0629 - val_mean_squared_error: 2.0629\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.7699 - mean_squared_error: 0.7699 - val_loss: 2.0657 - val_mean_squared_error: 2.0657\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7055 - mean_squared_error: 0.7055 - val_loss: 1.9317 - val_mean_squared_error: 1.9317\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7190 - mean_squared_error: 0.7190 - val_loss: 1.8247 - val_mean_squared_error: 1.8247\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.7175 - mean_squared_error: 0.7175 - val_loss: 1.9752 - val_mean_squared_error: 1.9752\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.7713 - mean_squared_error: 0.7713 - val_loss: 2.1596 - val_mean_squared_error: 2.1596\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.7584 - mean_squared_error: 0.7584 - val_loss: 2.2134 - val_mean_squared_error: 2.2134\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.7366 - mean_squared_error: 0.7366 - val_loss: 2.0033 - val_mean_squared_error: 2.0033\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.6595 - mean_squared_error: 0.6595 - val_loss: 1.9114 - val_mean_squared_error: 1.9114\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.6706 - mean_squared_error: 0.6706 - val_loss: 2.2313 - val_mean_squared_error: 2.2313\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.6997 - mean_squared_error: 0.6997 - val_loss: 1.9032 - val_mean_squared_error: 1.9032\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.6401 - mean_squared_error: 0.6401 - val_loss: 1.9113 - val_mean_squared_error: 1.9113\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.6358 - mean_squared_error: 0.6358 - val_loss: 1.9011 - val_mean_squared_error: 1.9011\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.6446 - mean_squared_error: 0.6446 - val_loss: 2.0946 - val_mean_squared_error: 2.0946\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.6927 - mean_squared_error: 0.6927 - val_loss: 2.2089 - val_mean_squared_error: 2.2089\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.6655 - mean_squared_error: 0.6655 - val_loss: 2.0435 - val_mean_squared_error: 2.0435\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.6672 - mean_squared_error: 0.6672 - val_loss: 1.9154 - val_mean_squared_error: 1.9154\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.6473 - mean_squared_error: 0.6473 - val_loss: 1.9620 - val_mean_squared_error: 1.9620\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.6838 - mean_squared_error: 0.6838 - val_loss: 1.9309 - val_mean_squared_error: 1.9309\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.6528 - mean_squared_error: 0.6528 - val_loss: 1.9668 - val_mean_squared_error: 1.9668\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.6231 - mean_squared_error: 0.6231 - val_loss: 1.9896 - val_mean_squared_error: 1.9896\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.6420 - mean_squared_error: 0.6420 - val_loss: 2.0535 - val_mean_squared_error: 2.0535\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6747 - mean_squared_error: 0.6747 - val_loss: 2.3597 - val_mean_squared_error: 2.3597\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.7425 - mean_squared_error: 0.7425 - val_loss: 1.9535 - val_mean_squared_error: 1.9535\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.6161 - mean_squared_error: 0.6161 - val_loss: 2.2181 - val_mean_squared_error: 2.2181\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5753 - mean_squared_error: 0.5753 - val_loss: 2.0099 - val_mean_squared_error: 2.0099\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.6937 - mean_squared_error: 0.6937 - val_loss: 2.0139 - val_mean_squared_error: 2.0139\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.6186 - mean_squared_error: 0.6186 - val_loss: 2.1634 - val_mean_squared_error: 2.1634\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6118 - mean_squared_error: 0.6118 - val_loss: 2.0973 - val_mean_squared_error: 2.0973\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.6464 - mean_squared_error: 0.6464 - val_loss: 2.0081 - val_mean_squared_error: 2.0081\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.5609 - mean_squared_error: 0.5609 - val_loss: 2.0123 - val_mean_squared_error: 2.0123\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.6380 - mean_squared_error: 0.6380 - val_loss: 2.1273 - val_mean_squared_error: 2.1273\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.5836 - mean_squared_error: 0.5836 - val_loss: 2.3937 - val_mean_squared_error: 2.3937\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.6429 - mean_squared_error: 0.6429 - val_loss: 1.8810 - val_mean_squared_error: 1.8810\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.5827 - mean_squared_error: 0.5827 - val_loss: 1.8954 - val_mean_squared_error: 1.8954\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.5847 - mean_squared_error: 0.5847 - val_loss: 2.0820 - val_mean_squared_error: 2.0820\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.5602 - mean_squared_error: 0.5602 - val_loss: 2.0053 - val_mean_squared_error: 2.0053\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.6679 - mean_squared_error: 0.6679 - val_loss: 1.8735 - val_mean_squared_error: 1.8735\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5411 - mean_squared_error: 0.5411 - val_loss: 1.8401 - val_mean_squared_error: 1.8401\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6135 - mean_squared_error: 0.6135 - val_loss: 1.9807 - val_mean_squared_error: 1.9807\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5874 - mean_squared_error: 0.5874 - val_loss: 1.9205 - val_mean_squared_error: 1.9205\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.5647 - mean_squared_error: 0.5647 - val_loss: 1.9536 - val_mean_squared_error: 1.9536\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5554 - mean_squared_error: 0.5554 - val_loss: 1.9571 - val_mean_squared_error: 1.9571\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.5595 - mean_squared_error: 0.5595 - val_loss: 2.0131 - val_mean_squared_error: 2.0131\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.6310 - mean_squared_error: 0.6310 - val_loss: 2.0444 - val_mean_squared_error: 2.0444\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.5683 - mean_squared_error: 0.5683 - val_loss: 2.1230 - val_mean_squared_error: 2.1230\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.5146 - mean_squared_error: 0.5146 - val_loss: 1.9313 - val_mean_squared_error: 1.9313\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.5711 - mean_squared_error: 0.5711 - val_loss: 1.9771 - val_mean_squared_error: 1.9771\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.5530 - mean_squared_error: 0.5530 - val_loss: 1.8885 - val_mean_squared_error: 1.8885\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.5393 - mean_squared_error: 0.5393 - val_loss: 1.9761 - val_mean_squared_error: 1.9761\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.6088 - mean_squared_error: 0.6088 - val_loss: 1.9876 - val_mean_squared_error: 1.9876\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.5455 - mean_squared_error: 0.5455 - val_loss: 2.0475 - val_mean_squared_error: 2.0475\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5421 - mean_squared_error: 0.5421 - val_loss: 1.8987 - val_mean_squared_error: 1.8987\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.5166 - mean_squared_error: 0.5166 - val_loss: 1.9749 - val_mean_squared_error: 1.9749\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.5458 - mean_squared_error: 0.5458 - val_loss: 1.8759 - val_mean_squared_error: 1.8759\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5163 - mean_squared_error: 0.5163 - val_loss: 1.8475 - val_mean_squared_error: 1.8475\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.5541 - mean_squared_error: 0.5541 - val_loss: 2.0343 - val_mean_squared_error: 2.0343\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.5607 - mean_squared_error: 0.5607 - val_loss: 1.9594 - val_mean_squared_error: 1.9594\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.4932 - mean_squared_error: 0.4932 - val_loss: 1.9775 - val_mean_squared_error: 1.9775\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.5317 - mean_squared_error: 0.5317 - val_loss: 1.9225 - val_mean_squared_error: 1.9225\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.5123 - mean_squared_error: 0.5123 - val_loss: 1.9009 - val_mean_squared_error: 1.9009\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.5182 - mean_squared_error: 0.5182 - val_loss: 1.9025 - val_mean_squared_error: 1.9025\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5453 - mean_squared_error: 0.5453 - val_loss: 1.8842 - val_mean_squared_error: 1.8842\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.5840 - mean_squared_error: 0.5840 - val_loss: 2.0067 - val_mean_squared_error: 2.0067\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5106 - mean_squared_error: 0.5106 - val_loss: 1.9368 - val_mean_squared_error: 1.9368\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.4618 - mean_squared_error: 0.4618 - val_loss: 1.9255 - val_mean_squared_error: 1.9255\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.5191 - mean_squared_error: 0.5191 - val_loss: 1.8955 - val_mean_squared_error: 1.8955\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.5825 - mean_squared_error: 0.5825 - val_loss: 2.0288 - val_mean_squared_error: 2.0288\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.5164 - mean_squared_error: 0.5164 - val_loss: 1.9289 - val_mean_squared_error: 1.9289\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.4917 - mean_squared_error: 0.4917 - val_loss: 1.9868 - val_mean_squared_error: 1.9868\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.5026 - mean_squared_error: 0.5026 - val_loss: 1.8252 - val_mean_squared_error: 1.8252\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.5130 - mean_squared_error: 0.5130 - val_loss: 1.8088 - val_mean_squared_error: 1.8088\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.4775 - mean_squared_error: 0.4775 - val_loss: 1.9680 - val_mean_squared_error: 1.9680\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.4900 - mean_squared_error: 0.4900 - val_loss: 1.9477 - val_mean_squared_error: 1.9477\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.5499 - mean_squared_error: 0.5499 - val_loss: 2.0119 - val_mean_squared_error: 2.0119\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.4807 - mean_squared_error: 0.4807 - val_loss: 1.8205 - val_mean_squared_error: 1.8205\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.4615 - mean_squared_error: 0.4615 - val_loss: 1.9731 - val_mean_squared_error: 1.9731\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.4862 - mean_squared_error: 0.4862 - val_loss: 1.9261 - val_mean_squared_error: 1.9261\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.4908 - mean_squared_error: 0.4908 - val_loss: 1.9343 - val_mean_squared_error: 1.9343\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.4593 - mean_squared_error: 0.4593 - val_loss: 2.0098 - val_mean_squared_error: 2.0098\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.4762 - mean_squared_error: 0.4762 - val_loss: 1.9949 - val_mean_squared_error: 1.9949\n",
            "==================================================\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_33 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_34 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_57 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_35 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_58 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_59 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 771us/sample - loss: 221.6025 - mean_squared_error: 221.6024 - val_loss: 227.3726 - val_mean_squared_error: 227.3727\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 370us/sample - loss: 19.7476 - mean_squared_error: 19.7476 - val_loss: 60.6190 - val_mean_squared_error: 60.6189\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 20.3314 - mean_squared_error: 20.3314 - val_loss: 27.6148 - val_mean_squared_error: 27.6148\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 367us/sample - loss: 15.8000 - mean_squared_error: 15.8000 - val_loss: 13.3160 - val_mean_squared_error: 13.3160\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 13.4612 - mean_squared_error: 13.4612 - val_loss: 10.6330 - val_mean_squared_error: 10.6330\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 12.9284 - mean_squared_error: 12.9284 - val_loss: 11.6784 - val_mean_squared_error: 11.6784\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 12.4543 - mean_squared_error: 12.4543 - val_loss: 13.6958 - val_mean_squared_error: 13.6958\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 10.8434 - mean_squared_error: 10.8434 - val_loss: 8.6489 - val_mean_squared_error: 8.6489\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 9.1943 - mean_squared_error: 9.1943 - val_loss: 8.6522 - val_mean_squared_error: 8.6522\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 7.9864 - mean_squared_error: 7.9864 - val_loss: 8.0180 - val_mean_squared_error: 8.0180\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 7.4352 - mean_squared_error: 7.4352 - val_loss: 7.5511 - val_mean_squared_error: 7.5511\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 6.5116 - mean_squared_error: 6.5117 - val_loss: 6.4300 - val_mean_squared_error: 6.4300\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 5.9682 - mean_squared_error: 5.9682 - val_loss: 7.3879 - val_mean_squared_error: 7.3879\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 5.1588 - mean_squared_error: 5.1588 - val_loss: 4.8262 - val_mean_squared_error: 4.8262\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 366us/sample - loss: 5.1391 - mean_squared_error: 5.1391 - val_loss: 4.8603 - val_mean_squared_error: 4.8603\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 4.4286 - mean_squared_error: 4.4286 - val_loss: 4.1359 - val_mean_squared_error: 4.1359\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 4.2189 - mean_squared_error: 4.2189 - val_loss: 3.7851 - val_mean_squared_error: 3.7851\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 370us/sample - loss: 3.8932 - mean_squared_error: 3.8932 - val_loss: 4.6495 - val_mean_squared_error: 4.6495\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 3.6589 - mean_squared_error: 3.6589 - val_loss: 3.4071 - val_mean_squared_error: 3.4071\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 3.5756 - mean_squared_error: 3.5756 - val_loss: 3.3670 - val_mean_squared_error: 3.3670\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 3.3830 - mean_squared_error: 3.3830 - val_loss: 3.3937 - val_mean_squared_error: 3.3937\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 3.1621 - mean_squared_error: 3.1621 - val_loss: 3.2724 - val_mean_squared_error: 3.2724\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 3.2104 - mean_squared_error: 3.2104 - val_loss: 3.5057 - val_mean_squared_error: 3.5057\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 367us/sample - loss: 2.9965 - mean_squared_error: 2.9965 - val_loss: 3.1932 - val_mean_squared_error: 3.1932\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 367us/sample - loss: 2.8825 - mean_squared_error: 2.8825 - val_loss: 2.9046 - val_mean_squared_error: 2.9046\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 2.7604 - mean_squared_error: 2.7604 - val_loss: 2.9763 - val_mean_squared_error: 2.9763\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 2.6107 - mean_squared_error: 2.6107 - val_loss: 2.9539 - val_mean_squared_error: 2.9539\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 2.5684 - mean_squared_error: 2.5684 - val_loss: 2.7813 - val_mean_squared_error: 2.7813\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 2.4756 - mean_squared_error: 2.4756 - val_loss: 2.3675 - val_mean_squared_error: 2.3675\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 2.3637 - mean_squared_error: 2.3637 - val_loss: 2.6846 - val_mean_squared_error: 2.6846\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 2.3593 - mean_squared_error: 2.3593 - val_loss: 2.3719 - val_mean_squared_error: 2.3719\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 2.2324 - mean_squared_error: 2.2324 - val_loss: 2.7633 - val_mean_squared_error: 2.7633\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 2.1698 - mean_squared_error: 2.1698 - val_loss: 2.8551 - val_mean_squared_error: 2.8551\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 2.0757 - mean_squared_error: 2.0757 - val_loss: 2.4447 - val_mean_squared_error: 2.4447\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 2.1409 - mean_squared_error: 2.1409 - val_loss: 2.4970 - val_mean_squared_error: 2.4970\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 2.1470 - mean_squared_error: 2.1470 - val_loss: 2.5473 - val_mean_squared_error: 2.5473\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 1.9769 - mean_squared_error: 1.9769 - val_loss: 2.4904 - val_mean_squared_error: 2.4904\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 1.9365 - mean_squared_error: 1.9365 - val_loss: 2.3647 - val_mean_squared_error: 2.3647\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 1.8937 - mean_squared_error: 1.8937 - val_loss: 3.0622 - val_mean_squared_error: 3.0622\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 2.0452 - mean_squared_error: 2.0452 - val_loss: 2.3682 - val_mean_squared_error: 2.3682\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.7737 - mean_squared_error: 1.7737 - val_loss: 2.2316 - val_mean_squared_error: 2.2316\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.8588 - mean_squared_error: 1.8588 - val_loss: 2.1940 - val_mean_squared_error: 2.1940\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.8077 - mean_squared_error: 1.8077 - val_loss: 3.2743 - val_mean_squared_error: 3.2743\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.7807 - mean_squared_error: 1.7807 - val_loss: 2.2920 - val_mean_squared_error: 2.2920\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.6905 - mean_squared_error: 1.6905 - val_loss: 2.2380 - val_mean_squared_error: 2.2380\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 374us/sample - loss: 1.6821 - mean_squared_error: 1.6821 - val_loss: 2.3450 - val_mean_squared_error: 2.3450\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 373us/sample - loss: 1.6672 - mean_squared_error: 1.6672 - val_loss: 2.0466 - val_mean_squared_error: 2.0466\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 1.6594 - mean_squared_error: 1.6594 - val_loss: 2.3397 - val_mean_squared_error: 2.3397\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 1.6521 - mean_squared_error: 1.6521 - val_loss: 2.3910 - val_mean_squared_error: 2.3910\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 1.7321 - mean_squared_error: 1.7321 - val_loss: 2.2035 - val_mean_squared_error: 2.2035\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 1.5945 - mean_squared_error: 1.5945 - val_loss: 1.9905 - val_mean_squared_error: 1.9905\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.4775 - mean_squared_error: 1.4775 - val_loss: 2.1149 - val_mean_squared_error: 2.1149\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 374us/sample - loss: 1.5198 - mean_squared_error: 1.5198 - val_loss: 2.1661 - val_mean_squared_error: 2.1661\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 1.4381 - mean_squared_error: 1.4381 - val_loss: 2.2614 - val_mean_squared_error: 2.2614\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 1.4401 - mean_squared_error: 1.4401 - val_loss: 2.1666 - val_mean_squared_error: 2.1666\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 368us/sample - loss: 1.4641 - mean_squared_error: 1.4641 - val_loss: 1.9062 - val_mean_squared_error: 1.9062\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 1.3508 - mean_squared_error: 1.3508 - val_loss: 2.2991 - val_mean_squared_error: 2.2991\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 366us/sample - loss: 1.3713 - mean_squared_error: 1.3713 - val_loss: 1.8822 - val_mean_squared_error: 1.8822\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 1.4453 - mean_squared_error: 1.4453 - val_loss: 1.9410 - val_mean_squared_error: 1.9410\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 1.3302 - mean_squared_error: 1.3302 - val_loss: 1.9566 - val_mean_squared_error: 1.9566\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 1.3536 - mean_squared_error: 1.3536 - val_loss: 1.9525 - val_mean_squared_error: 1.9525\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.3354 - mean_squared_error: 1.3354 - val_loss: 2.1001 - val_mean_squared_error: 2.1001\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.2544 - mean_squared_error: 1.2544 - val_loss: 2.2215 - val_mean_squared_error: 2.2215\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 1.2482 - mean_squared_error: 1.2482 - val_loss: 1.9875 - val_mean_squared_error: 1.9875\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.2772 - mean_squared_error: 1.2772 - val_loss: 2.2759 - val_mean_squared_error: 2.2759\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 1.3111 - mean_squared_error: 1.3111 - val_loss: 2.1708 - val_mean_squared_error: 2.1708\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 1.2364 - mean_squared_error: 1.2364 - val_loss: 1.9291 - val_mean_squared_error: 1.9291\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 1.2616 - mean_squared_error: 1.2616 - val_loss: 1.9908 - val_mean_squared_error: 1.9908\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 1.2402 - mean_squared_error: 1.2402 - val_loss: 1.9761 - val_mean_squared_error: 1.9761\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 1.2149 - mean_squared_error: 1.2149 - val_loss: 1.7774 - val_mean_squared_error: 1.7774\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.2557 - mean_squared_error: 1.2557 - val_loss: 2.0352 - val_mean_squared_error: 2.0352\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.1878 - mean_squared_error: 1.1878 - val_loss: 1.9605 - val_mean_squared_error: 1.9605\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 1.2149 - mean_squared_error: 1.2149 - val_loss: 2.0787 - val_mean_squared_error: 2.0787\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.1446 - mean_squared_error: 1.1446 - val_loss: 1.8644 - val_mean_squared_error: 1.8644\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 1.1358 - mean_squared_error: 1.1358 - val_loss: 1.9509 - val_mean_squared_error: 1.9509\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 1.1360 - mean_squared_error: 1.1360 - val_loss: 2.1203 - val_mean_squared_error: 2.1203\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.2274 - mean_squared_error: 1.2274 - val_loss: 2.1694 - val_mean_squared_error: 2.1694\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.0546 - mean_squared_error: 1.0546 - val_loss: 1.8583 - val_mean_squared_error: 1.8583\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 1.0938 - mean_squared_error: 1.0938 - val_loss: 1.9703 - val_mean_squared_error: 1.9703\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 1.1514 - mean_squared_error: 1.1514 - val_loss: 2.2485 - val_mean_squared_error: 2.2485\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.0763 - mean_squared_error: 1.0763 - val_loss: 2.0639 - val_mean_squared_error: 2.0639\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.0912 - mean_squared_error: 1.0912 - val_loss: 1.8615 - val_mean_squared_error: 1.8615\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 1.1701 - mean_squared_error: 1.1701 - val_loss: 2.0403 - val_mean_squared_error: 2.0403\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 1.0916 - mean_squared_error: 1.0916 - val_loss: 2.1065 - val_mean_squared_error: 2.1065\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.6972 - val_mean_squared_error: 1.6972\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.0523 - mean_squared_error: 1.0523 - val_loss: 1.9853 - val_mean_squared_error: 1.9853\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.9357 - val_mean_squared_error: 1.9357\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.0984 - mean_squared_error: 1.0984 - val_loss: 2.2212 - val_mean_squared_error: 2.2212\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.9891 - mean_squared_error: 0.9891 - val_loss: 1.8991 - val_mean_squared_error: 1.8991\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.0304 - mean_squared_error: 1.0304 - val_loss: 1.7783 - val_mean_squared_error: 1.7783\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 1.0487 - mean_squared_error: 1.0487 - val_loss: 2.0280 - val_mean_squared_error: 2.0280\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.0601 - mean_squared_error: 1.0601 - val_loss: 1.7345 - val_mean_squared_error: 1.7345\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.9862 - mean_squared_error: 0.9862 - val_loss: 1.9418 - val_mean_squared_error: 1.9418\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 366us/sample - loss: 0.9964 - mean_squared_error: 0.9964 - val_loss: 2.6774 - val_mean_squared_error: 2.6774\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.9043 - mean_squared_error: 0.9043 - val_loss: 1.8772 - val_mean_squared_error: 1.8772\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.9682 - mean_squared_error: 0.9682 - val_loss: 1.8259 - val_mean_squared_error: 1.8259\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.0187 - mean_squared_error: 1.0187 - val_loss: 1.8440 - val_mean_squared_error: 1.8440\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.9723 - mean_squared_error: 0.9723 - val_loss: 1.9173 - val_mean_squared_error: 1.9173\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.0344 - mean_squared_error: 1.0344 - val_loss: 1.9498 - val_mean_squared_error: 1.9498\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 0.9720 - mean_squared_error: 0.9720 - val_loss: 2.0510 - val_mean_squared_error: 2.0510\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.9498 - mean_squared_error: 0.9498 - val_loss: 1.8654 - val_mean_squared_error: 1.8654\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.8876 - mean_squared_error: 0.8876 - val_loss: 1.8607 - val_mean_squared_error: 1.8607\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 0.9133 - mean_squared_error: 0.9133 - val_loss: 1.8150 - val_mean_squared_error: 1.8150\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.8823 - mean_squared_error: 0.8823 - val_loss: 1.8259 - val_mean_squared_error: 1.8259\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.8569 - mean_squared_error: 0.8569 - val_loss: 1.8988 - val_mean_squared_error: 1.8988\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.9270 - mean_squared_error: 0.9270 - val_loss: 1.7834 - val_mean_squared_error: 1.7834\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 367us/sample - loss: 0.8927 - mean_squared_error: 0.8927 - val_loss: 2.0019 - val_mean_squared_error: 2.0019\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 0.9144 - mean_squared_error: 0.9144 - val_loss: 2.0123 - val_mean_squared_error: 2.0123\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.8256 - mean_squared_error: 0.8256 - val_loss: 1.8207 - val_mean_squared_error: 1.8207\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.9534 - mean_squared_error: 0.9534 - val_loss: 1.7460 - val_mean_squared_error: 1.7460\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.8621 - mean_squared_error: 0.8621 - val_loss: 1.9514 - val_mean_squared_error: 1.9514\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.8225 - mean_squared_error: 0.8225 - val_loss: 1.8279 - val_mean_squared_error: 1.8279\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.9122 - mean_squared_error: 0.9122 - val_loss: 1.7780 - val_mean_squared_error: 1.7780\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.7687 - mean_squared_error: 0.7687 - val_loss: 1.8196 - val_mean_squared_error: 1.8196\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.8734 - mean_squared_error: 0.8734 - val_loss: 1.7348 - val_mean_squared_error: 1.7348\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.8073 - mean_squared_error: 0.8073 - val_loss: 1.7233 - val_mean_squared_error: 1.7233\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 366us/sample - loss: 0.8301 - mean_squared_error: 0.8301 - val_loss: 2.0158 - val_mean_squared_error: 2.0158\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 366us/sample - loss: 0.8060 - mean_squared_error: 0.8060 - val_loss: 1.8002 - val_mean_squared_error: 1.8002\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.8365 - mean_squared_error: 0.8365 - val_loss: 1.7581 - val_mean_squared_error: 1.7581\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.8371 - mean_squared_error: 0.8371 - val_loss: 1.9399 - val_mean_squared_error: 1.9399\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.8470 - mean_squared_error: 0.8470 - val_loss: 1.8073 - val_mean_squared_error: 1.8073\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.7267 - mean_squared_error: 0.7267 - val_loss: 1.7605 - val_mean_squared_error: 1.7605\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.7993 - mean_squared_error: 0.7993 - val_loss: 1.8234 - val_mean_squared_error: 1.8234\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.7503 - mean_squared_error: 0.7503 - val_loss: 1.7884 - val_mean_squared_error: 1.7884\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.7839 - mean_squared_error: 0.7839 - val_loss: 1.7918 - val_mean_squared_error: 1.7918\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.7443 - mean_squared_error: 0.7443 - val_loss: 1.8417 - val_mean_squared_error: 1.8417\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.7898 - mean_squared_error: 0.7898 - val_loss: 1.8900 - val_mean_squared_error: 1.8900\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 0.8113 - mean_squared_error: 0.8113 - val_loss: 1.8316 - val_mean_squared_error: 1.8316\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.7831 - mean_squared_error: 0.7831 - val_loss: 1.7693 - val_mean_squared_error: 1.7693\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.7950 - mean_squared_error: 0.7950 - val_loss: 2.2905 - val_mean_squared_error: 2.2905\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.7084 - mean_squared_error: 0.7084 - val_loss: 1.8167 - val_mean_squared_error: 1.8167\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.7457 - mean_squared_error: 0.7457 - val_loss: 1.7374 - val_mean_squared_error: 1.7374\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.6682 - mean_squared_error: 0.6682 - val_loss: 1.9024 - val_mean_squared_error: 1.9024\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 0.7935 - mean_squared_error: 0.7935 - val_loss: 1.9666 - val_mean_squared_error: 1.9666\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.7348 - mean_squared_error: 0.7348 - val_loss: 1.7885 - val_mean_squared_error: 1.7885\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7259 - mean_squared_error: 0.7259 - val_loss: 1.7880 - val_mean_squared_error: 1.7880\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.6913 - mean_squared_error: 0.6913 - val_loss: 1.9704 - val_mean_squared_error: 1.9704\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.6727 - mean_squared_error: 0.6727 - val_loss: 1.7336 - val_mean_squared_error: 1.7336\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.7312 - mean_squared_error: 0.7312 - val_loss: 1.8379 - val_mean_squared_error: 1.8379\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.6662 - mean_squared_error: 0.6662 - val_loss: 1.7676 - val_mean_squared_error: 1.7676\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.6608 - mean_squared_error: 0.6608 - val_loss: 1.7621 - val_mean_squared_error: 1.7621\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.6891 - mean_squared_error: 0.6891 - val_loss: 1.8534 - val_mean_squared_error: 1.8534\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.7013 - mean_squared_error: 0.7013 - val_loss: 1.8317 - val_mean_squared_error: 1.8317\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.7350 - mean_squared_error: 0.7350 - val_loss: 1.7860 - val_mean_squared_error: 1.7860\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.7343 - mean_squared_error: 0.7343 - val_loss: 1.9021 - val_mean_squared_error: 1.9021\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.6884 - mean_squared_error: 0.6884 - val_loss: 1.7396 - val_mean_squared_error: 1.7396\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.6660 - mean_squared_error: 0.6660 - val_loss: 1.9153 - val_mean_squared_error: 1.9153\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7059 - mean_squared_error: 0.7059 - val_loss: 1.9798 - val_mean_squared_error: 1.9798\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.6750 - mean_squared_error: 0.6750 - val_loss: 1.6696 - val_mean_squared_error: 1.6696\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.6462 - mean_squared_error: 0.6462 - val_loss: 1.8894 - val_mean_squared_error: 1.8894\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.6938 - mean_squared_error: 0.6938 - val_loss: 1.7799 - val_mean_squared_error: 1.7799\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.6609 - mean_squared_error: 0.6609 - val_loss: 1.7358 - val_mean_squared_error: 1.7358\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.6236 - mean_squared_error: 0.6236 - val_loss: 1.6884 - val_mean_squared_error: 1.6884\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.6071 - mean_squared_error: 0.6071 - val_loss: 1.8805 - val_mean_squared_error: 1.8805\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.6642 - mean_squared_error: 0.6642 - val_loss: 1.9436 - val_mean_squared_error: 1.9436\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.6715 - mean_squared_error: 0.6715 - val_loss: 1.9138 - val_mean_squared_error: 1.9138\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 366us/sample - loss: 0.6781 - mean_squared_error: 0.6781 - val_loss: 1.7464 - val_mean_squared_error: 1.7464\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.6341 - mean_squared_error: 0.6341 - val_loss: 1.9360 - val_mean_squared_error: 1.9360\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.6147 - mean_squared_error: 0.6147 - val_loss: 1.7906 - val_mean_squared_error: 1.7906\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.6182 - mean_squared_error: 0.6182 - val_loss: 1.7074 - val_mean_squared_error: 1.7074\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 368us/sample - loss: 0.6387 - mean_squared_error: 0.6387 - val_loss: 1.8758 - val_mean_squared_error: 1.8758\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.6257 - mean_squared_error: 0.6257 - val_loss: 1.7948 - val_mean_squared_error: 1.7948\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6268 - mean_squared_error: 0.6268 - val_loss: 1.6966 - val_mean_squared_error: 1.6966\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.6202 - mean_squared_error: 0.6202 - val_loss: 1.7558 - val_mean_squared_error: 1.7558\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.5746 - mean_squared_error: 0.5746 - val_loss: 1.9701 - val_mean_squared_error: 1.9701\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6020 - mean_squared_error: 0.6020 - val_loss: 1.7537 - val_mean_squared_error: 1.7537\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.5863 - mean_squared_error: 0.5863 - val_loss: 1.7308 - val_mean_squared_error: 1.7308\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.5985 - mean_squared_error: 0.5985 - val_loss: 1.7026 - val_mean_squared_error: 1.7026\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5890 - mean_squared_error: 0.5890 - val_loss: 1.7344 - val_mean_squared_error: 1.7344\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.6255 - mean_squared_error: 0.6255 - val_loss: 1.8355 - val_mean_squared_error: 1.8355\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.6354 - mean_squared_error: 0.6354 - val_loss: 1.7898 - val_mean_squared_error: 1.7898\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.5674 - mean_squared_error: 0.5674 - val_loss: 1.8936 - val_mean_squared_error: 1.8936\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.5988 - mean_squared_error: 0.5988 - val_loss: 1.6354 - val_mean_squared_error: 1.6354\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5830 - mean_squared_error: 0.5830 - val_loss: 1.8507 - val_mean_squared_error: 1.8507\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.5655 - mean_squared_error: 0.5655 - val_loss: 1.8417 - val_mean_squared_error: 1.8417\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.6177 - mean_squared_error: 0.6177 - val_loss: 1.9029 - val_mean_squared_error: 1.9029\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.5715 - mean_squared_error: 0.5715 - val_loss: 1.7679 - val_mean_squared_error: 1.7679\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.6185 - mean_squared_error: 0.6185 - val_loss: 1.9228 - val_mean_squared_error: 1.9228\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.5699 - mean_squared_error: 0.5699 - val_loss: 1.7686 - val_mean_squared_error: 1.7686\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5671 - mean_squared_error: 0.5671 - val_loss: 1.9486 - val_mean_squared_error: 1.9486\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.5773 - mean_squared_error: 0.5773 - val_loss: 1.9411 - val_mean_squared_error: 1.9411\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5941 - mean_squared_error: 0.5941 - val_loss: 1.6953 - val_mean_squared_error: 1.6953\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.5346 - mean_squared_error: 0.5346 - val_loss: 1.7803 - val_mean_squared_error: 1.7803\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.5102 - mean_squared_error: 0.5102 - val_loss: 1.7422 - val_mean_squared_error: 1.7422\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.5397 - mean_squared_error: 0.5397 - val_loss: 1.7037 - val_mean_squared_error: 1.7037\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.5567 - mean_squared_error: 0.5567 - val_loss: 1.7221 - val_mean_squared_error: 1.7221\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.5394 - mean_squared_error: 0.5394 - val_loss: 1.7403 - val_mean_squared_error: 1.7403\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.5713 - mean_squared_error: 0.5713 - val_loss: 1.7796 - val_mean_squared_error: 1.7796\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5637 - mean_squared_error: 0.5637 - val_loss: 1.7519 - val_mean_squared_error: 1.7519\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.5676 - mean_squared_error: 0.5676 - val_loss: 1.7836 - val_mean_squared_error: 1.7836\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.5475 - mean_squared_error: 0.5475 - val_loss: 1.8316 - val_mean_squared_error: 1.8316\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.5601 - mean_squared_error: 0.5601 - val_loss: 1.7641 - val_mean_squared_error: 1.7641\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.5646 - mean_squared_error: 0.5646 - val_loss: 1.9031 - val_mean_squared_error: 1.9031\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.5394 - mean_squared_error: 0.5394 - val_loss: 1.7252 - val_mean_squared_error: 1.7252\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.5635 - mean_squared_error: 0.5635 - val_loss: 1.9499 - val_mean_squared_error: 1.9499\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.5051 - mean_squared_error: 0.5051 - val_loss: 1.7279 - val_mean_squared_error: 1.7279\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.5299 - mean_squared_error: 0.5299 - val_loss: 1.8161 - val_mean_squared_error: 1.8161\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5381 - mean_squared_error: 0.5381 - val_loss: 1.8425 - val_mean_squared_error: 1.8425\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5841 - mean_squared_error: 0.5841 - val_loss: 1.8028 - val_mean_squared_error: 1.8028\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.5486 - mean_squared_error: 0.5486 - val_loss: 1.7211 - val_mean_squared_error: 1.7211\n",
            "==================================================\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_36 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_60 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_37 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_38 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_50 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_51 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 757us/sample - loss: 225.8227 - mean_squared_error: 225.8227 - val_loss: 130.7595 - val_mean_squared_error: 130.7595\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 19.0552 - mean_squared_error: 19.0552 - val_loss: 49.5999 - val_mean_squared_error: 49.5999\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 19.2934 - mean_squared_error: 19.2934 - val_loss: 35.7771 - val_mean_squared_error: 35.7772\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 16.5490 - mean_squared_error: 16.5490 - val_loss: 19.1632 - val_mean_squared_error: 19.1632\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 14.0985 - mean_squared_error: 14.0985 - val_loss: 12.8358 - val_mean_squared_error: 12.8358\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 11.8114 - mean_squared_error: 11.8114 - val_loss: 10.2091 - val_mean_squared_error: 10.2091\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 11.8897 - mean_squared_error: 11.8897 - val_loss: 9.4801 - val_mean_squared_error: 9.4801\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 10.7165 - mean_squared_error: 10.7165 - val_loss: 10.3078 - val_mean_squared_error: 10.3078\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 9.3796 - mean_squared_error: 9.3796 - val_loss: 9.0058 - val_mean_squared_error: 9.0058\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 8.6063 - mean_squared_error: 8.6063 - val_loss: 7.7113 - val_mean_squared_error: 7.7113\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 6.6955 - mean_squared_error: 6.6955 - val_loss: 7.2252 - val_mean_squared_error: 7.2252\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 5.9014 - mean_squared_error: 5.9014 - val_loss: 5.8462 - val_mean_squared_error: 5.8462\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 5.0595 - mean_squared_error: 5.0595 - val_loss: 4.7280 - val_mean_squared_error: 4.7280\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 4.5997 - mean_squared_error: 4.5997 - val_loss: 4.3710 - val_mean_squared_error: 4.3710\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 4.0616 - mean_squared_error: 4.0616 - val_loss: 5.6640 - val_mean_squared_error: 5.6640\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 3.8872 - mean_squared_error: 3.8872 - val_loss: 4.2812 - val_mean_squared_error: 4.2812\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 374us/sample - loss: 3.8207 - mean_squared_error: 3.8207 - val_loss: 3.8618 - val_mean_squared_error: 3.8618\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 3.5365 - mean_squared_error: 3.5365 - val_loss: 3.5279 - val_mean_squared_error: 3.5279\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 3.1992 - mean_squared_error: 3.1992 - val_loss: 3.7513 - val_mean_squared_error: 3.7513\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 3.1808 - mean_squared_error: 3.1808 - val_loss: 3.8716 - val_mean_squared_error: 3.8716\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 2.9019 - mean_squared_error: 2.9019 - val_loss: 3.7506 - val_mean_squared_error: 3.7506\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 2.7783 - mean_squared_error: 2.7783 - val_loss: 4.2286 - val_mean_squared_error: 4.2286\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 2.6318 - mean_squared_error: 2.6318 - val_loss: 3.5830 - val_mean_squared_error: 3.5830\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 2.4644 - mean_squared_error: 2.4644 - val_loss: 3.1486 - val_mean_squared_error: 3.1486\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 2.5338 - mean_squared_error: 2.5338 - val_loss: 3.0858 - val_mean_squared_error: 3.0858\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 2.3739 - mean_squared_error: 2.3739 - val_loss: 3.0140 - val_mean_squared_error: 3.0140\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 2.2337 - mean_squared_error: 2.2337 - val_loss: 2.9079 - val_mean_squared_error: 2.9079\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 2.3145 - mean_squared_error: 2.3145 - val_loss: 2.9468 - val_mean_squared_error: 2.9468\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 2.3149 - mean_squared_error: 2.3149 - val_loss: 2.9369 - val_mean_squared_error: 2.9369\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 2.2053 - mean_squared_error: 2.2053 - val_loss: 3.1826 - val_mean_squared_error: 3.1826\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 2.0275 - mean_squared_error: 2.0275 - val_loss: 2.7885 - val_mean_squared_error: 2.7885\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 2.0278 - mean_squared_error: 2.0278 - val_loss: 2.6932 - val_mean_squared_error: 2.6932\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 2.1309 - mean_squared_error: 2.1309 - val_loss: 3.5828 - val_mean_squared_error: 3.5828\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 1.9467 - mean_squared_error: 1.9467 - val_loss: 2.7809 - val_mean_squared_error: 2.7809\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.8235 - mean_squared_error: 1.8235 - val_loss: 2.6282 - val_mean_squared_error: 2.6282\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 1.8413 - mean_squared_error: 1.8413 - val_loss: 2.6485 - val_mean_squared_error: 2.6485\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 1.8589 - mean_squared_error: 1.8589 - val_loss: 2.5173 - val_mean_squared_error: 2.5173\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.6248 - mean_squared_error: 1.6248 - val_loss: 2.4400 - val_mean_squared_error: 2.4400\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.7370 - mean_squared_error: 1.7370 - val_loss: 3.0081 - val_mean_squared_error: 3.0081\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 1.6782 - mean_squared_error: 1.6782 - val_loss: 2.5597 - val_mean_squared_error: 2.5597\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 1.5275 - mean_squared_error: 1.5275 - val_loss: 2.2279 - val_mean_squared_error: 2.2279\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 1.5370 - mean_squared_error: 1.5370 - val_loss: 2.5136 - val_mean_squared_error: 2.5136\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.5792 - mean_squared_error: 1.5792 - val_loss: 2.4019 - val_mean_squared_error: 2.4019\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 375us/sample - loss: 1.5308 - mean_squared_error: 1.5308 - val_loss: 2.3414 - val_mean_squared_error: 2.3414\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 375us/sample - loss: 1.4914 - mean_squared_error: 1.4914 - val_loss: 2.8932 - val_mean_squared_error: 2.8932\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 1.5943 - mean_squared_error: 1.5943 - val_loss: 2.2673 - val_mean_squared_error: 2.2673\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 1.5859 - mean_squared_error: 1.5859 - val_loss: 2.3597 - val_mean_squared_error: 2.3597\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 1.4156 - mean_squared_error: 1.4156 - val_loss: 2.5452 - val_mean_squared_error: 2.5452\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.4203 - mean_squared_error: 1.4202 - val_loss: 2.6457 - val_mean_squared_error: 2.6457\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 1.2999 - mean_squared_error: 1.2999 - val_loss: 2.5381 - val_mean_squared_error: 2.5381\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.2957 - mean_squared_error: 1.2957 - val_loss: 2.3595 - val_mean_squared_error: 2.3595\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.2827 - mean_squared_error: 1.2827 - val_loss: 2.3456 - val_mean_squared_error: 2.3456\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.2781 - mean_squared_error: 1.2781 - val_loss: 2.1993 - val_mean_squared_error: 2.1993\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 1.4258 - mean_squared_error: 1.4258 - val_loss: 2.7445 - val_mean_squared_error: 2.7445\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.3646 - mean_squared_error: 1.3646 - val_loss: 2.2809 - val_mean_squared_error: 2.2809\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 1.3166 - mean_squared_error: 1.3166 - val_loss: 2.2389 - val_mean_squared_error: 2.2389\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 1.3197 - mean_squared_error: 1.3197 - val_loss: 2.3156 - val_mean_squared_error: 2.3156\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 1.2257 - mean_squared_error: 1.2257 - val_loss: 2.2961 - val_mean_squared_error: 2.2961\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.1679 - mean_squared_error: 1.1679 - val_loss: 2.2416 - val_mean_squared_error: 2.2416\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 1.2314 - mean_squared_error: 1.2314 - val_loss: 2.3409 - val_mean_squared_error: 2.3409\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 1.1387 - mean_squared_error: 1.1387 - val_loss: 2.2918 - val_mean_squared_error: 2.2918\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 1.2152 - mean_squared_error: 1.2152 - val_loss: 2.3108 - val_mean_squared_error: 2.3108\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 1.2054 - mean_squared_error: 1.2054 - val_loss: 2.3571 - val_mean_squared_error: 2.3571\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 1.3126 - mean_squared_error: 1.3126 - val_loss: 2.0611 - val_mean_squared_error: 2.0611\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.1284 - mean_squared_error: 1.1284 - val_loss: 2.2383 - val_mean_squared_error: 2.2383\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.0960 - mean_squared_error: 1.0960 - val_loss: 2.1170 - val_mean_squared_error: 2.1170\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 2.3024 - val_mean_squared_error: 2.3024\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.1060 - mean_squared_error: 1.1060 - val_loss: 2.1530 - val_mean_squared_error: 2.1530\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.1589 - mean_squared_error: 1.1589 - val_loss: 2.3425 - val_mean_squared_error: 2.3425\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.1664 - mean_squared_error: 1.1664 - val_loss: 2.1970 - val_mean_squared_error: 2.1970\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.0221 - mean_squared_error: 1.0221 - val_loss: 2.0465 - val_mean_squared_error: 2.0465\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.2728 - mean_squared_error: 1.2728 - val_loss: 2.2395 - val_mean_squared_error: 2.2395\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.0963 - mean_squared_error: 1.0963 - val_loss: 1.9384 - val_mean_squared_error: 1.9384\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.0721 - mean_squared_error: 1.0721 - val_loss: 2.2155 - val_mean_squared_error: 2.2155\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0530 - mean_squared_error: 1.0530 - val_loss: 1.9652 - val_mean_squared_error: 1.9652\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.0233 - mean_squared_error: 1.0233 - val_loss: 2.1043 - val_mean_squared_error: 2.1043\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.0416 - mean_squared_error: 1.0416 - val_loss: 2.2996 - val_mean_squared_error: 2.2996\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.9557 - mean_squared_error: 0.9557 - val_loss: 2.1557 - val_mean_squared_error: 2.1557\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0006 - mean_squared_error: 1.0006 - val_loss: 2.0275 - val_mean_squared_error: 2.0275\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.9825 - mean_squared_error: 0.9825 - val_loss: 2.4176 - val_mean_squared_error: 2.4176\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 1.0464 - mean_squared_error: 1.0464 - val_loss: 2.1126 - val_mean_squared_error: 2.1126\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.9274 - mean_squared_error: 0.9274 - val_loss: 2.4050 - val_mean_squared_error: 2.4050\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.9065 - mean_squared_error: 0.9065 - val_loss: 1.9634 - val_mean_squared_error: 1.9634\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.9248 - mean_squared_error: 0.9248 - val_loss: 2.1863 - val_mean_squared_error: 2.1863\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.9425 - mean_squared_error: 0.9425 - val_loss: 2.1182 - val_mean_squared_error: 2.1182\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.9921 - mean_squared_error: 0.9921 - val_loss: 2.2488 - val_mean_squared_error: 2.2488\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.8485 - mean_squared_error: 0.8485 - val_loss: 2.1995 - val_mean_squared_error: 2.1995\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.9387 - mean_squared_error: 0.9387 - val_loss: 1.8563 - val_mean_squared_error: 1.8563\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.9167 - mean_squared_error: 0.9167 - val_loss: 2.1563 - val_mean_squared_error: 2.1563\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 1.0513 - mean_squared_error: 1.0513 - val_loss: 2.1305 - val_mean_squared_error: 2.1305\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.9557 - mean_squared_error: 0.9557 - val_loss: 2.5664 - val_mean_squared_error: 2.5664\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.8773 - mean_squared_error: 0.8773 - val_loss: 2.4792 - val_mean_squared_error: 2.4792\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.9710 - mean_squared_error: 0.9710 - val_loss: 2.0125 - val_mean_squared_error: 2.0125\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.9205 - mean_squared_error: 0.9205 - val_loss: 2.1790 - val_mean_squared_error: 2.1790\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.8470 - mean_squared_error: 0.8470 - val_loss: 1.9871 - val_mean_squared_error: 1.9871\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 1.9772 - val_mean_squared_error: 1.9772\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.8180 - mean_squared_error: 0.8180 - val_loss: 2.1000 - val_mean_squared_error: 2.1000\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.8140 - mean_squared_error: 0.8140 - val_loss: 1.9567 - val_mean_squared_error: 1.9567\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.8729 - mean_squared_error: 0.8729 - val_loss: 2.2966 - val_mean_squared_error: 2.2966\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.9300 - mean_squared_error: 0.9300 - val_loss: 1.9467 - val_mean_squared_error: 1.9467\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.7294 - mean_squared_error: 0.7294 - val_loss: 2.0092 - val_mean_squared_error: 2.0092\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.7535 - mean_squared_error: 0.7535 - val_loss: 1.8998 - val_mean_squared_error: 1.8998\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.7939 - mean_squared_error: 0.7939 - val_loss: 2.3174 - val_mean_squared_error: 2.3174\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.8717 - mean_squared_error: 0.8717 - val_loss: 1.8618 - val_mean_squared_error: 1.8618\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.8063 - mean_squared_error: 0.8063 - val_loss: 2.0794 - val_mean_squared_error: 2.0794\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 0.8879 - mean_squared_error: 0.8879 - val_loss: 1.9610 - val_mean_squared_error: 1.9610\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.8227 - mean_squared_error: 0.8227 - val_loss: 1.9727 - val_mean_squared_error: 1.9727\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.8092 - mean_squared_error: 0.8092 - val_loss: 1.9958 - val_mean_squared_error: 1.9958\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 0.7228 - mean_squared_error: 0.7228 - val_loss: 2.1896 - val_mean_squared_error: 2.1896\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.7828 - mean_squared_error: 0.7828 - val_loss: 1.9809 - val_mean_squared_error: 1.9809\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.7791 - mean_squared_error: 0.7791 - val_loss: 1.9842 - val_mean_squared_error: 1.9842\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.7288 - mean_squared_error: 0.7288 - val_loss: 2.1132 - val_mean_squared_error: 2.1132\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.7146 - mean_squared_error: 0.7146 - val_loss: 2.0183 - val_mean_squared_error: 2.0183\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.7608 - mean_squared_error: 0.7608 - val_loss: 1.9635 - val_mean_squared_error: 1.9635\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.7604 - mean_squared_error: 0.7604 - val_loss: 1.8311 - val_mean_squared_error: 1.8311\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.7867 - mean_squared_error: 0.7867 - val_loss: 1.9153 - val_mean_squared_error: 1.9153\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.6881 - mean_squared_error: 0.6881 - val_loss: 2.1397 - val_mean_squared_error: 2.1397\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.7247 - mean_squared_error: 0.7247 - val_loss: 1.9898 - val_mean_squared_error: 1.9898\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.7126 - mean_squared_error: 0.7126 - val_loss: 2.0121 - val_mean_squared_error: 2.0121\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.7063 - mean_squared_error: 0.7063 - val_loss: 2.0380 - val_mean_squared_error: 2.0380\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.7402 - mean_squared_error: 0.7402 - val_loss: 1.9669 - val_mean_squared_error: 1.9669\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.7413 - mean_squared_error: 0.7413 - val_loss: 1.7890 - val_mean_squared_error: 1.7890\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.7124 - mean_squared_error: 0.7124 - val_loss: 1.9680 - val_mean_squared_error: 1.9680\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.6869 - mean_squared_error: 0.6869 - val_loss: 1.8768 - val_mean_squared_error: 1.8768\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 375us/sample - loss: 0.6469 - mean_squared_error: 0.6469 - val_loss: 1.8883 - val_mean_squared_error: 1.8883\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.7054 - mean_squared_error: 0.7054 - val_loss: 2.2049 - val_mean_squared_error: 2.2049\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 374us/sample - loss: 0.7082 - mean_squared_error: 0.7082 - val_loss: 2.1446 - val_mean_squared_error: 2.1446\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.7095 - mean_squared_error: 0.7095 - val_loss: 1.9959 - val_mean_squared_error: 1.9959\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.7411 - mean_squared_error: 0.7411 - val_loss: 1.8716 - val_mean_squared_error: 1.8716\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.7004 - mean_squared_error: 0.7004 - val_loss: 1.8634 - val_mean_squared_error: 1.8634\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.7412 - mean_squared_error: 0.7412 - val_loss: 1.9627 - val_mean_squared_error: 1.9627\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.6720 - mean_squared_error: 0.6720 - val_loss: 1.8233 - val_mean_squared_error: 1.8233\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.6061 - mean_squared_error: 0.6061 - val_loss: 1.9098 - val_mean_squared_error: 1.9098\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.6299 - mean_squared_error: 0.6299 - val_loss: 1.8833 - val_mean_squared_error: 1.8833\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.6202 - mean_squared_error: 0.6202 - val_loss: 1.8838 - val_mean_squared_error: 1.8838\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.6363 - mean_squared_error: 0.6363 - val_loss: 1.8995 - val_mean_squared_error: 1.8995\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.6216 - mean_squared_error: 0.6216 - val_loss: 1.8825 - val_mean_squared_error: 1.8825\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.6170 - mean_squared_error: 0.6170 - val_loss: 2.0579 - val_mean_squared_error: 2.0579\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.6204 - mean_squared_error: 0.6204 - val_loss: 1.8559 - val_mean_squared_error: 1.8559\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.7575 - mean_squared_error: 0.7575 - val_loss: 1.8597 - val_mean_squared_error: 1.8597\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.6351 - mean_squared_error: 0.6351 - val_loss: 1.9092 - val_mean_squared_error: 1.9092\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.5918 - mean_squared_error: 0.5918 - val_loss: 1.8431 - val_mean_squared_error: 1.8431\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.6388 - mean_squared_error: 0.6388 - val_loss: 1.7952 - val_mean_squared_error: 1.7952\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.6241 - mean_squared_error: 0.6241 - val_loss: 1.9517 - val_mean_squared_error: 1.9517\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.6463 - mean_squared_error: 0.6463 - val_loss: 2.1648 - val_mean_squared_error: 2.1648\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.6084 - mean_squared_error: 0.6084 - val_loss: 1.9984 - val_mean_squared_error: 1.9984\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.6260 - mean_squared_error: 0.6260 - val_loss: 1.8813 - val_mean_squared_error: 1.8813\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.6523 - mean_squared_error: 0.6523 - val_loss: 1.8839 - val_mean_squared_error: 1.8839\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5619 - mean_squared_error: 0.5619 - val_loss: 1.9455 - val_mean_squared_error: 1.9455\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.5800 - mean_squared_error: 0.5800 - val_loss: 2.2475 - val_mean_squared_error: 2.2475\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.6523 - mean_squared_error: 0.6523 - val_loss: 1.8473 - val_mean_squared_error: 1.8473\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.6273 - mean_squared_error: 0.6273 - val_loss: 1.8266 - val_mean_squared_error: 1.8266\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.6204 - mean_squared_error: 0.6204 - val_loss: 1.9673 - val_mean_squared_error: 1.9673\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.5667 - mean_squared_error: 0.5667 - val_loss: 1.8290 - val_mean_squared_error: 1.8290\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.5353 - mean_squared_error: 0.5353 - val_loss: 1.9024 - val_mean_squared_error: 1.9024\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.6306 - mean_squared_error: 0.6306 - val_loss: 1.9882 - val_mean_squared_error: 1.9882\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.6433 - mean_squared_error: 0.6433 - val_loss: 1.9470 - val_mean_squared_error: 1.9470\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.5997 - mean_squared_error: 0.5997 - val_loss: 1.9111 - val_mean_squared_error: 1.9111\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.5958 - mean_squared_error: 0.5958 - val_loss: 1.8246 - val_mean_squared_error: 1.8246\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.5992 - mean_squared_error: 0.5992 - val_loss: 1.9598 - val_mean_squared_error: 1.9598\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5757 - mean_squared_error: 0.5757 - val_loss: 1.9763 - val_mean_squared_error: 1.9763\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.6494 - mean_squared_error: 0.6494 - val_loss: 1.9165 - val_mean_squared_error: 1.9165\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.5749 - mean_squared_error: 0.5749 - val_loss: 1.8309 - val_mean_squared_error: 1.8309\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 375us/sample - loss: 0.5484 - mean_squared_error: 0.5484 - val_loss: 1.8977 - val_mean_squared_error: 1.8977\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.5572 - mean_squared_error: 0.5572 - val_loss: 1.8618 - val_mean_squared_error: 1.8618\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.6108 - mean_squared_error: 0.6108 - val_loss: 1.8833 - val_mean_squared_error: 1.8833\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.5902 - mean_squared_error: 0.5902 - val_loss: 2.1113 - val_mean_squared_error: 2.1113\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.5829 - mean_squared_error: 0.5829 - val_loss: 1.8056 - val_mean_squared_error: 1.8056\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5918 - mean_squared_error: 0.5918 - val_loss: 1.9108 - val_mean_squared_error: 1.9108\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5759 - mean_squared_error: 0.5759 - val_loss: 1.9169 - val_mean_squared_error: 1.9169\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 0.5811 - mean_squared_error: 0.5811 - val_loss: 1.9524 - val_mean_squared_error: 1.9524\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5896 - mean_squared_error: 0.5896 - val_loss: 1.8325 - val_mean_squared_error: 1.8325\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 375us/sample - loss: 0.5486 - mean_squared_error: 0.5486 - val_loss: 1.8629 - val_mean_squared_error: 1.8629\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.6143 - mean_squared_error: 0.6143 - val_loss: 2.0627 - val_mean_squared_error: 2.0627\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.6106 - mean_squared_error: 0.6106 - val_loss: 2.0001 - val_mean_squared_error: 2.0001\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.5011 - mean_squared_error: 0.5011 - val_loss: 1.8873 - val_mean_squared_error: 1.8873\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5057 - mean_squared_error: 0.5057 - val_loss: 1.9193 - val_mean_squared_error: 1.9193\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.5299 - mean_squared_error: 0.5299 - val_loss: 1.8344 - val_mean_squared_error: 1.8344\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 373us/sample - loss: 0.5719 - mean_squared_error: 0.5719 - val_loss: 1.9515 - val_mean_squared_error: 1.9515\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.4852 - mean_squared_error: 0.4852 - val_loss: 1.9812 - val_mean_squared_error: 1.9812\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.4852 - mean_squared_error: 0.4852 - val_loss: 1.9149 - val_mean_squared_error: 1.9149\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.5572 - mean_squared_error: 0.5572 - val_loss: 1.9018 - val_mean_squared_error: 1.9018\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.5187 - mean_squared_error: 0.5187 - val_loss: 1.8117 - val_mean_squared_error: 1.8117\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5519 - mean_squared_error: 0.5519 - val_loss: 1.9390 - val_mean_squared_error: 1.9390\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.4858 - mean_squared_error: 0.4858 - val_loss: 2.0167 - val_mean_squared_error: 2.0167\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.5322 - mean_squared_error: 0.5322 - val_loss: 1.8898 - val_mean_squared_error: 1.8898\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.5016 - mean_squared_error: 0.5016 - val_loss: 1.8587 - val_mean_squared_error: 1.8587\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.4777 - mean_squared_error: 0.4777 - val_loss: 1.9032 - val_mean_squared_error: 1.9032\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.5525 - mean_squared_error: 0.5525 - val_loss: 1.8208 - val_mean_squared_error: 1.8208\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.4929 - mean_squared_error: 0.4929 - val_loss: 1.8860 - val_mean_squared_error: 1.8860\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.5059 - mean_squared_error: 0.5059 - val_loss: 1.9017 - val_mean_squared_error: 1.9017\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.5059 - mean_squared_error: 0.5059 - val_loss: 1.8903 - val_mean_squared_error: 1.8903\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.4918 - mean_squared_error: 0.4918 - val_loss: 1.9294 - val_mean_squared_error: 1.9294\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5027 - mean_squared_error: 0.5027 - val_loss: 1.8046 - val_mean_squared_error: 1.8046\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 0.4826 - mean_squared_error: 0.4826 - val_loss: 1.9769 - val_mean_squared_error: 1.9769\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.4580 - mean_squared_error: 0.4580 - val_loss: 1.8496 - val_mean_squared_error: 1.8496\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.4643 - mean_squared_error: 0.4643 - val_loss: 1.8178 - val_mean_squared_error: 1.8178\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.4790 - mean_squared_error: 0.4790 - val_loss: 1.8891 - val_mean_squared_error: 1.8891\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.4998 - mean_squared_error: 0.4998 - val_loss: 1.8495 - val_mean_squared_error: 1.8495\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.4844 - mean_squared_error: 0.4844 - val_loss: 1.8576 - val_mean_squared_error: 1.8576\n",
            "==================================================\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_39 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_39 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_52 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_40 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_53 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_41 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_54 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_68 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_55 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_69 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 226.7116 - mean_squared_error: 226.7116 - val_loss: 195.3012 - val_mean_squared_error: 195.3012\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 21.7994 - mean_squared_error: 21.7994 - val_loss: 40.3885 - val_mean_squared_error: 40.3885\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 17.1563 - mean_squared_error: 17.1563 - val_loss: 19.4497 - val_mean_squared_error: 19.4497\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 15.1951 - mean_squared_error: 15.1951 - val_loss: 11.3941 - val_mean_squared_error: 11.3941\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 13.1415 - mean_squared_error: 13.1415 - val_loss: 12.0542 - val_mean_squared_error: 12.0542\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 12.3726 - mean_squared_error: 12.3726 - val_loss: 10.6672 - val_mean_squared_error: 10.6672\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 10.6595 - mean_squared_error: 10.6595 - val_loss: 8.6612 - val_mean_squared_error: 8.6612\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 10.7983 - mean_squared_error: 10.7983 - val_loss: 9.5732 - val_mean_squared_error: 9.5732\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 9.1950 - mean_squared_error: 9.1950 - val_loss: 7.5182 - val_mean_squared_error: 7.5182\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 7.4789 - mean_squared_error: 7.4789 - val_loss: 6.6889 - val_mean_squared_error: 6.6889\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 6.6434 - mean_squared_error: 6.6434 - val_loss: 8.2187 - val_mean_squared_error: 8.2187\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 5.9679 - mean_squared_error: 5.9679 - val_loss: 6.1300 - val_mean_squared_error: 6.1300\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 5.7389 - mean_squared_error: 5.7389 - val_loss: 4.6019 - val_mean_squared_error: 4.6019\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 4.5371 - mean_squared_error: 4.5371 - val_loss: 4.1410 - val_mean_squared_error: 4.1410\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 4.6694 - mean_squared_error: 4.6694 - val_loss: 4.4732 - val_mean_squared_error: 4.4732\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 4.0016 - mean_squared_error: 4.0016 - val_loss: 4.4798 - val_mean_squared_error: 4.4798\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 3.8194 - mean_squared_error: 3.8194 - val_loss: 3.6181 - val_mean_squared_error: 3.6181\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 3.6642 - mean_squared_error: 3.6642 - val_loss: 3.3103 - val_mean_squared_error: 3.3103\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 3.3399 - mean_squared_error: 3.3399 - val_loss: 3.4885 - val_mean_squared_error: 3.4885\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 3.2487 - mean_squared_error: 3.2487 - val_loss: 3.0981 - val_mean_squared_error: 3.0981\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 3.0539 - mean_squared_error: 3.0539 - val_loss: 2.8957 - val_mean_squared_error: 2.8957\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 2.8795 - mean_squared_error: 2.8795 - val_loss: 2.7503 - val_mean_squared_error: 2.7503\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 2.8486 - mean_squared_error: 2.8486 - val_loss: 2.9517 - val_mean_squared_error: 2.9517\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 2.7904 - mean_squared_error: 2.7904 - val_loss: 2.7883 - val_mean_squared_error: 2.7883\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 2.7141 - mean_squared_error: 2.7141 - val_loss: 3.1325 - val_mean_squared_error: 3.1325\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.5780 - mean_squared_error: 2.5780 - val_loss: 2.9156 - val_mean_squared_error: 2.9156\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 2.6586 - mean_squared_error: 2.6586 - val_loss: 2.9203 - val_mean_squared_error: 2.9203\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 2.4915 - mean_squared_error: 2.4915 - val_loss: 3.2851 - val_mean_squared_error: 3.2851\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 2.5124 - mean_squared_error: 2.5124 - val_loss: 2.3334 - val_mean_squared_error: 2.3334\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 2.3314 - mean_squared_error: 2.3314 - val_loss: 2.9559 - val_mean_squared_error: 2.9559\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 2.2596 - mean_squared_error: 2.2596 - val_loss: 2.3835 - val_mean_squared_error: 2.3835\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 2.1398 - mean_squared_error: 2.1398 - val_loss: 2.6147 - val_mean_squared_error: 2.6147\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 2.1419 - mean_squared_error: 2.1419 - val_loss: 4.4464 - val_mean_squared_error: 4.4464\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 2.2843 - mean_squared_error: 2.2843 - val_loss: 2.3654 - val_mean_squared_error: 2.3654\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 2.0964 - mean_squared_error: 2.0964 - val_loss: 2.4153 - val_mean_squared_error: 2.4153\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 2.0567 - mean_squared_error: 2.0567 - val_loss: 2.2309 - val_mean_squared_error: 2.2309\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.9651 - mean_squared_error: 1.9651 - val_loss: 2.5773 - val_mean_squared_error: 2.5773\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.9904 - mean_squared_error: 1.9904 - val_loss: 2.1848 - val_mean_squared_error: 2.1848\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.8489 - mean_squared_error: 1.8489 - val_loss: 2.2348 - val_mean_squared_error: 2.2348\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.9079 - mean_squared_error: 1.9079 - val_loss: 2.4494 - val_mean_squared_error: 2.4494\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.8465 - mean_squared_error: 1.8465 - val_loss: 2.4689 - val_mean_squared_error: 2.4689\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 1.8292 - mean_squared_error: 1.8292 - val_loss: 2.2833 - val_mean_squared_error: 2.2833\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.9476 - mean_squared_error: 1.9476 - val_loss: 1.9874 - val_mean_squared_error: 1.9874\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.7314 - mean_squared_error: 1.7314 - val_loss: 2.0916 - val_mean_squared_error: 2.0916\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 1.8289 - mean_squared_error: 1.8289 - val_loss: 2.0800 - val_mean_squared_error: 2.0800\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.6677 - mean_squared_error: 1.6677 - val_loss: 2.3743 - val_mean_squared_error: 2.3743\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.6456 - mean_squared_error: 1.6456 - val_loss: 2.1810 - val_mean_squared_error: 2.1810\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.5934 - mean_squared_error: 1.5934 - val_loss: 2.0341 - val_mean_squared_error: 2.0341\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.6526 - mean_squared_error: 1.6526 - val_loss: 2.5021 - val_mean_squared_error: 2.5021\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 1.5714 - mean_squared_error: 1.5714 - val_loss: 2.4837 - val_mean_squared_error: 2.4837\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.5069 - mean_squared_error: 1.5069 - val_loss: 1.7929 - val_mean_squared_error: 1.7929\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.5373 - mean_squared_error: 1.5373 - val_loss: 2.8489 - val_mean_squared_error: 2.8489\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.5570 - mean_squared_error: 1.5570 - val_loss: 2.1694 - val_mean_squared_error: 2.1694\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.5306 - mean_squared_error: 1.5306 - val_loss: 2.1478 - val_mean_squared_error: 2.1478\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.5259 - mean_squared_error: 1.5259 - val_loss: 1.9519 - val_mean_squared_error: 1.9519\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.4139 - mean_squared_error: 1.4139 - val_loss: 1.8335 - val_mean_squared_error: 1.8335\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.4017 - mean_squared_error: 1.4017 - val_loss: 1.9740 - val_mean_squared_error: 1.9740\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.3732 - mean_squared_error: 1.3732 - val_loss: 1.9342 - val_mean_squared_error: 1.9342\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.4366 - mean_squared_error: 1.4366 - val_loss: 2.3596 - val_mean_squared_error: 2.3596\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.3350 - mean_squared_error: 1.3350 - val_loss: 2.5524 - val_mean_squared_error: 2.5524\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.3000 - mean_squared_error: 1.3000 - val_loss: 1.9104 - val_mean_squared_error: 1.9104\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.1956 - mean_squared_error: 1.1956 - val_loss: 1.9959 - val_mean_squared_error: 1.9959\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.3309 - mean_squared_error: 1.3309 - val_loss: 2.1704 - val_mean_squared_error: 2.1704\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.2812 - mean_squared_error: 1.2812 - val_loss: 2.1520 - val_mean_squared_error: 2.1520\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.2827 - mean_squared_error: 1.2827 - val_loss: 1.8521 - val_mean_squared_error: 1.8521\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1765 - mean_squared_error: 1.1765 - val_loss: 1.7024 - val_mean_squared_error: 1.7024\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.1573 - mean_squared_error: 1.1573 - val_loss: 1.7137 - val_mean_squared_error: 1.7137\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.3585 - mean_squared_error: 1.3585 - val_loss: 2.0026 - val_mean_squared_error: 2.0026\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.2925 - mean_squared_error: 1.2925 - val_loss: 1.9840 - val_mean_squared_error: 1.9840\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.2186 - mean_squared_error: 1.2186 - val_loss: 1.7089 - val_mean_squared_error: 1.7089\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.2838 - mean_squared_error: 1.2838 - val_loss: 1.9552 - val_mean_squared_error: 1.9552\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.2212 - mean_squared_error: 1.2212 - val_loss: 1.9813 - val_mean_squared_error: 1.9813\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 1.2192 - mean_squared_error: 1.2192 - val_loss: 1.7397 - val_mean_squared_error: 1.7397\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.2031 - mean_squared_error: 1.2031 - val_loss: 1.8565 - val_mean_squared_error: 1.8565\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.2051 - mean_squared_error: 1.2051 - val_loss: 2.1886 - val_mean_squared_error: 2.1886\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.1394 - mean_squared_error: 1.1394 - val_loss: 1.7788 - val_mean_squared_error: 1.7788\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.0911 - mean_squared_error: 1.0911 - val_loss: 1.8009 - val_mean_squared_error: 1.8009\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 2.0089 - val_mean_squared_error: 2.0089\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.1372 - mean_squared_error: 1.1372 - val_loss: 2.0776 - val_mean_squared_error: 2.0776\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.0931 - mean_squared_error: 1.0931 - val_loss: 1.8340 - val_mean_squared_error: 1.8340\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0180 - mean_squared_error: 1.0180 - val_loss: 1.6330 - val_mean_squared_error: 1.6330\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1203 - mean_squared_error: 1.1203 - val_loss: 1.5894 - val_mean_squared_error: 1.5894\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.0927 - mean_squared_error: 1.0927 - val_loss: 1.7501 - val_mean_squared_error: 1.7501\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.0600 - mean_squared_error: 1.0600 - val_loss: 1.7558 - val_mean_squared_error: 1.7558\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0039 - mean_squared_error: 1.0039 - val_loss: 1.9341 - val_mean_squared_error: 1.9341\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.0890 - mean_squared_error: 1.0890 - val_loss: 1.8289 - val_mean_squared_error: 1.8289\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.1661 - mean_squared_error: 1.1661 - val_loss: 1.7534 - val_mean_squared_error: 1.7534\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 1.0236 - mean_squared_error: 1.0236 - val_loss: 1.8866 - val_mean_squared_error: 1.8866\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.9834 - mean_squared_error: 0.9834 - val_loss: 1.7048 - val_mean_squared_error: 1.7048\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 403us/sample - loss: 0.9682 - mean_squared_error: 0.9682 - val_loss: 1.6071 - val_mean_squared_error: 1.6071\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.9102 - mean_squared_error: 0.9102 - val_loss: 1.7074 - val_mean_squared_error: 1.7074\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.0408 - mean_squared_error: 1.0408 - val_loss: 1.6851 - val_mean_squared_error: 1.6851\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 0.9689 - mean_squared_error: 0.9689 - val_loss: 1.7795 - val_mean_squared_error: 1.7795\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 2.3171 - val_mean_squared_error: 2.3171\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.0545 - mean_squared_error: 1.0545 - val_loss: 1.7365 - val_mean_squared_error: 1.7365\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.0382 - mean_squared_error: 1.0382 - val_loss: 1.6673 - val_mean_squared_error: 1.6673\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.9033 - mean_squared_error: 0.9033 - val_loss: 1.5928 - val_mean_squared_error: 1.5928\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.8872 - mean_squared_error: 0.8872 - val_loss: 1.8052 - val_mean_squared_error: 1.8052\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.9458 - mean_squared_error: 0.9458 - val_loss: 1.6996 - val_mean_squared_error: 1.6996\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.8841 - mean_squared_error: 0.8841 - val_loss: 1.6860 - val_mean_squared_error: 1.6860\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.8385 - mean_squared_error: 0.8385 - val_loss: 1.6044 - val_mean_squared_error: 1.6044\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8695 - mean_squared_error: 0.8695 - val_loss: 1.7177 - val_mean_squared_error: 1.7177\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.9479 - mean_squared_error: 0.9479 - val_loss: 1.7193 - val_mean_squared_error: 1.7193\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.8740 - mean_squared_error: 0.8740 - val_loss: 1.8277 - val_mean_squared_error: 1.8277\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8578 - mean_squared_error: 0.8578 - val_loss: 1.9621 - val_mean_squared_error: 1.9621\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9407 - mean_squared_error: 0.9407 - val_loss: 1.6854 - val_mean_squared_error: 1.6854\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8837 - mean_squared_error: 0.8837 - val_loss: 1.7270 - val_mean_squared_error: 1.7270\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.8474 - mean_squared_error: 0.8474 - val_loss: 1.6807 - val_mean_squared_error: 1.6807\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8965 - mean_squared_error: 0.8965 - val_loss: 1.6421 - val_mean_squared_error: 1.6421\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8499 - mean_squared_error: 0.8499 - val_loss: 1.9451 - val_mean_squared_error: 1.9451\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8133 - mean_squared_error: 0.8133 - val_loss: 1.5337 - val_mean_squared_error: 1.5337\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8684 - mean_squared_error: 0.8684 - val_loss: 1.7019 - val_mean_squared_error: 1.7019\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.7949 - mean_squared_error: 0.7949 - val_loss: 1.8548 - val_mean_squared_error: 1.8548\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8335 - mean_squared_error: 0.8335 - val_loss: 1.5307 - val_mean_squared_error: 1.5307\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8683 - mean_squared_error: 0.8683 - val_loss: 1.6430 - val_mean_squared_error: 1.6430\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8730 - mean_squared_error: 0.8730 - val_loss: 1.6957 - val_mean_squared_error: 1.6957\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7870 - mean_squared_error: 0.7870 - val_loss: 1.7350 - val_mean_squared_error: 1.7350\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8169 - mean_squared_error: 0.8169 - val_loss: 1.6904 - val_mean_squared_error: 1.6904\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7615 - mean_squared_error: 0.7615 - val_loss: 1.8338 - val_mean_squared_error: 1.8338\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7703 - mean_squared_error: 0.7703 - val_loss: 1.7521 - val_mean_squared_error: 1.7521\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.7534 - mean_squared_error: 0.7534 - val_loss: 1.6225 - val_mean_squared_error: 1.6225\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7552 - mean_squared_error: 0.7552 - val_loss: 1.7617 - val_mean_squared_error: 1.7617\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7589 - mean_squared_error: 0.7589 - val_loss: 1.9205 - val_mean_squared_error: 1.9205\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.8305 - mean_squared_error: 0.8305 - val_loss: 1.7864 - val_mean_squared_error: 1.7864\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8581 - mean_squared_error: 0.8581 - val_loss: 2.3539 - val_mean_squared_error: 2.3539\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.8290 - mean_squared_error: 0.8290 - val_loss: 1.5973 - val_mean_squared_error: 1.5973\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7233 - mean_squared_error: 0.7233 - val_loss: 1.7401 - val_mean_squared_error: 1.7401\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.7089 - mean_squared_error: 0.7089 - val_loss: 1.7109 - val_mean_squared_error: 1.7109\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7316 - mean_squared_error: 0.7316 - val_loss: 1.6614 - val_mean_squared_error: 1.6614\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.7057 - mean_squared_error: 0.7057 - val_loss: 1.7409 - val_mean_squared_error: 1.7409\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7644 - mean_squared_error: 0.7644 - val_loss: 1.7944 - val_mean_squared_error: 1.7944\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.8621 - mean_squared_error: 0.8621 - val_loss: 1.6923 - val_mean_squared_error: 1.6923\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.6763 - mean_squared_error: 0.6763 - val_loss: 1.6992 - val_mean_squared_error: 1.6992\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.7297 - mean_squared_error: 0.7297 - val_loss: 1.6208 - val_mean_squared_error: 1.6208\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7410 - mean_squared_error: 0.7410 - val_loss: 1.7825 - val_mean_squared_error: 1.7825\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.7384 - mean_squared_error: 0.7384 - val_loss: 1.7003 - val_mean_squared_error: 1.7003\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.7242 - mean_squared_error: 0.7242 - val_loss: 1.6580 - val_mean_squared_error: 1.6580\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7704 - mean_squared_error: 0.7704 - val_loss: 1.6357 - val_mean_squared_error: 1.6357\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.7416 - mean_squared_error: 0.7416 - val_loss: 1.8125 - val_mean_squared_error: 1.8125\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7236 - mean_squared_error: 0.7236 - val_loss: 1.7391 - val_mean_squared_error: 1.7391\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6986 - mean_squared_error: 0.6986 - val_loss: 1.5861 - val_mean_squared_error: 1.5861\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.6517 - mean_squared_error: 0.6517 - val_loss: 1.5971 - val_mean_squared_error: 1.5971\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6562 - mean_squared_error: 0.6562 - val_loss: 1.7245 - val_mean_squared_error: 1.7245\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.7078 - mean_squared_error: 0.7078 - val_loss: 1.8360 - val_mean_squared_error: 1.8360\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7006 - mean_squared_error: 0.7006 - val_loss: 1.8200 - val_mean_squared_error: 1.8200\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.7171 - mean_squared_error: 0.7171 - val_loss: 1.8786 - val_mean_squared_error: 1.8786\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.6634 - mean_squared_error: 0.6634 - val_loss: 1.6644 - val_mean_squared_error: 1.6644\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7927 - mean_squared_error: 0.7927 - val_loss: 2.0496 - val_mean_squared_error: 2.0496\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7191 - mean_squared_error: 0.7191 - val_loss: 1.8018 - val_mean_squared_error: 1.8018\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6535 - mean_squared_error: 0.6535 - val_loss: 1.6056 - val_mean_squared_error: 1.6056\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6832 - mean_squared_error: 0.6832 - val_loss: 1.9954 - val_mean_squared_error: 1.9954\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6278 - mean_squared_error: 0.6278 - val_loss: 1.8557 - val_mean_squared_error: 1.8557\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6204 - mean_squared_error: 0.6204 - val_loss: 1.6120 - val_mean_squared_error: 1.6120\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6398 - mean_squared_error: 0.6398 - val_loss: 1.7612 - val_mean_squared_error: 1.7612\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6386 - mean_squared_error: 0.6386 - val_loss: 1.6761 - val_mean_squared_error: 1.6761\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6318 - mean_squared_error: 0.6318 - val_loss: 1.6745 - val_mean_squared_error: 1.6745\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6381 - mean_squared_error: 0.6381 - val_loss: 1.7307 - val_mean_squared_error: 1.7307\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6529 - mean_squared_error: 0.6529 - val_loss: 1.8846 - val_mean_squared_error: 1.8846\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6298 - mean_squared_error: 0.6298 - val_loss: 1.8538 - val_mean_squared_error: 1.8538\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6749 - mean_squared_error: 0.6749 - val_loss: 1.6846 - val_mean_squared_error: 1.6846\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.5807 - mean_squared_error: 0.5807 - val_loss: 1.6724 - val_mean_squared_error: 1.6724\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6086 - mean_squared_error: 0.6086 - val_loss: 1.6359 - val_mean_squared_error: 1.6359\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.6243 - mean_squared_error: 0.6243 - val_loss: 1.6429 - val_mean_squared_error: 1.6429\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.6179 - mean_squared_error: 0.6179 - val_loss: 1.8083 - val_mean_squared_error: 1.8083\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.6231 - mean_squared_error: 0.6231 - val_loss: 1.7720 - val_mean_squared_error: 1.7720\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5983 - mean_squared_error: 0.5983 - val_loss: 1.6673 - val_mean_squared_error: 1.6673\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.5781 - mean_squared_error: 0.5781 - val_loss: 1.7970 - val_mean_squared_error: 1.7970\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5823 - mean_squared_error: 0.5823 - val_loss: 1.7078 - val_mean_squared_error: 1.7078\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6242 - mean_squared_error: 0.6242 - val_loss: 1.8457 - val_mean_squared_error: 1.8457\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6184 - mean_squared_error: 0.6184 - val_loss: 1.7061 - val_mean_squared_error: 1.7061\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5805 - mean_squared_error: 0.5805 - val_loss: 1.7608 - val_mean_squared_error: 1.7608\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.5653 - mean_squared_error: 0.5653 - val_loss: 1.7555 - val_mean_squared_error: 1.7555\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.5480 - mean_squared_error: 0.5480 - val_loss: 1.6350 - val_mean_squared_error: 1.6350\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5638 - mean_squared_error: 0.5638 - val_loss: 1.7144 - val_mean_squared_error: 1.7144\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6130 - mean_squared_error: 0.6130 - val_loss: 1.8270 - val_mean_squared_error: 1.8270\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.6138 - mean_squared_error: 0.6138 - val_loss: 1.6684 - val_mean_squared_error: 1.6684\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5883 - mean_squared_error: 0.5883 - val_loss: 1.7462 - val_mean_squared_error: 1.7462\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.5523 - mean_squared_error: 0.5523 - val_loss: 1.6906 - val_mean_squared_error: 1.6906\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 0.6035 - mean_squared_error: 0.6035 - val_loss: 1.6901 - val_mean_squared_error: 1.6901\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.5339 - mean_squared_error: 0.5339 - val_loss: 1.7461 - val_mean_squared_error: 1.7461\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6239 - mean_squared_error: 0.6239 - val_loss: 1.6709 - val_mean_squared_error: 1.6709\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5451 - mean_squared_error: 0.5451 - val_loss: 1.7992 - val_mean_squared_error: 1.7992\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5701 - mean_squared_error: 0.5701 - val_loss: 1.8067 - val_mean_squared_error: 1.8067\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 0.5580 - mean_squared_error: 0.5580 - val_loss: 1.6402 - val_mean_squared_error: 1.6402\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5976 - mean_squared_error: 0.5976 - val_loss: 1.6478 - val_mean_squared_error: 1.6478\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6027 - mean_squared_error: 0.6027 - val_loss: 1.7721 - val_mean_squared_error: 1.7721\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6061 - mean_squared_error: 0.6061 - val_loss: 1.7162 - val_mean_squared_error: 1.7162\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.5998 - mean_squared_error: 0.5998 - val_loss: 1.7008 - val_mean_squared_error: 1.7008\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5655 - mean_squared_error: 0.5655 - val_loss: 1.7972 - val_mean_squared_error: 1.7972\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.5670 - mean_squared_error: 0.5670 - val_loss: 1.8280 - val_mean_squared_error: 1.8280\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5578 - mean_squared_error: 0.5578 - val_loss: 1.8550 - val_mean_squared_error: 1.8550\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.5524 - mean_squared_error: 0.5524 - val_loss: 1.7676 - val_mean_squared_error: 1.7676\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5563 - mean_squared_error: 0.5563 - val_loss: 1.6515 - val_mean_squared_error: 1.6515\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.5268 - mean_squared_error: 0.5268 - val_loss: 1.6682 - val_mean_squared_error: 1.6682\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.5688 - mean_squared_error: 0.5688 - val_loss: 1.8779 - val_mean_squared_error: 1.8779\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5103 - mean_squared_error: 0.5103 - val_loss: 1.7405 - val_mean_squared_error: 1.7405\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5200 - mean_squared_error: 0.5200 - val_loss: 1.7556 - val_mean_squared_error: 1.7556\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5491 - mean_squared_error: 0.5491 - val_loss: 1.7033 - val_mean_squared_error: 1.7033\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5289 - mean_squared_error: 0.5289 - val_loss: 1.6752 - val_mean_squared_error: 1.6752\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.5052 - mean_squared_error: 0.5052 - val_loss: 1.8117 - val_mean_squared_error: 1.8117\n",
            "==================================================\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_42 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_42 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_56 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_43 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_57 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_58 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_73 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_59 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_74 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 773us/sample - loss: 171.5404 - mean_squared_error: 171.5405 - val_loss: 181.1619 - val_mean_squared_error: 181.1619\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 19.4808 - mean_squared_error: 19.4808 - val_loss: 57.7885 - val_mean_squared_error: 57.7885\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 17.2639 - mean_squared_error: 17.2639 - val_loss: 23.6416 - val_mean_squared_error: 23.6416\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 15.5013 - mean_squared_error: 15.5013 - val_loss: 18.6819 - val_mean_squared_error: 18.6819\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 13.8931 - mean_squared_error: 13.8931 - val_loss: 14.9824 - val_mean_squared_error: 14.9824\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 12.1536 - mean_squared_error: 12.1536 - val_loss: 14.1705 - val_mean_squared_error: 14.1705\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 12.2923 - mean_squared_error: 12.2923 - val_loss: 11.1057 - val_mean_squared_error: 11.1057\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 10.0742 - mean_squared_error: 10.0742 - val_loss: 10.4033 - val_mean_squared_error: 10.4033\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 8.7966 - mean_squared_error: 8.7966 - val_loss: 8.6144 - val_mean_squared_error: 8.6144\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 335us/sample - loss: 7.2644 - mean_squared_error: 7.2644 - val_loss: 8.6460 - val_mean_squared_error: 8.6460\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 335us/sample - loss: 6.1993 - mean_squared_error: 6.1993 - val_loss: 6.2802 - val_mean_squared_error: 6.2802\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 5.3166 - mean_squared_error: 5.3166 - val_loss: 5.7791 - val_mean_squared_error: 5.7791\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 4.6440 - mean_squared_error: 4.6440 - val_loss: 4.8422 - val_mean_squared_error: 4.8422\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 4.2184 - mean_squared_error: 4.2184 - val_loss: 4.7371 - val_mean_squared_error: 4.7371\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 333us/sample - loss: 3.7779 - mean_squared_error: 3.7779 - val_loss: 4.6109 - val_mean_squared_error: 4.6109\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 3.6085 - mean_squared_error: 3.6085 - val_loss: 4.4770 - val_mean_squared_error: 4.4770\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 3.3666 - mean_squared_error: 3.3666 - val_loss: 3.8256 - val_mean_squared_error: 3.8256\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 3.2268 - mean_squared_error: 3.2268 - val_loss: 4.2022 - val_mean_squared_error: 4.2022\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.9315 - mean_squared_error: 2.9315 - val_loss: 3.5476 - val_mean_squared_error: 3.5476\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 2.7567 - mean_squared_error: 2.7567 - val_loss: 3.0520 - val_mean_squared_error: 3.0520\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 2.8528 - mean_squared_error: 2.8528 - val_loss: 3.4776 - val_mean_squared_error: 3.4776\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 2.5053 - mean_squared_error: 2.5053 - val_loss: 2.8251 - val_mean_squared_error: 2.8251\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 2.5661 - mean_squared_error: 2.5661 - val_loss: 4.1363 - val_mean_squared_error: 4.1363\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.4408 - mean_squared_error: 2.4408 - val_loss: 2.8235 - val_mean_squared_error: 2.8235\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 2.4169 - mean_squared_error: 2.4169 - val_loss: 2.9146 - val_mean_squared_error: 2.9146\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.3518 - mean_squared_error: 2.3518 - val_loss: 2.9335 - val_mean_squared_error: 2.9335\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 2.1268 - mean_squared_error: 2.1268 - val_loss: 2.8541 - val_mean_squared_error: 2.8541\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.2651 - mean_squared_error: 2.2651 - val_loss: 3.5486 - val_mean_squared_error: 3.5486\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.1431 - mean_squared_error: 2.1431 - val_loss: 2.9024 - val_mean_squared_error: 2.9024\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 2.0162 - mean_squared_error: 2.0162 - val_loss: 2.6763 - val_mean_squared_error: 2.6763\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 2.0300 - mean_squared_error: 2.0300 - val_loss: 2.6624 - val_mean_squared_error: 2.6624\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.9178 - mean_squared_error: 1.9178 - val_loss: 2.5591 - val_mean_squared_error: 2.5591\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.8178 - mean_squared_error: 1.8178 - val_loss: 2.3958 - val_mean_squared_error: 2.3958\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 2.0908 - mean_squared_error: 2.0908 - val_loss: 2.6564 - val_mean_squared_error: 2.6564\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.8840 - mean_squared_error: 1.8840 - val_loss: 3.0869 - val_mean_squared_error: 3.0869\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.8442 - mean_squared_error: 1.8442 - val_loss: 2.7430 - val_mean_squared_error: 2.7430\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.8335 - mean_squared_error: 1.8335 - val_loss: 2.4883 - val_mean_squared_error: 2.4883\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.6246 - mean_squared_error: 1.6246 - val_loss: 2.2987 - val_mean_squared_error: 2.2987\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 1.5330 - mean_squared_error: 1.5330 - val_loss: 2.1632 - val_mean_squared_error: 2.1632\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.6668 - mean_squared_error: 1.6668 - val_loss: 2.4251 - val_mean_squared_error: 2.4251\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.8200 - mean_squared_error: 1.8200 - val_loss: 2.3905 - val_mean_squared_error: 2.3905\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.5378 - mean_squared_error: 1.5378 - val_loss: 2.3540 - val_mean_squared_error: 2.3540\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.6408 - mean_squared_error: 1.6408 - val_loss: 2.6868 - val_mean_squared_error: 2.6868\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 1.5319 - mean_squared_error: 1.5319 - val_loss: 2.0988 - val_mean_squared_error: 2.0988\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.5862 - mean_squared_error: 1.5862 - val_loss: 2.6334 - val_mean_squared_error: 2.6334\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 1.6568 - mean_squared_error: 1.6568 - val_loss: 2.2715 - val_mean_squared_error: 2.2715\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 1.4760 - mean_squared_error: 1.4760 - val_loss: 3.2187 - val_mean_squared_error: 3.2187\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.5358 - mean_squared_error: 1.5358 - val_loss: 2.2656 - val_mean_squared_error: 2.2656\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.4594 - mean_squared_error: 1.4594 - val_loss: 2.1045 - val_mean_squared_error: 2.1045\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.4397 - mean_squared_error: 1.4397 - val_loss: 2.1464 - val_mean_squared_error: 2.1464\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.3090 - mean_squared_error: 1.3090 - val_loss: 2.3231 - val_mean_squared_error: 2.3231\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.2583 - mean_squared_error: 1.2583 - val_loss: 2.3623 - val_mean_squared_error: 2.3623\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.3768 - mean_squared_error: 1.3768 - val_loss: 2.1756 - val_mean_squared_error: 2.1756\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.2905 - mean_squared_error: 1.2905 - val_loss: 2.3466 - val_mean_squared_error: 2.3466\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2681 - mean_squared_error: 1.2681 - val_loss: 2.5466 - val_mean_squared_error: 2.5466\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.2399 - mean_squared_error: 1.2399 - val_loss: 1.9911 - val_mean_squared_error: 1.9911\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 1.3601 - mean_squared_error: 1.3601 - val_loss: 2.3461 - val_mean_squared_error: 2.3461\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.2490 - mean_squared_error: 1.2490 - val_loss: 3.2938 - val_mean_squared_error: 3.2938\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.2995 - mean_squared_error: 1.2995 - val_loss: 2.5295 - val_mean_squared_error: 2.5295\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.1937 - mean_squared_error: 1.1937 - val_loss: 2.3381 - val_mean_squared_error: 2.3381\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.1138 - mean_squared_error: 1.1138 - val_loss: 3.0353 - val_mean_squared_error: 3.0353\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.1498 - mean_squared_error: 1.1498 - val_loss: 2.3183 - val_mean_squared_error: 2.3183\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2445 - mean_squared_error: 1.2445 - val_loss: 2.2272 - val_mean_squared_error: 2.2272\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.2727 - mean_squared_error: 1.2727 - val_loss: 2.6147 - val_mean_squared_error: 2.6147\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.1120 - mean_squared_error: 1.1120 - val_loss: 2.6823 - val_mean_squared_error: 2.6823\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.2044 - mean_squared_error: 1.2044 - val_loss: 2.1192 - val_mean_squared_error: 2.1192\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.0666 - mean_squared_error: 1.0666 - val_loss: 2.2004 - val_mean_squared_error: 2.2004\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.0711 - mean_squared_error: 1.0711 - val_loss: 2.4231 - val_mean_squared_error: 2.4231\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.1947 - mean_squared_error: 1.1947 - val_loss: 2.2371 - val_mean_squared_error: 2.2371\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 1.1112 - mean_squared_error: 1.1112 - val_loss: 2.7487 - val_mean_squared_error: 2.7487\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 1.1942 - mean_squared_error: 1.1942 - val_loss: 2.1762 - val_mean_squared_error: 2.1762\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.0931 - mean_squared_error: 1.0931 - val_loss: 2.1498 - val_mean_squared_error: 2.1498\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.1325 - mean_squared_error: 1.1325 - val_loss: 2.6020 - val_mean_squared_error: 2.6020\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.0642 - mean_squared_error: 1.0642 - val_loss: 2.3280 - val_mean_squared_error: 2.3280\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.1608 - mean_squared_error: 1.1608 - val_loss: 1.9472 - val_mean_squared_error: 1.9472\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.0837 - mean_squared_error: 1.0837 - val_loss: 2.1180 - val_mean_squared_error: 2.1180\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.0474 - mean_squared_error: 1.0474 - val_loss: 2.4383 - val_mean_squared_error: 2.4383\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 2.0961 - val_mean_squared_error: 2.0961\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.0010 - mean_squared_error: 1.0010 - val_loss: 2.1817 - val_mean_squared_error: 2.1817\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.9925 - val_mean_squared_error: 1.9925\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.0339 - mean_squared_error: 1.0339 - val_loss: 2.3407 - val_mean_squared_error: 2.3407\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.9377 - mean_squared_error: 0.9377 - val_loss: 2.0681 - val_mean_squared_error: 2.0681\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.0721 - mean_squared_error: 1.0721 - val_loss: 1.9872 - val_mean_squared_error: 1.9872\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.9020 - mean_squared_error: 0.9020 - val_loss: 2.2114 - val_mean_squared_error: 2.2114\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.9410 - mean_squared_error: 0.9410 - val_loss: 1.9056 - val_mean_squared_error: 1.9056\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.8665 - mean_squared_error: 0.8665 - val_loss: 2.2483 - val_mean_squared_error: 2.2483\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.9215 - mean_squared_error: 0.9215 - val_loss: 2.2530 - val_mean_squared_error: 2.2530\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.9458 - mean_squared_error: 0.9458 - val_loss: 2.1068 - val_mean_squared_error: 2.1068\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9479 - mean_squared_error: 0.9479 - val_loss: 2.0903 - val_mean_squared_error: 2.0903\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.9895 - mean_squared_error: 0.9895 - val_loss: 2.2738 - val_mean_squared_error: 2.2738\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.9254 - mean_squared_error: 0.9254 - val_loss: 2.2146 - val_mean_squared_error: 2.2146\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.8878 - mean_squared_error: 0.8878 - val_loss: 1.9877 - val_mean_squared_error: 1.9877\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.8722 - mean_squared_error: 0.8722 - val_loss: 2.4371 - val_mean_squared_error: 2.4371\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.8551 - mean_squared_error: 0.8551 - val_loss: 2.0628 - val_mean_squared_error: 2.0628\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.9380 - mean_squared_error: 0.9380 - val_loss: 2.2171 - val_mean_squared_error: 2.2171\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.8576 - mean_squared_error: 0.8576 - val_loss: 1.9407 - val_mean_squared_error: 1.9407\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.8244 - mean_squared_error: 0.8244 - val_loss: 2.2372 - val_mean_squared_error: 2.2372\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.8728 - mean_squared_error: 0.8728 - val_loss: 2.0467 - val_mean_squared_error: 2.0467\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.9395 - mean_squared_error: 0.9395 - val_loss: 2.1678 - val_mean_squared_error: 2.1678\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.9731 - mean_squared_error: 0.9731 - val_loss: 2.3618 - val_mean_squared_error: 2.3618\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 2.0581 - val_mean_squared_error: 2.0581\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.7983 - mean_squared_error: 0.7983 - val_loss: 2.0277 - val_mean_squared_error: 2.0277\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.8664 - mean_squared_error: 0.8664 - val_loss: 2.2349 - val_mean_squared_error: 2.2349\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.8104 - mean_squared_error: 0.8104 - val_loss: 2.0732 - val_mean_squared_error: 2.0732\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.7351 - mean_squared_error: 0.7351 - val_loss: 1.9180 - val_mean_squared_error: 1.9180\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.8126 - mean_squared_error: 0.8126 - val_loss: 2.4472 - val_mean_squared_error: 2.4472\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.9047 - mean_squared_error: 0.9047 - val_loss: 2.5355 - val_mean_squared_error: 2.5355\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.7601 - mean_squared_error: 0.7601 - val_loss: 1.9058 - val_mean_squared_error: 1.9058\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.7318 - mean_squared_error: 0.7318 - val_loss: 2.1204 - val_mean_squared_error: 2.1204\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.8243 - mean_squared_error: 0.8243 - val_loss: 2.2268 - val_mean_squared_error: 2.2268\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.7771 - mean_squared_error: 0.7771 - val_loss: 2.3191 - val_mean_squared_error: 2.3191\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.7187 - mean_squared_error: 0.7187 - val_loss: 2.1767 - val_mean_squared_error: 2.1767\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 0.7451 - mean_squared_error: 0.7451 - val_loss: 1.9685 - val_mean_squared_error: 1.9685\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7522 - mean_squared_error: 0.7522 - val_loss: 2.1161 - val_mean_squared_error: 2.1161\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.8762 - mean_squared_error: 0.8762 - val_loss: 1.8834 - val_mean_squared_error: 1.8834\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7432 - mean_squared_error: 0.7432 - val_loss: 2.3319 - val_mean_squared_error: 2.3319\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.7581 - mean_squared_error: 0.7581 - val_loss: 1.9438 - val_mean_squared_error: 1.9438\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7041 - mean_squared_error: 0.7041 - val_loss: 1.8755 - val_mean_squared_error: 1.8755\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.7289 - mean_squared_error: 0.7289 - val_loss: 2.0133 - val_mean_squared_error: 2.0133\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.7076 - mean_squared_error: 0.7076 - val_loss: 2.1084 - val_mean_squared_error: 2.1084\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.7304 - mean_squared_error: 0.7304 - val_loss: 2.1053 - val_mean_squared_error: 2.1053\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7250 - mean_squared_error: 0.7250 - val_loss: 2.0369 - val_mean_squared_error: 2.0369\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.7294 - mean_squared_error: 0.7294 - val_loss: 2.1099 - val_mean_squared_error: 2.1099\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.7673 - mean_squared_error: 0.7673 - val_loss: 1.9831 - val_mean_squared_error: 1.9831\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.7392 - mean_squared_error: 0.7392 - val_loss: 1.9840 - val_mean_squared_error: 1.9840\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.6974 - mean_squared_error: 0.6974 - val_loss: 2.0550 - val_mean_squared_error: 2.0550\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.7109 - mean_squared_error: 0.7109 - val_loss: 1.9088 - val_mean_squared_error: 1.9088\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.6523 - mean_squared_error: 0.6523 - val_loss: 1.8857 - val_mean_squared_error: 1.8857\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.8069 - mean_squared_error: 0.8069 - val_loss: 1.9254 - val_mean_squared_error: 1.9254\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.6676 - mean_squared_error: 0.6676 - val_loss: 1.8595 - val_mean_squared_error: 1.8595\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.5801 - mean_squared_error: 0.5801 - val_loss: 1.9638 - val_mean_squared_error: 1.9638\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.6894 - mean_squared_error: 0.6894 - val_loss: 1.9175 - val_mean_squared_error: 1.9175\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 0.6532 - mean_squared_error: 0.6532 - val_loss: 1.8617 - val_mean_squared_error: 1.8617\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.6543 - mean_squared_error: 0.6543 - val_loss: 1.9402 - val_mean_squared_error: 1.9402\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.6155 - mean_squared_error: 0.6155 - val_loss: 1.9639 - val_mean_squared_error: 1.9639\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.6560 - mean_squared_error: 0.6560 - val_loss: 2.2188 - val_mean_squared_error: 2.2188\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.6309 - mean_squared_error: 0.6309 - val_loss: 1.9293 - val_mean_squared_error: 1.9293\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 0.6745 - mean_squared_error: 0.6745 - val_loss: 2.1203 - val_mean_squared_error: 2.1203\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.6512 - mean_squared_error: 0.6512 - val_loss: 1.9925 - val_mean_squared_error: 1.9925\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.6879 - mean_squared_error: 0.6879 - val_loss: 1.8398 - val_mean_squared_error: 1.8398\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.6807 - mean_squared_error: 0.6807 - val_loss: 2.2472 - val_mean_squared_error: 2.2472\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.6495 - mean_squared_error: 0.6495 - val_loss: 2.1745 - val_mean_squared_error: 2.1745\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.6926 - mean_squared_error: 0.6926 - val_loss: 1.9937 - val_mean_squared_error: 1.9937\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.6001 - mean_squared_error: 0.6001 - val_loss: 1.9377 - val_mean_squared_error: 1.9377\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 335us/sample - loss: 0.6572 - mean_squared_error: 0.6572 - val_loss: 1.9536 - val_mean_squared_error: 1.9536\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.6075 - mean_squared_error: 0.6075 - val_loss: 2.0498 - val_mean_squared_error: 2.0498\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.6208 - mean_squared_error: 0.6208 - val_loss: 1.8925 - val_mean_squared_error: 1.8925\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.6508 - mean_squared_error: 0.6508 - val_loss: 1.9178 - val_mean_squared_error: 1.9178\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.6082 - mean_squared_error: 0.6082 - val_loss: 2.0120 - val_mean_squared_error: 2.0120\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.6203 - mean_squared_error: 0.6203 - val_loss: 2.0062 - val_mean_squared_error: 2.0062\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.6260 - mean_squared_error: 0.6260 - val_loss: 1.8835 - val_mean_squared_error: 1.8835\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.6019 - mean_squared_error: 0.6019 - val_loss: 2.0634 - val_mean_squared_error: 2.0634\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.5663 - mean_squared_error: 0.5663 - val_loss: 1.9450 - val_mean_squared_error: 1.9450\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5943 - mean_squared_error: 0.5943 - val_loss: 1.8241 - val_mean_squared_error: 1.8241\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5964 - mean_squared_error: 0.5964 - val_loss: 1.8331 - val_mean_squared_error: 1.8331\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.6104 - mean_squared_error: 0.6104 - val_loss: 1.8937 - val_mean_squared_error: 1.8937\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.6108 - mean_squared_error: 0.6108 - val_loss: 1.9638 - val_mean_squared_error: 1.9638\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5893 - mean_squared_error: 0.5893 - val_loss: 1.9130 - val_mean_squared_error: 1.9130\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.6049 - mean_squared_error: 0.6049 - val_loss: 2.0015 - val_mean_squared_error: 2.0015\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.5565 - mean_squared_error: 0.5565 - val_loss: 2.4140 - val_mean_squared_error: 2.4140\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.5975 - mean_squared_error: 0.5975 - val_loss: 2.2354 - val_mean_squared_error: 2.2354\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.6147 - mean_squared_error: 0.6147 - val_loss: 2.1118 - val_mean_squared_error: 2.1118\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.5540 - mean_squared_error: 0.5540 - val_loss: 1.9650 - val_mean_squared_error: 1.9650\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.5823 - mean_squared_error: 0.5823 - val_loss: 1.8549 - val_mean_squared_error: 1.8549\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 0.5165 - mean_squared_error: 0.5165 - val_loss: 1.9252 - val_mean_squared_error: 1.9252\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.5306 - mean_squared_error: 0.5306 - val_loss: 1.8720 - val_mean_squared_error: 1.8720\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 0.5030 - mean_squared_error: 0.5030 - val_loss: 1.8563 - val_mean_squared_error: 1.8563\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5519 - mean_squared_error: 0.5519 - val_loss: 2.0999 - val_mean_squared_error: 2.0999\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.6015 - mean_squared_error: 0.6015 - val_loss: 1.9736 - val_mean_squared_error: 1.9736\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.5245 - mean_squared_error: 0.5245 - val_loss: 1.9653 - val_mean_squared_error: 1.9653\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.5780 - mean_squared_error: 0.5780 - val_loss: 1.9301 - val_mean_squared_error: 1.9302\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.5417 - mean_squared_error: 0.5417 - val_loss: 1.8441 - val_mean_squared_error: 1.8441\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.4966 - mean_squared_error: 0.4966 - val_loss: 1.8969 - val_mean_squared_error: 1.8969\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.5113 - mean_squared_error: 0.5113 - val_loss: 2.2265 - val_mean_squared_error: 2.2265\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.5569 - mean_squared_error: 0.5569 - val_loss: 1.8910 - val_mean_squared_error: 1.8910\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.5251 - mean_squared_error: 0.5251 - val_loss: 1.9007 - val_mean_squared_error: 1.9007\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.5355 - mean_squared_error: 0.5355 - val_loss: 2.0246 - val_mean_squared_error: 2.0246\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.5218 - mean_squared_error: 0.5218 - val_loss: 1.8789 - val_mean_squared_error: 1.8789\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5341 - mean_squared_error: 0.5341 - val_loss: 1.8193 - val_mean_squared_error: 1.8193\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.5504 - mean_squared_error: 0.5504 - val_loss: 1.9335 - val_mean_squared_error: 1.9335\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5354 - mean_squared_error: 0.5354 - val_loss: 1.9636 - val_mean_squared_error: 1.9636\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.5072 - mean_squared_error: 0.5072 - val_loss: 1.9769 - val_mean_squared_error: 1.9769\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.5689 - mean_squared_error: 0.5689 - val_loss: 2.1626 - val_mean_squared_error: 2.1626\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.5285 - mean_squared_error: 0.5285 - val_loss: 2.0330 - val_mean_squared_error: 2.0330\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.5122 - mean_squared_error: 0.5122 - val_loss: 1.9923 - val_mean_squared_error: 1.9923\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.5098 - mean_squared_error: 0.5098 - val_loss: 1.8694 - val_mean_squared_error: 1.8694\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 2.1655 - val_mean_squared_error: 2.1655\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.4773 - mean_squared_error: 0.4773 - val_loss: 1.8871 - val_mean_squared_error: 1.8871\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.5418 - mean_squared_error: 0.5418 - val_loss: 1.8549 - val_mean_squared_error: 1.8549\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.4990 - mean_squared_error: 0.4990 - val_loss: 1.9710 - val_mean_squared_error: 1.9710\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5055 - mean_squared_error: 0.5055 - val_loss: 1.9474 - val_mean_squared_error: 1.9474\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 335us/sample - loss: 0.5248 - mean_squared_error: 0.5248 - val_loss: 2.0001 - val_mean_squared_error: 2.0001\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 0.4737 - mean_squared_error: 0.4737 - val_loss: 1.8012 - val_mean_squared_error: 1.8012\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.4855 - mean_squared_error: 0.4855 - val_loss: 1.9064 - val_mean_squared_error: 1.9064\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 0.4732 - mean_squared_error: 0.4732 - val_loss: 2.0041 - val_mean_squared_error: 2.0041\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.4954 - mean_squared_error: 0.4954 - val_loss: 1.8927 - val_mean_squared_error: 1.8927\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 0.4920 - mean_squared_error: 0.4920 - val_loss: 1.8338 - val_mean_squared_error: 1.8338\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 0.5467 - mean_squared_error: 0.5467 - val_loss: 1.8238 - val_mean_squared_error: 1.8238\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.4607 - mean_squared_error: 0.4607 - val_loss: 1.8363 - val_mean_squared_error: 1.8363\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 0.4667 - mean_squared_error: 0.4667 - val_loss: 1.8841 - val_mean_squared_error: 1.8841\n",
            "==================================================\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_45 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_75 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_60 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_76 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_61 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_77 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_62 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_78 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_63 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_79 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 837us/sample - loss: 178.0851 - mean_squared_error: 178.0851 - val_loss: 148.5042 - val_mean_squared_error: 148.5042\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 370us/sample - loss: 20.1247 - mean_squared_error: 20.1247 - val_loss: 41.2281 - val_mean_squared_error: 41.2281\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 18.4188 - mean_squared_error: 18.4188 - val_loss: 35.7206 - val_mean_squared_error: 35.7206\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 15.7851 - mean_squared_error: 15.7851 - val_loss: 16.1071 - val_mean_squared_error: 16.1071\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 12.9661 - mean_squared_error: 12.9661 - val_loss: 13.4622 - val_mean_squared_error: 13.4622\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 11.3201 - mean_squared_error: 11.3201 - val_loss: 10.2582 - val_mean_squared_error: 10.2582\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 10.4952 - mean_squared_error: 10.4952 - val_loss: 8.9945 - val_mean_squared_error: 8.9945\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 9.6000 - mean_squared_error: 9.6000 - val_loss: 8.6243 - val_mean_squared_error: 8.6243\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 7.9814 - mean_squared_error: 7.9814 - val_loss: 7.8942 - val_mean_squared_error: 7.8942\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 7.0495 - mean_squared_error: 7.0495 - val_loss: 6.2896 - val_mean_squared_error: 6.2896\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 5.9933 - mean_squared_error: 5.9933 - val_loss: 7.8542 - val_mean_squared_error: 7.8542\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 5.3036 - mean_squared_error: 5.3036 - val_loss: 6.5327 - val_mean_squared_error: 6.5327\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 4.6180 - mean_squared_error: 4.6180 - val_loss: 5.2348 - val_mean_squared_error: 5.2348\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 4.3621 - mean_squared_error: 4.3621 - val_loss: 5.5262 - val_mean_squared_error: 5.5262\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 3.8386 - mean_squared_error: 3.8386 - val_loss: 3.9531 - val_mean_squared_error: 3.9531\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 3.6521 - mean_squared_error: 3.6521 - val_loss: 5.1774 - val_mean_squared_error: 5.1774\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 3.6224 - mean_squared_error: 3.6224 - val_loss: 3.5195 - val_mean_squared_error: 3.5195\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 3.4532 - mean_squared_error: 3.4532 - val_loss: 4.3198 - val_mean_squared_error: 4.3198\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 3.0374 - mean_squared_error: 3.0374 - val_loss: 3.2855 - val_mean_squared_error: 3.2855\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 2.9802 - mean_squared_error: 2.9802 - val_loss: 3.2655 - val_mean_squared_error: 3.2655\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 2.9327 - mean_squared_error: 2.9327 - val_loss: 3.5552 - val_mean_squared_error: 3.5552\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 2.7050 - mean_squared_error: 2.7050 - val_loss: 3.1767 - val_mean_squared_error: 3.1767\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 2.6728 - mean_squared_error: 2.6728 - val_loss: 2.9702 - val_mean_squared_error: 2.9702\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 2.7487 - mean_squared_error: 2.7487 - val_loss: 3.7258 - val_mean_squared_error: 3.7258\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 2.5678 - mean_squared_error: 2.5678 - val_loss: 2.8895 - val_mean_squared_error: 2.8895\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 2.4430 - mean_squared_error: 2.4430 - val_loss: 2.6468 - val_mean_squared_error: 2.6468\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 2.4085 - mean_squared_error: 2.4085 - val_loss: 3.6279 - val_mean_squared_error: 3.6279\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 2.3947 - mean_squared_error: 2.3947 - val_loss: 2.7703 - val_mean_squared_error: 2.7703\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 2.3267 - mean_squared_error: 2.3267 - val_loss: 2.9517 - val_mean_squared_error: 2.9517\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 2.2242 - mean_squared_error: 2.2242 - val_loss: 3.6671 - val_mean_squared_error: 3.6671\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 2.1569 - mean_squared_error: 2.1569 - val_loss: 2.8192 - val_mean_squared_error: 2.8192\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 2.3343 - mean_squared_error: 2.3343 - val_loss: 3.8979 - val_mean_squared_error: 3.8979\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 2.0744 - mean_squared_error: 2.0744 - val_loss: 2.5899 - val_mean_squared_error: 2.5899\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 2.1079 - mean_squared_error: 2.1079 - val_loss: 2.7143 - val_mean_squared_error: 2.7143\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.8544 - mean_squared_error: 1.8544 - val_loss: 2.3849 - val_mean_squared_error: 2.3849\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.9785 - mean_squared_error: 1.9785 - val_loss: 2.1509 - val_mean_squared_error: 2.1509\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.8266 - mean_squared_error: 1.8266 - val_loss: 2.3047 - val_mean_squared_error: 2.3047\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.8295 - mean_squared_error: 1.8295 - val_loss: 2.6598 - val_mean_squared_error: 2.6598\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.9318 - mean_squared_error: 1.9318 - val_loss: 2.8790 - val_mean_squared_error: 2.8790\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.8768 - mean_squared_error: 1.8768 - val_loss: 2.8763 - val_mean_squared_error: 2.8763\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.8715 - mean_squared_error: 1.8715 - val_loss: 2.7580 - val_mean_squared_error: 2.7580\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.7993 - mean_squared_error: 1.7993 - val_loss: 2.1013 - val_mean_squared_error: 2.1013\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.6274 - mean_squared_error: 1.6274 - val_loss: 2.8878 - val_mean_squared_error: 2.8878\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.8280 - mean_squared_error: 1.8280 - val_loss: 2.1670 - val_mean_squared_error: 2.1670\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.7318 - mean_squared_error: 1.7318 - val_loss: 2.1597 - val_mean_squared_error: 2.1597\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.5495 - mean_squared_error: 1.5495 - val_loss: 2.5449 - val_mean_squared_error: 2.5449\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.5507 - mean_squared_error: 1.5507 - val_loss: 2.6335 - val_mean_squared_error: 2.6335\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.6467 - mean_squared_error: 1.6467 - val_loss: 2.1918 - val_mean_squared_error: 2.1918\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.4772 - mean_squared_error: 1.4772 - val_loss: 2.5946 - val_mean_squared_error: 2.5946\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.5929 - mean_squared_error: 1.5929 - val_loss: 2.6489 - val_mean_squared_error: 2.6489\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.7322 - mean_squared_error: 1.7322 - val_loss: 2.6757 - val_mean_squared_error: 2.6757\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.5005 - mean_squared_error: 1.5005 - val_loss: 2.0306 - val_mean_squared_error: 2.0306\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.5012 - mean_squared_error: 1.5012 - val_loss: 2.3327 - val_mean_squared_error: 2.3327\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.4992 - mean_squared_error: 1.4992 - val_loss: 2.1206 - val_mean_squared_error: 2.1206\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.4469 - mean_squared_error: 1.4469 - val_loss: 2.2072 - val_mean_squared_error: 2.2072\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.3199 - mean_squared_error: 1.3199 - val_loss: 1.9218 - val_mean_squared_error: 1.9218\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.5262 - mean_squared_error: 1.5262 - val_loss: 1.9284 - val_mean_squared_error: 1.9284\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.3908 - mean_squared_error: 1.3908 - val_loss: 2.1868 - val_mean_squared_error: 2.1868\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.3030 - mean_squared_error: 1.3030 - val_loss: 2.5566 - val_mean_squared_error: 2.5566\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 1.3048 - mean_squared_error: 1.3048 - val_loss: 2.0037 - val_mean_squared_error: 2.0037\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.4347 - mean_squared_error: 1.4347 - val_loss: 1.9478 - val_mean_squared_error: 1.9478\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.3349 - mean_squared_error: 1.3349 - val_loss: 2.2156 - val_mean_squared_error: 2.2156\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.3170 - mean_squared_error: 1.3170 - val_loss: 2.6160 - val_mean_squared_error: 2.6160\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.3466 - mean_squared_error: 1.3466 - val_loss: 1.8770 - val_mean_squared_error: 1.8770\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.2767 - mean_squared_error: 1.2767 - val_loss: 1.9859 - val_mean_squared_error: 1.9859\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 1.2500 - mean_squared_error: 1.2500 - val_loss: 1.8916 - val_mean_squared_error: 1.8916\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.2305 - mean_squared_error: 1.2305 - val_loss: 2.0366 - val_mean_squared_error: 2.0366\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.4337 - mean_squared_error: 1.4337 - val_loss: 2.0267 - val_mean_squared_error: 2.0267\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.3245 - mean_squared_error: 1.3245 - val_loss: 2.1414 - val_mean_squared_error: 2.1414\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 1.2515 - mean_squared_error: 1.2515 - val_loss: 1.8924 - val_mean_squared_error: 1.8924\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.2743 - mean_squared_error: 1.2743 - val_loss: 1.7486 - val_mean_squared_error: 1.7486\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 1.2996 - mean_squared_error: 1.2996 - val_loss: 1.9719 - val_mean_squared_error: 1.9719\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.3303 - mean_squared_error: 1.3303 - val_loss: 1.9518 - val_mean_squared_error: 1.9518\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.1037 - mean_squared_error: 1.1037 - val_loss: 2.0055 - val_mean_squared_error: 2.0055\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.2004 - mean_squared_error: 1.2004 - val_loss: 1.8164 - val_mean_squared_error: 1.8164\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.1602 - mean_squared_error: 1.1602 - val_loss: 1.9054 - val_mean_squared_error: 1.9054\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.2691 - mean_squared_error: 1.2691 - val_loss: 2.3512 - val_mean_squared_error: 2.3512\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.1759 - mean_squared_error: 1.1759 - val_loss: 1.8839 - val_mean_squared_error: 1.8839\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.1355 - mean_squared_error: 1.1355 - val_loss: 2.0270 - val_mean_squared_error: 2.0270\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.0834 - mean_squared_error: 1.0834 - val_loss: 1.9633 - val_mean_squared_error: 1.9633\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.1122 - mean_squared_error: 1.1122 - val_loss: 1.9780 - val_mean_squared_error: 1.9780\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.0258 - mean_squared_error: 1.0258 - val_loss: 1.9188 - val_mean_squared_error: 1.9188\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.0579 - mean_squared_error: 1.0579 - val_loss: 1.8294 - val_mean_squared_error: 1.8294\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.1557 - mean_squared_error: 1.1557 - val_loss: 1.8868 - val_mean_squared_error: 1.8868\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.0524 - mean_squared_error: 1.0524 - val_loss: 2.0133 - val_mean_squared_error: 2.0133\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.1158 - mean_squared_error: 1.1158 - val_loss: 1.8081 - val_mean_squared_error: 1.8081\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.0721 - mean_squared_error: 1.0721 - val_loss: 2.3246 - val_mean_squared_error: 2.3246\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.9329 - mean_squared_error: 0.9329 - val_loss: 2.0484 - val_mean_squared_error: 2.0484\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.9658 - mean_squared_error: 0.9658 - val_loss: 1.9722 - val_mean_squared_error: 1.9722\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9953 - mean_squared_error: 0.9953 - val_loss: 1.8598 - val_mean_squared_error: 1.8598\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 2.5065 - val_mean_squared_error: 2.5065\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.0222 - mean_squared_error: 1.0222 - val_loss: 2.2666 - val_mean_squared_error: 2.2666\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9914 - mean_squared_error: 0.9914 - val_loss: 2.0823 - val_mean_squared_error: 2.0823\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.8173 - val_mean_squared_error: 1.8173\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.0345 - mean_squared_error: 1.0345 - val_loss: 2.0230 - val_mean_squared_error: 2.0230\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9323 - mean_squared_error: 0.9323 - val_loss: 1.7611 - val_mean_squared_error: 1.7611\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.9138 - mean_squared_error: 0.9138 - val_loss: 1.8406 - val_mean_squared_error: 1.8406\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.8936 - mean_squared_error: 0.8936 - val_loss: 1.9517 - val_mean_squared_error: 1.9517\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.9271 - mean_squared_error: 0.9271 - val_loss: 1.9389 - val_mean_squared_error: 1.9389\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.9258 - mean_squared_error: 0.9258 - val_loss: 1.7838 - val_mean_squared_error: 1.7838\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.9110 - mean_squared_error: 0.9110 - val_loss: 1.8438 - val_mean_squared_error: 1.8438\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.8959 - mean_squared_error: 0.8959 - val_loss: 1.9256 - val_mean_squared_error: 1.9256\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8456 - mean_squared_error: 0.8456 - val_loss: 1.7884 - val_mean_squared_error: 1.7884\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.8483 - mean_squared_error: 0.8483 - val_loss: 1.8468 - val_mean_squared_error: 1.8468\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.9371 - mean_squared_error: 0.9371 - val_loss: 1.9846 - val_mean_squared_error: 1.9846\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8647 - mean_squared_error: 0.8647 - val_loss: 1.8282 - val_mean_squared_error: 1.8282\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.8686 - mean_squared_error: 0.8686 - val_loss: 1.7074 - val_mean_squared_error: 1.7074\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.8615 - mean_squared_error: 0.8615 - val_loss: 2.1633 - val_mean_squared_error: 2.1633\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.8912 - mean_squared_error: 0.8912 - val_loss: 2.0329 - val_mean_squared_error: 2.0329\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.8892 - mean_squared_error: 0.8892 - val_loss: 1.8038 - val_mean_squared_error: 1.8038\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.7987 - mean_squared_error: 0.7987 - val_loss: 1.9928 - val_mean_squared_error: 1.9928\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.9062 - mean_squared_error: 0.9062 - val_loss: 2.0411 - val_mean_squared_error: 2.0411\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.8330 - mean_squared_error: 0.8330 - val_loss: 1.8388 - val_mean_squared_error: 1.8388\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.8942 - mean_squared_error: 0.8942 - val_loss: 2.0263 - val_mean_squared_error: 2.0263\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.8428 - mean_squared_error: 0.8428 - val_loss: 1.8600 - val_mean_squared_error: 1.8600\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.8048 - mean_squared_error: 0.8048 - val_loss: 2.1216 - val_mean_squared_error: 2.1216\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8245 - mean_squared_error: 0.8245 - val_loss: 1.9683 - val_mean_squared_error: 1.9683\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.8552 - mean_squared_error: 0.8552 - val_loss: 1.9904 - val_mean_squared_error: 1.9904\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.7891 - mean_squared_error: 0.7891 - val_loss: 1.9978 - val_mean_squared_error: 1.9978\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.7431 - mean_squared_error: 0.7431 - val_loss: 1.8239 - val_mean_squared_error: 1.8239\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.8046 - mean_squared_error: 0.8046 - val_loss: 1.7673 - val_mean_squared_error: 1.7673\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.7694 - mean_squared_error: 0.7694 - val_loss: 2.3857 - val_mean_squared_error: 2.3857\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.8423 - mean_squared_error: 0.8423 - val_loss: 1.9382 - val_mean_squared_error: 1.9382\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.7501 - mean_squared_error: 0.7501 - val_loss: 1.7810 - val_mean_squared_error: 1.7810\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8240 - mean_squared_error: 0.8240 - val_loss: 1.8067 - val_mean_squared_error: 1.8067\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7647 - mean_squared_error: 0.7647 - val_loss: 1.7681 - val_mean_squared_error: 1.7681\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8530 - mean_squared_error: 0.8530 - val_loss: 1.8788 - val_mean_squared_error: 1.8788\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.7932 - mean_squared_error: 0.7932 - val_loss: 1.9215 - val_mean_squared_error: 1.9215\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.7754 - mean_squared_error: 0.7754 - val_loss: 1.8802 - val_mean_squared_error: 1.8802\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.7465 - mean_squared_error: 0.7465 - val_loss: 2.0590 - val_mean_squared_error: 2.0590\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.8104 - mean_squared_error: 0.8104 - val_loss: 1.7184 - val_mean_squared_error: 1.7184\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.7552 - mean_squared_error: 0.7552 - val_loss: 1.8422 - val_mean_squared_error: 1.8422\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.7216 - mean_squared_error: 0.7216 - val_loss: 1.8669 - val_mean_squared_error: 1.8669\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.7929 - mean_squared_error: 0.7929 - val_loss: 1.9585 - val_mean_squared_error: 1.9585\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.7444 - mean_squared_error: 0.7444 - val_loss: 1.8236 - val_mean_squared_error: 1.8236\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7265 - mean_squared_error: 0.7265 - val_loss: 1.9017 - val_mean_squared_error: 1.9017\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.6987 - mean_squared_error: 0.6987 - val_loss: 1.8455 - val_mean_squared_error: 1.8455\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7917 - mean_squared_error: 0.7917 - val_loss: 1.9941 - val_mean_squared_error: 1.9941\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.7281 - mean_squared_error: 0.7281 - val_loss: 1.7053 - val_mean_squared_error: 1.7053\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7184 - mean_squared_error: 0.7184 - val_loss: 2.0485 - val_mean_squared_error: 2.0485\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.7040 - mean_squared_error: 0.7040 - val_loss: 1.8743 - val_mean_squared_error: 1.8743\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.7450 - mean_squared_error: 0.7450 - val_loss: 2.0488 - val_mean_squared_error: 2.0488\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.7354 - mean_squared_error: 0.7354 - val_loss: 1.9146 - val_mean_squared_error: 1.9146\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6599 - mean_squared_error: 0.6599 - val_loss: 2.1125 - val_mean_squared_error: 2.1125\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.6973 - mean_squared_error: 0.6973 - val_loss: 1.8983 - val_mean_squared_error: 1.8983\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.7101 - mean_squared_error: 0.7101 - val_loss: 1.8343 - val_mean_squared_error: 1.8343\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6842 - mean_squared_error: 0.6842 - val_loss: 2.0298 - val_mean_squared_error: 2.0298\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7115 - mean_squared_error: 0.7115 - val_loss: 1.7624 - val_mean_squared_error: 1.7624\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.7255 - mean_squared_error: 0.7255 - val_loss: 2.0789 - val_mean_squared_error: 2.0789\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 0.6806 - mean_squared_error: 0.6806 - val_loss: 1.7437 - val_mean_squared_error: 1.7437\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.6707 - mean_squared_error: 0.6707 - val_loss: 1.8245 - val_mean_squared_error: 1.8245\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.6994 - mean_squared_error: 0.6994 - val_loss: 2.0165 - val_mean_squared_error: 2.0165\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.7458 - mean_squared_error: 0.7458 - val_loss: 1.9757 - val_mean_squared_error: 1.9757\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.6932 - mean_squared_error: 0.6932 - val_loss: 2.0140 - val_mean_squared_error: 2.0140\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.7024 - mean_squared_error: 0.7024 - val_loss: 1.7214 - val_mean_squared_error: 1.7214\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.8014 - mean_squared_error: 0.8014 - val_loss: 1.8351 - val_mean_squared_error: 1.8351\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.7560 - mean_squared_error: 0.7560 - val_loss: 1.9612 - val_mean_squared_error: 1.9612\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.6878 - mean_squared_error: 0.6878 - val_loss: 2.0998 - val_mean_squared_error: 2.0998\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7205 - mean_squared_error: 0.7205 - val_loss: 1.8266 - val_mean_squared_error: 1.8266\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.6513 - mean_squared_error: 0.6513 - val_loss: 1.8108 - val_mean_squared_error: 1.8108\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.6344 - mean_squared_error: 0.6344 - val_loss: 1.7291 - val_mean_squared_error: 1.7291\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.7087 - mean_squared_error: 0.7087 - val_loss: 1.7977 - val_mean_squared_error: 1.7977\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.6339 - mean_squared_error: 0.6339 - val_loss: 1.8069 - val_mean_squared_error: 1.8069\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.6971 - mean_squared_error: 0.6971 - val_loss: 1.8670 - val_mean_squared_error: 1.8670\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.6738 - mean_squared_error: 0.6738 - val_loss: 1.8543 - val_mean_squared_error: 1.8543\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.6463 - mean_squared_error: 0.6463 - val_loss: 1.9795 - val_mean_squared_error: 1.9795\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5936 - mean_squared_error: 0.5936 - val_loss: 1.8090 - val_mean_squared_error: 1.8090\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.6764 - mean_squared_error: 0.6764 - val_loss: 1.9229 - val_mean_squared_error: 1.9229\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.6705 - mean_squared_error: 0.6705 - val_loss: 2.0018 - val_mean_squared_error: 2.0018\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.6600 - mean_squared_error: 0.6600 - val_loss: 1.8115 - val_mean_squared_error: 1.8115\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.6101 - mean_squared_error: 0.6101 - val_loss: 2.0285 - val_mean_squared_error: 2.0285\n",
            "==================================================\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_48 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_80 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_48 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_64 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_81 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_49 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_65 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_82 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_50 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_66 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_16 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_83 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_67 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_84 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 898us/sample - loss: 174.1677 - mean_squared_error: 174.1677 - val_loss: 134.1559 - val_mean_squared_error: 134.1559\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 23.4646 - mean_squared_error: 23.4646 - val_loss: 84.7452 - val_mean_squared_error: 84.7452\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 15.2138 - mean_squared_error: 15.2138 - val_loss: 20.4356 - val_mean_squared_error: 20.4356\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 14.6220 - mean_squared_error: 14.6220 - val_loss: 12.4339 - val_mean_squared_error: 12.4339\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 12.8264 - mean_squared_error: 12.8264 - val_loss: 11.4635 - val_mean_squared_error: 11.4635\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 12.6991 - mean_squared_error: 12.6991 - val_loss: 10.4148 - val_mean_squared_error: 10.4148\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 10.8907 - mean_squared_error: 10.8907 - val_loss: 8.5082 - val_mean_squared_error: 8.5082\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 9.8573 - mean_squared_error: 9.8573 - val_loss: 8.3731 - val_mean_squared_error: 8.3731\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 8.2013 - mean_squared_error: 8.2013 - val_loss: 8.3531 - val_mean_squared_error: 8.3531\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 6.5947 - mean_squared_error: 6.5947 - val_loss: 6.3401 - val_mean_squared_error: 6.3401\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 5.5573 - mean_squared_error: 5.5573 - val_loss: 5.5322 - val_mean_squared_error: 5.5322\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 4.9015 - mean_squared_error: 4.9015 - val_loss: 5.1233 - val_mean_squared_error: 5.1233\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 4.3747 - mean_squared_error: 4.3747 - val_loss: 5.0620 - val_mean_squared_error: 5.0620\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 4.1520 - mean_squared_error: 4.1520 - val_loss: 5.0779 - val_mean_squared_error: 5.0779\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 3.6882 - mean_squared_error: 3.6882 - val_loss: 4.3590 - val_mean_squared_error: 4.3590\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 3.7763 - mean_squared_error: 3.7763 - val_loss: 4.2154 - val_mean_squared_error: 4.2154\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 3.2190 - mean_squared_error: 3.2190 - val_loss: 3.5004 - val_mean_squared_error: 3.5004\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 3.0563 - mean_squared_error: 3.0563 - val_loss: 3.5212 - val_mean_squared_error: 3.5212\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 2.9965 - mean_squared_error: 2.9965 - val_loss: 3.4645 - val_mean_squared_error: 3.4645\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 2.6261 - mean_squared_error: 2.6261 - val_loss: 2.9378 - val_mean_squared_error: 2.9378\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.5076 - mean_squared_error: 2.5076 - val_loss: 3.9568 - val_mean_squared_error: 3.9568\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 2.6676 - mean_squared_error: 2.6676 - val_loss: 2.9840 - val_mean_squared_error: 2.9840\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 2.4633 - mean_squared_error: 2.4633 - val_loss: 3.5598 - val_mean_squared_error: 3.5598\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 2.3592 - mean_squared_error: 2.3592 - val_loss: 3.0356 - val_mean_squared_error: 3.0356\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 2.3338 - mean_squared_error: 2.3338 - val_loss: 3.1454 - val_mean_squared_error: 3.1454\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 2.1244 - mean_squared_error: 2.1244 - val_loss: 2.5379 - val_mean_squared_error: 2.5379\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 2.1021 - mean_squared_error: 2.1021 - val_loss: 2.8456 - val_mean_squared_error: 2.8456\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 2.0325 - mean_squared_error: 2.0325 - val_loss: 2.8212 - val_mean_squared_error: 2.8212\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 2.0068 - mean_squared_error: 2.0068 - val_loss: 2.2991 - val_mean_squared_error: 2.2991\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.9776 - mean_squared_error: 1.9776 - val_loss: 3.5048 - val_mean_squared_error: 3.5048\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 1.9620 - mean_squared_error: 1.9620 - val_loss: 2.5513 - val_mean_squared_error: 2.5513\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.8738 - mean_squared_error: 1.8738 - val_loss: 2.4296 - val_mean_squared_error: 2.4296\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.9274 - mean_squared_error: 1.9274 - val_loss: 2.7593 - val_mean_squared_error: 2.7593\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.7464 - mean_squared_error: 1.7464 - val_loss: 2.6340 - val_mean_squared_error: 2.6340\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 1.7600 - mean_squared_error: 1.7600 - val_loss: 2.3562 - val_mean_squared_error: 2.3562\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 1.7148 - mean_squared_error: 1.7148 - val_loss: 2.3442 - val_mean_squared_error: 2.3442\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.7143 - mean_squared_error: 1.7143 - val_loss: 2.4762 - val_mean_squared_error: 2.4762\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 1.6602 - mean_squared_error: 1.6602 - val_loss: 2.6602 - val_mean_squared_error: 2.6602\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.6888 - mean_squared_error: 1.6888 - val_loss: 2.7029 - val_mean_squared_error: 2.7029\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 1.6367 - mean_squared_error: 1.6367 - val_loss: 2.1956 - val_mean_squared_error: 2.1956\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.5779 - mean_squared_error: 1.5779 - val_loss: 2.4344 - val_mean_squared_error: 2.4344\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.5768 - mean_squared_error: 1.5768 - val_loss: 2.3754 - val_mean_squared_error: 2.3754\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.5232 - mean_squared_error: 1.5232 - val_loss: 2.4237 - val_mean_squared_error: 2.4237\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.4874 - mean_squared_error: 1.4874 - val_loss: 2.4629 - val_mean_squared_error: 2.4629\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.5292 - mean_squared_error: 1.5292 - val_loss: 2.7408 - val_mean_squared_error: 2.7408\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.4696 - mean_squared_error: 1.4696 - val_loss: 2.3391 - val_mean_squared_error: 2.3391\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.4820 - mean_squared_error: 1.4820 - val_loss: 2.1973 - val_mean_squared_error: 2.1973\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.3058 - mean_squared_error: 1.3058 - val_loss: 2.4358 - val_mean_squared_error: 2.4358\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.3037 - mean_squared_error: 1.3037 - val_loss: 2.6362 - val_mean_squared_error: 2.6362\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 1.5418 - mean_squared_error: 1.5418 - val_loss: 2.8992 - val_mean_squared_error: 2.8992\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.6035 - mean_squared_error: 1.6035 - val_loss: 2.6172 - val_mean_squared_error: 2.6172\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 1.3871 - mean_squared_error: 1.3871 - val_loss: 2.1233 - val_mean_squared_error: 2.1233\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 1.4139 - mean_squared_error: 1.4139 - val_loss: 2.0416 - val_mean_squared_error: 2.0416\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.2358 - mean_squared_error: 1.2358 - val_loss: 2.1585 - val_mean_squared_error: 2.1585\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.3106 - mean_squared_error: 1.3106 - val_loss: 2.1467 - val_mean_squared_error: 2.1467\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.1715 - mean_squared_error: 1.1715 - val_loss: 2.2061 - val_mean_squared_error: 2.2061\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.2473 - mean_squared_error: 1.2473 - val_loss: 2.4022 - val_mean_squared_error: 2.4022\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.2465 - mean_squared_error: 1.2465 - val_loss: 2.2972 - val_mean_squared_error: 2.2972\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.2812 - mean_squared_error: 1.2812 - val_loss: 2.2607 - val_mean_squared_error: 2.2607\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.2363 - mean_squared_error: 1.2363 - val_loss: 2.1080 - val_mean_squared_error: 2.1080\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.2068 - mean_squared_error: 1.2068 - val_loss: 2.3561 - val_mean_squared_error: 2.3561\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1706 - mean_squared_error: 1.1706 - val_loss: 2.2048 - val_mean_squared_error: 2.2048\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.1098 - mean_squared_error: 1.1098 - val_loss: 1.8711 - val_mean_squared_error: 1.8711\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0803 - mean_squared_error: 1.0803 - val_loss: 2.1692 - val_mean_squared_error: 2.1692\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1325 - mean_squared_error: 1.1325 - val_loss: 3.3002 - val_mean_squared_error: 3.3002\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.2004 - mean_squared_error: 1.2004 - val_loss: 2.1093 - val_mean_squared_error: 2.1093\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.1356 - mean_squared_error: 1.1356 - val_loss: 2.0587 - val_mean_squared_error: 2.0587\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1144 - mean_squared_error: 1.1144 - val_loss: 1.8743 - val_mean_squared_error: 1.8743\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.9654 - mean_squared_error: 0.9654 - val_loss: 2.0646 - val_mean_squared_error: 2.0646\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.0529 - mean_squared_error: 1.0529 - val_loss: 2.0270 - val_mean_squared_error: 2.0270\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.1846 - mean_squared_error: 1.1846 - val_loss: 2.2396 - val_mean_squared_error: 2.2396\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.0881 - mean_squared_error: 1.0881 - val_loss: 2.5457 - val_mean_squared_error: 2.5457\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.1239 - mean_squared_error: 1.1239 - val_loss: 2.1901 - val_mean_squared_error: 2.1901\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.0225 - mean_squared_error: 1.0225 - val_loss: 2.1498 - val_mean_squared_error: 2.1498\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 1.0257 - mean_squared_error: 1.0257 - val_loss: 1.8613 - val_mean_squared_error: 1.8613\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.9657 - mean_squared_error: 0.9657 - val_loss: 2.1414 - val_mean_squared_error: 2.1414\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.9111 - mean_squared_error: 0.9111 - val_loss: 2.2314 - val_mean_squared_error: 2.2314\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.1308 - mean_squared_error: 1.1308 - val_loss: 1.9005 - val_mean_squared_error: 1.9005\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8980 - mean_squared_error: 0.8980 - val_loss: 2.0802 - val_mean_squared_error: 2.0802\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8438 - mean_squared_error: 0.8438 - val_loss: 1.9721 - val_mean_squared_error: 1.9721\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.9060 - mean_squared_error: 0.9060 - val_loss: 1.8838 - val_mean_squared_error: 1.8838\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8705 - mean_squared_error: 0.8705 - val_loss: 2.1696 - val_mean_squared_error: 2.1696\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0268 - mean_squared_error: 1.0268 - val_loss: 1.9640 - val_mean_squared_error: 1.9640\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.0217 - mean_squared_error: 1.0217 - val_loss: 2.0332 - val_mean_squared_error: 2.0332\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.9628 - val_mean_squared_error: 1.9628\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9387 - mean_squared_error: 0.9387 - val_loss: 2.0516 - val_mean_squared_error: 2.0516\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8106 - mean_squared_error: 0.8106 - val_loss: 2.5792 - val_mean_squared_error: 2.5792\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0238 - mean_squared_error: 1.0238 - val_loss: 1.9212 - val_mean_squared_error: 1.9212\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.9247 - mean_squared_error: 0.9247 - val_loss: 2.0030 - val_mean_squared_error: 2.0030\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.9150 - mean_squared_error: 0.9150 - val_loss: 1.9962 - val_mean_squared_error: 1.9962\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.8764 - mean_squared_error: 0.8764 - val_loss: 1.9530 - val_mean_squared_error: 1.9530\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.8463 - mean_squared_error: 0.8463 - val_loss: 2.0002 - val_mean_squared_error: 2.0002\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.8618 - mean_squared_error: 0.8618 - val_loss: 2.0343 - val_mean_squared_error: 2.0343\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8333 - mean_squared_error: 0.8333 - val_loss: 1.9462 - val_mean_squared_error: 1.9462\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.9550 - mean_squared_error: 0.9550 - val_loss: 2.2280 - val_mean_squared_error: 2.2280\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.9716 - mean_squared_error: 0.9716 - val_loss: 2.0840 - val_mean_squared_error: 2.0840\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.7956 - mean_squared_error: 0.7956 - val_loss: 1.8753 - val_mean_squared_error: 1.8753\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.7756 - mean_squared_error: 0.7756 - val_loss: 2.0634 - val_mean_squared_error: 2.0634\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8570 - mean_squared_error: 0.8570 - val_loss: 1.8874 - val_mean_squared_error: 1.8874\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8343 - mean_squared_error: 0.8343 - val_loss: 1.9644 - val_mean_squared_error: 1.9644\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8106 - mean_squared_error: 0.8106 - val_loss: 1.8067 - val_mean_squared_error: 1.8067\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.7905 - mean_squared_error: 0.7905 - val_loss: 2.6233 - val_mean_squared_error: 2.6233\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.8529 - mean_squared_error: 0.8529 - val_loss: 1.7809 - val_mean_squared_error: 1.7809\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8863 - mean_squared_error: 0.8863 - val_loss: 1.9333 - val_mean_squared_error: 1.9333\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.7556 - mean_squared_error: 0.7556 - val_loss: 2.0824 - val_mean_squared_error: 2.0824\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.7408 - mean_squared_error: 0.7408 - val_loss: 1.8877 - val_mean_squared_error: 1.8877\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7074 - mean_squared_error: 0.7074 - val_loss: 2.0764 - val_mean_squared_error: 2.0764\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.8176 - mean_squared_error: 0.8176 - val_loss: 1.8510 - val_mean_squared_error: 1.8510\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7479 - mean_squared_error: 0.7479 - val_loss: 2.2155 - val_mean_squared_error: 2.2155\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8176 - mean_squared_error: 0.8176 - val_loss: 2.0087 - val_mean_squared_error: 2.0087\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8613 - mean_squared_error: 0.8613 - val_loss: 2.0473 - val_mean_squared_error: 2.0473\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7693 - mean_squared_error: 0.7693 - val_loss: 2.0648 - val_mean_squared_error: 2.0648\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7659 - mean_squared_error: 0.7659 - val_loss: 2.0283 - val_mean_squared_error: 2.0283\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.7635 - mean_squared_error: 0.7635 - val_loss: 2.2167 - val_mean_squared_error: 2.2167\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7512 - mean_squared_error: 0.7512 - val_loss: 2.1302 - val_mean_squared_error: 2.1302\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7457 - mean_squared_error: 0.7457 - val_loss: 1.9213 - val_mean_squared_error: 1.9213\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7500 - mean_squared_error: 0.7500 - val_loss: 1.8515 - val_mean_squared_error: 1.8515\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8018 - mean_squared_error: 0.8018 - val_loss: 1.9181 - val_mean_squared_error: 1.9181\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8209 - mean_squared_error: 0.8209 - val_loss: 1.8657 - val_mean_squared_error: 1.8657\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6982 - mean_squared_error: 0.6982 - val_loss: 1.8518 - val_mean_squared_error: 1.8518\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.7326 - mean_squared_error: 0.7326 - val_loss: 1.9332 - val_mean_squared_error: 1.9332\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.7702 - mean_squared_error: 0.7702 - val_loss: 2.5164 - val_mean_squared_error: 2.5164\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.7360 - mean_squared_error: 0.7360 - val_loss: 2.0374 - val_mean_squared_error: 2.0374\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7003 - mean_squared_error: 0.7003 - val_loss: 1.8768 - val_mean_squared_error: 1.8768\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7742 - mean_squared_error: 0.7742 - val_loss: 1.8646 - val_mean_squared_error: 1.8646\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.7210 - mean_squared_error: 0.7210 - val_loss: 1.8484 - val_mean_squared_error: 1.8484\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.7505 - mean_squared_error: 0.7505 - val_loss: 1.7416 - val_mean_squared_error: 1.7416\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7063 - mean_squared_error: 0.7063 - val_loss: 1.7361 - val_mean_squared_error: 1.7361\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6367 - mean_squared_error: 0.6367 - val_loss: 1.8336 - val_mean_squared_error: 1.8336\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6774 - mean_squared_error: 0.6774 - val_loss: 1.9578 - val_mean_squared_error: 1.9578\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7097 - mean_squared_error: 0.7097 - val_loss: 1.8134 - val_mean_squared_error: 1.8134\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.6559 - mean_squared_error: 0.6559 - val_loss: 1.8808 - val_mean_squared_error: 1.8808\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6806 - mean_squared_error: 0.6806 - val_loss: 1.8016 - val_mean_squared_error: 1.8016\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6249 - mean_squared_error: 0.6249 - val_loss: 1.8732 - val_mean_squared_error: 1.8732\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.5852 - mean_squared_error: 0.5852 - val_loss: 1.8172 - val_mean_squared_error: 1.8172\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.6798 - mean_squared_error: 0.6798 - val_loss: 2.2154 - val_mean_squared_error: 2.2154\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6395 - mean_squared_error: 0.6395 - val_loss: 2.0028 - val_mean_squared_error: 2.0028\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6285 - mean_squared_error: 0.6285 - val_loss: 1.8453 - val_mean_squared_error: 1.8453\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7067 - mean_squared_error: 0.7067 - val_loss: 1.7997 - val_mean_squared_error: 1.7997\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6428 - mean_squared_error: 0.6428 - val_loss: 1.7615 - val_mean_squared_error: 1.7615\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6309 - mean_squared_error: 0.6309 - val_loss: 2.3196 - val_mean_squared_error: 2.3196\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.7148 - mean_squared_error: 0.7148 - val_loss: 2.1559 - val_mean_squared_error: 2.1559\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.6657 - mean_squared_error: 0.6657 - val_loss: 1.8221 - val_mean_squared_error: 1.8221\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6462 - mean_squared_error: 0.6462 - val_loss: 2.3135 - val_mean_squared_error: 2.3135\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6065 - mean_squared_error: 0.6065 - val_loss: 2.0182 - val_mean_squared_error: 2.0182\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.6156 - mean_squared_error: 0.6156 - val_loss: 1.9412 - val_mean_squared_error: 1.9412\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5835 - mean_squared_error: 0.5835 - val_loss: 1.9860 - val_mean_squared_error: 1.9860\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.6591 - mean_squared_error: 0.6591 - val_loss: 1.8120 - val_mean_squared_error: 1.8120\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6636 - mean_squared_error: 0.6636 - val_loss: 1.9604 - val_mean_squared_error: 1.9604\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.5858 - mean_squared_error: 0.5858 - val_loss: 1.7519 - val_mean_squared_error: 1.7519\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.6471 - mean_squared_error: 0.6471 - val_loss: 1.9192 - val_mean_squared_error: 1.9192\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6305 - mean_squared_error: 0.6305 - val_loss: 2.0953 - val_mean_squared_error: 2.0953\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5999 - mean_squared_error: 0.5999 - val_loss: 1.9526 - val_mean_squared_error: 1.9526\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5756 - mean_squared_error: 0.5756 - val_loss: 1.9420 - val_mean_squared_error: 1.9420\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5883 - mean_squared_error: 0.5883 - val_loss: 1.9980 - val_mean_squared_error: 1.9980\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5682 - mean_squared_error: 0.5682 - val_loss: 1.8161 - val_mean_squared_error: 1.8161\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.5699 - mean_squared_error: 0.5699 - val_loss: 1.7874 - val_mean_squared_error: 1.7874\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5801 - mean_squared_error: 0.5801 - val_loss: 1.9343 - val_mean_squared_error: 1.9343\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6139 - mean_squared_error: 0.6139 - val_loss: 1.8832 - val_mean_squared_error: 1.8832\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5929 - mean_squared_error: 0.5929 - val_loss: 1.8341 - val_mean_squared_error: 1.8341\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.5683 - mean_squared_error: 0.5683 - val_loss: 1.8994 - val_mean_squared_error: 1.8994\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.5459 - mean_squared_error: 0.5459 - val_loss: 1.7874 - val_mean_squared_error: 1.7874\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.5703 - mean_squared_error: 0.5703 - val_loss: 1.8850 - val_mean_squared_error: 1.8850\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5581 - mean_squared_error: 0.5581 - val_loss: 2.0897 - val_mean_squared_error: 2.0897\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.5653 - mean_squared_error: 0.5653 - val_loss: 1.7380 - val_mean_squared_error: 1.7380\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5406 - mean_squared_error: 0.5406 - val_loss: 1.8931 - val_mean_squared_error: 1.8931\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5947 - mean_squared_error: 0.5947 - val_loss: 1.8709 - val_mean_squared_error: 1.8709\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.6388 - mean_squared_error: 0.6388 - val_loss: 1.7818 - val_mean_squared_error: 1.7818\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5405 - mean_squared_error: 0.5405 - val_loss: 1.7256 - val_mean_squared_error: 1.7256\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6078 - mean_squared_error: 0.6078 - val_loss: 1.9756 - val_mean_squared_error: 1.9756\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.6223 - mean_squared_error: 0.6223 - val_loss: 1.9866 - val_mean_squared_error: 1.9866\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.5793 - mean_squared_error: 0.5793 - val_loss: 1.9015 - val_mean_squared_error: 1.9015\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.5274 - mean_squared_error: 0.5274 - val_loss: 1.7996 - val_mean_squared_error: 1.7996\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.5693 - mean_squared_error: 0.5693 - val_loss: 1.8804 - val_mean_squared_error: 1.8804\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5512 - mean_squared_error: 0.5512 - val_loss: 1.8228 - val_mean_squared_error: 1.8228\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5993 - mean_squared_error: 0.5993 - val_loss: 1.7444 - val_mean_squared_error: 1.7444\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.5509 - mean_squared_error: 0.5509 - val_loss: 1.8202 - val_mean_squared_error: 1.8202\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.5113 - mean_squared_error: 0.5113 - val_loss: 1.8513 - val_mean_squared_error: 1.8513\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.5185 - mean_squared_error: 0.5185 - val_loss: 1.8217 - val_mean_squared_error: 1.8217\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.5536 - mean_squared_error: 0.5536 - val_loss: 1.7814 - val_mean_squared_error: 1.7814\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5588 - mean_squared_error: 0.5588 - val_loss: 1.8394 - val_mean_squared_error: 1.8394\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5111 - mean_squared_error: 0.5111 - val_loss: 1.8452 - val_mean_squared_error: 1.8452\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.4929 - mean_squared_error: 0.4929 - val_loss: 1.7842 - val_mean_squared_error: 1.7842\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.5083 - mean_squared_error: 0.5083 - val_loss: 1.7484 - val_mean_squared_error: 1.7484\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5963 - mean_squared_error: 0.5963 - val_loss: 1.9151 - val_mean_squared_error: 1.9151\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5003 - mean_squared_error: 0.5003 - val_loss: 1.8257 - val_mean_squared_error: 1.8257\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.5657 - mean_squared_error: 0.5657 - val_loss: 1.7892 - val_mean_squared_error: 1.7892\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.4729 - mean_squared_error: 0.4729 - val_loss: 1.7466 - val_mean_squared_error: 1.7466\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.5228 - mean_squared_error: 0.5228 - val_loss: 1.8057 - val_mean_squared_error: 1.8057\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.4464 - mean_squared_error: 0.4464 - val_loss: 1.8086 - val_mean_squared_error: 1.8086\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.4914 - mean_squared_error: 0.4914 - val_loss: 1.7148 - val_mean_squared_error: 1.7148\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5203 - mean_squared_error: 0.5203 - val_loss: 1.6896 - val_mean_squared_error: 1.6896\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.5063 - mean_squared_error: 0.5063 - val_loss: 1.8056 - val_mean_squared_error: 1.8056\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.5088 - mean_squared_error: 0.5088 - val_loss: 1.7510 - val_mean_squared_error: 1.7510\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 405us/sample - loss: 0.4640 - mean_squared_error: 0.4640 - val_loss: 1.8264 - val_mean_squared_error: 1.8264\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 0.5419 - mean_squared_error: 0.5419 - val_loss: 1.7943 - val_mean_squared_error: 1.7943\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 0.5547 - mean_squared_error: 0.5547 - val_loss: 1.8916 - val_mean_squared_error: 1.8916\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.5513 - mean_squared_error: 0.5513 - val_loss: 1.8420 - val_mean_squared_error: 1.8420\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.4908 - mean_squared_error: 0.4908 - val_loss: 1.7970 - val_mean_squared_error: 1.7970\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.4912 - mean_squared_error: 0.4912 - val_loss: 1.8407 - val_mean_squared_error: 1.8407\n",
            "==================================================\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_51 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_85 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_51 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_68 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_86 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_52 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_69 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_87 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_53 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_70 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_17 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_88 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_71 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_89 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 962us/sample - loss: 175.7548 - mean_squared_error: 175.7548 - val_loss: 184.6983 - val_mean_squared_error: 184.6983\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 18.0932 - mean_squared_error: 18.0932 - val_loss: 42.3898 - val_mean_squared_error: 42.3898\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 15.0298 - mean_squared_error: 15.0298 - val_loss: 15.4117 - val_mean_squared_error: 15.4117\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 13.2383 - mean_squared_error: 13.2383 - val_loss: 10.8765 - val_mean_squared_error: 10.8765\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 11.8380 - mean_squared_error: 11.8380 - val_loss: 12.2057 - val_mean_squared_error: 12.2057\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 11.2210 - mean_squared_error: 11.2210 - val_loss: 9.6636 - val_mean_squared_error: 9.6636\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 9.7814 - mean_squared_error: 9.7814 - val_loss: 9.2379 - val_mean_squared_error: 9.2379\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 8.8784 - mean_squared_error: 8.8784 - val_loss: 8.5248 - val_mean_squared_error: 8.5248\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 7.3313 - mean_squared_error: 7.3313 - val_loss: 7.0702 - val_mean_squared_error: 7.0702\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 6.3316 - mean_squared_error: 6.3316 - val_loss: 9.2696 - val_mean_squared_error: 9.2696\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 5.7886 - mean_squared_error: 5.7886 - val_loss: 6.2462 - val_mean_squared_error: 6.2462\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 405us/sample - loss: 5.0124 - mean_squared_error: 5.0124 - val_loss: 5.3610 - val_mean_squared_error: 5.3610\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 403us/sample - loss: 4.8843 - mean_squared_error: 4.8843 - val_loss: 6.1606 - val_mean_squared_error: 6.1606\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 4.3352 - mean_squared_error: 4.3352 - val_loss: 6.1841 - val_mean_squared_error: 6.1841\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 4.1334 - mean_squared_error: 4.1334 - val_loss: 4.8764 - val_mean_squared_error: 4.8764\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 3.6760 - mean_squared_error: 3.6760 - val_loss: 3.5600 - val_mean_squared_error: 3.5600\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 3.5002 - mean_squared_error: 3.5002 - val_loss: 4.2830 - val_mean_squared_error: 4.2830\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 3.5143 - mean_squared_error: 3.5143 - val_loss: 4.2615 - val_mean_squared_error: 4.2615\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 408us/sample - loss: 3.1286 - mean_squared_error: 3.1286 - val_loss: 3.2947 - val_mean_squared_error: 3.2947\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 403us/sample - loss: 3.0248 - mean_squared_error: 3.0248 - val_loss: 3.9911 - val_mean_squared_error: 3.9911\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 403us/sample - loss: 3.0598 - mean_squared_error: 3.0598 - val_loss: 3.1733 - val_mean_squared_error: 3.1733\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 2.7373 - mean_squared_error: 2.7373 - val_loss: 2.8047 - val_mean_squared_error: 2.8047\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 2.6651 - mean_squared_error: 2.6651 - val_loss: 2.6243 - val_mean_squared_error: 2.6243\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 2.5788 - mean_squared_error: 2.5788 - val_loss: 2.7293 - val_mean_squared_error: 2.7293\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 2.6109 - mean_squared_error: 2.6109 - val_loss: 2.7882 - val_mean_squared_error: 2.7882\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 2.3919 - mean_squared_error: 2.3919 - val_loss: 2.6195 - val_mean_squared_error: 2.6195\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 2.4005 - mean_squared_error: 2.4005 - val_loss: 3.2495 - val_mean_squared_error: 3.2495\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 2.3251 - mean_squared_error: 2.3251 - val_loss: 3.1919 - val_mean_squared_error: 3.1919\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 403us/sample - loss: 2.2203 - mean_squared_error: 2.2203 - val_loss: 2.8754 - val_mean_squared_error: 2.8754\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 2.1855 - mean_squared_error: 2.1855 - val_loss: 2.5250 - val_mean_squared_error: 2.5250\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 2.1955 - mean_squared_error: 2.1955 - val_loss: 2.5627 - val_mean_squared_error: 2.5627\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 2.0545 - mean_squared_error: 2.0545 - val_loss: 2.6894 - val_mean_squared_error: 2.6894\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 2.0084 - mean_squared_error: 2.0084 - val_loss: 2.9582 - val_mean_squared_error: 2.9582\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.9637 - mean_squared_error: 1.9637 - val_loss: 2.3727 - val_mean_squared_error: 2.3727\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.9479 - mean_squared_error: 1.9479 - val_loss: 2.0513 - val_mean_squared_error: 2.0513\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.9386 - mean_squared_error: 1.9386 - val_loss: 2.3631 - val_mean_squared_error: 2.3631\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 1.8477 - mean_squared_error: 1.8477 - val_loss: 2.1436 - val_mean_squared_error: 2.1436\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 1.8077 - mean_squared_error: 1.8077 - val_loss: 2.6437 - val_mean_squared_error: 2.6437\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.7500 - mean_squared_error: 1.7500 - val_loss: 2.1670 - val_mean_squared_error: 2.1670\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.7951 - mean_squared_error: 1.7951 - val_loss: 2.1883 - val_mean_squared_error: 2.1883\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.6850 - mean_squared_error: 1.6850 - val_loss: 2.4617 - val_mean_squared_error: 2.4617\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.8985 - mean_squared_error: 1.8985 - val_loss: 2.9073 - val_mean_squared_error: 2.9073\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.6460 - mean_squared_error: 1.6460 - val_loss: 2.8960 - val_mean_squared_error: 2.8960\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.6442 - mean_squared_error: 1.6442 - val_loss: 2.4684 - val_mean_squared_error: 2.4684\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.6144 - mean_squared_error: 1.6144 - val_loss: 2.8482 - val_mean_squared_error: 2.8482\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.6573 - mean_squared_error: 1.6573 - val_loss: 2.2006 - val_mean_squared_error: 2.2006\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.6843 - mean_squared_error: 1.6843 - val_loss: 2.2293 - val_mean_squared_error: 2.2293\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.4220 - mean_squared_error: 1.4220 - val_loss: 2.1190 - val_mean_squared_error: 2.1190\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.5463 - mean_squared_error: 1.5463 - val_loss: 2.6614 - val_mean_squared_error: 2.6614\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.4843 - mean_squared_error: 1.4843 - val_loss: 2.2111 - val_mean_squared_error: 2.2111\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.4431 - mean_squared_error: 1.4431 - val_loss: 2.1980 - val_mean_squared_error: 2.1980\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.4389 - mean_squared_error: 1.4389 - val_loss: 2.0407 - val_mean_squared_error: 2.0407\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.3820 - mean_squared_error: 1.3820 - val_loss: 1.9872 - val_mean_squared_error: 1.9872\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.4508 - mean_squared_error: 1.4508 - val_loss: 2.2747 - val_mean_squared_error: 2.2747\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 1.4129 - mean_squared_error: 1.4129 - val_loss: 1.8879 - val_mean_squared_error: 1.8879\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.3324 - mean_squared_error: 1.3324 - val_loss: 1.9457 - val_mean_squared_error: 1.9457\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.3575 - mean_squared_error: 1.3575 - val_loss: 2.1455 - val_mean_squared_error: 2.1455\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.3412 - mean_squared_error: 1.3412 - val_loss: 2.0380 - val_mean_squared_error: 2.0380\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.3236 - mean_squared_error: 1.3236 - val_loss: 1.9563 - val_mean_squared_error: 1.9563\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.3675 - mean_squared_error: 1.3675 - val_loss: 2.5719 - val_mean_squared_error: 2.5719\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.4244 - mean_squared_error: 1.4244 - val_loss: 2.1401 - val_mean_squared_error: 2.1401\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 1.2686 - mean_squared_error: 1.2686 - val_loss: 1.9149 - val_mean_squared_error: 1.9149\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 1.3596 - mean_squared_error: 1.3596 - val_loss: 2.2526 - val_mean_squared_error: 2.2526\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 1.3334 - mean_squared_error: 1.3334 - val_loss: 2.2999 - val_mean_squared_error: 2.2999\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.3657 - mean_squared_error: 1.3657 - val_loss: 2.3232 - val_mean_squared_error: 2.3232\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.1975 - mean_squared_error: 1.1975 - val_loss: 1.9495 - val_mean_squared_error: 1.9495\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.3413 - mean_squared_error: 1.3413 - val_loss: 2.1746 - val_mean_squared_error: 2.1746\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.1536 - mean_squared_error: 1.1536 - val_loss: 1.9767 - val_mean_squared_error: 1.9767\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.1905 - mean_squared_error: 1.1905 - val_loss: 2.5113 - val_mean_squared_error: 2.5113\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.2909 - mean_squared_error: 1.2909 - val_loss: 2.1605 - val_mean_squared_error: 2.1605\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.2848 - mean_squared_error: 1.2848 - val_loss: 1.7580 - val_mean_squared_error: 1.7580\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.2562 - mean_squared_error: 1.2562 - val_loss: 2.2283 - val_mean_squared_error: 2.2283\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.1330 - mean_squared_error: 1.1330 - val_loss: 2.1527 - val_mean_squared_error: 2.1527\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.2377 - mean_squared_error: 1.2377 - val_loss: 1.9833 - val_mean_squared_error: 1.9833\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.2525 - mean_squared_error: 1.2525 - val_loss: 1.7448 - val_mean_squared_error: 1.7448\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.1573 - mean_squared_error: 1.1573 - val_loss: 2.2030 - val_mean_squared_error: 2.2030\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.1623 - mean_squared_error: 1.1623 - val_loss: 2.0017 - val_mean_squared_error: 2.0017\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.1393 - mean_squared_error: 1.1393 - val_loss: 2.0533 - val_mean_squared_error: 2.0533\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.0980 - mean_squared_error: 1.0980 - val_loss: 2.0798 - val_mean_squared_error: 2.0798\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.1824 - mean_squared_error: 1.1824 - val_loss: 2.1237 - val_mean_squared_error: 2.1237\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.1512 - mean_squared_error: 1.1512 - val_loss: 1.9005 - val_mean_squared_error: 1.9005\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.9954 - val_mean_squared_error: 1.9954\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.0799 - mean_squared_error: 1.0799 - val_loss: 1.9509 - val_mean_squared_error: 1.9509\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 405us/sample - loss: 1.0892 - mean_squared_error: 1.0892 - val_loss: 1.9272 - val_mean_squared_error: 1.9272\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 1.0711 - mean_squared_error: 1.0711 - val_loss: 2.1126 - val_mean_squared_error: 2.1126\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 405us/sample - loss: 1.0171 - mean_squared_error: 1.0171 - val_loss: 1.9875 - val_mean_squared_error: 1.9875\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 405us/sample - loss: 1.0834 - mean_squared_error: 1.0834 - val_loss: 1.7842 - val_mean_squared_error: 1.7842\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 1.0537 - mean_squared_error: 1.0537 - val_loss: 1.8498 - val_mean_squared_error: 1.8498\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 1.0620 - mean_squared_error: 1.0620 - val_loss: 2.0510 - val_mean_squared_error: 2.0510\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.9918 - mean_squared_error: 0.9918 - val_loss: 1.7243 - val_mean_squared_error: 1.7243\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 1.0197 - mean_squared_error: 1.0197 - val_loss: 1.9174 - val_mean_squared_error: 1.9174\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.0005 - mean_squared_error: 1.0005 - val_loss: 1.7545 - val_mean_squared_error: 1.7545\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.9970 - mean_squared_error: 0.9970 - val_loss: 1.8458 - val_mean_squared_error: 1.8458\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.0037 - mean_squared_error: 1.0037 - val_loss: 1.9446 - val_mean_squared_error: 1.9446\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.9849 - mean_squared_error: 0.9849 - val_loss: 1.8512 - val_mean_squared_error: 1.8512\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 1.0449 - mean_squared_error: 1.0449 - val_loss: 1.9882 - val_mean_squared_error: 1.9882\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.9567 - mean_squared_error: 0.9567 - val_loss: 1.8960 - val_mean_squared_error: 1.8960\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.0940 - mean_squared_error: 1.0940 - val_loss: 1.9284 - val_mean_squared_error: 1.9284\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.9951 - mean_squared_error: 0.9951 - val_loss: 1.8609 - val_mean_squared_error: 1.8609\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.9961 - mean_squared_error: 0.9961 - val_loss: 1.9053 - val_mean_squared_error: 1.9053\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 2.0262 - val_mean_squared_error: 2.0262\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.9521 - mean_squared_error: 0.9521 - val_loss: 1.8112 - val_mean_squared_error: 1.8112\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.9342 - mean_squared_error: 0.9342 - val_loss: 2.3787 - val_mean_squared_error: 2.3787\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0875 - mean_squared_error: 1.0875 - val_loss: 1.8687 - val_mean_squared_error: 1.8687\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 1.7680 - val_mean_squared_error: 1.7680\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.9208 - mean_squared_error: 0.9208 - val_loss: 1.8727 - val_mean_squared_error: 1.8727\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8801 - mean_squared_error: 0.8801 - val_loss: 1.7865 - val_mean_squared_error: 1.7865\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.9099 - mean_squared_error: 0.9099 - val_loss: 1.8082 - val_mean_squared_error: 1.8082\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.9271 - mean_squared_error: 0.9271 - val_loss: 2.1607 - val_mean_squared_error: 2.1607\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.9199 - mean_squared_error: 0.9199 - val_loss: 1.9323 - val_mean_squared_error: 1.9323\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8210 - mean_squared_error: 0.8210 - val_loss: 1.8432 - val_mean_squared_error: 1.8432\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.8429 - mean_squared_error: 0.8429 - val_loss: 1.7555 - val_mean_squared_error: 1.7555\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 1.7667 - val_mean_squared_error: 1.7667\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8249 - mean_squared_error: 0.8249 - val_loss: 1.7660 - val_mean_squared_error: 1.7660\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8459 - mean_squared_error: 0.8459 - val_loss: 1.6755 - val_mean_squared_error: 1.6755\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.9065 - mean_squared_error: 0.9065 - val_loss: 1.9898 - val_mean_squared_error: 1.9898\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8763 - mean_squared_error: 0.8763 - val_loss: 1.9095 - val_mean_squared_error: 1.9095\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8680 - mean_squared_error: 0.8680 - val_loss: 1.9858 - val_mean_squared_error: 1.9858\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8457 - mean_squared_error: 0.8457 - val_loss: 1.7331 - val_mean_squared_error: 1.7331\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.8837 - mean_squared_error: 0.8837 - val_loss: 2.0502 - val_mean_squared_error: 2.0502\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8157 - mean_squared_error: 0.8157 - val_loss: 1.7960 - val_mean_squared_error: 1.7960\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.9402 - mean_squared_error: 0.9402 - val_loss: 1.9295 - val_mean_squared_error: 1.9295\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8100 - mean_squared_error: 0.8100 - val_loss: 1.7528 - val_mean_squared_error: 1.7528\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7776 - mean_squared_error: 0.7776 - val_loss: 1.7169 - val_mean_squared_error: 1.7169\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7944 - mean_squared_error: 0.7944 - val_loss: 1.9079 - val_mean_squared_error: 1.9079\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7720 - mean_squared_error: 0.7720 - val_loss: 1.7122 - val_mean_squared_error: 1.7122\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7870 - mean_squared_error: 0.7870 - val_loss: 1.7442 - val_mean_squared_error: 1.7442\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.7789 - mean_squared_error: 0.7789 - val_loss: 1.7128 - val_mean_squared_error: 1.7128\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8706 - mean_squared_error: 0.8706 - val_loss: 2.1115 - val_mean_squared_error: 2.1115\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.7435 - mean_squared_error: 0.7435 - val_loss: 1.8635 - val_mean_squared_error: 1.8635\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.7536 - mean_squared_error: 0.7536 - val_loss: 1.7437 - val_mean_squared_error: 1.7437\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.7704 - mean_squared_error: 0.7704 - val_loss: 1.9012 - val_mean_squared_error: 1.9012\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.7946 - mean_squared_error: 0.7946 - val_loss: 1.9471 - val_mean_squared_error: 1.9471\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7463 - mean_squared_error: 0.7463 - val_loss: 1.7795 - val_mean_squared_error: 1.7795\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.7473 - mean_squared_error: 0.7473 - val_loss: 1.6500 - val_mean_squared_error: 1.6500\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.8184 - mean_squared_error: 0.8184 - val_loss: 2.1023 - val_mean_squared_error: 2.1023\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7880 - mean_squared_error: 0.7880 - val_loss: 1.9716 - val_mean_squared_error: 1.9716\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.7565 - mean_squared_error: 0.7565 - val_loss: 1.7150 - val_mean_squared_error: 1.7150\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.7983 - mean_squared_error: 0.7983 - val_loss: 1.9417 - val_mean_squared_error: 1.9417\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7088 - mean_squared_error: 0.7088 - val_loss: 1.6990 - val_mean_squared_error: 1.6990\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7322 - mean_squared_error: 0.7322 - val_loss: 1.9264 - val_mean_squared_error: 1.9264\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.7621 - mean_squared_error: 0.7621 - val_loss: 2.0781 - val_mean_squared_error: 2.0781\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.7395 - mean_squared_error: 0.7395 - val_loss: 1.7335 - val_mean_squared_error: 1.7335\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.6831 - mean_squared_error: 0.6831 - val_loss: 1.7742 - val_mean_squared_error: 1.7742\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.7409 - mean_squared_error: 0.7409 - val_loss: 1.8058 - val_mean_squared_error: 1.8058\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.7034 - mean_squared_error: 0.7034 - val_loss: 1.6930 - val_mean_squared_error: 1.6930\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.7468 - mean_squared_error: 0.7468 - val_loss: 1.7547 - val_mean_squared_error: 1.7547\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.7196 - mean_squared_error: 0.7196 - val_loss: 1.8169 - val_mean_squared_error: 1.8169\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.7145 - mean_squared_error: 0.7145 - val_loss: 1.7096 - val_mean_squared_error: 1.7096\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6882 - mean_squared_error: 0.6882 - val_loss: 1.7408 - val_mean_squared_error: 1.7408\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.7175 - mean_squared_error: 0.7175 - val_loss: 1.7958 - val_mean_squared_error: 1.7958\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6823 - mean_squared_error: 0.6823 - val_loss: 1.6946 - val_mean_squared_error: 1.6946\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6886 - mean_squared_error: 0.6886 - val_loss: 2.2996 - val_mean_squared_error: 2.2996\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.6904 - mean_squared_error: 0.6904 - val_loss: 1.8631 - val_mean_squared_error: 1.8631\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.6870 - mean_squared_error: 0.6870 - val_loss: 1.7272 - val_mean_squared_error: 1.7272\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.7101 - mean_squared_error: 0.7101 - val_loss: 1.7397 - val_mean_squared_error: 1.7397\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.6918 - mean_squared_error: 0.6918 - val_loss: 1.7316 - val_mean_squared_error: 1.7316\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6944 - mean_squared_error: 0.6944 - val_loss: 1.6536 - val_mean_squared_error: 1.6536\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6384 - mean_squared_error: 0.6384 - val_loss: 1.8943 - val_mean_squared_error: 1.8943\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6455 - mean_squared_error: 0.6455 - val_loss: 1.7085 - val_mean_squared_error: 1.7085\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.6239 - mean_squared_error: 0.6239 - val_loss: 1.7473 - val_mean_squared_error: 1.7473\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.7007 - mean_squared_error: 0.7007 - val_loss: 1.7239 - val_mean_squared_error: 1.7239\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.6941 - mean_squared_error: 0.6941 - val_loss: 1.8019 - val_mean_squared_error: 1.8019\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.7101 - mean_squared_error: 0.7101 - val_loss: 1.8302 - val_mean_squared_error: 1.8302\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.7253 - mean_squared_error: 0.7253 - val_loss: 1.6442 - val_mean_squared_error: 1.6442\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6912 - mean_squared_error: 0.6912 - val_loss: 1.6100 - val_mean_squared_error: 1.6100\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.6529 - mean_squared_error: 0.6529 - val_loss: 1.7780 - val_mean_squared_error: 1.7780\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6150 - mean_squared_error: 0.6150 - val_loss: 1.7249 - val_mean_squared_error: 1.7249\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.6255 - mean_squared_error: 0.6255 - val_loss: 1.7892 - val_mean_squared_error: 1.7892\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.5963 - mean_squared_error: 0.5963 - val_loss: 1.7583 - val_mean_squared_error: 1.7583\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.5868 - mean_squared_error: 0.5868 - val_loss: 1.7253 - val_mean_squared_error: 1.7253\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.6298 - mean_squared_error: 0.6298 - val_loss: 1.9002 - val_mean_squared_error: 1.9002\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6031 - mean_squared_error: 0.6031 - val_loss: 1.8727 - val_mean_squared_error: 1.8727\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6720 - mean_squared_error: 0.6720 - val_loss: 1.8223 - val_mean_squared_error: 1.8223\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.6356 - mean_squared_error: 0.6356 - val_loss: 1.7548 - val_mean_squared_error: 1.7548\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6142 - mean_squared_error: 0.6142 - val_loss: 1.6258 - val_mean_squared_error: 1.6258\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.6732 - mean_squared_error: 0.6732 - val_loss: 1.8753 - val_mean_squared_error: 1.8753\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6397 - mean_squared_error: 0.6397 - val_loss: 1.6997 - val_mean_squared_error: 1.6997\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6036 - mean_squared_error: 0.6036 - val_loss: 1.8023 - val_mean_squared_error: 1.8023\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6151 - mean_squared_error: 0.6151 - val_loss: 1.7313 - val_mean_squared_error: 1.7313\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6188 - mean_squared_error: 0.6188 - val_loss: 1.8753 - val_mean_squared_error: 1.8753\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6084 - mean_squared_error: 0.6084 - val_loss: 1.8374 - val_mean_squared_error: 1.8374\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.6434 - mean_squared_error: 0.6434 - val_loss: 1.7924 - val_mean_squared_error: 1.7924\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.5675 - mean_squared_error: 0.5675 - val_loss: 1.7033 - val_mean_squared_error: 1.7033\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 405us/sample - loss: 0.5909 - mean_squared_error: 0.5909 - val_loss: 1.8037 - val_mean_squared_error: 1.8037\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.6053 - mean_squared_error: 0.6053 - val_loss: 1.6995 - val_mean_squared_error: 1.6995\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.6180 - mean_squared_error: 0.6180 - val_loss: 1.7295 - val_mean_squared_error: 1.7295\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6133 - mean_squared_error: 0.6133 - val_loss: 1.8860 - val_mean_squared_error: 1.8860\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5558 - mean_squared_error: 0.5558 - val_loss: 1.7225 - val_mean_squared_error: 1.7225\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.5623 - mean_squared_error: 0.5623 - val_loss: 1.7350 - val_mean_squared_error: 1.7350\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6140 - mean_squared_error: 0.6140 - val_loss: 1.7875 - val_mean_squared_error: 1.7875\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5653 - mean_squared_error: 0.5653 - val_loss: 1.6392 - val_mean_squared_error: 1.6392\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6463 - mean_squared_error: 0.6463 - val_loss: 1.7681 - val_mean_squared_error: 1.7681\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6120 - mean_squared_error: 0.6120 - val_loss: 1.7785 - val_mean_squared_error: 1.7785\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.6096 - mean_squared_error: 0.6096 - val_loss: 1.6898 - val_mean_squared_error: 1.6898\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.5668 - mean_squared_error: 0.5668 - val_loss: 1.7671 - val_mean_squared_error: 1.7671\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.5760 - mean_squared_error: 0.5760 - val_loss: 1.8832 - val_mean_squared_error: 1.8832\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.6125 - mean_squared_error: 0.6125 - val_loss: 1.8093 - val_mean_squared_error: 1.8093\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.5615 - mean_squared_error: 0.5615 - val_loss: 1.6633 - val_mean_squared_error: 1.6633\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 411us/sample - loss: 0.6094 - mean_squared_error: 0.6094 - val_loss: 1.7399 - val_mean_squared_error: 1.7399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsVp-fvCqlcd",
        "colab_type": "code",
        "outputId": "175e9e0a-1e1d-442f-819b-b9cb822d4f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Use an early stopping callback and our timing callback\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                              patience=100, mode='auto')\n",
        "time_callback = TimeHistory()\n",
        "\n",
        "# Initialize a new data frame to hold our output data\n",
        "cnn_flipped_df = pd.DataFrame()\n",
        "\n",
        "# Start filter list\n",
        "start_filters = [12, 16]\n",
        "\n",
        "# Create a list of initial dropout values and steps to increase\n",
        "dropouts = [(0.00,0.10)]\n",
        "\n",
        "for lr_factor in [10]: \n",
        "    for start_filter in start_filters:\n",
        "        for d in dropouts:\n",
        "            model = create_bn_cnn_model(start_filter, d[0], d[1], fc1=500, fc2=500)\n",
        "            # Use the adam optimizer with the default learning rate\n",
        "            adam = optimizers.Adam(lr=0.001*lr_factor, beta_1=0.9, beta_2=0.999)\n",
        "            model.compile(\n",
        "                  optimizer=adam,\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_squared_error'])\n",
        "            history = model.fit(\n",
        "                flipped_X.astype(np.float32), flipped_y.astype(np.float32),\n",
        "                epochs=200,\n",
        "                validation_split=0.15, callbacks=[time_callback, early_stop])\n",
        "            times = time_callback.times\n",
        "\n",
        "            # Convert to dataframe\n",
        "            hist = pd.DataFrame(history.history)\n",
        "            hist['epoch'] = history.epoch\n",
        "            hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "            hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "            hist['times'] = times\n",
        "            hist['starting_filter'] = start_filter\n",
        "            hist['layers'] = 3\n",
        "            hist['pooling'] = 'yes'\n",
        "            hist['fc_layer1'] = 500\n",
        "            hist['fc_layer2'] = 500\n",
        "            hist['activation'] = 'relu'\n",
        "            hist['optimizer'] = 'adam'\n",
        "            hist['lrate'] = adam.get_config()['learning_rate']\n",
        "            hist['dropout_initial'] = d[0]\n",
        "            hist['dropout_step'] = d[1]\n",
        "            hist['batch_norm'] = 1\n",
        "            hist['bias'] = 0\n",
        "            hist['stride'] = 1\n",
        "            hist['flipped'] = 1.0\n",
        "\n",
        "            # Keep concatenating to dataframe\n",
        "            cnn_flipped_df = pd.concat([cnn_flipped_df,hist])\n",
        "\n",
        "            # Re-pickle after every model to retain progress\n",
        "            cnn_flipped_df.to_pickle(drive_path+\"OutputData/cnn_flipped_df6.pkl\")\n",
        "\n",
        "            # Save models.\n",
        "            filename = \"cnn_flipped6_{}_d{}_s{}_sf{}_lrfactor{}_flipped_100\".format('adam', d[0], d[1], start_filter, lr_factor)\n",
        "            model.save(drive_path+\"Models/\"+filename+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_42 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_42 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_56 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_43 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_57 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_58 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_73 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_59 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_74 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 837us/sample - loss: 230.1333 - mean_squared_error: 230.1334 - val_loss: 233.5911 - val_mean_squared_error: 233.5911\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 21.9216 - mean_squared_error: 21.9216 - val_loss: 68.3068 - val_mean_squared_error: 68.3068\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 18.3456 - mean_squared_error: 18.3456 - val_loss: 23.3899 - val_mean_squared_error: 23.3899\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 16.2826 - mean_squared_error: 16.2826 - val_loss: 15.9336 - val_mean_squared_error: 15.9336\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 15.0244 - mean_squared_error: 15.0244 - val_loss: 23.1279 - val_mean_squared_error: 23.1279\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 14.3419 - mean_squared_error: 14.3419 - val_loss: 15.0854 - val_mean_squared_error: 15.0854\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 12.0839 - mean_squared_error: 12.0839 - val_loss: 10.8079 - val_mean_squared_error: 10.8079\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 11.4763 - mean_squared_error: 11.4763 - val_loss: 11.5478 - val_mean_squared_error: 11.5478\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 10.0495 - mean_squared_error: 10.0495 - val_loss: 9.2513 - val_mean_squared_error: 9.2513\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 9.8827 - mean_squared_error: 9.8827 - val_loss: 8.0024 - val_mean_squared_error: 8.0024\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 8.4692 - mean_squared_error: 8.4692 - val_loss: 7.4662 - val_mean_squared_error: 7.4662\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 7.4583 - mean_squared_error: 7.4583 - val_loss: 6.2666 - val_mean_squared_error: 6.2666\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 6.7544 - mean_squared_error: 6.7544 - val_loss: 6.5478 - val_mean_squared_error: 6.5478\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 6.0218 - mean_squared_error: 6.0218 - val_loss: 5.9917 - val_mean_squared_error: 5.9917\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 5.7824 - mean_squared_error: 5.7824 - val_loss: 4.3376 - val_mean_squared_error: 4.3376\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 5.1199 - mean_squared_error: 5.1199 - val_loss: 4.9593 - val_mean_squared_error: 4.9593\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 4.7682 - mean_squared_error: 4.7682 - val_loss: 3.9804 - val_mean_squared_error: 3.9804\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 4.5882 - mean_squared_error: 4.5882 - val_loss: 4.0979 - val_mean_squared_error: 4.0979\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 4.4420 - mean_squared_error: 4.4420 - val_loss: 3.7960 - val_mean_squared_error: 3.7960\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 4.3229 - mean_squared_error: 4.3229 - val_loss: 4.2997 - val_mean_squared_error: 4.2997\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 4.2955 - mean_squared_error: 4.2955 - val_loss: 3.4209 - val_mean_squared_error: 3.4209\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 3.9560 - mean_squared_error: 3.9560 - val_loss: 4.4654 - val_mean_squared_error: 4.4654\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 3.7738 - mean_squared_error: 3.7738 - val_loss: 2.9772 - val_mean_squared_error: 2.9772\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 3.6051 - mean_squared_error: 3.6051 - val_loss: 2.9260 - val_mean_squared_error: 2.9260\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 3.6060 - mean_squared_error: 3.6060 - val_loss: 2.8402 - val_mean_squared_error: 2.8402\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 3.2797 - mean_squared_error: 3.2797 - val_loss: 3.2450 - val_mean_squared_error: 3.2450\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 3.3472 - mean_squared_error: 3.3472 - val_loss: 2.9707 - val_mean_squared_error: 2.9707\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 3.1748 - mean_squared_error: 3.1748 - val_loss: 3.1572 - val_mean_squared_error: 3.1572\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 3.1017 - mean_squared_error: 3.1017 - val_loss: 2.9390 - val_mean_squared_error: 2.9390\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 3.1163 - mean_squared_error: 3.1163 - val_loss: 2.6118 - val_mean_squared_error: 2.6118\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 2.9944 - mean_squared_error: 2.9944 - val_loss: 2.5662 - val_mean_squared_error: 2.5662\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 2.9646 - mean_squared_error: 2.9646 - val_loss: 2.7880 - val_mean_squared_error: 2.7880\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 2.7362 - mean_squared_error: 2.7362 - val_loss: 2.3848 - val_mean_squared_error: 2.3848\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 2.7499 - mean_squared_error: 2.7499 - val_loss: 2.4331 - val_mean_squared_error: 2.4331\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 2.6782 - mean_squared_error: 2.6782 - val_loss: 2.2909 - val_mean_squared_error: 2.2909\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 2.5849 - mean_squared_error: 2.5849 - val_loss: 2.3235 - val_mean_squared_error: 2.3235\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 2.5506 - mean_squared_error: 2.5506 - val_loss: 2.7084 - val_mean_squared_error: 2.7084\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 2.4069 - mean_squared_error: 2.4069 - val_loss: 2.2721 - val_mean_squared_error: 2.2721\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 2.6217 - mean_squared_error: 2.6217 - val_loss: 2.6510 - val_mean_squared_error: 2.6510\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 2.3428 - mean_squared_error: 2.3428 - val_loss: 2.2836 - val_mean_squared_error: 2.2836\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.3437 - mean_squared_error: 2.3437 - val_loss: 2.7037 - val_mean_squared_error: 2.7037\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 2.3959 - mean_squared_error: 2.3959 - val_loss: 2.6272 - val_mean_squared_error: 2.6272\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.1745 - mean_squared_error: 2.1745 - val_loss: 1.9364 - val_mean_squared_error: 1.9364\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 2.1412 - mean_squared_error: 2.1412 - val_loss: 2.5140 - val_mean_squared_error: 2.5140\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 2.0327 - mean_squared_error: 2.0327 - val_loss: 2.1235 - val_mean_squared_error: 2.1235\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 2.0727 - mean_squared_error: 2.0727 - val_loss: 1.8860 - val_mean_squared_error: 1.8860\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.0700 - mean_squared_error: 2.0700 - val_loss: 2.2739 - val_mean_squared_error: 2.2739\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 2.0760 - mean_squared_error: 2.0760 - val_loss: 2.0013 - val_mean_squared_error: 2.0013\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 2.0290 - mean_squared_error: 2.0290 - val_loss: 1.8519 - val_mean_squared_error: 1.8519\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 2.0492 - mean_squared_error: 2.0492 - val_loss: 1.9240 - val_mean_squared_error: 1.9240\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.9780 - mean_squared_error: 1.9780 - val_loss: 2.1972 - val_mean_squared_error: 2.1972\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.9742 - mean_squared_error: 1.9742 - val_loss: 2.0443 - val_mean_squared_error: 2.0443\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.8391 - mean_squared_error: 1.8391 - val_loss: 1.8686 - val_mean_squared_error: 1.8686\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.8194 - mean_squared_error: 1.8194 - val_loss: 1.8867 - val_mean_squared_error: 1.8867\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.7523 - mean_squared_error: 1.7523 - val_loss: 2.1721 - val_mean_squared_error: 2.1721\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.7926 - mean_squared_error: 1.7926 - val_loss: 2.3080 - val_mean_squared_error: 2.3080\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.6867 - mean_squared_error: 1.6867 - val_loss: 1.9891 - val_mean_squared_error: 1.9891\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.7126 - mean_squared_error: 1.7126 - val_loss: 1.9220 - val_mean_squared_error: 1.9220\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.7713 - mean_squared_error: 1.7713 - val_loss: 1.9013 - val_mean_squared_error: 1.9013\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.6939 - mean_squared_error: 1.6939 - val_loss: 1.7365 - val_mean_squared_error: 1.7365\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.6358 - mean_squared_error: 1.6358 - val_loss: 1.9299 - val_mean_squared_error: 1.9299\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.7074 - mean_squared_error: 1.7074 - val_loss: 2.0788 - val_mean_squared_error: 2.0788\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.6331 - mean_squared_error: 1.6331 - val_loss: 1.9074 - val_mean_squared_error: 1.9074\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.5850 - mean_squared_error: 1.5850 - val_loss: 2.0268 - val_mean_squared_error: 2.0268\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.6403 - mean_squared_error: 1.6403 - val_loss: 1.7257 - val_mean_squared_error: 1.7257\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.6302 - mean_squared_error: 1.6302 - val_loss: 1.7299 - val_mean_squared_error: 1.7299\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.6388 - mean_squared_error: 1.6388 - val_loss: 1.6430 - val_mean_squared_error: 1.6430\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.6239 - mean_squared_error: 1.6239 - val_loss: 1.6821 - val_mean_squared_error: 1.6821\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.7037 - mean_squared_error: 1.7037 - val_loss: 1.7526 - val_mean_squared_error: 1.7526\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.5393 - mean_squared_error: 1.5393 - val_loss: 1.7286 - val_mean_squared_error: 1.7286\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.5689 - mean_squared_error: 1.5689 - val_loss: 1.9809 - val_mean_squared_error: 1.9809\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.6811 - mean_squared_error: 1.6811 - val_loss: 1.8676 - val_mean_squared_error: 1.8676\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.4348 - mean_squared_error: 1.4348 - val_loss: 2.1427 - val_mean_squared_error: 2.1427\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.5170 - mean_squared_error: 1.5170 - val_loss: 1.5674 - val_mean_squared_error: 1.5674\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.5529 - mean_squared_error: 1.5529 - val_loss: 1.6775 - val_mean_squared_error: 1.6775\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.4476 - mean_squared_error: 1.4476 - val_loss: 1.7336 - val_mean_squared_error: 1.7336\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.4454 - mean_squared_error: 1.4454 - val_loss: 1.6502 - val_mean_squared_error: 1.6502\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.3833 - mean_squared_error: 1.3833 - val_loss: 1.8646 - val_mean_squared_error: 1.8646\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.5215 - mean_squared_error: 1.5215 - val_loss: 1.8217 - val_mean_squared_error: 1.8217\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.4268 - mean_squared_error: 1.4268 - val_loss: 1.5672 - val_mean_squared_error: 1.5672\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.4122 - mean_squared_error: 1.4122 - val_loss: 1.6736 - val_mean_squared_error: 1.6736\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.4270 - mean_squared_error: 1.4270 - val_loss: 2.0412 - val_mean_squared_error: 2.0412\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.4469 - mean_squared_error: 1.4469 - val_loss: 1.7207 - val_mean_squared_error: 1.7207\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.3674 - mean_squared_error: 1.3674 - val_loss: 1.8242 - val_mean_squared_error: 1.8242\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.3397 - mean_squared_error: 1.3397 - val_loss: 1.6644 - val_mean_squared_error: 1.6644\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.3633 - mean_squared_error: 1.3633 - val_loss: 2.1426 - val_mean_squared_error: 2.1426\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.3847 - mean_squared_error: 1.3847 - val_loss: 2.1218 - val_mean_squared_error: 2.1218\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.3668 - mean_squared_error: 1.3668 - val_loss: 1.8632 - val_mean_squared_error: 1.8632\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.2940 - mean_squared_error: 1.2940 - val_loss: 1.6955 - val_mean_squared_error: 1.6955\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.4067 - mean_squared_error: 1.4067 - val_loss: 1.7115 - val_mean_squared_error: 1.7115\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.3319 - mean_squared_error: 1.3319 - val_loss: 1.5796 - val_mean_squared_error: 1.5796\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.3089 - mean_squared_error: 1.3089 - val_loss: 1.7287 - val_mean_squared_error: 1.7287\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.4024 - mean_squared_error: 1.4024 - val_loss: 1.6545 - val_mean_squared_error: 1.6545\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.3825 - mean_squared_error: 1.3825 - val_loss: 1.6912 - val_mean_squared_error: 1.6912\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.1905 - mean_squared_error: 1.1905 - val_loss: 1.6798 - val_mean_squared_error: 1.6798\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.2898 - mean_squared_error: 1.2898 - val_loss: 1.6114 - val_mean_squared_error: 1.6114\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.2900 - mean_squared_error: 1.2900 - val_loss: 1.7266 - val_mean_squared_error: 1.7266\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2038 - mean_squared_error: 1.2038 - val_loss: 1.7081 - val_mean_squared_error: 1.7081\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.2512 - mean_squared_error: 1.2512 - val_loss: 1.7629 - val_mean_squared_error: 1.7629\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.2166 - mean_squared_error: 1.2166 - val_loss: 1.7268 - val_mean_squared_error: 1.7268\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.2339 - mean_squared_error: 1.2339 - val_loss: 2.0743 - val_mean_squared_error: 2.0743\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.2355 - mean_squared_error: 1.2355 - val_loss: 1.5951 - val_mean_squared_error: 1.5951\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.1839 - mean_squared_error: 1.1839 - val_loss: 1.6800 - val_mean_squared_error: 1.6800\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2124 - mean_squared_error: 1.2124 - val_loss: 1.6656 - val_mean_squared_error: 1.6656\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.2188 - mean_squared_error: 1.2188 - val_loss: 1.7096 - val_mean_squared_error: 1.7096\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2061 - mean_squared_error: 1.2061 - val_loss: 1.6643 - val_mean_squared_error: 1.6643\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.3163 - mean_squared_error: 1.3163 - val_loss: 1.5102 - val_mean_squared_error: 1.5102\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.1490 - mean_squared_error: 1.1490 - val_loss: 1.6717 - val_mean_squared_error: 1.6717\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.2362 - mean_squared_error: 1.2362 - val_loss: 1.5599 - val_mean_squared_error: 1.5599\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2200 - mean_squared_error: 1.2200 - val_loss: 1.6092 - val_mean_squared_error: 1.6092\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.1409 - mean_squared_error: 1.1409 - val_loss: 1.6113 - val_mean_squared_error: 1.6113\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.1858 - mean_squared_error: 1.1858 - val_loss: 1.7435 - val_mean_squared_error: 1.7435\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.1111 - mean_squared_error: 1.1111 - val_loss: 1.8302 - val_mean_squared_error: 1.8302\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2412 - mean_squared_error: 1.2412 - val_loss: 1.9499 - val_mean_squared_error: 1.9499\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.1633 - mean_squared_error: 1.1633 - val_loss: 2.0933 - val_mean_squared_error: 2.0933\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.1098 - mean_squared_error: 1.1098 - val_loss: 1.6425 - val_mean_squared_error: 1.6425\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.1016 - mean_squared_error: 1.1016 - val_loss: 1.5535 - val_mean_squared_error: 1.5535\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.1675 - mean_squared_error: 1.1675 - val_loss: 1.7340 - val_mean_squared_error: 1.7340\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.1717 - mean_squared_error: 1.1717 - val_loss: 1.7893 - val_mean_squared_error: 1.7893\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.1659 - mean_squared_error: 1.1659 - val_loss: 1.7112 - val_mean_squared_error: 1.7112\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.1757 - mean_squared_error: 1.1757 - val_loss: 1.6818 - val_mean_squared_error: 1.6818\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.1517 - mean_squared_error: 1.1517 - val_loss: 1.7289 - val_mean_squared_error: 1.7289\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.1225 - mean_squared_error: 1.1225 - val_loss: 2.0171 - val_mean_squared_error: 2.0171\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.1909 - mean_squared_error: 1.1909 - val_loss: 1.6635 - val_mean_squared_error: 1.6635\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.0919 - mean_squared_error: 1.0919 - val_loss: 1.8945 - val_mean_squared_error: 1.8945\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.1147 - mean_squared_error: 1.1147 - val_loss: 1.8370 - val_mean_squared_error: 1.8370\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.1479 - mean_squared_error: 1.1479 - val_loss: 1.6168 - val_mean_squared_error: 1.6168\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.0928 - mean_squared_error: 1.0928 - val_loss: 1.5816 - val_mean_squared_error: 1.5816\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.1376 - mean_squared_error: 1.1376 - val_loss: 1.7296 - val_mean_squared_error: 1.7296\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0853 - mean_squared_error: 1.0853 - val_loss: 1.6499 - val_mean_squared_error: 1.6499\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.1159 - mean_squared_error: 1.1159 - val_loss: 1.7532 - val_mean_squared_error: 1.7532\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.0329 - mean_squared_error: 1.0329 - val_loss: 2.0669 - val_mean_squared_error: 2.0669\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0898 - mean_squared_error: 1.0898 - val_loss: 1.7655 - val_mean_squared_error: 1.7655\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0830 - mean_squared_error: 1.0830 - val_loss: 1.7227 - val_mean_squared_error: 1.7227\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.9943 - mean_squared_error: 0.9943 - val_loss: 1.5620 - val_mean_squared_error: 1.5620\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0233 - mean_squared_error: 1.0233 - val_loss: 1.6684 - val_mean_squared_error: 1.6684\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0776 - mean_squared_error: 1.0776 - val_loss: 1.6472 - val_mean_squared_error: 1.6472\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.0800 - mean_squared_error: 1.0800 - val_loss: 1.6479 - val_mean_squared_error: 1.6479\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0808 - mean_squared_error: 1.0808 - val_loss: 1.5830 - val_mean_squared_error: 1.5830\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.0521 - mean_squared_error: 1.0521 - val_loss: 1.7081 - val_mean_squared_error: 1.7081\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.0597 - mean_squared_error: 1.0597 - val_loss: 1.7934 - val_mean_squared_error: 1.7934\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.0280 - mean_squared_error: 1.0280 - val_loss: 1.6118 - val_mean_squared_error: 1.6118\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0669 - mean_squared_error: 1.0669 - val_loss: 1.6094 - val_mean_squared_error: 1.6094\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.9788 - mean_squared_error: 0.9788 - val_loss: 1.5895 - val_mean_squared_error: 1.5895\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.0376 - mean_squared_error: 1.0376 - val_loss: 1.6589 - val_mean_squared_error: 1.6589\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.6533 - val_mean_squared_error: 1.6533\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 1.0537 - mean_squared_error: 1.0537 - val_loss: 1.6206 - val_mean_squared_error: 1.6206\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.0031 - mean_squared_error: 1.0031 - val_loss: 1.7377 - val_mean_squared_error: 1.7377\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.9811 - mean_squared_error: 0.9811 - val_loss: 1.5836 - val_mean_squared_error: 1.5836\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.9901 - mean_squared_error: 0.9901 - val_loss: 1.8639 - val_mean_squared_error: 1.8639\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.0468 - mean_squared_error: 1.0468 - val_loss: 1.6596 - val_mean_squared_error: 1.6596\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.0324 - mean_squared_error: 1.0324 - val_loss: 1.6531 - val_mean_squared_error: 1.6531\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.9491 - mean_squared_error: 0.9491 - val_loss: 1.6728 - val_mean_squared_error: 1.6728\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9835 - mean_squared_error: 0.9835 - val_loss: 1.5604 - val_mean_squared_error: 1.5604\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.9906 - mean_squared_error: 0.9906 - val_loss: 1.9184 - val_mean_squared_error: 1.9184\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.0249 - mean_squared_error: 1.0249 - val_loss: 1.6947 - val_mean_squared_error: 1.6947\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9851 - mean_squared_error: 0.9851 - val_loss: 1.6645 - val_mean_squared_error: 1.6645\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.9551 - mean_squared_error: 0.9551 - val_loss: 1.8248 - val_mean_squared_error: 1.8248\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.9474 - mean_squared_error: 0.9474 - val_loss: 1.7632 - val_mean_squared_error: 1.7632\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.0372 - mean_squared_error: 1.0372 - val_loss: 1.8196 - val_mean_squared_error: 1.8196\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.0569 - mean_squared_error: 1.0569 - val_loss: 1.6064 - val_mean_squared_error: 1.6064\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.6420 - val_mean_squared_error: 1.6420\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.9448 - mean_squared_error: 0.9448 - val_loss: 1.6172 - val_mean_squared_error: 1.6172\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9852 - mean_squared_error: 0.9852 - val_loss: 1.6628 - val_mean_squared_error: 1.6628\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.9115 - mean_squared_error: 0.9115 - val_loss: 1.6043 - val_mean_squared_error: 1.6043\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.9615 - mean_squared_error: 0.9615 - val_loss: 1.7190 - val_mean_squared_error: 1.7190\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9347 - mean_squared_error: 0.9347 - val_loss: 1.6517 - val_mean_squared_error: 1.6517\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9258 - mean_squared_error: 0.9258 - val_loss: 1.6772 - val_mean_squared_error: 1.6772\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.9261 - mean_squared_error: 0.9261 - val_loss: 1.5908 - val_mean_squared_error: 1.5908\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9360 - mean_squared_error: 0.9360 - val_loss: 1.6432 - val_mean_squared_error: 1.6432\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.9418 - mean_squared_error: 0.9418 - val_loss: 1.6754 - val_mean_squared_error: 1.6754\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.9941 - mean_squared_error: 0.9941 - val_loss: 1.7864 - val_mean_squared_error: 1.7864\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.9246 - mean_squared_error: 0.9246 - val_loss: 1.7682 - val_mean_squared_error: 1.7682\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.9023 - mean_squared_error: 0.9023 - val_loss: 1.5170 - val_mean_squared_error: 1.5170\n",
            "==================================================\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_45 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_75 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_60 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_76 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_61 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_77 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_62 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_78 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_63 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_79 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 916us/sample - loss: 228.3768 - mean_squared_error: 228.3769 - val_loss: 174.2572 - val_mean_squared_error: 174.2572\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 20.5152 - mean_squared_error: 20.5152 - val_loss: 43.4856 - val_mean_squared_error: 43.4856\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 15.7205 - mean_squared_error: 15.7205 - val_loss: 12.7312 - val_mean_squared_error: 12.7312\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 14.9349 - mean_squared_error: 14.9349 - val_loss: 11.5483 - val_mean_squared_error: 11.5483\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 13.3080 - mean_squared_error: 13.3080 - val_loss: 11.0735 - val_mean_squared_error: 11.0735\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 12.1526 - mean_squared_error: 12.1526 - val_loss: 12.0810 - val_mean_squared_error: 12.0810\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 11.6030 - mean_squared_error: 11.6030 - val_loss: 10.5068 - val_mean_squared_error: 10.5068\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 10.4850 - mean_squared_error: 10.4850 - val_loss: 9.7831 - val_mean_squared_error: 9.7831\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 9.4118 - mean_squared_error: 9.4118 - val_loss: 11.4880 - val_mean_squared_error: 11.4880\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 8.5468 - mean_squared_error: 8.5468 - val_loss: 8.7952 - val_mean_squared_error: 8.7952\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 7.6789 - mean_squared_error: 7.6789 - val_loss: 7.3553 - val_mean_squared_error: 7.3553\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 6.6784 - mean_squared_error: 6.6784 - val_loss: 5.6911 - val_mean_squared_error: 5.6911\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 6.1154 - mean_squared_error: 6.1154 - val_loss: 5.1822 - val_mean_squared_error: 5.1822\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 5.5963 - mean_squared_error: 5.5963 - val_loss: 4.5712 - val_mean_squared_error: 4.5712\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 5.4388 - mean_squared_error: 5.4388 - val_loss: 4.4158 - val_mean_squared_error: 4.4158\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 4.9673 - mean_squared_error: 4.9673 - val_loss: 3.9888 - val_mean_squared_error: 3.9888\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 4.5428 - mean_squared_error: 4.5428 - val_loss: 3.7005 - val_mean_squared_error: 3.7005\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 4.4148 - mean_squared_error: 4.4148 - val_loss: 5.6247 - val_mean_squared_error: 5.6247\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 4.3763 - mean_squared_error: 4.3763 - val_loss: 3.9845 - val_mean_squared_error: 3.9845\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 4.0323 - mean_squared_error: 4.0323 - val_loss: 3.6174 - val_mean_squared_error: 3.6174\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 3.9770 - mean_squared_error: 3.9770 - val_loss: 3.1888 - val_mean_squared_error: 3.1888\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 3.6883 - mean_squared_error: 3.6883 - val_loss: 2.8964 - val_mean_squared_error: 2.8964\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 3.7223 - mean_squared_error: 3.7223 - val_loss: 2.8386 - val_mean_squared_error: 2.8386\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 3.5818 - mean_squared_error: 3.5818 - val_loss: 3.0218 - val_mean_squared_error: 3.0218\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 3.4399 - mean_squared_error: 3.4399 - val_loss: 3.2340 - val_mean_squared_error: 3.2340\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 3.1958 - mean_squared_error: 3.1958 - val_loss: 3.0831 - val_mean_squared_error: 3.0831\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 3.1393 - mean_squared_error: 3.1393 - val_loss: 2.7872 - val_mean_squared_error: 2.7872\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 2.9904 - mean_squared_error: 2.9904 - val_loss: 2.7058 - val_mean_squared_error: 2.7058\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 3.0022 - mean_squared_error: 3.0022 - val_loss: 3.0049 - val_mean_squared_error: 3.0049\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.9642 - mean_squared_error: 2.9642 - val_loss: 2.8634 - val_mean_squared_error: 2.8634\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 2.8122 - mean_squared_error: 2.8122 - val_loss: 2.9611 - val_mean_squared_error: 2.9611\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.7238 - mean_squared_error: 2.7238 - val_loss: 2.6597 - val_mean_squared_error: 2.6597\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 2.6528 - mean_squared_error: 2.6528 - val_loss: 2.5337 - val_mean_squared_error: 2.5337\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 2.7306 - mean_squared_error: 2.7306 - val_loss: 2.5820 - val_mean_squared_error: 2.5820\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 2.6636 - mean_squared_error: 2.6636 - val_loss: 2.4101 - val_mean_squared_error: 2.4101\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 2.5729 - mean_squared_error: 2.5729 - val_loss: 2.0959 - val_mean_squared_error: 2.0959\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 2.4347 - mean_squared_error: 2.4347 - val_loss: 2.1637 - val_mean_squared_error: 2.1637\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.4209 - mean_squared_error: 2.4209 - val_loss: 2.1538 - val_mean_squared_error: 2.1538\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 2.3599 - mean_squared_error: 2.3599 - val_loss: 2.1472 - val_mean_squared_error: 2.1472\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 2.2854 - mean_squared_error: 2.2854 - val_loss: 1.9835 - val_mean_squared_error: 1.9835\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 2.4214 - mean_squared_error: 2.4214 - val_loss: 2.0987 - val_mean_squared_error: 2.0987\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.2267 - mean_squared_error: 2.2267 - val_loss: 1.9505 - val_mean_squared_error: 1.9505\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 2.1891 - mean_squared_error: 2.1891 - val_loss: 2.1218 - val_mean_squared_error: 2.1218\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 2.0994 - mean_squared_error: 2.0994 - val_loss: 2.2900 - val_mean_squared_error: 2.2900\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.1343 - mean_squared_error: 2.1343 - val_loss: 1.9685 - val_mean_squared_error: 1.9685\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 2.0921 - mean_squared_error: 2.0921 - val_loss: 1.9044 - val_mean_squared_error: 1.9044\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 2.0195 - mean_squared_error: 2.0195 - val_loss: 1.9561 - val_mean_squared_error: 1.9561\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 2.0469 - mean_squared_error: 2.0469 - val_loss: 2.4396 - val_mean_squared_error: 2.4396\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.9089 - mean_squared_error: 1.9089 - val_loss: 1.8075 - val_mean_squared_error: 1.8075\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.9438 - mean_squared_error: 1.9438 - val_loss: 2.0380 - val_mean_squared_error: 2.0380\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.9641 - mean_squared_error: 1.9641 - val_loss: 1.8972 - val_mean_squared_error: 1.8972\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.8149 - mean_squared_error: 1.8149 - val_loss: 1.9728 - val_mean_squared_error: 1.9728\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.9811 - mean_squared_error: 1.9811 - val_loss: 2.6854 - val_mean_squared_error: 2.6854\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.8247 - mean_squared_error: 1.8248 - val_loss: 1.7529 - val_mean_squared_error: 1.7529\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.8756 - mean_squared_error: 1.8756 - val_loss: 1.9952 - val_mean_squared_error: 1.9952\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.7205 - mean_squared_error: 1.7205 - val_loss: 2.1848 - val_mean_squared_error: 2.1848\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.6976 - mean_squared_error: 1.6976 - val_loss: 2.0938 - val_mean_squared_error: 2.0938\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.7157 - mean_squared_error: 1.7157 - val_loss: 2.0553 - val_mean_squared_error: 2.0553\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.7042 - mean_squared_error: 1.7042 - val_loss: 1.8310 - val_mean_squared_error: 1.8310\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.6506 - mean_squared_error: 1.6506 - val_loss: 1.8740 - val_mean_squared_error: 1.8740\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.6945 - mean_squared_error: 1.6945 - val_loss: 1.7346 - val_mean_squared_error: 1.7346\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.5484 - mean_squared_error: 1.5484 - val_loss: 2.0006 - val_mean_squared_error: 2.0006\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.5901 - mean_squared_error: 1.5901 - val_loss: 1.8754 - val_mean_squared_error: 1.8754\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.7344 - mean_squared_error: 1.7344 - val_loss: 1.7961 - val_mean_squared_error: 1.7961\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.5655 - mean_squared_error: 1.5655 - val_loss: 2.0739 - val_mean_squared_error: 2.0739\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.5689 - mean_squared_error: 1.5689 - val_loss: 1.6457 - val_mean_squared_error: 1.6457\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.5034 - mean_squared_error: 1.5034 - val_loss: 1.8409 - val_mean_squared_error: 1.8409\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.6020 - mean_squared_error: 1.6020 - val_loss: 1.7562 - val_mean_squared_error: 1.7562\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.5026 - mean_squared_error: 1.5026 - val_loss: 1.9685 - val_mean_squared_error: 1.9685\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.5269 - mean_squared_error: 1.5269 - val_loss: 1.8400 - val_mean_squared_error: 1.8400\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.4497 - mean_squared_error: 1.4497 - val_loss: 1.6737 - val_mean_squared_error: 1.6737\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.4686 - mean_squared_error: 1.4686 - val_loss: 1.7037 - val_mean_squared_error: 1.7037\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.4664 - mean_squared_error: 1.4664 - val_loss: 1.6971 - val_mean_squared_error: 1.6971\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.4512 - mean_squared_error: 1.4512 - val_loss: 1.7890 - val_mean_squared_error: 1.7890\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.4176 - mean_squared_error: 1.4176 - val_loss: 1.6882 - val_mean_squared_error: 1.6882\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.4880 - mean_squared_error: 1.4880 - val_loss: 1.6898 - val_mean_squared_error: 1.6898\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.4638 - mean_squared_error: 1.4638 - val_loss: 1.9206 - val_mean_squared_error: 1.9206\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.4667 - mean_squared_error: 1.4667 - val_loss: 1.9450 - val_mean_squared_error: 1.9450\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.3868 - mean_squared_error: 1.3868 - val_loss: 1.9832 - val_mean_squared_error: 1.9832\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.4332 - mean_squared_error: 1.4332 - val_loss: 1.5886 - val_mean_squared_error: 1.5886\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.3983 - mean_squared_error: 1.3983 - val_loss: 1.6899 - val_mean_squared_error: 1.6899\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.3199 - mean_squared_error: 1.3199 - val_loss: 1.8004 - val_mean_squared_error: 1.8004\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.3254 - mean_squared_error: 1.3254 - val_loss: 1.6511 - val_mean_squared_error: 1.6511\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.4849 - mean_squared_error: 1.4849 - val_loss: 1.9747 - val_mean_squared_error: 1.9747\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.3196 - mean_squared_error: 1.3196 - val_loss: 1.7058 - val_mean_squared_error: 1.7058\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.3178 - mean_squared_error: 1.3178 - val_loss: 1.7598 - val_mean_squared_error: 1.7598\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.2864 - mean_squared_error: 1.2864 - val_loss: 1.6917 - val_mean_squared_error: 1.6917\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.2890 - mean_squared_error: 1.2890 - val_loss: 1.9304 - val_mean_squared_error: 1.9304\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.3113 - mean_squared_error: 1.3113 - val_loss: 1.6147 - val_mean_squared_error: 1.6147\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.3384 - mean_squared_error: 1.3384 - val_loss: 1.6460 - val_mean_squared_error: 1.6460\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.3209 - mean_squared_error: 1.3209 - val_loss: 1.8709 - val_mean_squared_error: 1.8709\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.3150 - mean_squared_error: 1.3150 - val_loss: 1.6069 - val_mean_squared_error: 1.6069\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.3929 - mean_squared_error: 1.3929 - val_loss: 1.7801 - val_mean_squared_error: 1.7801\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1890 - mean_squared_error: 1.1890 - val_loss: 1.8147 - val_mean_squared_error: 1.8147\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.3775 - mean_squared_error: 1.3775 - val_loss: 1.6183 - val_mean_squared_error: 1.6183\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1625 - mean_squared_error: 1.1625 - val_loss: 1.6499 - val_mean_squared_error: 1.6499\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.2160 - mean_squared_error: 1.2160 - val_loss: 1.6104 - val_mean_squared_error: 1.6104\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.2085 - mean_squared_error: 1.2085 - val_loss: 1.6568 - val_mean_squared_error: 1.6568\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.3010 - mean_squared_error: 1.3010 - val_loss: 1.6315 - val_mean_squared_error: 1.6315\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.2632 - mean_squared_error: 1.2632 - val_loss: 1.7427 - val_mean_squared_error: 1.7427\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1958 - mean_squared_error: 1.1958 - val_loss: 1.5431 - val_mean_squared_error: 1.5431\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.2375 - mean_squared_error: 1.2375 - val_loss: 1.8113 - val_mean_squared_error: 1.8113\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.2114 - mean_squared_error: 1.2114 - val_loss: 1.7882 - val_mean_squared_error: 1.7882\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.2230 - mean_squared_error: 1.2230 - val_loss: 1.7286 - val_mean_squared_error: 1.7286\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.1646 - mean_squared_error: 1.1646 - val_loss: 1.6315 - val_mean_squared_error: 1.6315\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.1129 - mean_squared_error: 1.1129 - val_loss: 1.6899 - val_mean_squared_error: 1.6899\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.1918 - mean_squared_error: 1.1918 - val_loss: 1.8828 - val_mean_squared_error: 1.8828\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.1905 - mean_squared_error: 1.1905 - val_loss: 1.6197 - val_mean_squared_error: 1.6197\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.2612 - mean_squared_error: 1.2612 - val_loss: 2.0361 - val_mean_squared_error: 2.0361\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.0967 - mean_squared_error: 1.0967 - val_loss: 1.7677 - val_mean_squared_error: 1.7677\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.1511 - mean_squared_error: 1.1511 - val_loss: 2.0853 - val_mean_squared_error: 2.0853\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.1144 - mean_squared_error: 1.1144 - val_loss: 1.6055 - val_mean_squared_error: 1.6055\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.1358 - mean_squared_error: 1.1358 - val_loss: 1.7142 - val_mean_squared_error: 1.7142\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.1317 - mean_squared_error: 1.1317 - val_loss: 1.7565 - val_mean_squared_error: 1.7565\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.1119 - mean_squared_error: 1.1119 - val_loss: 1.6041 - val_mean_squared_error: 1.6041\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.1684 - mean_squared_error: 1.1684 - val_loss: 1.7989 - val_mean_squared_error: 1.7989\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.0653 - mean_squared_error: 1.0653 - val_loss: 1.7738 - val_mean_squared_error: 1.7738\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.1359 - mean_squared_error: 1.1359 - val_loss: 1.9257 - val_mean_squared_error: 1.9257\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 1.7056 - val_mean_squared_error: 1.7056\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 406us/sample - loss: 1.1296 - mean_squared_error: 1.1296 - val_loss: 2.0779 - val_mean_squared_error: 2.0779\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 1.2529 - mean_squared_error: 1.2529 - val_loss: 1.6448 - val_mean_squared_error: 1.6448\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 1.0717 - mean_squared_error: 1.0717 - val_loss: 1.7575 - val_mean_squared_error: 1.7575\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.0737 - mean_squared_error: 1.0737 - val_loss: 1.6359 - val_mean_squared_error: 1.6359\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.0690 - mean_squared_error: 1.0690 - val_loss: 1.6204 - val_mean_squared_error: 1.6204\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.0379 - mean_squared_error: 1.0379 - val_loss: 2.5999 - val_mean_squared_error: 2.5999\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.0523 - mean_squared_error: 1.0523 - val_loss: 1.7212 - val_mean_squared_error: 1.7212\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.0291 - mean_squared_error: 1.0291 - val_loss: 1.6611 - val_mean_squared_error: 1.6611\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0849 - mean_squared_error: 1.0849 - val_loss: 1.5953 - val_mean_squared_error: 1.5953\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.0213 - mean_squared_error: 1.0213 - val_loss: 1.6260 - val_mean_squared_error: 1.6260\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0905 - mean_squared_error: 1.0905 - val_loss: 1.7584 - val_mean_squared_error: 1.7584\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.0985 - mean_squared_error: 1.0985 - val_loss: 1.6443 - val_mean_squared_error: 1.6443\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.0369 - mean_squared_error: 1.0369 - val_loss: 1.7666 - val_mean_squared_error: 1.7666\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.0490 - mean_squared_error: 1.0490 - val_loss: 1.7444 - val_mean_squared_error: 1.7444\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.0305 - mean_squared_error: 1.0305 - val_loss: 1.6457 - val_mean_squared_error: 1.6457\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0647 - mean_squared_error: 1.0647 - val_loss: 1.7293 - val_mean_squared_error: 1.7293\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.6977 - val_mean_squared_error: 1.6977\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9737 - mean_squared_error: 0.9737 - val_loss: 1.6160 - val_mean_squared_error: 1.6160\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.9981 - mean_squared_error: 0.9981 - val_loss: 1.6912 - val_mean_squared_error: 1.6912\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.0255 - mean_squared_error: 1.0255 - val_loss: 1.7398 - val_mean_squared_error: 1.7398\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.7451 - val_mean_squared_error: 1.7451\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.0289 - mean_squared_error: 1.0289 - val_loss: 1.7014 - val_mean_squared_error: 1.7014\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.9725 - mean_squared_error: 0.9725 - val_loss: 1.7877 - val_mean_squared_error: 1.7877\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.9726 - mean_squared_error: 0.9726 - val_loss: 1.6400 - val_mean_squared_error: 1.6400\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.0001 - mean_squared_error: 1.0001 - val_loss: 1.5485 - val_mean_squared_error: 1.5485\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.9816 - mean_squared_error: 0.9816 - val_loss: 1.7270 - val_mean_squared_error: 1.7270\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.9860 - mean_squared_error: 0.9860 - val_loss: 1.6046 - val_mean_squared_error: 1.6046\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.6005 - val_mean_squared_error: 1.6005\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.0212 - mean_squared_error: 1.0212 - val_loss: 1.7311 - val_mean_squared_error: 1.7311\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9782 - mean_squared_error: 0.9782 - val_loss: 1.6406 - val_mean_squared_error: 1.6406\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9501 - mean_squared_error: 0.9501 - val_loss: 1.5966 - val_mean_squared_error: 1.5966\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.9809 - mean_squared_error: 0.9809 - val_loss: 1.7350 - val_mean_squared_error: 1.7350\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.9702 - mean_squared_error: 0.9702 - val_loss: 1.6146 - val_mean_squared_error: 1.6146\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.9420 - mean_squared_error: 0.9420 - val_loss: 1.7158 - val_mean_squared_error: 1.7158\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.9376 - mean_squared_error: 0.9376 - val_loss: 1.7328 - val_mean_squared_error: 1.7328\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.9224 - mean_squared_error: 0.9224 - val_loss: 1.5739 - val_mean_squared_error: 1.5739\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.9279 - mean_squared_error: 0.9279 - val_loss: 1.7619 - val_mean_squared_error: 1.7619\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.0180 - mean_squared_error: 1.0180 - val_loss: 1.7576 - val_mean_squared_error: 1.7576\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.9489 - mean_squared_error: 0.9489 - val_loss: 1.5962 - val_mean_squared_error: 1.5962\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9231 - mean_squared_error: 0.9231 - val_loss: 1.6188 - val_mean_squared_error: 1.6189\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9487 - mean_squared_error: 0.9487 - val_loss: 1.6442 - val_mean_squared_error: 1.6442\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9438 - mean_squared_error: 0.9438 - val_loss: 1.6826 - val_mean_squared_error: 1.6826\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.9790 - mean_squared_error: 0.9790 - val_loss: 1.6523 - val_mean_squared_error: 1.6523\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.9392 - mean_squared_error: 0.9392 - val_loss: 1.6583 - val_mean_squared_error: 1.6583\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8835 - mean_squared_error: 0.8835 - val_loss: 1.6112 - val_mean_squared_error: 1.6112\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9062 - mean_squared_error: 0.9062 - val_loss: 1.6775 - val_mean_squared_error: 1.6775\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.8925 - mean_squared_error: 0.8925 - val_loss: 1.6165 - val_mean_squared_error: 1.6165\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8702 - mean_squared_error: 0.8702 - val_loss: 1.7379 - val_mean_squared_error: 1.7379\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9239 - mean_squared_error: 0.9239 - val_loss: 1.7782 - val_mean_squared_error: 1.7782\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9749 - mean_squared_error: 0.9749 - val_loss: 1.6226 - val_mean_squared_error: 1.6226\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9159 - mean_squared_error: 0.9159 - val_loss: 1.6544 - val_mean_squared_error: 1.6544\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8742 - mean_squared_error: 0.8742 - val_loss: 1.6392 - val_mean_squared_error: 1.6392\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.9097 - mean_squared_error: 0.9097 - val_loss: 1.8307 - val_mean_squared_error: 1.8307\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9088 - mean_squared_error: 0.9088 - val_loss: 1.6260 - val_mean_squared_error: 1.6260\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.9080 - mean_squared_error: 0.9080 - val_loss: 1.6421 - val_mean_squared_error: 1.6421\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.8626 - mean_squared_error: 0.8626 - val_loss: 1.6603 - val_mean_squared_error: 1.6603\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8552 - mean_squared_error: 0.8552 - val_loss: 1.6059 - val_mean_squared_error: 1.6059\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.8609 - mean_squared_error: 0.8609 - val_loss: 1.5937 - val_mean_squared_error: 1.5937\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8991 - mean_squared_error: 0.8991 - val_loss: 1.7119 - val_mean_squared_error: 1.7119\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9274 - mean_squared_error: 0.9274 - val_loss: 1.6606 - val_mean_squared_error: 1.6606\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8632 - mean_squared_error: 0.8632 - val_loss: 1.7058 - val_mean_squared_error: 1.7058\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8629 - mean_squared_error: 0.8629 - val_loss: 1.5902 - val_mean_squared_error: 1.5902\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9161 - mean_squared_error: 0.9161 - val_loss: 1.8186 - val_mean_squared_error: 1.8186\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8882 - mean_squared_error: 0.8882 - val_loss: 1.6583 - val_mean_squared_error: 1.6583\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 406us/sample - loss: 0.9176 - mean_squared_error: 0.9176 - val_loss: 1.7590 - val_mean_squared_error: 1.7590\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8912 - mean_squared_error: 0.8912 - val_loss: 1.7101 - val_mean_squared_error: 1.7101\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.8742 - mean_squared_error: 0.8742 - val_loss: 1.8685 - val_mean_squared_error: 1.8685\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.8615 - mean_squared_error: 0.8615 - val_loss: 1.5933 - val_mean_squared_error: 1.5933\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.8193 - mean_squared_error: 0.8193 - val_loss: 1.6387 - val_mean_squared_error: 1.6387\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.8839 - mean_squared_error: 0.8839 - val_loss: 1.6154 - val_mean_squared_error: 1.6154\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.8383 - mean_squared_error: 0.8383 - val_loss: 1.6469 - val_mean_squared_error: 1.6469\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8420 - mean_squared_error: 0.8420 - val_loss: 1.7093 - val_mean_squared_error: 1.7093\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.8973 - mean_squared_error: 0.8973 - val_loss: 1.5820 - val_mean_squared_error: 1.5820\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8281 - mean_squared_error: 0.8281 - val_loss: 1.5854 - val_mean_squared_error: 1.5854\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8359 - mean_squared_error: 0.8359 - val_loss: 1.7653 - val_mean_squared_error: 1.7653\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8548 - mean_squared_error: 0.8548 - val_loss: 1.6032 - val_mean_squared_error: 1.6032\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8243 - mean_squared_error: 0.8243 - val_loss: 1.8416 - val_mean_squared_error: 1.8416\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8880 - mean_squared_error: 0.8880 - val_loss: 1.6708 - val_mean_squared_error: 1.6708\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8477 - mean_squared_error: 0.8477 - val_loss: 1.5756 - val_mean_squared_error: 1.5756\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8354 - mean_squared_error: 0.8354 - val_loss: 1.6074 - val_mean_squared_error: 1.6074\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.8282 - mean_squared_error: 0.8282 - val_loss: 1.7118 - val_mean_squared_error: 1.7118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yfMbGZhXTrL",
        "colab_type": "text"
      },
      "source": [
        "## VGG Net Inspired Models\n",
        "\n",
        "As discussed in this [Analytics Vidhya blogpost](https://www.analyticsvidhya.com/blog/2017/08/10-advanced-deep-learning-architectures-data-scientists/), the VGGNet was developed by researchers of the Visual Graphics Group at Oxford University. This model is characteristically different from the AlexNet inspired models the team had been using due to its stacked convolution layers prior to pooling. This gives the VGG Net models additional depth (and additional parameters to train). As the VGG Nets themselves start with different input sizes than we are working with for this task, the team simply used the VGG Net architecture as inspiration for their own deeper network. The network model iteself is coded below. \n",
        "\n",
        "As can be seen through inspection of the code cell the strategy employed looked like the following:\n",
        "\n",
        "* Convolute Input with 3x3 kernel and Same Padding and User Specified Filter Depth (sf) ) - Output Shape (96x96xsf)\n",
        "* Convolute with 3x3 kernel and Same Padding and User Specified Filter Depth (sf) ) - Output Shape (96x96xsf)\n",
        "* Max Pool with 2x2 Kernel  - Output Shape (48x48xsf)\n",
        "* Convolute with 3x3 kernel and Same Padding and double filter depth - Output Shape (48x48x(2x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (48x48x(2x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (48x48x(2x sf))\n",
        "* Max Pool with 2x2 Kernel  - Output Shape (24 x 24 x (2xsf) )\n",
        "* Convolute with 3x3 kernel and Same Padding and double filter depth - Output Shape (24x24x(4x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (24x24x(4x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (24x24x(4x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (24x24x(4x sf))\n",
        "* Max Pool with 2x2 Kernel  - Output Shape (12 x 12 x (4xsf) )\n",
        "* Convolute with 3x3 kernel and Same Padding and double filter depth - Output Shape (12x12x(8x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (12x12x(8x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (12x12x(8x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (12x12x(8x sf))\n",
        "* Max Pool with 2x2 Kernel  - Output Shape (6 x 6 x (8xsf) )\n",
        "* Dense Layer with 500 Hidden Units\n",
        "* Dense Layer with 500 Hidden Units\n",
        "* Output Layer with 30 outputs\n",
        "\n",
        "It was obviously expected that the additional layers would add to model runtime, but the big question was how would it perform? This question is answered by the plot below. The VGG net in the end performed similarly to the AlexNet version, in terms of overall validation set RMSE. However, as one can tell from the plots below, the RMSE and the epoch training time was extremely variable. It was unclear what this performance issue was due to. Even without the spikes in epoch run time the median epoch training time for the VGG Net is approximately twice that of the AlexNet architecture. A few variations on the VGG Net architecture were pursued, but none should significantly better performance than the AlexNet model. For this reason, additional VGG Net studies were foregone.\n",
        "\n",
        "![alt text](https://github.com/tomgoter/w207_finalproject/raw/master/Images/vgg_performance_V2.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PVltysLKpme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_vgg_model(start_filter, d, step, bias):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model \n",
        "    '''\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "    \n",
        "    # Input layer is our grayscale image that is 96 pixels by 96 pixels\n",
        "    cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    \n",
        "    # Add our first convolution layers which is two back-to-back conv with 3x3 kernel and same padding\n",
        "    # Add depth with filters\n",
        "    # Our output from these convolutions will be (96,96,start_filter)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (48,48,32)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add some random dropout\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    \n",
        "    # Add our second convolution layers which is three back-to-back conv with 3x3 kernel and same padding\n",
        "    # Double filter depth - output layer will be (48,48,start_filter*2)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "    \n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (24,24,start_filter*2)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add some random dropout\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    \n",
        "    # Add our third convolution layers which is three back-to-back conv with 3x3 kernel and same padding\n",
        "    # Double filter depth again - output layer will be (24,24,start_filter*4)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "    \n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (12,12,start_filter*4)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add some random dropout\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+2*step))\n",
        "    \n",
        "    # Add our fourth and final convolution layers which is three back-to-back conv with 3x3 kernel and same padding\n",
        "    # Double filter depth again - output layer will be (12,12,start_filter*8)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        \n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (6,6,start_filter*8)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Flatten and transition to fully connected layers\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(500))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(500))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(30))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdJY734byccZ",
        "colab_type": "code",
        "outputId": "d1f86772-9ea8-4f0f-dccb-e489d4d3476b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Redefine optimizer list to just focus on adam and sgd\n",
        "opt_list = {'adam':adam}\n",
        "\n",
        "# Use an early stopping callback and our timing callback\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                              patience=100, mode='auto')\n",
        "time_callback = TimeHistory()\n",
        "\n",
        "# Initialize a new data frame to hold our output data\n",
        "cnn_vgg_flipped_df = pd.DataFrame()\n",
        "\n",
        "# Start filter list\n",
        "start_filters = [16]\n",
        "\n",
        "# Flag for using or not using bias term\n",
        "biases = [False]\n",
        "\n",
        "# Create a list of initial dropout values and steps to increase\n",
        "dropouts = [(0.0,0.02), (0.0,0.05)]\n",
        "\n",
        "\n",
        "for lr_factor in [10]:\n",
        "  for opt_name, opt in opt_list.items():\n",
        "      for start_filter in start_filters:\n",
        "          for bias in biases:\n",
        "              for d in dropouts:\n",
        "                  adam = optimizers.Adam(lr=0.001*lr_factor, beta_1=0.9, beta_2=0.999)\n",
        "                  model = create_vgg_model(start_filter, d[0], d[1], bias)\n",
        "                  model.compile(\n",
        "                        optimizer=opt,\n",
        "                        loss='mean_squared_error',\n",
        "                        metrics=['mean_squared_error'])\n",
        "                  history = model.fit(\n",
        "                      flipped_X.astype(np.float32), flipped_y.astype(np.float32),\n",
        "                      epochs=200,\n",
        "                      validation_split=0.15, callbacks=[time_callback, early_stop])\n",
        "                  times = time_callback.times\n",
        "\n",
        "                  # Convert to dataframe\n",
        "                  hist = pd.DataFrame(history.history)\n",
        "                  hist['epoch'] = history.epoch\n",
        "                  hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "                  hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "                  hist['times'] = times\n",
        "                  hist['starting_filter'] = start_filter\n",
        "                  hist['layers'] = 4\n",
        "                  hist['pooling'] = 'yes'\n",
        "                  hist['fc_layer'] = 500\n",
        "                  hist['activation'] = 'relu'\n",
        "                  hist['optimizer'] = opt_name\n",
        "                  hist['lrate'] = opt.get_config()['learning_rate']\n",
        "                  hist['dropout_initial'] = d[0]\n",
        "                  hist['dropout_step'] = d[1]\n",
        "                  hist['batch_norm'] = 1\n",
        "                  hist['bias'] = int(bias)\n",
        "                  hist['arch'] = 'vgg'\n",
        "\n",
        "                  # Keep concatenating to dataframe\n",
        "                  cnn_vgg_flipped_df = pd.concat([cnn_vgg_flipped_df,hist])\n",
        "\n",
        "                  # Re-pickle after every model to retain progress\n",
        "                  cnn_vgg_flipped_df.to_pickle(drive_path + \"OutputData/cnn_vgg_flipped4_df.pkl\")\n",
        "\n",
        "                  # Save models.\n",
        "                  filename = \"cnn_vgg_flipped4_model_{}_d{}_s{}_sf{}\".format(opt_name, d[0], d[1], start_filter)\n",
        "                  model.save(drive_path + \"Models/\"+filename+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0725 22:08:00.813345 140301373831040 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 96, 96, 16)        144       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 96, 96, 16)        2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 96, 96, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 48, 48, 32)        4608      \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 24, 24, 64)        18432     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 12, 12, 128)       73728     \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               2304500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,245,598\n",
            "Trainable params: 3,243,118\n",
            "Non-trainable params: 2,480\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 7s 2ms/sample - loss: 1503.0691 - mean_squared_error: 1503.0686 - val_loss: 902.9318 - val_mean_squared_error: 902.9318\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 3s 785us/sample - loss: 231.0996 - mean_squared_error: 231.0995 - val_loss: 77.6797 - val_mean_squared_error: 77.6797\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 3s 791us/sample - loss: 32.0186 - mean_squared_error: 32.0186 - val_loss: 24.6080 - val_mean_squared_error: 24.6080\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 3s 794us/sample - loss: 20.9619 - mean_squared_error: 20.9619 - val_loss: 28.6376 - val_mean_squared_error: 28.6376\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 3s 795us/sample - loss: 15.5489 - mean_squared_error: 15.5489 - val_loss: 14.0135 - val_mean_squared_error: 14.0135\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 3s 792us/sample - loss: 15.2131 - mean_squared_error: 15.2131 - val_loss: 14.5299 - val_mean_squared_error: 14.5299\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 3s 796us/sample - loss: 13.0577 - mean_squared_error: 13.0577 - val_loss: 12.9371 - val_mean_squared_error: 12.9371\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 3s 796us/sample - loss: 12.2642 - mean_squared_error: 12.2642 - val_loss: 12.6177 - val_mean_squared_error: 12.6177\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 3s 800us/sample - loss: 11.5952 - mean_squared_error: 11.5952 - val_loss: 10.3358 - val_mean_squared_error: 10.3358\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 3s 801us/sample - loss: 11.3017 - mean_squared_error: 11.3017 - val_loss: 12.2963 - val_mean_squared_error: 12.2963\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 3s 803us/sample - loss: 10.9783 - mean_squared_error: 10.9783 - val_loss: 10.7083 - val_mean_squared_error: 10.7083\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 10.7583 - mean_squared_error: 10.7583 - val_loss: 10.8048 - val_mean_squared_error: 10.8048\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 10.6175 - mean_squared_error: 10.6175 - val_loss: 11.4092 - val_mean_squared_error: 11.4092\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 10.5943 - mean_squared_error: 10.5943 - val_loss: 10.4830 - val_mean_squared_error: 10.4830\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 10.5227 - mean_squared_error: 10.5227 - val_loss: 13.9042 - val_mean_squared_error: 13.9042\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 10.2316 - mean_squared_error: 10.2316 - val_loss: 9.6822 - val_mean_squared_error: 9.6822\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 10.2786 - mean_squared_error: 10.2786 - val_loss: 10.4266 - val_mean_squared_error: 10.4266\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 10.3456 - mean_squared_error: 10.3456 - val_loss: 9.7416 - val_mean_squared_error: 9.7416\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 10.1203 - mean_squared_error: 10.1202 - val_loss: 11.4136 - val_mean_squared_error: 11.4136\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 9.8893 - mean_squared_error: 9.8893 - val_loss: 9.9292 - val_mean_squared_error: 9.9292\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 9.6789 - mean_squared_error: 9.6789 - val_loss: 9.6395 - val_mean_squared_error: 9.6395\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 3s 822us/sample - loss: 9.5863 - mean_squared_error: 9.5863 - val_loss: 14.9666 - val_mean_squared_error: 14.9666\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 3s 822us/sample - loss: 9.6267 - mean_squared_error: 9.6267 - val_loss: 9.9612 - val_mean_squared_error: 9.9612\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 9.3602 - mean_squared_error: 9.3602 - val_loss: 10.5398 - val_mean_squared_error: 10.5398\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 8.9657 - mean_squared_error: 8.9657 - val_loss: 12.1236 - val_mean_squared_error: 12.1236\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 8.3030 - mean_squared_error: 8.3030 - val_loss: 11.4900 - val_mean_squared_error: 11.4900\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 8.0048 - mean_squared_error: 8.0048 - val_loss: 7.7885 - val_mean_squared_error: 7.7885\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 7.6340 - mean_squared_error: 7.6340 - val_loss: 10.2349 - val_mean_squared_error: 10.2349\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 7.4121 - mean_squared_error: 7.4121 - val_loss: 8.5402 - val_mean_squared_error: 8.5402\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 6.7272 - mean_squared_error: 6.7272 - val_loss: 6.4859 - val_mean_squared_error: 6.4859\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 6.2125 - mean_squared_error: 6.2125 - val_loss: 9.7414 - val_mean_squared_error: 9.7414\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 6.2266 - mean_squared_error: 6.2266 - val_loss: 12.3595 - val_mean_squared_error: 12.3595\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 5.8923 - mean_squared_error: 5.8923 - val_loss: 5.5178 - val_mean_squared_error: 5.5178\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 5.2831 - mean_squared_error: 5.2831 - val_loss: 5.3895 - val_mean_squared_error: 5.3895\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 5.2139 - mean_squared_error: 5.2139 - val_loss: 5.0656 - val_mean_squared_error: 5.0656\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 4.9461 - mean_squared_error: 4.9461 - val_loss: 5.7699 - val_mean_squared_error: 5.7699\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 3s 802us/sample - loss: 4.3066 - mean_squared_error: 4.3066 - val_loss: 4.4356 - val_mean_squared_error: 4.4356\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 3s 802us/sample - loss: 4.1310 - mean_squared_error: 4.1310 - val_loss: 3.9020 - val_mean_squared_error: 3.9020\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 4.0614 - mean_squared_error: 4.0614 - val_loss: 4.0883 - val_mean_squared_error: 4.0883\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 4.1426 - mean_squared_error: 4.1426 - val_loss: 5.2771 - val_mean_squared_error: 5.2771\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 3.8258 - mean_squared_error: 3.8258 - val_loss: 3.9562 - val_mean_squared_error: 3.9562\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 3.7033 - mean_squared_error: 3.7033 - val_loss: 4.7001 - val_mean_squared_error: 4.7001\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 4.1275 - mean_squared_error: 4.1275 - val_loss: 4.0003 - val_mean_squared_error: 4.0003\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 3s 805us/sample - loss: 3.3979 - mean_squared_error: 3.3979 - val_loss: 3.2287 - val_mean_squared_error: 3.2287\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 3.2528 - mean_squared_error: 3.2528 - val_loss: 3.9348 - val_mean_squared_error: 3.9348\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 3.3443 - mean_squared_error: 3.3443 - val_loss: 3.3301 - val_mean_squared_error: 3.3301\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 3.2545 - mean_squared_error: 3.2545 - val_loss: 3.5569 - val_mean_squared_error: 3.5569\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 3.0308 - mean_squared_error: 3.0308 - val_loss: 3.0240 - val_mean_squared_error: 3.0240\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 2.8805 - mean_squared_error: 2.8805 - val_loss: 3.1963 - val_mean_squared_error: 3.1963\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 2.7823 - mean_squared_error: 2.7823 - val_loss: 3.0328 - val_mean_squared_error: 3.0328\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 3s 827us/sample - loss: 2.9146 - mean_squared_error: 2.9146 - val_loss: 3.4589 - val_mean_squared_error: 3.4589\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 2.7606 - mean_squared_error: 2.7606 - val_loss: 3.3263 - val_mean_squared_error: 3.3263\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.6750 - mean_squared_error: 2.6750 - val_loss: 2.7002 - val_mean_squared_error: 2.7002\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 2.6626 - mean_squared_error: 2.6626 - val_loss: 2.4706 - val_mean_squared_error: 2.4706\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 2.6441 - mean_squared_error: 2.6441 - val_loss: 4.1419 - val_mean_squared_error: 4.1419\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 2.7798 - mean_squared_error: 2.7798 - val_loss: 2.8001 - val_mean_squared_error: 2.8001\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.5343 - mean_squared_error: 2.5343 - val_loss: 2.7543 - val_mean_squared_error: 2.7543\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 2.5694 - mean_squared_error: 2.5694 - val_loss: 2.6659 - val_mean_squared_error: 2.6659\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.5152 - mean_squared_error: 2.5152 - val_loss: 2.7019 - val_mean_squared_error: 2.7019\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 2.4735 - mean_squared_error: 2.4735 - val_loss: 2.7386 - val_mean_squared_error: 2.7386\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.3519 - mean_squared_error: 2.3519 - val_loss: 2.4482 - val_mean_squared_error: 2.4482\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 2.4132 - mean_squared_error: 2.4132 - val_loss: 2.7022 - val_mean_squared_error: 2.7022\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 2.2927 - mean_squared_error: 2.2927 - val_loss: 2.4846 - val_mean_squared_error: 2.4846\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 2.4958 - mean_squared_error: 2.4958 - val_loss: 3.6533 - val_mean_squared_error: 3.6533\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 2.4997 - mean_squared_error: 2.4997 - val_loss: 2.4060 - val_mean_squared_error: 2.4060\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 2.3777 - mean_squared_error: 2.3777 - val_loss: 4.7044 - val_mean_squared_error: 4.7044\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.3814 - mean_squared_error: 2.3814 - val_loss: 2.6793 - val_mean_squared_error: 2.6793\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.2522 - mean_squared_error: 2.2522 - val_loss: 2.9455 - val_mean_squared_error: 2.9455\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 2.1121 - mean_squared_error: 2.1121 - val_loss: 2.6455 - val_mean_squared_error: 2.6455\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.1318 - mean_squared_error: 2.1318 - val_loss: 2.2845 - val_mean_squared_error: 2.2845\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 2.1261 - mean_squared_error: 2.1261 - val_loss: 2.4760 - val_mean_squared_error: 2.4760\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 2.1347 - mean_squared_error: 2.1347 - val_loss: 2.5176 - val_mean_squared_error: 2.5176\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.9396 - mean_squared_error: 1.9396 - val_loss: 2.2484 - val_mean_squared_error: 2.2484\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.9848 - mean_squared_error: 1.9848 - val_loss: 2.0955 - val_mean_squared_error: 2.0955\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.8994 - mean_squared_error: 1.8994 - val_loss: 2.2606 - val_mean_squared_error: 2.2606\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.9766 - mean_squared_error: 1.9766 - val_loss: 2.6509 - val_mean_squared_error: 2.6509\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.8706 - mean_squared_error: 1.8706 - val_loss: 2.3129 - val_mean_squared_error: 2.3129\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.9293 - mean_squared_error: 1.9293 - val_loss: 2.5570 - val_mean_squared_error: 2.5570\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 2.0685 - mean_squared_error: 2.0685 - val_loss: 2.5901 - val_mean_squared_error: 2.5901\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.8957 - mean_squared_error: 1.8957 - val_loss: 2.1503 - val_mean_squared_error: 2.1503\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 1.8593 - mean_squared_error: 1.8593 - val_loss: 2.1262 - val_mean_squared_error: 2.1262\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.8238 - mean_squared_error: 1.8238 - val_loss: 2.2783 - val_mean_squared_error: 2.2783\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.8163 - mean_squared_error: 1.8163 - val_loss: 2.3301 - val_mean_squared_error: 2.3301\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.8169 - mean_squared_error: 1.8169 - val_loss: 2.0574 - val_mean_squared_error: 2.0574\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.8313 - mean_squared_error: 1.8313 - val_loss: 2.3836 - val_mean_squared_error: 2.3836\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.7657 - mean_squared_error: 1.7657 - val_loss: 2.4654 - val_mean_squared_error: 2.4654\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.7929 - mean_squared_error: 1.7929 - val_loss: 2.0292 - val_mean_squared_error: 2.0292\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.7475 - mean_squared_error: 1.7475 - val_loss: 2.3269 - val_mean_squared_error: 2.3269\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.7887 - mean_squared_error: 1.7887 - val_loss: 2.2048 - val_mean_squared_error: 2.2048\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.7190 - mean_squared_error: 1.7190 - val_loss: 1.8778 - val_mean_squared_error: 1.8778\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.7100 - mean_squared_error: 1.7100 - val_loss: 2.2725 - val_mean_squared_error: 2.2725\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 2.5097 - mean_squared_error: 2.5097 - val_loss: 2.7079 - val_mean_squared_error: 2.7079\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.7623 - mean_squared_error: 1.7623 - val_loss: 2.1005 - val_mean_squared_error: 2.1005\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.6673 - mean_squared_error: 1.6673 - val_loss: 1.9159 - val_mean_squared_error: 1.9159\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 1.6941 - mean_squared_error: 1.6941 - val_loss: 2.0220 - val_mean_squared_error: 2.0220\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.6115 - mean_squared_error: 1.6115 - val_loss: 1.8630 - val_mean_squared_error: 1.8630\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.5463 - mean_squared_error: 1.5463 - val_loss: 1.9573 - val_mean_squared_error: 1.9573\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.5950 - mean_squared_error: 1.5950 - val_loss: 1.8909 - val_mean_squared_error: 1.8909\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.5851 - mean_squared_error: 1.5851 - val_loss: 2.1106 - val_mean_squared_error: 2.1106\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.5458 - mean_squared_error: 1.5458 - val_loss: 1.8748 - val_mean_squared_error: 1.8748\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.6409 - mean_squared_error: 1.6409 - val_loss: 2.2934 - val_mean_squared_error: 2.2934\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.5652 - mean_squared_error: 1.5652 - val_loss: 1.9642 - val_mean_squared_error: 1.9642\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.6893 - mean_squared_error: 1.6893 - val_loss: 2.2211 - val_mean_squared_error: 2.2211\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.5597 - mean_squared_error: 1.5597 - val_loss: 1.8295 - val_mean_squared_error: 1.8295\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.4955 - mean_squared_error: 1.4955 - val_loss: 1.9565 - val_mean_squared_error: 1.9565\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.5962 - mean_squared_error: 1.5962 - val_loss: 2.1047 - val_mean_squared_error: 2.1047\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.5077 - mean_squared_error: 1.5077 - val_loss: 1.9444 - val_mean_squared_error: 1.9444\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.4053 - mean_squared_error: 1.4053 - val_loss: 1.9084 - val_mean_squared_error: 1.9084\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.5548 - mean_squared_error: 1.5548 - val_loss: 2.1765 - val_mean_squared_error: 2.1765\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.5425 - mean_squared_error: 1.5425 - val_loss: 2.0048 - val_mean_squared_error: 2.0048\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.4078 - mean_squared_error: 1.4078 - val_loss: 1.9682 - val_mean_squared_error: 1.9682\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.4739 - mean_squared_error: 1.4739 - val_loss: 1.8651 - val_mean_squared_error: 1.8651\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.3717 - mean_squared_error: 1.3717 - val_loss: 1.9898 - val_mean_squared_error: 1.9898\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.3974 - mean_squared_error: 1.3974 - val_loss: 1.9178 - val_mean_squared_error: 1.9178\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.4548 - mean_squared_error: 1.4548 - val_loss: 2.1050 - val_mean_squared_error: 2.1050\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.4541 - mean_squared_error: 1.4541 - val_loss: 2.1339 - val_mean_squared_error: 2.1339\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.3923 - mean_squared_error: 1.3923 - val_loss: 2.0165 - val_mean_squared_error: 2.0165\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.4403 - mean_squared_error: 1.4403 - val_loss: 1.8839 - val_mean_squared_error: 1.8839\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.2914 - mean_squared_error: 1.2914 - val_loss: 1.8169 - val_mean_squared_error: 1.8169\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.3645 - mean_squared_error: 1.3645 - val_loss: 2.0411 - val_mean_squared_error: 2.0411\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.3972 - mean_squared_error: 1.3972 - val_loss: 1.8647 - val_mean_squared_error: 1.8647\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.4032 - mean_squared_error: 1.4032 - val_loss: 2.1487 - val_mean_squared_error: 2.1487\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.3643 - mean_squared_error: 1.3643 - val_loss: 1.9899 - val_mean_squared_error: 1.9899\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.3461 - mean_squared_error: 1.3461 - val_loss: 1.8254 - val_mean_squared_error: 1.8254\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.3433 - mean_squared_error: 1.3433 - val_loss: 1.8368 - val_mean_squared_error: 1.8368\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.2782 - mean_squared_error: 1.2782 - val_loss: 1.9814 - val_mean_squared_error: 1.9814\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.2820 - mean_squared_error: 1.2820 - val_loss: 1.8315 - val_mean_squared_error: 1.8315\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.2815 - mean_squared_error: 1.2815 - val_loss: 1.8975 - val_mean_squared_error: 1.8975\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.2823 - mean_squared_error: 1.2823 - val_loss: 1.9023 - val_mean_squared_error: 1.9023\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.3234 - mean_squared_error: 1.3234 - val_loss: 1.9524 - val_mean_squared_error: 1.9524\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.3512 - mean_squared_error: 1.3512 - val_loss: 2.1729 - val_mean_squared_error: 2.1729\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.3333 - mean_squared_error: 1.3333 - val_loss: 1.8951 - val_mean_squared_error: 1.8951\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.3042 - mean_squared_error: 1.3042 - val_loss: 2.4911 - val_mean_squared_error: 2.4911\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.3043 - mean_squared_error: 1.3043 - val_loss: 1.9161 - val_mean_squared_error: 1.9161\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.2298 - mean_squared_error: 1.2298 - val_loss: 1.9100 - val_mean_squared_error: 1.9100\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 1.3306 - mean_squared_error: 1.3306 - val_loss: 1.8345 - val_mean_squared_error: 1.8345\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.3490 - mean_squared_error: 1.3490 - val_loss: 1.9363 - val_mean_squared_error: 1.9363\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 1.2960 - mean_squared_error: 1.2960 - val_loss: 1.7568 - val_mean_squared_error: 1.7568\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.2330 - mean_squared_error: 1.2330 - val_loss: 1.7237 - val_mean_squared_error: 1.7237\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.2602 - mean_squared_error: 1.2602 - val_loss: 1.7288 - val_mean_squared_error: 1.7288\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.2043 - mean_squared_error: 1.2043 - val_loss: 2.1854 - val_mean_squared_error: 2.1854\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.2420 - mean_squared_error: 1.2420 - val_loss: 1.8229 - val_mean_squared_error: 1.8229\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.2355 - mean_squared_error: 1.2355 - val_loss: 1.8637 - val_mean_squared_error: 1.8637\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.1862 - mean_squared_error: 1.1862 - val_loss: 1.6333 - val_mean_squared_error: 1.6333\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.2561 - mean_squared_error: 1.2561 - val_loss: 1.8678 - val_mean_squared_error: 1.8678\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 1.1514 - mean_squared_error: 1.1514 - val_loss: 1.7591 - val_mean_squared_error: 1.7591\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.1946 - mean_squared_error: 1.1946 - val_loss: 1.7072 - val_mean_squared_error: 1.7072\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.1521 - mean_squared_error: 1.1521 - val_loss: 1.9323 - val_mean_squared_error: 1.9323\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.2263 - mean_squared_error: 1.2263 - val_loss: 1.9031 - val_mean_squared_error: 1.9031\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.2521 - mean_squared_error: 1.2521 - val_loss: 2.3134 - val_mean_squared_error: 2.3134\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.1752 - mean_squared_error: 1.1752 - val_loss: 1.7707 - val_mean_squared_error: 1.7707\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.1611 - mean_squared_error: 1.1611 - val_loss: 1.8069 - val_mean_squared_error: 1.8069\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.1111 - mean_squared_error: 1.1111 - val_loss: 1.7490 - val_mean_squared_error: 1.7490\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.1238 - mean_squared_error: 1.1238 - val_loss: 2.1677 - val_mean_squared_error: 2.1677\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.0931 - mean_squared_error: 1.0931 - val_loss: 1.8050 - val_mean_squared_error: 1.8050\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 3s 823us/sample - loss: 1.1622 - mean_squared_error: 1.1622 - val_loss: 1.8085 - val_mean_squared_error: 1.8085\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 3s 824us/sample - loss: 1.1549 - mean_squared_error: 1.1549 - val_loss: 1.8930 - val_mean_squared_error: 1.8930\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 1.1057 - mean_squared_error: 1.1057 - val_loss: 1.6695 - val_mean_squared_error: 1.6695\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.1699 - mean_squared_error: 1.1699 - val_loss: 1.9364 - val_mean_squared_error: 1.9364\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.1191 - mean_squared_error: 1.1191 - val_loss: 1.9345 - val_mean_squared_error: 1.9345\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.0893 - mean_squared_error: 1.0893 - val_loss: 1.7383 - val_mean_squared_error: 1.7383\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.1360 - mean_squared_error: 1.1360 - val_loss: 1.8963 - val_mean_squared_error: 1.8963\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.1666 - mean_squared_error: 1.1666 - val_loss: 2.0031 - val_mean_squared_error: 2.0031\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.1376 - mean_squared_error: 1.1376 - val_loss: 1.6715 - val_mean_squared_error: 1.6715\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.1099 - mean_squared_error: 1.1099 - val_loss: 1.7668 - val_mean_squared_error: 1.7668\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.0868 - mean_squared_error: 1.0868 - val_loss: 1.8729 - val_mean_squared_error: 1.8729\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.0355 - mean_squared_error: 1.0355 - val_loss: 1.7582 - val_mean_squared_error: 1.7582\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.0847 - mean_squared_error: 1.0847 - val_loss: 1.6786 - val_mean_squared_error: 1.6786\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.0650 - mean_squared_error: 1.0650 - val_loss: 1.6590 - val_mean_squared_error: 1.6590\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.0740 - mean_squared_error: 1.0740 - val_loss: 1.7560 - val_mean_squared_error: 1.7560\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.0690 - mean_squared_error: 1.0690 - val_loss: 1.7621 - val_mean_squared_error: 1.7621\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.0904 - mean_squared_error: 1.0904 - val_loss: 1.6967 - val_mean_squared_error: 1.6967\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.1241 - mean_squared_error: 1.1241 - val_loss: 1.7269 - val_mean_squared_error: 1.7269\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.0216 - mean_squared_error: 1.0216 - val_loss: 1.6187 - val_mean_squared_error: 1.6187\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.0451 - mean_squared_error: 1.0451 - val_loss: 1.8779 - val_mean_squared_error: 1.8779\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.8412 - val_mean_squared_error: 1.8412\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.1191 - mean_squared_error: 1.1191 - val_loss: 1.7714 - val_mean_squared_error: 1.7714\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 1.1152 - mean_squared_error: 1.1152 - val_loss: 2.0291 - val_mean_squared_error: 2.0291\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.9098 - val_mean_squared_error: 1.9098\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.7931 - val_mean_squared_error: 1.7931\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.0292 - mean_squared_error: 1.0292 - val_loss: 1.7716 - val_mean_squared_error: 1.7716\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.0232 - mean_squared_error: 1.0232 - val_loss: 1.7122 - val_mean_squared_error: 1.7122\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.0354 - mean_squared_error: 1.0354 - val_loss: 1.8117 - val_mean_squared_error: 1.8117\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.0015 - mean_squared_error: 1.0015 - val_loss: 2.1174 - val_mean_squared_error: 2.1174\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 0.9996 - mean_squared_error: 0.9996 - val_loss: 1.7951 - val_mean_squared_error: 1.7951\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 1.6512 - val_mean_squared_error: 1.6512\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.0227 - mean_squared_error: 1.0227 - val_loss: 1.6900 - val_mean_squared_error: 1.6900\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.0275 - mean_squared_error: 1.0275 - val_loss: 1.7239 - val_mean_squared_error: 1.7239\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 0.9872 - mean_squared_error: 0.9872 - val_loss: 1.6981 - val_mean_squared_error: 1.6981\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.8362 - val_mean_squared_error: 1.8362\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.0740 - mean_squared_error: 1.0740 - val_loss: 2.1267 - val_mean_squared_error: 2.1267\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 0.9954 - mean_squared_error: 0.9954 - val_loss: 1.8639 - val_mean_squared_error: 1.8639\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 0.9363 - mean_squared_error: 0.9363 - val_loss: 1.8845 - val_mean_squared_error: 1.8845\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 0.9662 - mean_squared_error: 0.9662 - val_loss: 1.7055 - val_mean_squared_error: 1.7055\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 0.9716 - mean_squared_error: 0.9716 - val_loss: 1.9042 - val_mean_squared_error: 1.9042\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 0.9432 - mean_squared_error: 0.9432 - val_loss: 1.8178 - val_mean_squared_error: 1.8178\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 0.9161 - mean_squared_error: 0.9161 - val_loss: 1.6802 - val_mean_squared_error: 1.6802\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.0386 - mean_squared_error: 1.0386 - val_loss: 1.7276 - val_mean_squared_error: 1.7276\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.0305 - mean_squared_error: 1.0305 - val_loss: 1.6825 - val_mean_squared_error: 1.6825\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 0.9307 - mean_squared_error: 0.9307 - val_loss: 1.8480 - val_mean_squared_error: 1.8480\n",
            "==================================================\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 96, 96, 16)        144       \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 96, 96, 16)        2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 96, 96, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 48, 48, 32)        4608      \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 24, 24, 64)        18432     \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 12, 12, 128)       73728     \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 500)               2304500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,245,598\n",
            "Trainable params: 3,243,118\n",
            "Non-trainable params: 2,480\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 4s 1ms/sample - loss: 359.6200 - mean_squared_error: 359.6201 - val_loss: 7973.7003 - val_mean_squared_error: 7973.7012\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 23.8004 - mean_squared_error: 23.8004 - val_loss: 25.0413 - val_mean_squared_error: 25.0413\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 20.6984 - mean_squared_error: 20.6984 - val_loss: 76.9191 - val_mean_squared_error: 76.9191\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 17.3981 - mean_squared_error: 17.3981 - val_loss: 156.9874 - val_mean_squared_error: 156.9875\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 17.6791 - mean_squared_error: 17.6791 - val_loss: 13.6750 - val_mean_squared_error: 13.6750\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 16.5321 - mean_squared_error: 16.5321 - val_loss: 11.9649 - val_mean_squared_error: 11.9649\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 15.7400 - mean_squared_error: 15.7400 - val_loss: 10.9494 - val_mean_squared_error: 10.9495\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 15.4877 - mean_squared_error: 15.4877 - val_loss: 10.1268 - val_mean_squared_error: 10.1268\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 15.8821 - mean_squared_error: 15.8821 - val_loss: 12.1346 - val_mean_squared_error: 12.1346\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 14.1005 - mean_squared_error: 14.1005 - val_loss: 10.1803 - val_mean_squared_error: 10.1803\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 13.2821 - mean_squared_error: 13.2821 - val_loss: 11.5947 - val_mean_squared_error: 11.5947\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 13.2061 - mean_squared_error: 13.2061 - val_loss: 13.5007 - val_mean_squared_error: 13.5007\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 13.0883 - mean_squared_error: 13.0883 - val_loss: 9.9821 - val_mean_squared_error: 9.9821\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 12.6410 - mean_squared_error: 12.6410 - val_loss: 9.9776 - val_mean_squared_error: 9.9776\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 12.3707 - mean_squared_error: 12.3707 - val_loss: 12.2840 - val_mean_squared_error: 12.2840\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 11.5549 - mean_squared_error: 11.5549 - val_loss: 9.8940 - val_mean_squared_error: 9.8940\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 11.6520 - mean_squared_error: 11.6520 - val_loss: 10.8655 - val_mean_squared_error: 10.8655\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 11.5297 - mean_squared_error: 11.5297 - val_loss: 10.3046 - val_mean_squared_error: 10.3046\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 11.6861 - mean_squared_error: 11.6861 - val_loss: 9.7664 - val_mean_squared_error: 9.7664\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 11.4645 - mean_squared_error: 11.4645 - val_loss: 10.3496 - val_mean_squared_error: 10.3496\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 11.2142 - mean_squared_error: 11.2142 - val_loss: 10.1159 - val_mean_squared_error: 10.1159\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 11.0739 - mean_squared_error: 11.0739 - val_loss: 9.7431 - val_mean_squared_error: 9.7431\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 10.7695 - mean_squared_error: 10.7695 - val_loss: 10.7072 - val_mean_squared_error: 10.7072\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 10.9254 - mean_squared_error: 10.9254 - val_loss: 10.2044 - val_mean_squared_error: 10.2044\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 10.6208 - mean_squared_error: 10.6208 - val_loss: 9.9746 - val_mean_squared_error: 9.9746\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 10.5558 - mean_squared_error: 10.5558 - val_loss: 9.7736 - val_mean_squared_error: 9.7736\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 10.5311 - mean_squared_error: 10.5311 - val_loss: 11.0120 - val_mean_squared_error: 11.0120\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 10.3229 - mean_squared_error: 10.3229 - val_loss: 9.2332 - val_mean_squared_error: 9.2332\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 10.0644 - mean_squared_error: 10.0644 - val_loss: 9.6001 - val_mean_squared_error: 9.6001\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 10.1466 - mean_squared_error: 10.1466 - val_loss: 9.2063 - val_mean_squared_error: 9.2063\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 10.1588 - mean_squared_error: 10.1588 - val_loss: 9.6684 - val_mean_squared_error: 9.6684\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 9.9150 - mean_squared_error: 9.9150 - val_loss: 9.3432 - val_mean_squared_error: 9.3432\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 9.8422 - mean_squared_error: 9.8422 - val_loss: 8.9940 - val_mean_squared_error: 8.9940\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 9.6811 - mean_squared_error: 9.6811 - val_loss: 9.3499 - val_mean_squared_error: 9.3499\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 9.5079 - mean_squared_error: 9.5079 - val_loss: 9.3007 - val_mean_squared_error: 9.3007\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 9.4958 - mean_squared_error: 9.4958 - val_loss: 8.9605 - val_mean_squared_error: 8.9605\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 9.1721 - mean_squared_error: 9.1721 - val_loss: 8.9975 - val_mean_squared_error: 8.9975\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 8.9831 - mean_squared_error: 8.9831 - val_loss: 8.9376 - val_mean_squared_error: 8.9376\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 8.8930 - mean_squared_error: 8.8930 - val_loss: 9.0930 - val_mean_squared_error: 9.0930\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 8.8104 - mean_squared_error: 8.8104 - val_loss: 8.4789 - val_mean_squared_error: 8.4789\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 8.5353 - mean_squared_error: 8.5353 - val_loss: 8.4068 - val_mean_squared_error: 8.4068\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 8.4202 - mean_squared_error: 8.4202 - val_loss: 8.4547 - val_mean_squared_error: 8.4547\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 8.1379 - mean_squared_error: 8.1379 - val_loss: 7.8324 - val_mean_squared_error: 7.8324\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 7.9939 - mean_squared_error: 7.9939 - val_loss: 8.1304 - val_mean_squared_error: 8.1304\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 7.9221 - mean_squared_error: 7.9221 - val_loss: 8.5291 - val_mean_squared_error: 8.5291\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 7.8250 - mean_squared_error: 7.8250 - val_loss: 8.1635 - val_mean_squared_error: 8.1635\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 7.6487 - mean_squared_error: 7.6487 - val_loss: 7.5735 - val_mean_squared_error: 7.5735\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 7.4298 - mean_squared_error: 7.4298 - val_loss: 7.6809 - val_mean_squared_error: 7.6809\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 7.3114 - mean_squared_error: 7.3114 - val_loss: 6.9536 - val_mean_squared_error: 6.9536\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 6.9902 - mean_squared_error: 6.9902 - val_loss: 8.0645 - val_mean_squared_error: 8.0645\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 6.7743 - mean_squared_error: 6.7743 - val_loss: 6.8398 - val_mean_squared_error: 6.8398\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 6.3468 - mean_squared_error: 6.3468 - val_loss: 6.0552 - val_mean_squared_error: 6.0552\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 6.0621 - mean_squared_error: 6.0621 - val_loss: 6.4335 - val_mean_squared_error: 6.4335\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 5.7854 - mean_squared_error: 5.7854 - val_loss: 5.4773 - val_mean_squared_error: 5.4773\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 3s 823us/sample - loss: 5.6389 - mean_squared_error: 5.6389 - val_loss: 6.3317 - val_mean_squared_error: 6.3317\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 5.4856 - mean_squared_error: 5.4856 - val_loss: 5.7900 - val_mean_squared_error: 5.7900\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 5.2958 - mean_squared_error: 5.2958 - val_loss: 6.4532 - val_mean_squared_error: 6.4532\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 5.0501 - mean_squared_error: 5.0501 - val_loss: 5.1896 - val_mean_squared_error: 5.1896\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 5.0475 - mean_squared_error: 5.0475 - val_loss: 4.8536 - val_mean_squared_error: 4.8536\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 4.5868 - mean_squared_error: 4.5868 - val_loss: 4.8903 - val_mean_squared_error: 4.8903\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 4.5303 - mean_squared_error: 4.5303 - val_loss: 4.5040 - val_mean_squared_error: 4.5040\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 4.4301 - mean_squared_error: 4.4301 - val_loss: 4.8573 - val_mean_squared_error: 4.8573\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 4.3250 - mean_squared_error: 4.3250 - val_loss: 4.5516 - val_mean_squared_error: 4.5516\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 4.2365 - mean_squared_error: 4.2365 - val_loss: 4.2204 - val_mean_squared_error: 4.2204\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 4.0213 - mean_squared_error: 4.0213 - val_loss: 4.3960 - val_mean_squared_error: 4.3960\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 3.9296 - mean_squared_error: 3.9296 - val_loss: 4.0308 - val_mean_squared_error: 4.0308\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 3.9039 - mean_squared_error: 3.9039 - val_loss: 4.5603 - val_mean_squared_error: 4.5603\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 3.8146 - mean_squared_error: 3.8146 - val_loss: 5.2740 - val_mean_squared_error: 5.2740\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 3.7382 - mean_squared_error: 3.7382 - val_loss: 4.1732 - val_mean_squared_error: 4.1732\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 3.4778 - mean_squared_error: 3.4778 - val_loss: 3.6776 - val_mean_squared_error: 3.6776\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 3.5835 - mean_squared_error: 3.5835 - val_loss: 3.7223 - val_mean_squared_error: 3.7223\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 3.4265 - mean_squared_error: 3.4265 - val_loss: 3.3943 - val_mean_squared_error: 3.3943\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 3.3526 - mean_squared_error: 3.3526 - val_loss: 3.5536 - val_mean_squared_error: 3.5536\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 3.2433 - mean_squared_error: 3.2433 - val_loss: 4.6982 - val_mean_squared_error: 4.6982\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 3.2148 - mean_squared_error: 3.2148 - val_loss: 3.4698 - val_mean_squared_error: 3.4698\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 3.1963 - mean_squared_error: 3.1963 - val_loss: 3.3397 - val_mean_squared_error: 3.3397\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 3.1514 - mean_squared_error: 3.1514 - val_loss: 3.4802 - val_mean_squared_error: 3.4802\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 3.0419 - mean_squared_error: 3.0419 - val_loss: 3.2063 - val_mean_squared_error: 3.2063\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.9725 - mean_squared_error: 2.9725 - val_loss: 3.1267 - val_mean_squared_error: 3.1267\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.9223 - mean_squared_error: 2.9223 - val_loss: 3.1691 - val_mean_squared_error: 3.1691\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 3s 823us/sample - loss: 2.8206 - mean_squared_error: 2.8206 - val_loss: 3.2628 - val_mean_squared_error: 3.2628\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 2.9340 - mean_squared_error: 2.9340 - val_loss: 3.5922 - val_mean_squared_error: 3.5922\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 2.7511 - mean_squared_error: 2.7511 - val_loss: 3.0034 - val_mean_squared_error: 3.0034\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 3.0216 - mean_squared_error: 3.0216 - val_loss: 3.5171 - val_mean_squared_error: 3.5171\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.6508 - mean_squared_error: 2.6508 - val_loss: 3.0134 - val_mean_squared_error: 3.0134\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.6981 - mean_squared_error: 2.6981 - val_loss: 3.0064 - val_mean_squared_error: 3.0064\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 2.6140 - mean_squared_error: 2.6140 - val_loss: 3.1889 - val_mean_squared_error: 3.1889\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.5478 - mean_squared_error: 2.5478 - val_loss: 2.9409 - val_mean_squared_error: 2.9409\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 2.5059 - mean_squared_error: 2.5059 - val_loss: 3.1482 - val_mean_squared_error: 3.1482\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.4985 - mean_squared_error: 2.4985 - val_loss: 3.2798 - val_mean_squared_error: 3.2798\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 3s 822us/sample - loss: 2.5086 - mean_squared_error: 2.5086 - val_loss: 3.1131 - val_mean_squared_error: 3.1131\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.4404 - mean_squared_error: 2.4404 - val_loss: 2.8794 - val_mean_squared_error: 2.8794\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 2.3644 - mean_squared_error: 2.3644 - val_loss: 2.8144 - val_mean_squared_error: 2.8144\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 2.4653 - mean_squared_error: 2.4653 - val_loss: 3.0579 - val_mean_squared_error: 3.0579\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 2.3987 - mean_squared_error: 2.3987 - val_loss: 2.7019 - val_mean_squared_error: 2.7019\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 2.3926 - mean_squared_error: 2.3926 - val_loss: 3.0377 - val_mean_squared_error: 3.0377\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.3482 - mean_squared_error: 2.3482 - val_loss: 2.8627 - val_mean_squared_error: 2.8627\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.2632 - mean_squared_error: 2.2632 - val_loss: 2.9966 - val_mean_squared_error: 2.9966\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.2774 - mean_squared_error: 2.2774 - val_loss: 2.7916 - val_mean_squared_error: 2.7916\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.2089 - mean_squared_error: 2.2089 - val_loss: 2.5826 - val_mean_squared_error: 2.5826\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.2447 - mean_squared_error: 2.2447 - val_loss: 2.5615 - val_mean_squared_error: 2.5615\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.2301 - mean_squared_error: 2.2301 - val_loss: 3.0712 - val_mean_squared_error: 3.0712\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.0920 - mean_squared_error: 2.0920 - val_loss: 2.6030 - val_mean_squared_error: 2.6030\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 2.1285 - mean_squared_error: 2.1285 - val_loss: 2.5648 - val_mean_squared_error: 2.5648\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 2.1420 - mean_squared_error: 2.1420 - val_loss: 2.5643 - val_mean_squared_error: 2.5643\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 2.0600 - mean_squared_error: 2.0600 - val_loss: 2.7024 - val_mean_squared_error: 2.7024\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.1119 - mean_squared_error: 2.1119 - val_loss: 2.6188 - val_mean_squared_error: 2.6188\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.0171 - mean_squared_error: 2.0171 - val_loss: 2.6572 - val_mean_squared_error: 2.6572\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.0441 - mean_squared_error: 2.0441 - val_loss: 2.8237 - val_mean_squared_error: 2.8237\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.9447 - mean_squared_error: 1.9447 - val_loss: 2.5299 - val_mean_squared_error: 2.5299\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.9826 - mean_squared_error: 1.9826 - val_loss: 2.6204 - val_mean_squared_error: 2.6204\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.9921 - mean_squared_error: 1.9921 - val_loss: 2.7583 - val_mean_squared_error: 2.7583\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.9381 - mean_squared_error: 1.9381 - val_loss: 2.5447 - val_mean_squared_error: 2.5447\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.9693 - mean_squared_error: 1.9693 - val_loss: 2.5477 - val_mean_squared_error: 2.5477\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.8951 - mean_squared_error: 1.8951 - val_loss: 2.4096 - val_mean_squared_error: 2.4096\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.8211 - mean_squared_error: 1.8211 - val_loss: 2.3764 - val_mean_squared_error: 2.3764\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.8307 - mean_squared_error: 1.8307 - val_loss: 2.4654 - val_mean_squared_error: 2.4654\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.8703 - mean_squared_error: 1.8703 - val_loss: 2.5337 - val_mean_squared_error: 2.5337\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.8900 - mean_squared_error: 1.8900 - val_loss: 2.6673 - val_mean_squared_error: 2.6673\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.8624 - mean_squared_error: 1.8624 - val_loss: 2.5291 - val_mean_squared_error: 2.5291\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.7986 - mean_squared_error: 1.7986 - val_loss: 2.4719 - val_mean_squared_error: 2.4719\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.7893 - mean_squared_error: 1.7893 - val_loss: 2.6164 - val_mean_squared_error: 2.6164\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.8238 - mean_squared_error: 1.8238 - val_loss: 2.4905 - val_mean_squared_error: 2.4905\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.8246 - mean_squared_error: 1.8246 - val_loss: 2.4323 - val_mean_squared_error: 2.4323\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.7191 - mean_squared_error: 1.7191 - val_loss: 2.5202 - val_mean_squared_error: 2.5202\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.7771 - mean_squared_error: 1.7771 - val_loss: 2.4548 - val_mean_squared_error: 2.4548\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.7094 - mean_squared_error: 1.7094 - val_loss: 2.6318 - val_mean_squared_error: 2.6318\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.7729 - mean_squared_error: 1.7729 - val_loss: 2.4078 - val_mean_squared_error: 2.4078\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.7313 - mean_squared_error: 1.7313 - val_loss: 2.4034 - val_mean_squared_error: 2.4034\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.6588 - mean_squared_error: 1.6588 - val_loss: 2.4682 - val_mean_squared_error: 2.4682\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.6772 - mean_squared_error: 1.6772 - val_loss: 2.5472 - val_mean_squared_error: 2.5472\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.7121 - mean_squared_error: 1.7121 - val_loss: 2.5819 - val_mean_squared_error: 2.5819\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.7150 - mean_squared_error: 1.7150 - val_loss: 2.5585 - val_mean_squared_error: 2.5585\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.6961 - mean_squared_error: 1.6961 - val_loss: 2.3468 - val_mean_squared_error: 2.3468\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.6137 - mean_squared_error: 1.6137 - val_loss: 2.3838 - val_mean_squared_error: 2.3838\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.6084 - mean_squared_error: 1.6084 - val_loss: 2.6342 - val_mean_squared_error: 2.6342\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.6499 - mean_squared_error: 1.6499 - val_loss: 2.2645 - val_mean_squared_error: 2.2645\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.6166 - mean_squared_error: 1.6166 - val_loss: 2.2921 - val_mean_squared_error: 2.2921\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.5730 - mean_squared_error: 1.5730 - val_loss: 2.2975 - val_mean_squared_error: 2.2975\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.5480 - mean_squared_error: 1.5480 - val_loss: 2.2755 - val_mean_squared_error: 2.2755\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.5715 - mean_squared_error: 1.5715 - val_loss: 2.3500 - val_mean_squared_error: 2.3500\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.6063 - mean_squared_error: 1.6063 - val_loss: 2.4623 - val_mean_squared_error: 2.4623\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.5324 - mean_squared_error: 1.5324 - val_loss: 2.5655 - val_mean_squared_error: 2.5655\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.5658 - mean_squared_error: 1.5658 - val_loss: 2.6717 - val_mean_squared_error: 2.6717\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 1.5833 - mean_squared_error: 1.5833 - val_loss: 2.5186 - val_mean_squared_error: 2.5186\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.5359 - mean_squared_error: 1.5359 - val_loss: 2.3668 - val_mean_squared_error: 2.3668\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.4705 - mean_squared_error: 1.4705 - val_loss: 2.3126 - val_mean_squared_error: 2.3126\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.5291 - mean_squared_error: 1.5291 - val_loss: 2.6165 - val_mean_squared_error: 2.6165\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.4419 - mean_squared_error: 1.4419 - val_loss: 2.2157 - val_mean_squared_error: 2.2157\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 1.5068 - mean_squared_error: 1.5068 - val_loss: 2.6012 - val_mean_squared_error: 2.6012\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.4741 - mean_squared_error: 1.4741 - val_loss: 2.2453 - val_mean_squared_error: 2.2453\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.4214 - mean_squared_error: 1.4214 - val_loss: 2.2380 - val_mean_squared_error: 2.2380\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.4495 - mean_squared_error: 1.4495 - val_loss: 2.3959 - val_mean_squared_error: 2.3959\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.5265 - mean_squared_error: 1.5265 - val_loss: 2.2333 - val_mean_squared_error: 2.2333\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.4759 - mean_squared_error: 1.4759 - val_loss: 2.2498 - val_mean_squared_error: 2.2498\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.4739 - mean_squared_error: 1.4739 - val_loss: 2.3853 - val_mean_squared_error: 2.3853\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.4498 - mean_squared_error: 1.4498 - val_loss: 2.2532 - val_mean_squared_error: 2.2532\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.4430 - mean_squared_error: 1.4430 - val_loss: 2.3275 - val_mean_squared_error: 2.3275\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.3723 - mean_squared_error: 1.3723 - val_loss: 2.2530 - val_mean_squared_error: 2.2530\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.4162 - mean_squared_error: 1.4162 - val_loss: 2.3067 - val_mean_squared_error: 2.3067\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 1.3707 - mean_squared_error: 1.3707 - val_loss: 2.2355 - val_mean_squared_error: 2.2355\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.4046 - mean_squared_error: 1.4046 - val_loss: 2.1292 - val_mean_squared_error: 2.1292\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 3s 824us/sample - loss: 1.4052 - mean_squared_error: 1.4052 - val_loss: 2.2936 - val_mean_squared_error: 2.2936\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 1.3420 - mean_squared_error: 1.3420 - val_loss: 2.2712 - val_mean_squared_error: 2.2712\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 3s 825us/sample - loss: 1.3499 - mean_squared_error: 1.3499 - val_loss: 2.3464 - val_mean_squared_error: 2.3464\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.3985 - mean_squared_error: 1.3985 - val_loss: 2.2478 - val_mean_squared_error: 2.2478\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.3315 - mean_squared_error: 1.3315 - val_loss: 2.4021 - val_mean_squared_error: 2.4021\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.3944 - mean_squared_error: 1.3944 - val_loss: 2.2359 - val_mean_squared_error: 2.2359\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.3462 - mean_squared_error: 1.3462 - val_loss: 2.3668 - val_mean_squared_error: 2.3668\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.4040 - mean_squared_error: 1.4040 - val_loss: 2.3425 - val_mean_squared_error: 2.3425\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.3355 - mean_squared_error: 1.3355 - val_loss: 2.2036 - val_mean_squared_error: 2.2036\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.2965 - mean_squared_error: 1.2965 - val_loss: 2.1066 - val_mean_squared_error: 2.1066\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.2929 - mean_squared_error: 1.2929 - val_loss: 2.2440 - val_mean_squared_error: 2.2440\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.3316 - mean_squared_error: 1.3316 - val_loss: 2.2216 - val_mean_squared_error: 2.2216\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.3195 - mean_squared_error: 1.3195 - val_loss: 2.3684 - val_mean_squared_error: 2.3684\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.2631 - mean_squared_error: 1.2631 - val_loss: 2.2069 - val_mean_squared_error: 2.2069\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.3020 - mean_squared_error: 1.3020 - val_loss: 2.1915 - val_mean_squared_error: 2.1915\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 1.3379 - mean_squared_error: 1.3379 - val_loss: 2.1487 - val_mean_squared_error: 2.1487\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.3134 - mean_squared_error: 1.3134 - val_loss: 2.2481 - val_mean_squared_error: 2.2481\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.2696 - mean_squared_error: 1.2696 - val_loss: 2.4526 - val_mean_squared_error: 2.4526\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.2778 - mean_squared_error: 1.2778 - val_loss: 2.2099 - val_mean_squared_error: 2.2099\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.2521 - mean_squared_error: 1.2521 - val_loss: 2.2603 - val_mean_squared_error: 2.2603\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.2632 - mean_squared_error: 1.2632 - val_loss: 2.3422 - val_mean_squared_error: 2.3422\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.2624 - mean_squared_error: 1.2624 - val_loss: 2.2580 - val_mean_squared_error: 2.2580\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.2754 - mean_squared_error: 1.2754 - val_loss: 2.1856 - val_mean_squared_error: 2.1856\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.2333 - mean_squared_error: 1.2333 - val_loss: 2.2056 - val_mean_squared_error: 2.2056\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.2111 - mean_squared_error: 1.2111 - val_loss: 2.2010 - val_mean_squared_error: 2.2010\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.2644 - mean_squared_error: 1.2644 - val_loss: 2.4299 - val_mean_squared_error: 2.4299\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.2570 - mean_squared_error: 1.2570 - val_loss: 2.1820 - val_mean_squared_error: 2.1820\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.2024 - mean_squared_error: 1.2024 - val_loss: 2.2777 - val_mean_squared_error: 2.2777\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.1996 - mean_squared_error: 1.1996 - val_loss: 2.3330 - val_mean_squared_error: 2.3330\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.2197 - mean_squared_error: 1.2197 - val_loss: 2.2204 - val_mean_squared_error: 2.2204\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.1592 - mean_squared_error: 1.1592 - val_loss: 2.1616 - val_mean_squared_error: 2.1616\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.1746 - mean_squared_error: 1.1746 - val_loss: 2.2421 - val_mean_squared_error: 2.2421\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.2054 - mean_squared_error: 1.2054 - val_loss: 2.2645 - val_mean_squared_error: 2.2645\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.1760 - mean_squared_error: 1.1760 - val_loss: 2.5849 - val_mean_squared_error: 2.5849\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.2002 - mean_squared_error: 1.2002 - val_loss: 2.2459 - val_mean_squared_error: 2.2459\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.1759 - mean_squared_error: 1.1759 - val_loss: 2.3514 - val_mean_squared_error: 2.3514\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.1586 - mean_squared_error: 1.1586 - val_loss: 2.2307 - val_mean_squared_error: 2.2307\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.1708 - mean_squared_error: 1.1708 - val_loss: 2.2136 - val_mean_squared_error: 2.2136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQlwrYKlvHzd",
        "colab_type": "text"
      },
      "source": [
        "## Specialization Time\n",
        "\n",
        "After getting lackluster test scores using our AlexNet and VGG Net models which made use of only the complete training examples (with data augmentation), the team determined there may be fruit in moving towards highly specialized models. Instead of having one CNN that is training to try to simultaneously identify 15 different keypoints, lets try to develop 15 models that each identify only one keypoint.\n",
        "\n",
        "The benefit of this approach is twofold.\n",
        "\n",
        "1.   Maximize use of available training data - there is no longer any need to throw any examples away. Each model will be trained on all available training examples for that specific keypoint. This is a huge benefit to keypoints such as nose_tip and eye centers as they have more than 7000 training examples; whereas, we were only previously making use of ~2100.\n",
        "2.   The entire model parameter training is focused on identifying a single keypoint. Thus the usefulness of each layer and each hidden unit is maximized.\n",
        "\n",
        "Obviously the downside to this approach is the 15x increase in model training time, and that doesn't include any\n",
        "\n",
        "Since we now need to train 15 models, we transfer our weights from our combined model to hopefully speed up the learning process. We will train for additional epochs but implement an early stopping criteria.\n",
        "\n",
        "As we are now making use of all of the data, we have updated our DataExploration file in the team's [GitHub repo](https://github.com/tomgoter/w207_finalproject) to generated, pickle and save the complete dataset. The code cell below is used to simply read that in from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfvwtwK2XzlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Load the complete dataframe with xflips from the pickle file\n",
        "df = pd.read_pickle(drive_path + \"df_nostache_w_flip.pkl\")\n",
        "\n",
        "# Don't use flipped data for sensitivities\n",
        "df_noflip = pd.read_pickle(drive_path + \"df_nostache.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KamAqR1D6RSm",
        "colab_type": "text"
      },
      "source": [
        "## Define Specialized CNN model\n",
        "\n",
        "We chose to start with the architecture from our best performing AlexNet inspired CNN. This model had three convolution layers and two fully connected layers leading to the output layer. It is important to note (probably obvious) that the output layer has been modified from 30 outputs (corresponding to the x and y-coordinates of all 15 keypoints) to instead only have two outputs (x,y for single keypoint). We also name the output layer, as this was determined to be an easy way to avoid conflict when bringing the weights from our combined model (assigned by name, so having a new name for this output layer means we won't be trying to assign weights from 30 units to 2 units)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsGPjPUp6Raq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_spec_bn_cnn_model(start_filter, d, step, fc1=500, fc2=500):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model \n",
        "    '''\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "#     cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='valid', \n",
        "                                         input_shape=(96, 96, 1),activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (2, 2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (2, 2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+2*step))\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc1))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc2))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(2, name='Specialist'))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq6_vi6l3juz",
        "colab_type": "text"
      },
      "source": [
        "## Train Keypoint Specific Models\n",
        "\n",
        "The function below looks a lot similar to earlier functions that were used to wrap around model building, training and output storing. There are a few notable differences however. \n",
        "  \n",
        "\n",
        "*   The first is that we have a 'name' parameter which is simply a link to the saved combined model from which we will be loading weights.\n",
        "*   Our early stopping callback is now set to save the best weights (we should have been doing this earlier). We don't expect any of these models to need 300 epochs to train. So this callback will likely be invoked for each model. Using the restore_best_weights option, we ensure our model that we are selecting (and eventually using for predicting the test keypoints) is using the most optimized model weights (as judged by our validation scores).\n",
        "*  We load the weights from our specified model, after first constructing the model using the same architecture\n",
        "*  The training data is subsetted to only contain y-values for a single specified keypoint which is an input parameter to our function (i.e., 'keypoint')\n",
        "* Similar to before, all models will be saved to Google Drive to allow us to make predictions on the test set for each keypoint and then combine and submit\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JngibKC4vbHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_specialists(df, keypoint, name='cnn_flipped6_adam_d0.0_s0.1_sf12_lrfactor10_flipped_100.h5'):\n",
        "  # Use an early stopping callback and our timing callback\n",
        "  early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                                patience=100, mode='auto', restore_best_weights = True)\n",
        "  time_callback = TimeHistory()\n",
        "  \n",
        "  \n",
        "  # Use the adam optimizer with the default learning rate\n",
        "  adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
        "  \n",
        "  \n",
        "  # Path to the model - we will just load the weights from it\n",
        "  model_path = drive_path + 'Models/' + name \n",
        "\n",
        "  \n",
        "  \n",
        "  # Compile the model\n",
        "  \n",
        "  model = create_spec_bn_cnn_model(12, 0.0, 0.10)\n",
        "      \n",
        "  # Load the weights from our no nan model as a starting point\n",
        "  model.load_weights(model_path, by_name=True)\n",
        "  \n",
        "  model.compile(\n",
        "      optimizer=adam,\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['mean_squared_error'])\n",
        "  \n",
        "  df_keypoint = df[[keypoint + '_x',keypoint+'_y', 'Image']]\n",
        "  print(df_keypoint.columns)\n",
        "  df_keypoint = df_keypoint.dropna().reset_index(drop=True)\n",
        "  \n",
        "  # Grab the last column - that is our image data for X matrix\n",
        "  train_X = df_keypoint.iloc[:, -1]\n",
        "\n",
        "  # Convert from a series of arrays to an NDarray\n",
        "  train_X = np.array([x.reshape(96,96,1) for x in train_X])\n",
        " \n",
        "  \n",
        "  # Get the Y data for just the specified keypoint\n",
        "  train_y = np.array(df_keypoint.iloc[:,:-1])\n",
        " \n",
        "  history = model.fit(\n",
        "      train_X.astype(np.float32), train_y.astype(np.float32),\n",
        "      epochs=300,\n",
        "      validation_split=0.20,\n",
        "      callbacks=[time_callback, early_stop])\n",
        "  times = time_callback.times\n",
        "\n",
        "  # Convert to dataframe\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "  hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "  hist['times'] = times\n",
        "  hist['starting_filter'] = 12\n",
        "  hist['layers'] = 3\n",
        "  hist['pooling'] = 'yes'\n",
        "  hist['fc_layer1'] = 500\n",
        "  hist['fc_layer2'] = 500\n",
        "  hist['activation'] = 'relu'\n",
        "  hist['optimizer'] = 'adam'\n",
        "  hist['lrate'] = adam.get_config()['learning_rate']\n",
        "  hist['dropout_initial'] = 0.0\n",
        "  hist['dropout_step'] = 0.1\n",
        "  hist['batch_norm'] = 1\n",
        "  hist['bias'] = 0\n",
        "  hist['stride'] = 1\n",
        "  hist['flipped'] = 1.0\n",
        "  hist['keypoint'] = keypoint\n",
        "\n",
        "  # Re-pickle after every model to retain progress\n",
        "  hist.to_pickle(drive_path+\"OutputData/spec_{}.pkl\".format(keypoint))\n",
        "\n",
        "  # Save models.\n",
        "  filename = \"cnn_spec_{}_d0_s10_sf12_lrfactor1_flipped_100\".format(keypoint)\n",
        "  model.save(drive_path+\"Models/\"+filename+\".h5\")\n",
        "  return hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXZrCYW0CdOV",
        "colab_type": "text"
      },
      "source": [
        "## Train Each Model\n",
        "\n",
        "Run the function below for each keypoint. The first go around no additional optimization of the models was pursued due to the runtime required to train the 15 different models with over 3 million parameters to train for each model. The training was done using the simple line below which was executed 15 times, once for each keypoint. Each execution of the train specialists function results in a dataframe of output and a saved model file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWW-rv1t2Jus",
        "colab_type": "code",
        "outputId": "e1610d71-e32c-49cb-8129-a132616bb50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Initialize a new data frame to hold our output data\n",
        "specialist_df = train_specialists(df, 'mouth_center_bottom_lip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0727 15:48:04.860363 140272171808640 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "Specialist (Dense)           (None, 2)                 1002      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 3,166,206\n",
            "Trainable params: 3,164,038\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Index(['mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y', 'Image'], dtype='object')\n",
            "Train on 11224 samples, validate on 2806 samples\n",
            "Epoch 1/300\n",
            "11224/11224 [==============================] - 10s 931us/sample - loss: 166.3862 - mean_squared_error: 166.3863 - val_loss: 32.1779 - val_mean_squared_error: 32.1779\n",
            "Epoch 2/300\n",
            "11224/11224 [==============================] - 7s 601us/sample - loss: 28.5072 - mean_squared_error: 28.5072 - val_loss: 23.1099 - val_mean_squared_error: 23.1099\n",
            "Epoch 3/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 21.1354 - mean_squared_error: 21.1354 - val_loss: 18.9494 - val_mean_squared_error: 18.9494\n",
            "Epoch 4/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 15.7803 - mean_squared_error: 15.7803 - val_loss: 14.1359 - val_mean_squared_error: 14.1359\n",
            "Epoch 5/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 12.9508 - mean_squared_error: 12.9508 - val_loss: 11.1376 - val_mean_squared_error: 11.1376\n",
            "Epoch 6/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 11.4619 - mean_squared_error: 11.4619 - val_loss: 10.9137 - val_mean_squared_error: 10.9137\n",
            "Epoch 7/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 10.7150 - mean_squared_error: 10.7150 - val_loss: 11.7024 - val_mean_squared_error: 11.7024\n",
            "Epoch 8/300\n",
            "11224/11224 [==============================] - 7s 604us/sample - loss: 9.4589 - mean_squared_error: 9.4589 - val_loss: 9.4017 - val_mean_squared_error: 9.4017\n",
            "Epoch 9/300\n",
            "11224/11224 [==============================] - 7s 605us/sample - loss: 8.9746 - mean_squared_error: 8.9746 - val_loss: 9.3576 - val_mean_squared_error: 9.3576\n",
            "Epoch 10/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 8.4599 - mean_squared_error: 8.4599 - val_loss: 10.0454 - val_mean_squared_error: 10.0454\n",
            "Epoch 11/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 7.6235 - mean_squared_error: 7.6235 - val_loss: 12.1137 - val_mean_squared_error: 12.1137\n",
            "Epoch 12/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 6.9940 - mean_squared_error: 6.9940 - val_loss: 8.3925 - val_mean_squared_error: 8.3925\n",
            "Epoch 13/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 6.5702 - mean_squared_error: 6.5702 - val_loss: 9.1276 - val_mean_squared_error: 9.1276\n",
            "Epoch 14/300\n",
            "11224/11224 [==============================] - 7s 609us/sample - loss: 6.0347 - mean_squared_error: 6.0347 - val_loss: 8.8838 - val_mean_squared_error: 8.8838\n",
            "Epoch 15/300\n",
            "11224/11224 [==============================] - 7s 609us/sample - loss: 5.7161 - mean_squared_error: 5.7161 - val_loss: 8.8581 - val_mean_squared_error: 8.8581\n",
            "Epoch 16/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 5.8700 - mean_squared_error: 5.8700 - val_loss: 10.1397 - val_mean_squared_error: 10.1397\n",
            "Epoch 17/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 5.5906 - mean_squared_error: 5.5906 - val_loss: 9.1881 - val_mean_squared_error: 9.1881\n",
            "Epoch 18/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 5.1712 - mean_squared_error: 5.1712 - val_loss: 11.2490 - val_mean_squared_error: 11.2490\n",
            "Epoch 19/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 4.7811 - mean_squared_error: 4.7811 - val_loss: 8.5590 - val_mean_squared_error: 8.5590\n",
            "Epoch 20/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 4.9751 - mean_squared_error: 4.9751 - val_loss: 10.0889 - val_mean_squared_error: 10.0889\n",
            "Epoch 21/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 4.5916 - mean_squared_error: 4.5916 - val_loss: 9.3839 - val_mean_squared_error: 9.3839\n",
            "Epoch 22/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 4.6254 - mean_squared_error: 4.6254 - val_loss: 9.1777 - val_mean_squared_error: 9.1777\n",
            "Epoch 23/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 4.1844 - mean_squared_error: 4.1844 - val_loss: 9.2528 - val_mean_squared_error: 9.2528\n",
            "Epoch 24/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 4.0265 - mean_squared_error: 4.0265 - val_loss: 11.5981 - val_mean_squared_error: 11.5981\n",
            "Epoch 25/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 4.2185 - mean_squared_error: 4.2185 - val_loss: 9.9581 - val_mean_squared_error: 9.9581\n",
            "Epoch 26/300\n",
            "11224/11224 [==============================] - 7s 601us/sample - loss: 3.8847 - mean_squared_error: 3.8847 - val_loss: 9.5121 - val_mean_squared_error: 9.5121\n",
            "Epoch 27/300\n",
            "11224/11224 [==============================] - 7s 603us/sample - loss: 3.9022 - mean_squared_error: 3.9022 - val_loss: 9.3064 - val_mean_squared_error: 9.3064\n",
            "Epoch 28/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 3.6455 - mean_squared_error: 3.6455 - val_loss: 10.5734 - val_mean_squared_error: 10.5734\n",
            "Epoch 29/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 3.6491 - mean_squared_error: 3.6491 - val_loss: 9.3377 - val_mean_squared_error: 9.3377\n",
            "Epoch 30/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 3.4385 - mean_squared_error: 3.4385 - val_loss: 11.6536 - val_mean_squared_error: 11.6536\n",
            "Epoch 31/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 3.4647 - mean_squared_error: 3.4647 - val_loss: 11.1657 - val_mean_squared_error: 11.1657\n",
            "Epoch 32/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 3.6443 - mean_squared_error: 3.6443 - val_loss: 9.7314 - val_mean_squared_error: 9.7314\n",
            "Epoch 33/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 3.1881 - mean_squared_error: 3.1881 - val_loss: 10.8785 - val_mean_squared_error: 10.8785\n",
            "Epoch 34/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 3.2441 - mean_squared_error: 3.2441 - val_loss: 8.4020 - val_mean_squared_error: 8.4020\n",
            "Epoch 35/300\n",
            "11224/11224 [==============================] - 7s 593us/sample - loss: 3.2652 - mean_squared_error: 3.2652 - val_loss: 11.2950 - val_mean_squared_error: 11.2950\n",
            "Epoch 36/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 3.2429 - mean_squared_error: 3.2429 - val_loss: 11.8995 - val_mean_squared_error: 11.8995\n",
            "Epoch 37/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 3.1539 - mean_squared_error: 3.1539 - val_loss: 13.1377 - val_mean_squared_error: 13.1377\n",
            "Epoch 38/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 3.1294 - mean_squared_error: 3.1294 - val_loss: 10.2872 - val_mean_squared_error: 10.2872\n",
            "Epoch 39/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 2.9429 - mean_squared_error: 2.9429 - val_loss: 11.9942 - val_mean_squared_error: 11.9942\n",
            "Epoch 40/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 3.0296 - mean_squared_error: 3.0296 - val_loss: 10.0049 - val_mean_squared_error: 10.0049\n",
            "Epoch 41/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 2.7941 - mean_squared_error: 2.7941 - val_loss: 10.6158 - val_mean_squared_error: 10.6158\n",
            "Epoch 42/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 2.6234 - mean_squared_error: 2.6234 - val_loss: 12.1483 - val_mean_squared_error: 12.1483\n",
            "Epoch 43/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 2.7361 - mean_squared_error: 2.7361 - val_loss: 10.7333 - val_mean_squared_error: 10.7333\n",
            "Epoch 44/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.6032 - mean_squared_error: 2.6032 - val_loss: 9.8596 - val_mean_squared_error: 9.8596\n",
            "Epoch 45/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 2.5395 - mean_squared_error: 2.5395 - val_loss: 10.7666 - val_mean_squared_error: 10.7666\n",
            "Epoch 46/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.6347 - mean_squared_error: 2.6347 - val_loss: 10.8272 - val_mean_squared_error: 10.8272\n",
            "Epoch 47/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 2.5417 - mean_squared_error: 2.5417 - val_loss: 9.0584 - val_mean_squared_error: 9.0584\n",
            "Epoch 48/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 2.5796 - mean_squared_error: 2.5796 - val_loss: 11.7893 - val_mean_squared_error: 11.7893\n",
            "Epoch 49/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.6230 - mean_squared_error: 2.6230 - val_loss: 9.6174 - val_mean_squared_error: 9.6174\n",
            "Epoch 50/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 2.3272 - mean_squared_error: 2.3272 - val_loss: 10.2643 - val_mean_squared_error: 10.2643\n",
            "Epoch 51/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 2.2871 - mean_squared_error: 2.2871 - val_loss: 10.9192 - val_mean_squared_error: 10.9192\n",
            "Epoch 52/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 2.3804 - mean_squared_error: 2.3804 - val_loss: 13.3764 - val_mean_squared_error: 13.3764\n",
            "Epoch 53/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.4318 - mean_squared_error: 2.4318 - val_loss: 10.0927 - val_mean_squared_error: 10.0927\n",
            "Epoch 54/300\n",
            "11224/11224 [==============================] - 7s 610us/sample - loss: 2.3770 - mean_squared_error: 2.3770 - val_loss: 12.9976 - val_mean_squared_error: 12.9976\n",
            "Epoch 55/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 2.4732 - mean_squared_error: 2.4732 - val_loss: 10.5091 - val_mean_squared_error: 10.5091\n",
            "Epoch 56/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.2497 - mean_squared_error: 2.2497 - val_loss: 10.5901 - val_mean_squared_error: 10.5901\n",
            "Epoch 57/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 2.1670 - mean_squared_error: 2.1670 - val_loss: 11.5544 - val_mean_squared_error: 11.5544\n",
            "Epoch 58/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 2.2429 - mean_squared_error: 2.2429 - val_loss: 9.8691 - val_mean_squared_error: 9.8691\n",
            "Epoch 59/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.3066 - mean_squared_error: 2.3066 - val_loss: 10.4940 - val_mean_squared_error: 10.4940\n",
            "Epoch 60/300\n",
            "11224/11224 [==============================] - 7s 605us/sample - loss: 2.1321 - mean_squared_error: 2.1321 - val_loss: 9.1842 - val_mean_squared_error: 9.1842\n",
            "Epoch 61/300\n",
            "11224/11224 [==============================] - 7s 616us/sample - loss: 2.0641 - mean_squared_error: 2.0641 - val_loss: 9.5009 - val_mean_squared_error: 9.5009\n",
            "Epoch 62/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.0558 - mean_squared_error: 2.0558 - val_loss: 10.6910 - val_mean_squared_error: 10.6910\n",
            "Epoch 63/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 2.0706 - mean_squared_error: 2.0706 - val_loss: 10.9768 - val_mean_squared_error: 10.9768\n",
            "Epoch 64/300\n",
            "11224/11224 [==============================] - 7s 593us/sample - loss: 2.2437 - mean_squared_error: 2.2437 - val_loss: 10.2110 - val_mean_squared_error: 10.2110\n",
            "Epoch 65/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 2.0452 - mean_squared_error: 2.0451 - val_loss: 10.2117 - val_mean_squared_error: 10.2117\n",
            "Epoch 66/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 2.0678 - mean_squared_error: 2.0678 - val_loss: 10.9321 - val_mean_squared_error: 10.9321\n",
            "Epoch 67/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 1.9506 - mean_squared_error: 1.9506 - val_loss: 10.2620 - val_mean_squared_error: 10.2620\n",
            "Epoch 68/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.1329 - mean_squared_error: 2.1329 - val_loss: 11.2880 - val_mean_squared_error: 11.2880\n",
            "Epoch 69/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 1.9885 - mean_squared_error: 1.9885 - val_loss: 10.1301 - val_mean_squared_error: 10.1301\n",
            "Epoch 70/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 1.9477 - mean_squared_error: 1.9477 - val_loss: 9.5525 - val_mean_squared_error: 9.5525\n",
            "Epoch 71/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 2.0414 - mean_squared_error: 2.0414 - val_loss: 11.8213 - val_mean_squared_error: 11.8213\n",
            "Epoch 72/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.9268 - mean_squared_error: 1.9268 - val_loss: 8.9484 - val_mean_squared_error: 8.9484\n",
            "Epoch 73/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 1.9989 - mean_squared_error: 1.9989 - val_loss: 9.8604 - val_mean_squared_error: 9.8604\n",
            "Epoch 74/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 1.9260 - mean_squared_error: 1.9260 - val_loss: 10.4302 - val_mean_squared_error: 10.4302\n",
            "Epoch 75/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 1.9129 - mean_squared_error: 1.9129 - val_loss: 10.9411 - val_mean_squared_error: 10.9411\n",
            "Epoch 76/300\n",
            "11224/11224 [==============================] - 7s 602us/sample - loss: 1.8448 - mean_squared_error: 1.8448 - val_loss: 9.9548 - val_mean_squared_error: 9.9548\n",
            "Epoch 77/300\n",
            "11224/11224 [==============================] - 7s 592us/sample - loss: 1.8558 - mean_squared_error: 1.8558 - val_loss: 9.2382 - val_mean_squared_error: 9.2382\n",
            "Epoch 78/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 1.7167 - mean_squared_error: 1.7167 - val_loss: 11.5651 - val_mean_squared_error: 11.5651\n",
            "Epoch 79/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 1.9431 - mean_squared_error: 1.9431 - val_loss: 10.5454 - val_mean_squared_error: 10.5454\n",
            "Epoch 80/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.8703 - mean_squared_error: 1.8703 - val_loss: 11.5071 - val_mean_squared_error: 11.5071\n",
            "Epoch 81/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.7858 - mean_squared_error: 1.7858 - val_loss: 11.0635 - val_mean_squared_error: 11.0635\n",
            "Epoch 82/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 1.8605 - mean_squared_error: 1.8605 - val_loss: 10.3657 - val_mean_squared_error: 10.3657\n",
            "Epoch 83/300\n",
            "11224/11224 [==============================] - 7s 591us/sample - loss: 1.8526 - mean_squared_error: 1.8526 - val_loss: 10.8657 - val_mean_squared_error: 10.8657\n",
            "Epoch 84/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.6802 - mean_squared_error: 1.6802 - val_loss: 10.8227 - val_mean_squared_error: 10.8227\n",
            "Epoch 85/300\n",
            "11224/11224 [==============================] - 7s 602us/sample - loss: 1.8512 - mean_squared_error: 1.8512 - val_loss: 12.1353 - val_mean_squared_error: 12.1353\n",
            "Epoch 86/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 1.8726 - mean_squared_error: 1.8726 - val_loss: 11.4388 - val_mean_squared_error: 11.4388\n",
            "Epoch 87/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.7423 - mean_squared_error: 1.7423 - val_loss: 9.9396 - val_mean_squared_error: 9.9396\n",
            "Epoch 88/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 1.7357 - mean_squared_error: 1.7357 - val_loss: 11.1935 - val_mean_squared_error: 11.1935\n",
            "Epoch 89/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.6608 - mean_squared_error: 1.6608 - val_loss: 11.4153 - val_mean_squared_error: 11.4153\n",
            "Epoch 90/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 1.8831 - mean_squared_error: 1.8831 - val_loss: 10.9537 - val_mean_squared_error: 10.9537\n",
            "Epoch 91/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 1.7491 - mean_squared_error: 1.7491 - val_loss: 9.6476 - val_mean_squared_error: 9.6476\n",
            "Epoch 92/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 1.5639 - mean_squared_error: 1.5639 - val_loss: 9.5865 - val_mean_squared_error: 9.5865\n",
            "Epoch 93/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 1.6905 - mean_squared_error: 1.6905 - val_loss: 11.6873 - val_mean_squared_error: 11.6873\n",
            "Epoch 94/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 1.7199 - mean_squared_error: 1.7199 - val_loss: 10.9966 - val_mean_squared_error: 10.9966\n",
            "Epoch 95/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 1.7339 - mean_squared_error: 1.7339 - val_loss: 10.5042 - val_mean_squared_error: 10.5042\n",
            "Epoch 96/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 1.7083 - mean_squared_error: 1.7083 - val_loss: 10.5614 - val_mean_squared_error: 10.5614\n",
            "Epoch 97/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 1.6525 - mean_squared_error: 1.6525 - val_loss: 11.9250 - val_mean_squared_error: 11.9250\n",
            "Epoch 98/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 1.6536 - mean_squared_error: 1.6536 - val_loss: 11.0450 - val_mean_squared_error: 11.0450\n",
            "Epoch 99/300\n",
            "11224/11224 [==============================] - 7s 606us/sample - loss: 1.6707 - mean_squared_error: 1.6707 - val_loss: 12.1873 - val_mean_squared_error: 12.1873\n",
            "Epoch 100/300\n",
            "11224/11224 [==============================] - 7s 604us/sample - loss: 1.6210 - mean_squared_error: 1.6210 - val_loss: 9.9100 - val_mean_squared_error: 9.9100\n",
            "Epoch 101/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.6722 - mean_squared_error: 1.6722 - val_loss: 9.7796 - val_mean_squared_error: 9.7796\n",
            "Epoch 102/300\n",
            "11224/11224 [==============================] - 7s 601us/sample - loss: 1.6132 - mean_squared_error: 1.6132 - val_loss: 10.4941 - val_mean_squared_error: 10.4941\n",
            "Epoch 103/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 1.5222 - mean_squared_error: 1.5222 - val_loss: 9.3724 - val_mean_squared_error: 9.3724\n",
            "Epoch 104/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.5836 - mean_squared_error: 1.5836 - val_loss: 11.1845 - val_mean_squared_error: 11.1845\n",
            "Epoch 105/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.6296 - mean_squared_error: 1.6296 - val_loss: 9.7118 - val_mean_squared_error: 9.7118\n",
            "Epoch 106/300\n",
            "11224/11224 [==============================] - 7s 602us/sample - loss: 1.6491 - mean_squared_error: 1.6491 - val_loss: 11.3507 - val_mean_squared_error: 11.3507\n",
            "Epoch 107/300\n",
            "11224/11224 [==============================] - 7s 612us/sample - loss: 1.5178 - mean_squared_error: 1.5178 - val_loss: 10.5453 - val_mean_squared_error: 10.5453\n",
            "Epoch 108/300\n",
            "11224/11224 [==============================] - 7s 601us/sample - loss: 1.6014 - mean_squared_error: 1.6014 - val_loss: 12.0440 - val_mean_squared_error: 12.0440\n",
            "Epoch 109/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.5330 - mean_squared_error: 1.5330 - val_loss: 10.0148 - val_mean_squared_error: 10.0148\n",
            "Epoch 110/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 1.4666 - mean_squared_error: 1.4666 - val_loss: 11.0279 - val_mean_squared_error: 11.0279\n",
            "Epoch 111/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 1.6252 - mean_squared_error: 1.6252 - val_loss: 10.5352 - val_mean_squared_error: 10.5352\n",
            "Epoch 112/300\n",
            "11224/11224 [==============================] - 7s 611us/sample - loss: 1.4610 - mean_squared_error: 1.4610 - val_loss: 9.5276 - val_mean_squared_error: 9.5276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNnic9ZXmXMZ",
        "colab_type": "text"
      },
      "source": [
        "## Collect the Specialist Epoch Data\n",
        "\n",
        "Once all the specialized training models were trained, the output data from each was combined into a single pickle file. This was useful when comparing relative accuracies and runtimes for models for the different keypoints. This comparison is completed with the neural_net_analysis and Bokeh App in the team's GitHub repository.\n",
        "\n",
        "Once the 15 models were all trained, predictions on the test images were made using all of these models (see submission_notebook). Using no additional optimization over the combined model, the test predictions from our specialized model saw a reduction in RMSE from 2.87 to 2.18 which would put the team in 23rd place on the Kaggle Public Leaderboard and a reduction from 2.76 to 1.78 on the private score which would have been 12th place on the Kaggle Private Leaderboard (acknowledging that the leaderboard has been frozen since the competition closed). This benefit was huge and totally worth the hassle of training 15 models! The next question is, \"Can we continue to improve?\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFQq6Lk4mXRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combine_spec_data():\n",
        "  # Create a path to the Google Drive location of the output\n",
        "  output_path = drive_path+\"OutputData/\"\n",
        "  \n",
        "  # Initialize an empty dataframe\n",
        "  spec_df = pd.DataFrame()\n",
        "  \n",
        "  # Loop over the 15 keypoints to bring in all of the specialist training data\n",
        "  for keypoint in df.columns[:-1:2]:\n",
        "    \n",
        "    # Read the individual keypoint model data from the pkl file\n",
        "    temp_df = pd.read_pickle(output_path+\"spec_{}.pkl\".format(keypoint[:-2]))\n",
        "\n",
        "    # Concatenate the specialist dataframe to the combined dataframe\n",
        "    spec_df = pd.concat([spec_df, temp_df])\n",
        "  \n",
        "  # Return the complete dataframe\n",
        "  return spec_df\n",
        "    \n",
        "  #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpQ7XsyOoHPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute the function above\n",
        "spec_df = combine_spec_data()\n",
        "\n",
        "# Write the combined dataframe to a pickle file\n",
        "spec_df.to_pickle(drive_path+\"OutputData/spec_01.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3cvrhFWpdiD",
        "colab_type": "code",
        "outputId": "0543b54d-4f7e-4de2-d53d-1e5925a3a08a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print(spec_df.columns)\n",
        "print(spec_df.groupby('keypoint').val_RMSE.min().sort_values(ascending=False))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['loss', 'mean_squared_error', 'val_loss', 'val_mean_squared_error',\n",
            "       'epoch', 'RMSE', 'val_RMSE', 'times', 'starting_filter', 'layers',\n",
            "       'pooling', 'fc_layer1', 'fc_layer2', 'activation', 'optimizer', 'lrate',\n",
            "       'dropout_initial', 'dropout_step', 'batch_norm', 'bias', 'stride',\n",
            "       'flipped', 'keypoint'],\n",
            "      dtype='object')\n",
            "keypoint\n",
            "mouth_center_bottom_lip    2.896987\n",
            "nose_tip                   2.649517\n",
            "left_eye_center            2.039986\n",
            "right_eye_center           1.940586\n",
            "left_eyebrow_outer_end     1.862300\n",
            "right_eyebrow_outer_end    1.716092\n",
            "right_eyebrow_inner_end    1.638469\n",
            "mouth_right_corner         1.542316\n",
            "left_eyebrow_inner_end     1.510707\n",
            "mouth_center_top_lip       1.481612\n",
            "mouth_left_corner          1.422979\n",
            "right_eye_outer_corner     1.396080\n",
            "left_eye_outer_corner      1.303956\n",
            "right_eye_inner_corner     1.101775\n",
            "left_eye_inner_corner      1.048386\n",
            "Name: val_RMSE, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlFI5h9n3Fpg",
        "colab_type": "text"
      },
      "source": [
        "## Improved Specialists\n",
        "Our specialist models showed significant improvement over our combined CNN. However, we had higher than desired validation errors for some keypoints. Interestingly enough, the keypoints with the highest errors had the largest set of available training data. Let's see if we can improve the performance of these few keypoints (listed below) through tuning of only these models. The list below is the ranked (worst to best) specialist model RMSE values. We will prioritize model improvements relative to this list.\n",
        "\n",
        "1.   mouth_center_bottom_lip 2.90 \n",
        "2.   nose_tip  2.65\n",
        "3.   left_eye_center 2.04\n",
        "4.   right_eye_center           1.94\n",
        "5.   left_eyebrow_outer_end     1.86\n",
        "6.   right_eyebrow_outer_end    1.72\n",
        "7.   right_eyebrow_inner_end    1.64\n",
        "8.   mouth_right_corner         1.54\n",
        "9. left_eyebrow_inner_end     1.51\n",
        "10. mouth_center_top_lip       1.48\n",
        "11. mouth_left_corner          1.42\n",
        "12. right_eye_outer_corner     1.40\n",
        "13. left_eye_outer_corner      1.30\n",
        "14. right_eye_inner_corner     1.10\n",
        "15. left_eye_inner_corner      1.05\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j39RU4Y33F8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_improved_specialists(df, keypoint, sf=12, doi=0.0, dos=0.1, lrf=10, fc1=500, fc2=500):\n",
        "  # Use an early stopping callback and our timing callback\n",
        "  early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                                patience=100, mode='auto', restore_best_weights = True)\n",
        "  time_callback = TimeHistory()\n",
        "  \n",
        "  \n",
        "  # Use the adam optimizer with the default learning rate\n",
        "  adam = optimizers.Adam(lr=0.001*lrf, beta_1=0.9, beta_2=0.999)\n",
        "    \n",
        "  # Create and Compile the model\n",
        "  model = create_spec_bn_cnn_model(sf, doi, dos, fc1=fc1, fc2=fc2)\n",
        "  model.compile(\n",
        "      optimizer=adam,\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['mean_squared_error'])\n",
        "  \n",
        "  \n",
        "  # Subset the image/keypoint dataframe to just have the training locations for \n",
        "  # the keypoints of interest\n",
        "  df_keypoint = df[[keypoint + '_x',keypoint+'_y', 'Image']]\n",
        "  \n",
        "  # Print the remaining columns as a QA check\n",
        "  print(df_keypoint.columns)\n",
        "  \n",
        "  # Drop the NA values\n",
        "  df_keypoint = df_keypoint.dropna().reset_index(drop=True)\n",
        "  \n",
        "  # Grab the last column - that is our image data for X matrix\n",
        "  train_X = df_keypoint.iloc[:, -1]\n",
        "\n",
        "  # Convert from a series of arrays to an NDarray\n",
        "  train_X = np.array([x.reshape(96,96,1) for x in train_X])\n",
        " \n",
        "  \n",
        "  # Get the Y data for just the specified keypoint\n",
        "  train_y = np.array(df_keypoint.iloc[:,:-1])\n",
        " \n",
        "  history = model.fit(\n",
        "      train_X.astype(np.float32), train_y.astype(np.float32),\n",
        "      epochs=300,\n",
        "      validation_split=0.20,\n",
        "      callbacks=[time_callback, early_stop])\n",
        "  times = time_callback.times\n",
        "\n",
        "  # Convert to dataframe\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "  hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "  hist['times'] = times\n",
        "  hist['starting_filter'] = sf\n",
        "  hist['layers'] = 3\n",
        "  hist['pooling'] = 'yes'\n",
        "  hist['fc_layer1'] = fc1\n",
        "  hist['fc_layer2'] = fc2\n",
        "  hist['activation'] = 'relu'\n",
        "  hist['optimizer'] = 'adam'\n",
        "  hist['lrate'] = adam.get_config()['learning_rate']\n",
        "  hist['dropout_initial'] = doi\n",
        "  hist['dropout_step'] = dos\n",
        "  hist['batch_norm'] = 1\n",
        "  hist['bias'] = 0\n",
        "  hist['stride'] = 1\n",
        "  hist['flipped'] = 1.0\n",
        "  hist['keypoint'] = keypoint\n",
        "\n",
        "  # Re-pickle after every model to retain progress\n",
        "  hist.to_pickle(drive_path+\"OutputData/cnn_3l_spec_{}_d{}_s{}_sf{}_lrf{}_flipped_100.pkl\".format(keypoint, doi, dos, sf, lrf))\n",
        "\n",
        "  # Save models.\n",
        "  filename = \"cnn_spec_{}_d{}_s{}_sf{}_lrfactor{}_flipped_100\".format(keypoint, doi, dos, sf, lrf)\n",
        "  model.save(drive_path+\"Models/\"+filename+\".h5\")\n",
        "  return hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk7vMAwJ6eNr",
        "colab_type": "text"
      },
      "source": [
        "## Train some improved models (hopefully)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crED-6Ts6d48",
        "colab_type": "code",
        "outputId": "80179639-b6ae-40e7-8ac0-7cbb97b3de54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "improved_specialist_df = train_improved_specialists(df, 'mouth_center_bottom_lip', sf=16, doi=0.10, dos=0.10, lrf=10, fc1=100, fc2=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 94, 94, 4)         36        \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 94, 94, 4)         16        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 47, 47, 4)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 47, 47, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 46, 46, 8)         128       \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 46, 46, 8)         32        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 23, 23, 8)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 23, 23, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 22, 22, 16)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1936)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                96850     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "Specialist (Dense)           (None, 2)                 102       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 100,690\n",
            "Trainable params: 100,434\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Index(['mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y', 'Image'], dtype='object')\n",
            "Train on 11224 samples, validate on 2806 samples\n",
            "Epoch 1/300\n",
            "11224/11224 [==============================] - 7s 657us/sample - loss: 438.9785 - mean_squared_error: 438.9786 - val_loss: 27.5374 - val_mean_squared_error: 27.5374\n",
            "Epoch 2/300\n",
            "11224/11224 [==============================] - 6s 562us/sample - loss: 29.1563 - mean_squared_error: 29.1563 - val_loss: 32.0204 - val_mean_squared_error: 32.0204\n",
            "Epoch 3/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 23.7582 - mean_squared_error: 23.7582 - val_loss: 35.1630 - val_mean_squared_error: 35.1630\n",
            "Epoch 4/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 18.8875 - mean_squared_error: 18.8875 - val_loss: 19.1946 - val_mean_squared_error: 19.1946\n",
            "Epoch 5/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 15.7735 - mean_squared_error: 15.7734 - val_loss: 16.2234 - val_mean_squared_error: 16.2234\n",
            "Epoch 6/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 13.8829 - mean_squared_error: 13.8829 - val_loss: 12.1693 - val_mean_squared_error: 12.1693\n",
            "Epoch 7/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 13.1520 - mean_squared_error: 13.1520 - val_loss: 12.4870 - val_mean_squared_error: 12.4870\n",
            "Epoch 8/300\n",
            "11224/11224 [==============================] - 6s 555us/sample - loss: 12.1919 - mean_squared_error: 12.1919 - val_loss: 12.4375 - val_mean_squared_error: 12.4375\n",
            "Epoch 9/300\n",
            "11224/11224 [==============================] - 6s 556us/sample - loss: 11.8714 - mean_squared_error: 11.8714 - val_loss: 11.0968 - val_mean_squared_error: 11.0968\n",
            "Epoch 10/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 11.4733 - mean_squared_error: 11.4733 - val_loss: 12.1071 - val_mean_squared_error: 12.1072\n",
            "Epoch 11/300\n",
            "11224/11224 [==============================] - 6s 572us/sample - loss: 11.3535 - mean_squared_error: 11.3535 - val_loss: 9.5859 - val_mean_squared_error: 9.5859\n",
            "Epoch 12/300\n",
            "11224/11224 [==============================] - 7s 586us/sample - loss: 10.7507 - mean_squared_error: 10.7507 - val_loss: 10.7390 - val_mean_squared_error: 10.7390\n",
            "Epoch 13/300\n",
            "11224/11224 [==============================] - 7s 611us/sample - loss: 10.4079 - mean_squared_error: 10.4079 - val_loss: 9.4820 - val_mean_squared_error: 9.4820\n",
            "Epoch 14/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 10.1812 - mean_squared_error: 10.1812 - val_loss: 10.2504 - val_mean_squared_error: 10.2504\n",
            "Epoch 15/300\n",
            "11224/11224 [==============================] - 6s 570us/sample - loss: 10.1992 - mean_squared_error: 10.1992 - val_loss: 11.5275 - val_mean_squared_error: 11.5275\n",
            "Epoch 16/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 9.9497 - mean_squared_error: 9.9497 - val_loss: 9.2354 - val_mean_squared_error: 9.2354\n",
            "Epoch 17/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 9.6138 - mean_squared_error: 9.6138 - val_loss: 9.9306 - val_mean_squared_error: 9.9306\n",
            "Epoch 18/300\n",
            "11224/11224 [==============================] - 6s 574us/sample - loss: 9.5203 - mean_squared_error: 9.5203 - val_loss: 10.9124 - val_mean_squared_error: 10.9124\n",
            "Epoch 19/300\n",
            "11224/11224 [==============================] - 6s 579us/sample - loss: 9.4510 - mean_squared_error: 9.4510 - val_loss: 12.1238 - val_mean_squared_error: 12.1238\n",
            "Epoch 20/300\n",
            "11224/11224 [==============================] - 6s 570us/sample - loss: 9.6123 - mean_squared_error: 9.6123 - val_loss: 8.1126 - val_mean_squared_error: 8.1126\n",
            "Epoch 21/300\n",
            "11224/11224 [==============================] - 6s 573us/sample - loss: 9.4743 - mean_squared_error: 9.4743 - val_loss: 10.6267 - val_mean_squared_error: 10.6267\n",
            "Epoch 22/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 9.0694 - mean_squared_error: 9.0694 - val_loss: 10.8385 - val_mean_squared_error: 10.8385\n",
            "Epoch 23/300\n",
            "11224/11224 [==============================] - 6s 551us/sample - loss: 9.1306 - mean_squared_error: 9.1306 - val_loss: 12.1282 - val_mean_squared_error: 12.1282\n",
            "Epoch 24/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 8.7344 - mean_squared_error: 8.7344 - val_loss: 9.6550 - val_mean_squared_error: 9.6550\n",
            "Epoch 25/300\n",
            "11224/11224 [==============================] - 6s 542us/sample - loss: 8.9102 - mean_squared_error: 8.9102 - val_loss: 9.6339 - val_mean_squared_error: 9.6339\n",
            "Epoch 26/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 8.8678 - mean_squared_error: 8.8678 - val_loss: 9.4497 - val_mean_squared_error: 9.4497\n",
            "Epoch 27/300\n",
            "11224/11224 [==============================] - 7s 585us/sample - loss: 8.9774 - mean_squared_error: 8.9774 - val_loss: 9.2100 - val_mean_squared_error: 9.2100\n",
            "Epoch 28/300\n",
            "11224/11224 [==============================] - 6s 566us/sample - loss: 8.6907 - mean_squared_error: 8.6907 - val_loss: 9.5336 - val_mean_squared_error: 9.5336\n",
            "Epoch 29/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 8.6060 - mean_squared_error: 8.6060 - val_loss: 8.5604 - val_mean_squared_error: 8.5604\n",
            "Epoch 30/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 8.6601 - mean_squared_error: 8.6601 - val_loss: 9.3492 - val_mean_squared_error: 9.3492\n",
            "Epoch 31/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 8.4763 - mean_squared_error: 8.4763 - val_loss: 9.9081 - val_mean_squared_error: 9.9081\n",
            "Epoch 32/300\n",
            "11224/11224 [==============================] - 6s 548us/sample - loss: 8.6429 - mean_squared_error: 8.6429 - val_loss: 9.0076 - val_mean_squared_error: 9.0076\n",
            "Epoch 33/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 8.7764 - mean_squared_error: 8.7764 - val_loss: 8.9297 - val_mean_squared_error: 8.9297\n",
            "Epoch 34/300\n",
            "11224/11224 [==============================] - 6s 544us/sample - loss: 8.3981 - mean_squared_error: 8.3981 - val_loss: 10.0273 - val_mean_squared_error: 10.0273\n",
            "Epoch 35/300\n",
            "11224/11224 [==============================] - 6s 572us/sample - loss: 8.0797 - mean_squared_error: 8.0797 - val_loss: 8.4146 - val_mean_squared_error: 8.4146\n",
            "Epoch 36/300\n",
            "11224/11224 [==============================] - 7s 592us/sample - loss: 8.1594 - mean_squared_error: 8.1594 - val_loss: 8.1155 - val_mean_squared_error: 8.1155\n",
            "Epoch 37/300\n",
            "11224/11224 [==============================] - 6s 579us/sample - loss: 8.2703 - mean_squared_error: 8.2703 - val_loss: 8.8129 - val_mean_squared_error: 8.8129\n",
            "Epoch 38/300\n",
            "11224/11224 [==============================] - 6s 576us/sample - loss: 7.9256 - mean_squared_error: 7.9256 - val_loss: 9.4224 - val_mean_squared_error: 9.4224\n",
            "Epoch 39/300\n",
            "11224/11224 [==============================] - 6s 562us/sample - loss: 8.1254 - mean_squared_error: 8.1254 - val_loss: 9.1780 - val_mean_squared_error: 9.1780\n",
            "Epoch 40/300\n",
            "11224/11224 [==============================] - 6s 563us/sample - loss: 7.9655 - mean_squared_error: 7.9655 - val_loss: 8.2614 - val_mean_squared_error: 8.2614\n",
            "Epoch 41/300\n",
            "11224/11224 [==============================] - 6s 572us/sample - loss: 8.0925 - mean_squared_error: 8.0925 - val_loss: 10.4832 - val_mean_squared_error: 10.4832\n",
            "Epoch 42/300\n",
            "11224/11224 [==============================] - 6s 567us/sample - loss: 7.8796 - mean_squared_error: 7.8796 - val_loss: 10.5516 - val_mean_squared_error: 10.5516\n",
            "Epoch 43/300\n",
            "11224/11224 [==============================] - 6s 561us/sample - loss: 7.9978 - mean_squared_error: 7.9978 - val_loss: 9.7201 - val_mean_squared_error: 9.7201\n",
            "Epoch 44/300\n",
            "11224/11224 [==============================] - 6s 563us/sample - loss: 8.1364 - mean_squared_error: 8.1364 - val_loss: 9.3578 - val_mean_squared_error: 9.3578\n",
            "Epoch 45/300\n",
            "11224/11224 [==============================] - 6s 572us/sample - loss: 8.0065 - mean_squared_error: 8.0065 - val_loss: 9.4041 - val_mean_squared_error: 9.4041\n",
            "Epoch 46/300\n",
            "11224/11224 [==============================] - 6s 573us/sample - loss: 7.6971 - mean_squared_error: 7.6971 - val_loss: 9.8468 - val_mean_squared_error: 9.8468\n",
            "Epoch 47/300\n",
            "11224/11224 [==============================] - 6s 571us/sample - loss: 7.8681 - mean_squared_error: 7.8681 - val_loss: 8.8480 - val_mean_squared_error: 8.8480\n",
            "Epoch 48/300\n",
            "11224/11224 [==============================] - 6s 567us/sample - loss: 7.7933 - mean_squared_error: 7.7933 - val_loss: 11.4950 - val_mean_squared_error: 11.4950\n",
            "Epoch 49/300\n",
            "11224/11224 [==============================] - 6s 551us/sample - loss: 7.7576 - mean_squared_error: 7.7576 - val_loss: 10.2658 - val_mean_squared_error: 10.2658\n",
            "Epoch 50/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 7.8973 - mean_squared_error: 7.8973 - val_loss: 8.3836 - val_mean_squared_error: 8.3836\n",
            "Epoch 51/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 7.6256 - mean_squared_error: 7.6256 - val_loss: 8.4850 - val_mean_squared_error: 8.4850\n",
            "Epoch 52/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 7.4258 - mean_squared_error: 7.4258 - val_loss: 8.2751 - val_mean_squared_error: 8.2751\n",
            "Epoch 53/300\n",
            "11224/11224 [==============================] - 6s 555us/sample - loss: 7.5977 - mean_squared_error: 7.5977 - val_loss: 8.8407 - val_mean_squared_error: 8.8407\n",
            "Epoch 54/300\n",
            "11224/11224 [==============================] - 6s 557us/sample - loss: 7.5362 - mean_squared_error: 7.5362 - val_loss: 8.2792 - val_mean_squared_error: 8.2792\n",
            "Epoch 55/300\n",
            "11224/11224 [==============================] - 6s 548us/sample - loss: 7.7151 - mean_squared_error: 7.7151 - val_loss: 8.8584 - val_mean_squared_error: 8.8584\n",
            "Epoch 56/300\n",
            "11224/11224 [==============================] - 6s 555us/sample - loss: 7.4554 - mean_squared_error: 7.4554 - val_loss: 8.3963 - val_mean_squared_error: 8.3963\n",
            "Epoch 57/300\n",
            "11224/11224 [==============================] - 6s 546us/sample - loss: 7.6237 - mean_squared_error: 7.6237 - val_loss: 8.7418 - val_mean_squared_error: 8.7418\n",
            "Epoch 58/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 7.3576 - mean_squared_error: 7.3576 - val_loss: 9.4310 - val_mean_squared_error: 9.4310\n",
            "Epoch 59/300\n",
            "11224/11224 [==============================] - 6s 579us/sample - loss: 7.5236 - mean_squared_error: 7.5236 - val_loss: 9.1627 - val_mean_squared_error: 9.1627\n",
            "Epoch 60/300\n",
            "11224/11224 [==============================] - 6s 566us/sample - loss: 7.4811 - mean_squared_error: 7.4811 - val_loss: 8.1114 - val_mean_squared_error: 8.1114\n",
            "Epoch 61/300\n",
            "11224/11224 [==============================] - 6s 555us/sample - loss: 7.3487 - mean_squared_error: 7.3487 - val_loss: 9.5425 - val_mean_squared_error: 9.5425\n",
            "Epoch 62/300\n",
            "11224/11224 [==============================] - 6s 566us/sample - loss: 7.4883 - mean_squared_error: 7.4883 - val_loss: 8.5798 - val_mean_squared_error: 8.5798\n",
            "Epoch 63/300\n",
            "11224/11224 [==============================] - 6s 561us/sample - loss: 7.3754 - mean_squared_error: 7.3754 - val_loss: 9.1504 - val_mean_squared_error: 9.1504\n",
            "Epoch 64/300\n",
            "11224/11224 [==============================] - 6s 552us/sample - loss: 7.3732 - mean_squared_error: 7.3732 - val_loss: 9.3743 - val_mean_squared_error: 9.3743\n",
            "Epoch 65/300\n",
            "11224/11224 [==============================] - 6s 545us/sample - loss: 7.3420 - mean_squared_error: 7.3420 - val_loss: 10.1818 - val_mean_squared_error: 10.1818\n",
            "Epoch 66/300\n",
            "11224/11224 [==============================] - 6s 548us/sample - loss: 7.2701 - mean_squared_error: 7.2701 - val_loss: 9.0546 - val_mean_squared_error: 9.0546\n",
            "Epoch 67/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 7.5652 - mean_squared_error: 7.5652 - val_loss: 8.9615 - val_mean_squared_error: 8.9615\n",
            "Epoch 68/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 7.1657 - mean_squared_error: 7.1657 - val_loss: 9.1250 - val_mean_squared_error: 9.1250\n",
            "Epoch 69/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 7.3790 - mean_squared_error: 7.3790 - val_loss: 9.0475 - val_mean_squared_error: 9.0475\n",
            "Epoch 70/300\n",
            "11224/11224 [==============================] - 6s 551us/sample - loss: 7.3436 - mean_squared_error: 7.3436 - val_loss: 9.6967 - val_mean_squared_error: 9.6967\n",
            "Epoch 71/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 7.1875 - mean_squared_error: 7.1875 - val_loss: 8.5076 - val_mean_squared_error: 8.5076\n",
            "Epoch 72/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 7.5803 - mean_squared_error: 7.5803 - val_loss: 8.3448 - val_mean_squared_error: 8.3448\n",
            "Epoch 73/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 7.3019 - mean_squared_error: 7.3019 - val_loss: 8.2934 - val_mean_squared_error: 8.2934\n",
            "Epoch 74/300\n",
            "11224/11224 [==============================] - 6s 564us/sample - loss: 7.3627 - mean_squared_error: 7.3627 - val_loss: 9.3077 - val_mean_squared_error: 9.3077\n",
            "Epoch 75/300\n",
            "11224/11224 [==============================] - 6s 569us/sample - loss: 7.2452 - mean_squared_error: 7.2452 - val_loss: 9.4922 - val_mean_squared_error: 9.4922\n",
            "Epoch 76/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 7.3774 - mean_squared_error: 7.3774 - val_loss: 9.2438 - val_mean_squared_error: 9.2438\n",
            "Epoch 77/300\n",
            "11224/11224 [==============================] - 6s 563us/sample - loss: 7.2540 - mean_squared_error: 7.2540 - val_loss: 9.1553 - val_mean_squared_error: 9.1553\n",
            "Epoch 78/300\n",
            "11224/11224 [==============================] - 6s 549us/sample - loss: 7.1523 - mean_squared_error: 7.1523 - val_loss: 8.6123 - val_mean_squared_error: 8.6123\n",
            "Epoch 79/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 7.3256 - mean_squared_error: 7.3256 - val_loss: 8.2878 - val_mean_squared_error: 8.2878\n",
            "Epoch 80/300\n",
            "11224/11224 [==============================] - 6s 564us/sample - loss: 7.1256 - mean_squared_error: 7.1256 - val_loss: 8.7150 - val_mean_squared_error: 8.7150\n",
            "Epoch 81/300\n",
            "11224/11224 [==============================] - 6s 568us/sample - loss: 7.1626 - mean_squared_error: 7.1626 - val_loss: 9.9042 - val_mean_squared_error: 9.9042\n",
            "Epoch 82/300\n",
            "11224/11224 [==============================] - 7s 583us/sample - loss: 6.9730 - mean_squared_error: 6.9730 - val_loss: 10.6249 - val_mean_squared_error: 10.6249\n",
            "Epoch 83/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 7.0310 - mean_squared_error: 7.0310 - val_loss: 8.5471 - val_mean_squared_error: 8.5471\n",
            "Epoch 84/300\n",
            "11224/11224 [==============================] - 7s 591us/sample - loss: 7.0390 - mean_squared_error: 7.0390 - val_loss: 8.2525 - val_mean_squared_error: 8.2525\n",
            "Epoch 85/300\n",
            "11224/11224 [==============================] - 6s 575us/sample - loss: 6.9613 - mean_squared_error: 6.9613 - val_loss: 8.3097 - val_mean_squared_error: 8.3097\n",
            "Epoch 86/300\n",
            "11224/11224 [==============================] - 6s 570us/sample - loss: 7.0498 - mean_squared_error: 7.0498 - val_loss: 8.6639 - val_mean_squared_error: 8.6639\n",
            "Epoch 87/300\n",
            "11224/11224 [==============================] - 6s 569us/sample - loss: 6.9302 - mean_squared_error: 6.9302 - val_loss: 8.6986 - val_mean_squared_error: 8.6986\n",
            "Epoch 88/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 6.8118 - mean_squared_error: 6.8118 - val_loss: 8.9761 - val_mean_squared_error: 8.9761\n",
            "Epoch 89/300\n",
            "11224/11224 [==============================] - 6s 577us/sample - loss: 7.2025 - mean_squared_error: 7.2025 - val_loss: 7.8163 - val_mean_squared_error: 7.8163\n",
            "Epoch 90/300\n",
            "11224/11224 [==============================] - 6s 557us/sample - loss: 6.8898 - mean_squared_error: 6.8898 - val_loss: 10.0420 - val_mean_squared_error: 10.0420\n",
            "Epoch 91/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 6.9140 - mean_squared_error: 6.9140 - val_loss: 10.9191 - val_mean_squared_error: 10.9191\n",
            "Epoch 92/300\n",
            "11224/11224 [==============================] - 6s 561us/sample - loss: 7.0483 - mean_squared_error: 7.0483 - val_loss: 8.6121 - val_mean_squared_error: 8.6121\n",
            "Epoch 93/300\n",
            "11224/11224 [==============================] - 6s 555us/sample - loss: 7.0462 - mean_squared_error: 7.0462 - val_loss: 8.5855 - val_mean_squared_error: 8.5855\n",
            "Epoch 94/300\n",
            "11224/11224 [==============================] - 6s 557us/sample - loss: 6.7451 - mean_squared_error: 6.7451 - val_loss: 9.1105 - val_mean_squared_error: 9.1105\n",
            "Epoch 95/300\n",
            "11224/11224 [==============================] - 6s 568us/sample - loss: 6.9382 - mean_squared_error: 6.9382 - val_loss: 9.1169 - val_mean_squared_error: 9.1169\n",
            "Epoch 96/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 6.6502 - mean_squared_error: 6.6502 - val_loss: 8.5890 - val_mean_squared_error: 8.5890\n",
            "Epoch 97/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 6.8331 - mean_squared_error: 6.8331 - val_loss: 9.4256 - val_mean_squared_error: 9.4256\n",
            "Epoch 98/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 6.9988 - mean_squared_error: 6.9988 - val_loss: 10.7724 - val_mean_squared_error: 10.7724\n",
            "Epoch 99/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 6.7185 - mean_squared_error: 6.7185 - val_loss: 8.4142 - val_mean_squared_error: 8.4142\n",
            "Epoch 100/300\n",
            "11224/11224 [==============================] - 6s 546us/sample - loss: 6.8159 - mean_squared_error: 6.8159 - val_loss: 8.8844 - val_mean_squared_error: 8.8844\n",
            "Epoch 101/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 6.7843 - mean_squared_error: 6.7843 - val_loss: 10.4451 - val_mean_squared_error: 10.4451\n",
            "Epoch 102/300\n",
            "11224/11224 [==============================] - 6s 542us/sample - loss: 6.9234 - mean_squared_error: 6.9234 - val_loss: 8.2903 - val_mean_squared_error: 8.2903\n",
            "Epoch 103/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 6.6836 - mean_squared_error: 6.6836 - val_loss: 8.4837 - val_mean_squared_error: 8.4837\n",
            "Epoch 104/300\n",
            "11224/11224 [==============================] - 6s 550us/sample - loss: 6.6273 - mean_squared_error: 6.6273 - val_loss: 8.4869 - val_mean_squared_error: 8.4869\n",
            "Epoch 105/300\n",
            "11224/11224 [==============================] - 6s 547us/sample - loss: 6.8819 - mean_squared_error: 6.8819 - val_loss: 9.2971 - val_mean_squared_error: 9.2971\n",
            "Epoch 106/300\n",
            "11224/11224 [==============================] - 6s 545us/sample - loss: 6.6082 - mean_squared_error: 6.6082 - val_loss: 9.9064 - val_mean_squared_error: 9.9064\n",
            "Epoch 107/300\n",
            "11224/11224 [==============================] - 6s 541us/sample - loss: 6.8380 - mean_squared_error: 6.8380 - val_loss: 10.0317 - val_mean_squared_error: 10.0317\n",
            "Epoch 108/300\n",
            "11224/11224 [==============================] - 6s 536us/sample - loss: 6.7768 - mean_squared_error: 6.7768 - val_loss: 9.0190 - val_mean_squared_error: 9.0190\n",
            "Epoch 109/300\n",
            "11224/11224 [==============================] - 6s 532us/sample - loss: 6.8190 - mean_squared_error: 6.8190 - val_loss: 8.7234 - val_mean_squared_error: 8.7234\n",
            "Epoch 110/300\n",
            "11224/11224 [==============================] - 6s 544us/sample - loss: 6.8114 - mean_squared_error: 6.8114 - val_loss: 8.8734 - val_mean_squared_error: 8.8734\n",
            "Epoch 111/300\n",
            "11224/11224 [==============================] - 6s 549us/sample - loss: 6.8585 - mean_squared_error: 6.8585 - val_loss: 8.8966 - val_mean_squared_error: 8.8966\n",
            "Epoch 112/300\n",
            "11224/11224 [==============================] - 6s 536us/sample - loss: 6.8094 - mean_squared_error: 6.8094 - val_loss: 8.7111 - val_mean_squared_error: 8.7111\n",
            "Epoch 113/300\n",
            "11224/11224 [==============================] - 6s 573us/sample - loss: 6.8509 - mean_squared_error: 6.8509 - val_loss: 9.0076 - val_mean_squared_error: 9.0076\n",
            "Epoch 114/300\n",
            "11224/11224 [==============================] - 7s 608us/sample - loss: 6.6609 - mean_squared_error: 6.6609 - val_loss: 9.0938 - val_mean_squared_error: 9.0938\n",
            "Epoch 115/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 6.8599 - mean_squared_error: 6.8599 - val_loss: 8.8708 - val_mean_squared_error: 8.8708\n",
            "Epoch 116/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 6.7310 - mean_squared_error: 6.7310 - val_loss: 8.3775 - val_mean_squared_error: 8.3775\n",
            "Epoch 117/300\n",
            "11224/11224 [==============================] - 7s 591us/sample - loss: 6.7922 - mean_squared_error: 6.7922 - val_loss: 10.9353 - val_mean_squared_error: 10.9353\n",
            "Epoch 118/300\n",
            "11224/11224 [==============================] - 6s 569us/sample - loss: 6.5850 - mean_squared_error: 6.5850 - val_loss: 8.5429 - val_mean_squared_error: 8.5429\n",
            "Epoch 119/300\n",
            "11224/11224 [==============================] - 6s 556us/sample - loss: 6.5433 - mean_squared_error: 6.5433 - val_loss: 8.3596 - val_mean_squared_error: 8.3596\n",
            "Epoch 120/300\n",
            "11224/11224 [==============================] - 6s 557us/sample - loss: 6.9858 - mean_squared_error: 6.9858 - val_loss: 8.7500 - val_mean_squared_error: 8.7500\n",
            "Epoch 121/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 6.7628 - mean_squared_error: 6.7628 - val_loss: 8.6519 - val_mean_squared_error: 8.6519\n",
            "Epoch 122/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 6.6708 - mean_squared_error: 6.6708 - val_loss: 9.9031 - val_mean_squared_error: 9.9031\n",
            "Epoch 123/300\n",
            "11224/11224 [==============================] - 6s 575us/sample - loss: 6.5945 - mean_squared_error: 6.5945 - val_loss: 9.4229 - val_mean_squared_error: 9.4229\n",
            "Epoch 124/300\n",
            "11224/11224 [==============================] - 6s 574us/sample - loss: 6.6096 - mean_squared_error: 6.6096 - val_loss: 9.3869 - val_mean_squared_error: 9.3869\n",
            "Epoch 125/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 6.5741 - mean_squared_error: 6.5741 - val_loss: 8.8122 - val_mean_squared_error: 8.8122\n",
            "Epoch 126/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 6.4791 - mean_squared_error: 6.4791 - val_loss: 8.4926 - val_mean_squared_error: 8.4926\n",
            "Epoch 127/300\n",
            "11224/11224 [==============================] - 6s 550us/sample - loss: 6.8485 - mean_squared_error: 6.8485 - val_loss: 11.2326 - val_mean_squared_error: 11.2326\n",
            "Epoch 128/300\n",
            "11224/11224 [==============================] - 6s 548us/sample - loss: 6.7364 - mean_squared_error: 6.7364 - val_loss: 8.5782 - val_mean_squared_error: 8.5782\n",
            "Epoch 129/300\n",
            "11224/11224 [==============================] - 6s 547us/sample - loss: 6.5558 - mean_squared_error: 6.5558 - val_loss: 8.7228 - val_mean_squared_error: 8.7228\n",
            "Epoch 130/300\n",
            "11224/11224 [==============================] - 6s 556us/sample - loss: 6.6859 - mean_squared_error: 6.6859 - val_loss: 8.4272 - val_mean_squared_error: 8.4272\n",
            "Epoch 131/300\n",
            "11224/11224 [==============================] - 6s 556us/sample - loss: 6.5091 - mean_squared_error: 6.5091 - val_loss: 9.0712 - val_mean_squared_error: 9.0712\n",
            "Epoch 132/300\n",
            "11224/11224 [==============================] - 6s 569us/sample - loss: 6.5575 - mean_squared_error: 6.5575 - val_loss: 8.8853 - val_mean_squared_error: 8.8853\n",
            "Epoch 133/300\n",
            "11224/11224 [==============================] - 7s 607us/sample - loss: 6.6937 - mean_squared_error: 6.6937 - val_loss: 8.3492 - val_mean_squared_error: 8.3492\n",
            "Epoch 134/300\n",
            "11224/11224 [==============================] - 7s 616us/sample - loss: 6.6384 - mean_squared_error: 6.6384 - val_loss: 8.5517 - val_mean_squared_error: 8.5517\n",
            "Epoch 135/300\n",
            "11224/11224 [==============================] - 7s 630us/sample - loss: 6.6600 - mean_squared_error: 6.6600 - val_loss: 8.5363 - val_mean_squared_error: 8.5363\n",
            "Epoch 136/300\n",
            "11224/11224 [==============================] - 7s 614us/sample - loss: 6.5836 - mean_squared_error: 6.5836 - val_loss: 8.6610 - val_mean_squared_error: 8.6610\n",
            "Epoch 137/300\n",
            "11224/11224 [==============================] - 7s 586us/sample - loss: 6.5562 - mean_squared_error: 6.5562 - val_loss: 8.4029 - val_mean_squared_error: 8.4029\n",
            "Epoch 138/300\n",
            "11224/11224 [==============================] - 6s 576us/sample - loss: 6.4682 - mean_squared_error: 6.4682 - val_loss: 8.5786 - val_mean_squared_error: 8.5786\n",
            "Epoch 139/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 6.6103 - mean_squared_error: 6.6103 - val_loss: 8.6878 - val_mean_squared_error: 8.6878\n",
            "Epoch 140/300\n",
            "11224/11224 [==============================] - 7s 579us/sample - loss: 6.4204 - mean_squared_error: 6.4204 - val_loss: 9.1446 - val_mean_squared_error: 9.1446\n",
            "Epoch 141/300\n",
            "11224/11224 [==============================] - 6s 562us/sample - loss: 6.6668 - mean_squared_error: 6.6668 - val_loss: 10.5675 - val_mean_squared_error: 10.5675\n",
            "Epoch 142/300\n",
            "11224/11224 [==============================] - 6s 573us/sample - loss: 6.7543 - mean_squared_error: 6.7543 - val_loss: 8.3503 - val_mean_squared_error: 8.3503\n",
            "Epoch 143/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 6.4097 - mean_squared_error: 6.4097 - val_loss: 8.7332 - val_mean_squared_error: 8.7332\n",
            "Epoch 144/300\n",
            "11224/11224 [==============================] - 7s 585us/sample - loss: 6.3592 - mean_squared_error: 6.3592 - val_loss: 8.0579 - val_mean_squared_error: 8.0579\n",
            "Epoch 145/300\n",
            "11224/11224 [==============================] - 7s 607us/sample - loss: 6.4203 - mean_squared_error: 6.4203 - val_loss: 8.1864 - val_mean_squared_error: 8.1864\n",
            "Epoch 146/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 6.4319 - mean_squared_error: 6.4319 - val_loss: 9.2459 - val_mean_squared_error: 9.2459\n",
            "Epoch 147/300\n",
            "11224/11224 [==============================] - 6s 577us/sample - loss: 6.5365 - mean_squared_error: 6.5365 - val_loss: 8.2355 - val_mean_squared_error: 8.2355\n",
            "Epoch 148/300\n",
            "11224/11224 [==============================] - 6s 577us/sample - loss: 6.5576 - mean_squared_error: 6.5576 - val_loss: 8.6411 - val_mean_squared_error: 8.6411\n",
            "Epoch 149/300\n",
            "11224/11224 [==============================] - 7s 586us/sample - loss: 6.3899 - mean_squared_error: 6.3899 - val_loss: 8.5265 - val_mean_squared_error: 8.5265\n",
            "Epoch 150/300\n",
            "11224/11224 [==============================] - 6s 566us/sample - loss: 6.4235 - mean_squared_error: 6.4235 - val_loss: 13.3596 - val_mean_squared_error: 13.3596\n",
            "Epoch 151/300\n",
            "11224/11224 [==============================] - 6s 573us/sample - loss: 6.4748 - mean_squared_error: 6.4748 - val_loss: 8.4661 - val_mean_squared_error: 8.4661\n",
            "Epoch 152/300\n",
            "11224/11224 [==============================] - 6s 555us/sample - loss: 6.2563 - mean_squared_error: 6.2563 - val_loss: 9.8821 - val_mean_squared_error: 9.8821\n",
            "Epoch 153/300\n",
            "11224/11224 [==============================] - 6s 556us/sample - loss: 6.4367 - mean_squared_error: 6.4367 - val_loss: 9.5723 - val_mean_squared_error: 9.5723\n",
            "Epoch 154/300\n",
            "11224/11224 [==============================] - 6s 562us/sample - loss: 6.3567 - mean_squared_error: 6.3567 - val_loss: 8.7219 - val_mean_squared_error: 8.7219\n",
            "Epoch 155/300\n",
            "11224/11224 [==============================] - 6s 557us/sample - loss: 6.4862 - mean_squared_error: 6.4862 - val_loss: 8.6261 - val_mean_squared_error: 8.6261\n",
            "Epoch 156/300\n",
            "11224/11224 [==============================] - 6s 547us/sample - loss: 6.3709 - mean_squared_error: 6.3709 - val_loss: 9.0639 - val_mean_squared_error: 9.0639\n",
            "Epoch 157/300\n",
            "11224/11224 [==============================] - 6s 549us/sample - loss: 6.4303 - mean_squared_error: 6.4303 - val_loss: 8.7495 - val_mean_squared_error: 8.7495\n",
            "Epoch 158/300\n",
            "11224/11224 [==============================] - 6s 556us/sample - loss: 6.5035 - mean_squared_error: 6.5035 - val_loss: 10.6513 - val_mean_squared_error: 10.6513\n",
            "Epoch 159/300\n",
            "11224/11224 [==============================] - 6s 543us/sample - loss: 6.3415 - mean_squared_error: 6.3415 - val_loss: 9.0323 - val_mean_squared_error: 9.0323\n",
            "Epoch 160/300\n",
            "11224/11224 [==============================] - 6s 544us/sample - loss: 6.3959 - mean_squared_error: 6.3959 - val_loss: 9.2187 - val_mean_squared_error: 9.2187\n",
            "Epoch 161/300\n",
            "11224/11224 [==============================] - 6s 545us/sample - loss: 6.3260 - mean_squared_error: 6.3260 - val_loss: 8.6863 - val_mean_squared_error: 8.6863\n",
            "Epoch 162/300\n",
            "11224/11224 [==============================] - 6s 550us/sample - loss: 6.3582 - mean_squared_error: 6.3582 - val_loss: 9.0109 - val_mean_squared_error: 9.0109\n",
            "Epoch 163/300\n",
            "11224/11224 [==============================] - 6s 546us/sample - loss: 6.4380 - mean_squared_error: 6.4380 - val_loss: 8.8003 - val_mean_squared_error: 8.8003\n",
            "Epoch 164/300\n",
            "11224/11224 [==============================] - 6s 545us/sample - loss: 6.3565 - mean_squared_error: 6.3565 - val_loss: 9.0015 - val_mean_squared_error: 9.0015\n",
            "Epoch 165/300\n",
            "11224/11224 [==============================] - 6s 546us/sample - loss: 6.4185 - mean_squared_error: 6.4185 - val_loss: 9.5119 - val_mean_squared_error: 9.5119\n",
            "Epoch 166/300\n",
            "11224/11224 [==============================] - 6s 534us/sample - loss: 6.3335 - mean_squared_error: 6.3335 - val_loss: 8.9621 - val_mean_squared_error: 8.9621\n",
            "Epoch 167/300\n",
            "11224/11224 [==============================] - 6s 548us/sample - loss: 6.4126 - mean_squared_error: 6.4126 - val_loss: 10.7404 - val_mean_squared_error: 10.7404\n",
            "Epoch 168/300\n",
            "11224/11224 [==============================] - 6s 538us/sample - loss: 6.4032 - mean_squared_error: 6.4032 - val_loss: 8.7105 - val_mean_squared_error: 8.7105\n",
            "Epoch 169/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 6.3268 - mean_squared_error: 6.3268 - val_loss: 9.0662 - val_mean_squared_error: 9.0662\n",
            "Epoch 170/300\n",
            "11224/11224 [==============================] - 6s 542us/sample - loss: 6.4812 - mean_squared_error: 6.4812 - val_loss: 8.6048 - val_mean_squared_error: 8.6048\n",
            "Epoch 171/300\n",
            "11224/11224 [==============================] - 6s 549us/sample - loss: 6.3874 - mean_squared_error: 6.3874 - val_loss: 8.7327 - val_mean_squared_error: 8.7327\n",
            "Epoch 172/300\n",
            "11224/11224 [==============================] - 7s 587us/sample - loss: 6.4164 - mean_squared_error: 6.4164 - val_loss: 8.8576 - val_mean_squared_error: 8.8576\n",
            "Epoch 173/300\n",
            "11224/11224 [==============================] - 7s 582us/sample - loss: 6.4281 - mean_squared_error: 6.4281 - val_loss: 9.1770 - val_mean_squared_error: 9.1770\n",
            "Epoch 174/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 6.2404 - mean_squared_error: 6.2404 - val_loss: 8.8227 - val_mean_squared_error: 8.8227\n",
            "Epoch 175/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 6.3494 - mean_squared_error: 6.3494 - val_loss: 9.1181 - val_mean_squared_error: 9.1181\n",
            "Epoch 176/300\n",
            "11224/11224 [==============================] - 6s 566us/sample - loss: 6.2991 - mean_squared_error: 6.2991 - val_loss: 9.2921 - val_mean_squared_error: 9.2921\n",
            "Epoch 177/300\n",
            "11224/11224 [==============================] - 6s 576us/sample - loss: 6.3588 - mean_squared_error: 6.3588 - val_loss: 9.4609 - val_mean_squared_error: 9.4609\n",
            "Epoch 178/300\n",
            "11224/11224 [==============================] - 6s 568us/sample - loss: 6.4024 - mean_squared_error: 6.4024 - val_loss: 10.8043 - val_mean_squared_error: 10.8043\n",
            "Epoch 179/300\n",
            "11224/11224 [==============================] - 6s 564us/sample - loss: 6.3508 - mean_squared_error: 6.3508 - val_loss: 9.5929 - val_mean_squared_error: 9.5929\n",
            "Epoch 180/300\n",
            "11224/11224 [==============================] - 6s 575us/sample - loss: 6.4084 - mean_squared_error: 6.4084 - val_loss: 8.6909 - val_mean_squared_error: 8.6909\n",
            "Epoch 181/300\n",
            "11224/11224 [==============================] - 6s 574us/sample - loss: 6.0515 - mean_squared_error: 6.0515 - val_loss: 8.8053 - val_mean_squared_error: 8.8053\n",
            "Epoch 182/300\n",
            "11224/11224 [==============================] - 6s 561us/sample - loss: 6.2735 - mean_squared_error: 6.2735 - val_loss: 9.0267 - val_mean_squared_error: 9.0267\n",
            "Epoch 183/300\n",
            "11224/11224 [==============================] - 6s 568us/sample - loss: 6.2406 - mean_squared_error: 6.2406 - val_loss: 9.7252 - val_mean_squared_error: 9.7252\n",
            "Epoch 184/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 6.3768 - mean_squared_error: 6.3768 - val_loss: 9.7331 - val_mean_squared_error: 9.7331\n",
            "Epoch 185/300\n",
            "11224/11224 [==============================] - 6s 568us/sample - loss: 6.3558 - mean_squared_error: 6.3558 - val_loss: 9.2764 - val_mean_squared_error: 9.2764\n",
            "Epoch 186/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 6.2013 - mean_squared_error: 6.2013 - val_loss: 8.8600 - val_mean_squared_error: 8.8600\n",
            "Epoch 187/300\n",
            "11224/11224 [==============================] - 6s 562us/sample - loss: 6.3377 - mean_squared_error: 6.3377 - val_loss: 9.0824 - val_mean_squared_error: 9.0824\n",
            "Epoch 188/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 6.3259 - mean_squared_error: 6.3259 - val_loss: 8.6587 - val_mean_squared_error: 8.6587\n",
            "Epoch 189/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 6.0810 - mean_squared_error: 6.0810 - val_loss: 9.2363 - val_mean_squared_error: 9.2363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTya_ZwmQ4Uj",
        "colab_type": "text"
      },
      "source": [
        "## Try a different model architecture\n",
        "\n",
        "We didn't have much success improving our mouth_center_bottom_lip predictions using our existing network structure. We decided to go deeper by adding an additional convolution and pooling layer, but still applying the same increasing filter depth and dropout rate strategy from before. We found that with this deeper network we showed good validation results by increasing starting filter depth. This would tend to increase the number of parameters we are training and lead to additional overfitting. We counteracted this by reducing the hidden units in our fully connected layers from 500 to 200, and increasing our dropout rate. Using this method we were able to reduce our validation RMSE scores for our worst two keypoints (i.e., mouth_center_bottom_lip and nose_tip).  The resulting validation scores for our improved models were:\n",
        "\n",
        "\n",
        "\n",
        "1.   mouth_center_bottom_lip 2.90  -->  **2.54**\n",
        "2.   nose_tip  2.65 --> **2.41**\n",
        "\n",
        "With improving only our two worst performing specialist models, we scored against our test set and showed public/private Kaggle scores of 2.09/1.71. In fact the private score was good enough to knock us into 9th place on the private leaderboard.  Not bad!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYeLVOt4Q4ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_4layer_spec_model(start_filter, d, step, fc1=500, fc2=500):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model \n",
        "    '''\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "#     cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter, (2, 2), padding='same', \n",
        "                                         input_shape=(96, 96, 1),activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (4, 4), padding='same', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+2*step))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*5, (5, 5), padding='same', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc1))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc2))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(2, name='Specialist'))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model\n",
        "  \n",
        "def train_improved_4layer_specialists(df, keypoint, sf=12, doi=0.0, dos=0.1, lrf=10, fc1=500, fc2=500):\n",
        "  # Use an early stopping callback and our timing callback\n",
        "  early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                                patience=100, mode='auto', restore_best_weights = True)\n",
        "  time_callback = TimeHistory()\n",
        "  \n",
        "  \n",
        "  # Use the adam optimizer with the default learning rate\n",
        "  adam = optimizers.Adam(lr=0.001*lrf, beta_1=0.9, beta_2=0.999)\n",
        "    \n",
        "  # Create and Compile the model\n",
        "  model = create_4layer_spec_model(sf, doi, dos, fc1=fc1, fc2=fc2)\n",
        "  model.compile(\n",
        "      optimizer=adam,\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['mean_squared_error'])\n",
        "  \n",
        "  \n",
        "  # Subset the image/keypoint dataframe to just have the training locations for \n",
        "  # the keypoints of interest\n",
        "  df_keypoint = df[[keypoint + '_x',keypoint+'_y', 'Image']]\n",
        "  \n",
        "  # Print the remaining columns as a QA check\n",
        "  print(df_keypoint.columns)\n",
        "  \n",
        "  # Drop the NA values\n",
        "  df_keypoint = df_keypoint.dropna().reset_index(drop=True)\n",
        "  \n",
        "  # Grab the last column - that is our image data for X matrix\n",
        "  train_X = df_keypoint.iloc[:, -1]\n",
        "\n",
        "  # Convert from a series of arrays to an NDarray\n",
        "  train_X = np.array([x.reshape(96,96,1) for x in train_X])\n",
        " \n",
        "  \n",
        "  # Get the Y data for just the specified keypoint\n",
        "  train_y = np.array(df_keypoint.iloc[:,:-1])\n",
        " \n",
        "  history = model.fit(\n",
        "      train_X.astype(np.float32), train_y.astype(np.float32),\n",
        "      epochs=300,\n",
        "      validation_split=0.20,\n",
        "      callbacks=[time_callback, early_stop])\n",
        "  times = time_callback.times\n",
        "\n",
        "  # Convert to dataframe\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "  hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "  hist['times'] = times\n",
        "  hist['starting_filter'] = sf\n",
        "  hist['layers'] = 4\n",
        "  hist['pooling'] = 'yes'\n",
        "  hist['fc_layer1'] = fc1\n",
        "  hist['fc_layer2'] = fc2\n",
        "  hist['activation'] = 'relu'\n",
        "  hist['optimizer'] = 'adam'\n",
        "  hist['lrate'] = adam.get_config()['learning_rate']\n",
        "  hist['dropout_initial'] = doi\n",
        "  hist['dropout_step'] = dos\n",
        "  hist['batch_norm'] = 1\n",
        "  hist['bias'] = 0\n",
        "  hist['stride'] = 1\n",
        "  hist['flipped'] = 1.0\n",
        "  hist['keypoint'] = keypoint\n",
        "\n",
        "  # Re-pickle after every model to retain progress\n",
        "  hist.to_pickle(drive_path+\"OutputData/4l_spec_{}_d{}_s{}_sf{}_lrf{}_fc1{}_fc2{}_kern2345.pkl\".format(keypoint, doi, dos, sf, lrf, fc1, fc2))\n",
        "\n",
        "  # Save models.\n",
        "  filename = \"cnn_4l_spec_{}_d{}_s{}_sf{}_lrfactor{}_flipped_100_fc1{}_fc2{}_kern2345\".format(keypoint, doi, dos, sf, lrf, fc1, fc2)\n",
        "  model.save(drive_path+\"Models/\"+filename+\".h5\")\n",
        "  return hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NKjeEiZR1To",
        "colab_type": "code",
        "outputId": "801f1a6c-5a5b-47b8-f879-c773075b413f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "improved_specialist_df = train_improved_4layer_specialists(df, 'right_eye_center', sf=6, doi=0.00, dos=0.17, lrf=10, fc1=200, fc2=200)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0803 12:51:07.190214 139767557695360 nn_ops.py:4224] Large dropout rate: 0.51 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0803 12:51:07.319262 139767557695360 nn_ops.py:4224] Large dropout rate: 0.51 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 96, 96, 6)         24        \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 96, 96, 6)         24        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 48, 48, 6)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 48, 48, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 12)        648       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 48, 48, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 24)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 24, 24, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 12, 12, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 30)        18000     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 30)        120       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 30)          0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 6, 6, 30)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1080)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 200)               216200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 200)               40200     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "Specialist (Dense)           (None, 2)                 402       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 281,970\n",
            "Trainable params: 281,026\n",
            "Non-trainable params: 944\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Index(['right_eye_center_x', 'right_eye_center_y', 'Image'], dtype='object')\n",
            "Train on 11255 samples, validate on 2814 samples\n",
            "Epoch 1/300\n",
            "11255/11255 [==============================] - 7s 631us/sample - loss: 57.6686 - mean_squared_error: 57.6686 - val_loss: 12.5177 - val_mean_squared_error: 12.5177\n",
            "Epoch 2/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 12.9025 - mean_squared_error: 12.9025 - val_loss: 10.8997 - val_mean_squared_error: 10.8997\n",
            "Epoch 3/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 11.9535 - mean_squared_error: 11.9535 - val_loss: 10.3473 - val_mean_squared_error: 10.3473\n",
            "Epoch 4/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 10.9291 - mean_squared_error: 10.9291 - val_loss: 9.7213 - val_mean_squared_error: 9.7213\n",
            "Epoch 5/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 9.7473 - mean_squared_error: 9.7473 - val_loss: 8.1200 - val_mean_squared_error: 8.1200\n",
            "Epoch 6/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 8.6950 - mean_squared_error: 8.6950 - val_loss: 7.7472 - val_mean_squared_error: 7.7472\n",
            "Epoch 7/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 7.7512 - mean_squared_error: 7.7512 - val_loss: 7.7653 - val_mean_squared_error: 7.7653\n",
            "Epoch 8/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 7.3672 - mean_squared_error: 7.3672 - val_loss: 6.2554 - val_mean_squared_error: 6.2554\n",
            "Epoch 9/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 6.9197 - mean_squared_error: 6.9197 - val_loss: 6.0314 - val_mean_squared_error: 6.0314\n",
            "Epoch 10/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 6.5670 - mean_squared_error: 6.5670 - val_loss: 5.9496 - val_mean_squared_error: 5.9496\n",
            "Epoch 11/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 6.4719 - mean_squared_error: 6.4719 - val_loss: 6.0739 - val_mean_squared_error: 6.0739\n",
            "Epoch 12/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 6.0658 - mean_squared_error: 6.0658 - val_loss: 5.1404 - val_mean_squared_error: 5.1404\n",
            "Epoch 13/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 5.5122 - mean_squared_error: 5.5122 - val_loss: 5.7279 - val_mean_squared_error: 5.7279\n",
            "Epoch 14/300\n",
            "11255/11255 [==============================] - 6s 532us/sample - loss: 5.8288 - mean_squared_error: 5.8288 - val_loss: 5.1896 - val_mean_squared_error: 5.1896\n",
            "Epoch 15/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 5.4311 - mean_squared_error: 5.4311 - val_loss: 5.0095 - val_mean_squared_error: 5.0095\n",
            "Epoch 16/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 5.1971 - mean_squared_error: 5.1970 - val_loss: 5.2577 - val_mean_squared_error: 5.2577\n",
            "Epoch 17/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 5.2078 - mean_squared_error: 5.2078 - val_loss: 6.2596 - val_mean_squared_error: 6.2596\n",
            "Epoch 18/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 4.9780 - mean_squared_error: 4.9780 - val_loss: 5.1142 - val_mean_squared_error: 5.1142\n",
            "Epoch 19/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 4.9253 - mean_squared_error: 4.9253 - val_loss: 5.0111 - val_mean_squared_error: 5.0111\n",
            "Epoch 20/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 4.7234 - mean_squared_error: 4.7234 - val_loss: 4.7875 - val_mean_squared_error: 4.7875\n",
            "Epoch 21/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 4.7322 - mean_squared_error: 4.7322 - val_loss: 5.0446 - val_mean_squared_error: 5.0446\n",
            "Epoch 22/300\n",
            "11255/11255 [==============================] - 6s 511us/sample - loss: 4.7285 - mean_squared_error: 4.7285 - val_loss: 5.0980 - val_mean_squared_error: 5.0980\n",
            "Epoch 23/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 4.5043 - mean_squared_error: 4.5042 - val_loss: 5.0782 - val_mean_squared_error: 5.0782\n",
            "Epoch 24/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 4.2687 - mean_squared_error: 4.2687 - val_loss: 4.6903 - val_mean_squared_error: 4.6903\n",
            "Epoch 25/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 4.3794 - mean_squared_error: 4.3794 - val_loss: 5.3298 - val_mean_squared_error: 5.3298\n",
            "Epoch 26/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 4.4253 - mean_squared_error: 4.4253 - val_loss: 4.5522 - val_mean_squared_error: 4.5522\n",
            "Epoch 27/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 4.2120 - mean_squared_error: 4.2120 - val_loss: 4.8602 - val_mean_squared_error: 4.8602\n",
            "Epoch 28/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 4.2235 - mean_squared_error: 4.2235 - val_loss: 4.7224 - val_mean_squared_error: 4.7224\n",
            "Epoch 29/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 3.9539 - mean_squared_error: 3.9539 - val_loss: 4.6617 - val_mean_squared_error: 4.6617\n",
            "Epoch 30/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 3.9273 - mean_squared_error: 3.9273 - val_loss: 4.8068 - val_mean_squared_error: 4.8068\n",
            "Epoch 31/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 3.8313 - mean_squared_error: 3.8313 - val_loss: 4.5707 - val_mean_squared_error: 4.5707\n",
            "Epoch 32/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 3.9620 - mean_squared_error: 3.9620 - val_loss: 5.1792 - val_mean_squared_error: 5.1792\n",
            "Epoch 33/300\n",
            "11255/11255 [==============================] - 6s 513us/sample - loss: 3.8257 - mean_squared_error: 3.8257 - val_loss: 4.5185 - val_mean_squared_error: 4.5185\n",
            "Epoch 34/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 3.6065 - mean_squared_error: 3.6065 - val_loss: 5.3235 - val_mean_squared_error: 5.3235\n",
            "Epoch 35/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 3.6556 - mean_squared_error: 3.6556 - val_loss: 4.6279 - val_mean_squared_error: 4.6279\n",
            "Epoch 36/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 3.5734 - mean_squared_error: 3.5734 - val_loss: 5.0400 - val_mean_squared_error: 5.0400\n",
            "Epoch 37/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 3.5005 - mean_squared_error: 3.5005 - val_loss: 4.1442 - val_mean_squared_error: 4.1442\n",
            "Epoch 38/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 3.4587 - mean_squared_error: 3.4587 - val_loss: 5.0875 - val_mean_squared_error: 5.0875\n",
            "Epoch 39/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 3.4134 - mean_squared_error: 3.4134 - val_loss: 4.4585 - val_mean_squared_error: 4.4585\n",
            "Epoch 40/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 3.4388 - mean_squared_error: 3.4388 - val_loss: 4.4652 - val_mean_squared_error: 4.4652\n",
            "Epoch 41/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 3.4401 - mean_squared_error: 3.4401 - val_loss: 4.2628 - val_mean_squared_error: 4.2628\n",
            "Epoch 42/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 3.5936 - mean_squared_error: 3.5936 - val_loss: 4.3991 - val_mean_squared_error: 4.3991\n",
            "Epoch 43/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 3.3428 - mean_squared_error: 3.3428 - val_loss: 4.7982 - val_mean_squared_error: 4.7982\n",
            "Epoch 44/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 3.4644 - mean_squared_error: 3.4644 - val_loss: 4.6041 - val_mean_squared_error: 4.6041\n",
            "Epoch 45/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 3.2534 - mean_squared_error: 3.2534 - val_loss: 4.2920 - val_mean_squared_error: 4.2920\n",
            "Epoch 46/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 3.1482 - mean_squared_error: 3.1482 - val_loss: 4.1371 - val_mean_squared_error: 4.1371\n",
            "Epoch 47/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 3.2500 - mean_squared_error: 3.2500 - val_loss: 4.5050 - val_mean_squared_error: 4.5050\n",
            "Epoch 48/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 3.1958 - mean_squared_error: 3.1958 - val_loss: 4.3486 - val_mean_squared_error: 4.3486\n",
            "Epoch 49/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 3.2327 - mean_squared_error: 3.2327 - val_loss: 4.5436 - val_mean_squared_error: 4.5436\n",
            "Epoch 50/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 3.2156 - mean_squared_error: 3.2156 - val_loss: 5.2236 - val_mean_squared_error: 5.2236\n",
            "Epoch 51/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 3.2764 - mean_squared_error: 3.2764 - val_loss: 4.7821 - val_mean_squared_error: 4.7821\n",
            "Epoch 52/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 3.1770 - mean_squared_error: 3.1770 - val_loss: 5.1590 - val_mean_squared_error: 5.1590\n",
            "Epoch 53/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 3.4550 - mean_squared_error: 3.4550 - val_loss: 4.4995 - val_mean_squared_error: 4.4995\n",
            "Epoch 54/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 3.0990 - mean_squared_error: 3.0990 - val_loss: 4.7466 - val_mean_squared_error: 4.7466\n",
            "Epoch 55/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 3.1991 - mean_squared_error: 3.1991 - val_loss: 4.9006 - val_mean_squared_error: 4.9006\n",
            "Epoch 56/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 3.1293 - mean_squared_error: 3.1293 - val_loss: 4.7691 - val_mean_squared_error: 4.7691\n",
            "Epoch 57/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 3.0320 - mean_squared_error: 3.0320 - val_loss: 4.3656 - val_mean_squared_error: 4.3656\n",
            "Epoch 58/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 3.0754 - mean_squared_error: 3.0754 - val_loss: 4.6651 - val_mean_squared_error: 4.6651\n",
            "Epoch 59/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 3.0745 - mean_squared_error: 3.0745 - val_loss: 4.5016 - val_mean_squared_error: 4.5016\n",
            "Epoch 60/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 3.0274 - mean_squared_error: 3.0274 - val_loss: 5.0127 - val_mean_squared_error: 5.0127\n",
            "Epoch 61/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 3.1107 - mean_squared_error: 3.1107 - val_loss: 4.6132 - val_mean_squared_error: 4.6132\n",
            "Epoch 62/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 3.0377 - mean_squared_error: 3.0377 - val_loss: 4.6049 - val_mean_squared_error: 4.6048\n",
            "Epoch 63/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 3.0467 - mean_squared_error: 3.0467 - val_loss: 4.1159 - val_mean_squared_error: 4.1159\n",
            "Epoch 64/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.8755 - mean_squared_error: 2.8755 - val_loss: 5.0304 - val_mean_squared_error: 5.0304\n",
            "Epoch 65/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.9141 - mean_squared_error: 2.9141 - val_loss: 4.5670 - val_mean_squared_error: 4.5670\n",
            "Epoch 66/300\n",
            "11255/11255 [==============================] - 6s 542us/sample - loss: 3.0051 - mean_squared_error: 3.0051 - val_loss: 4.3163 - val_mean_squared_error: 4.3163\n",
            "Epoch 67/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 2.9879 - mean_squared_error: 2.9879 - val_loss: 3.9154 - val_mean_squared_error: 3.9154\n",
            "Epoch 68/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.9043 - mean_squared_error: 2.9043 - val_loss: 4.4908 - val_mean_squared_error: 4.4908\n",
            "Epoch 69/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 3.0623 - mean_squared_error: 3.0623 - val_loss: 4.2206 - val_mean_squared_error: 4.2206\n",
            "Epoch 70/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.9197 - mean_squared_error: 2.9197 - val_loss: 4.3258 - val_mean_squared_error: 4.3258\n",
            "Epoch 71/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.9201 - mean_squared_error: 2.9201 - val_loss: 4.3942 - val_mean_squared_error: 4.3942\n",
            "Epoch 72/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.8608 - mean_squared_error: 2.8608 - val_loss: 4.6692 - val_mean_squared_error: 4.6692\n",
            "Epoch 73/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.8764 - mean_squared_error: 2.8764 - val_loss: 4.4902 - val_mean_squared_error: 4.4902\n",
            "Epoch 74/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.8068 - mean_squared_error: 2.8068 - val_loss: 4.4177 - val_mean_squared_error: 4.4177\n",
            "Epoch 75/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.9572 - mean_squared_error: 2.9572 - val_loss: 3.9241 - val_mean_squared_error: 3.9241\n",
            "Epoch 76/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.8310 - mean_squared_error: 2.8310 - val_loss: 4.1452 - val_mean_squared_error: 4.1452\n",
            "Epoch 77/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.9051 - mean_squared_error: 2.9051 - val_loss: 4.9660 - val_mean_squared_error: 4.9660\n",
            "Epoch 78/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.9020 - mean_squared_error: 2.9020 - val_loss: 4.1162 - val_mean_squared_error: 4.1162\n",
            "Epoch 79/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.7093 - mean_squared_error: 2.7093 - val_loss: 3.8819 - val_mean_squared_error: 3.8819\n",
            "Epoch 80/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.7719 - mean_squared_error: 2.7719 - val_loss: 4.2845 - val_mean_squared_error: 4.2845\n",
            "Epoch 81/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.8697 - mean_squared_error: 2.8697 - val_loss: 4.5114 - val_mean_squared_error: 4.5114\n",
            "Epoch 82/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.7480 - mean_squared_error: 2.7480 - val_loss: 4.2044 - val_mean_squared_error: 4.2044\n",
            "Epoch 83/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.9379 - mean_squared_error: 2.9379 - val_loss: 4.1151 - val_mean_squared_error: 4.1151\n",
            "Epoch 84/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.8293 - mean_squared_error: 2.8293 - val_loss: 4.5623 - val_mean_squared_error: 4.5623\n",
            "Epoch 85/300\n",
            "11255/11255 [==============================] - 6s 531us/sample - loss: 2.6614 - mean_squared_error: 2.6614 - val_loss: 4.3552 - val_mean_squared_error: 4.3552\n",
            "Epoch 86/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.6947 - mean_squared_error: 2.6947 - val_loss: 4.1107 - val_mean_squared_error: 4.1107\n",
            "Epoch 87/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.7694 - mean_squared_error: 2.7694 - val_loss: 4.4999 - val_mean_squared_error: 4.4999\n",
            "Epoch 88/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.7863 - mean_squared_error: 2.7863 - val_loss: 5.0527 - val_mean_squared_error: 5.0527\n",
            "Epoch 89/300\n",
            "11255/11255 [==============================] - 6s 530us/sample - loss: 2.7089 - mean_squared_error: 2.7089 - val_loss: 4.2896 - val_mean_squared_error: 4.2896\n",
            "Epoch 90/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.8022 - mean_squared_error: 2.8022 - val_loss: 4.5011 - val_mean_squared_error: 4.5011\n",
            "Epoch 91/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.7076 - mean_squared_error: 2.7076 - val_loss: 4.3344 - val_mean_squared_error: 4.3344\n",
            "Epoch 92/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.7094 - mean_squared_error: 2.7094 - val_loss: 4.5561 - val_mean_squared_error: 4.5561\n",
            "Epoch 93/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.6621 - mean_squared_error: 2.6621 - val_loss: 4.2729 - val_mean_squared_error: 4.2729\n",
            "Epoch 94/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 2.5892 - mean_squared_error: 2.5892 - val_loss: 4.1975 - val_mean_squared_error: 4.1975\n",
            "Epoch 95/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.6513 - mean_squared_error: 2.6513 - val_loss: 4.4094 - val_mean_squared_error: 4.4094\n",
            "Epoch 96/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.6726 - mean_squared_error: 2.6726 - val_loss: 4.1190 - val_mean_squared_error: 4.1190\n",
            "Epoch 97/300\n",
            "11255/11255 [==============================] - 6s 538us/sample - loss: 2.7628 - mean_squared_error: 2.7628 - val_loss: 3.9554 - val_mean_squared_error: 3.9554\n",
            "Epoch 98/300\n",
            "11255/11255 [==============================] - 6s 545us/sample - loss: 2.7142 - mean_squared_error: 2.7142 - val_loss: 3.8085 - val_mean_squared_error: 3.8085\n",
            "Epoch 99/300\n",
            "11255/11255 [==============================] - 6s 532us/sample - loss: 2.7162 - mean_squared_error: 2.7162 - val_loss: 4.1205 - val_mean_squared_error: 4.1205\n",
            "Epoch 100/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.7175 - mean_squared_error: 2.7175 - val_loss: 3.8589 - val_mean_squared_error: 3.8589\n",
            "Epoch 101/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.6457 - mean_squared_error: 2.6457 - val_loss: 4.3058 - val_mean_squared_error: 4.3058\n",
            "Epoch 102/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 2.6973 - mean_squared_error: 2.6973 - val_loss: 3.8532 - val_mean_squared_error: 3.8532\n",
            "Epoch 103/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.7695 - mean_squared_error: 2.7695 - val_loss: 3.8407 - val_mean_squared_error: 3.8407\n",
            "Epoch 104/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.6582 - mean_squared_error: 2.6582 - val_loss: 3.9816 - val_mean_squared_error: 3.9816\n",
            "Epoch 105/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.6371 - mean_squared_error: 2.6371 - val_loss: 4.3238 - val_mean_squared_error: 4.3238\n",
            "Epoch 106/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.6806 - mean_squared_error: 2.6806 - val_loss: 4.2072 - val_mean_squared_error: 4.2072\n",
            "Epoch 107/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.5759 - mean_squared_error: 2.5759 - val_loss: 4.1281 - val_mean_squared_error: 4.1281\n",
            "Epoch 108/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.5833 - mean_squared_error: 2.5833 - val_loss: 3.8492 - val_mean_squared_error: 3.8491\n",
            "Epoch 109/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 2.6431 - mean_squared_error: 2.6431 - val_loss: 3.9165 - val_mean_squared_error: 3.9165\n",
            "Epoch 110/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.6185 - mean_squared_error: 2.6185 - val_loss: 4.0472 - val_mean_squared_error: 4.0472\n",
            "Epoch 111/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.5767 - mean_squared_error: 2.5767 - val_loss: 4.9171 - val_mean_squared_error: 4.9171\n",
            "Epoch 112/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.7358 - mean_squared_error: 2.7358 - val_loss: 4.0409 - val_mean_squared_error: 4.0409\n",
            "Epoch 113/300\n",
            "11255/11255 [==============================] - 6s 530us/sample - loss: 2.6173 - mean_squared_error: 2.6173 - val_loss: 3.8717 - val_mean_squared_error: 3.8717\n",
            "Epoch 114/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.5616 - mean_squared_error: 2.5616 - val_loss: 4.0596 - val_mean_squared_error: 4.0596\n",
            "Epoch 115/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.5879 - mean_squared_error: 2.5879 - val_loss: 3.9469 - val_mean_squared_error: 3.9469\n",
            "Epoch 116/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 2.6032 - mean_squared_error: 2.6032 - val_loss: 4.3902 - val_mean_squared_error: 4.3902\n",
            "Epoch 117/300\n",
            "11255/11255 [==============================] - 6s 533us/sample - loss: 2.5439 - mean_squared_error: 2.5439 - val_loss: 4.2786 - val_mean_squared_error: 4.2786\n",
            "Epoch 118/300\n",
            "11255/11255 [==============================] - 6s 537us/sample - loss: 2.6208 - mean_squared_error: 2.6208 - val_loss: 3.9411 - val_mean_squared_error: 3.9411\n",
            "Epoch 119/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.5826 - mean_squared_error: 2.5826 - val_loss: 4.0571 - val_mean_squared_error: 4.0571\n",
            "Epoch 120/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.4739 - mean_squared_error: 2.4739 - val_loss: 4.0434 - val_mean_squared_error: 4.0434\n",
            "Epoch 121/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.5465 - mean_squared_error: 2.5465 - val_loss: 4.2316 - val_mean_squared_error: 4.2316\n",
            "Epoch 122/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.5662 - mean_squared_error: 2.5662 - val_loss: 4.0533 - val_mean_squared_error: 4.0533\n",
            "Epoch 123/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 2.6069 - mean_squared_error: 2.6069 - val_loss: 4.3122 - val_mean_squared_error: 4.3122\n",
            "Epoch 124/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.6267 - mean_squared_error: 2.6267 - val_loss: 4.5184 - val_mean_squared_error: 4.5184\n",
            "Epoch 125/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.5328 - mean_squared_error: 2.5328 - val_loss: 4.2345 - val_mean_squared_error: 4.2345\n",
            "Epoch 126/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.7442 - mean_squared_error: 2.7442 - val_loss: 4.2518 - val_mean_squared_error: 4.2518\n",
            "Epoch 127/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.5688 - mean_squared_error: 2.5688 - val_loss: 3.7697 - val_mean_squared_error: 3.7697\n",
            "Epoch 128/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.4819 - mean_squared_error: 2.4819 - val_loss: 4.0791 - val_mean_squared_error: 4.0791\n",
            "Epoch 129/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.6323 - mean_squared_error: 2.6323 - val_loss: 4.4048 - val_mean_squared_error: 4.4048\n",
            "Epoch 130/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.5711 - mean_squared_error: 2.5711 - val_loss: 4.4588 - val_mean_squared_error: 4.4588\n",
            "Epoch 131/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.6023 - mean_squared_error: 2.6023 - val_loss: 3.8083 - val_mean_squared_error: 3.8083\n",
            "Epoch 132/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.5203 - mean_squared_error: 2.5203 - val_loss: 4.0397 - val_mean_squared_error: 4.0397\n",
            "Epoch 133/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.4926 - mean_squared_error: 2.4926 - val_loss: 4.0015 - val_mean_squared_error: 4.0015\n",
            "Epoch 134/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.4854 - mean_squared_error: 2.4854 - val_loss: 4.3648 - val_mean_squared_error: 4.3648\n",
            "Epoch 135/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.4271 - mean_squared_error: 2.4271 - val_loss: 4.2690 - val_mean_squared_error: 4.2690\n",
            "Epoch 136/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.4578 - mean_squared_error: 2.4578 - val_loss: 4.3065 - val_mean_squared_error: 4.3065\n",
            "Epoch 137/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.4723 - mean_squared_error: 2.4723 - val_loss: 4.4479 - val_mean_squared_error: 4.4479\n",
            "Epoch 138/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.5347 - mean_squared_error: 2.5347 - val_loss: 4.1656 - val_mean_squared_error: 4.1656\n",
            "Epoch 139/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 2.5574 - mean_squared_error: 2.5574 - val_loss: 3.9922 - val_mean_squared_error: 3.9922\n",
            "Epoch 140/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.4162 - mean_squared_error: 2.4162 - val_loss: 4.0755 - val_mean_squared_error: 4.0755\n",
            "Epoch 141/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.4992 - mean_squared_error: 2.4992 - val_loss: 3.7968 - val_mean_squared_error: 3.7968\n",
            "Epoch 142/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.4373 - mean_squared_error: 2.4373 - val_loss: 3.7901 - val_mean_squared_error: 3.7901\n",
            "Epoch 143/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.4273 - mean_squared_error: 2.4273 - val_loss: 3.9541 - val_mean_squared_error: 3.9542\n",
            "Epoch 144/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.4610 - mean_squared_error: 2.4610 - val_loss: 3.9620 - val_mean_squared_error: 3.9620\n",
            "Epoch 145/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.5081 - mean_squared_error: 2.5081 - val_loss: 3.9726 - val_mean_squared_error: 3.9726\n",
            "Epoch 146/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.4247 - mean_squared_error: 2.4247 - val_loss: 4.0245 - val_mean_squared_error: 4.0245\n",
            "Epoch 147/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.5201 - mean_squared_error: 2.5201 - val_loss: 3.7527 - val_mean_squared_error: 3.7527\n",
            "Epoch 148/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 2.5093 - mean_squared_error: 2.5093 - val_loss: 4.0020 - val_mean_squared_error: 4.0020\n",
            "Epoch 149/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.4949 - mean_squared_error: 2.4949 - val_loss: 3.8418 - val_mean_squared_error: 3.8418\n",
            "Epoch 150/300\n",
            "11255/11255 [==============================] - 6s 533us/sample - loss: 2.4302 - mean_squared_error: 2.4302 - val_loss: 4.5640 - val_mean_squared_error: 4.5640\n",
            "Epoch 151/300\n",
            "11255/11255 [==============================] - 6s 532us/sample - loss: 2.4543 - mean_squared_error: 2.4543 - val_loss: 4.3430 - val_mean_squared_error: 4.3430\n",
            "Epoch 152/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.5025 - mean_squared_error: 2.5025 - val_loss: 4.1283 - val_mean_squared_error: 4.1283\n",
            "Epoch 153/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.4637 - mean_squared_error: 2.4637 - val_loss: 4.0968 - val_mean_squared_error: 4.0968\n",
            "Epoch 154/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.3790 - mean_squared_error: 2.3790 - val_loss: 4.4207 - val_mean_squared_error: 4.4207\n",
            "Epoch 155/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.4256 - mean_squared_error: 2.4256 - val_loss: 4.2156 - val_mean_squared_error: 4.2156\n",
            "Epoch 156/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.3785 - mean_squared_error: 2.3785 - val_loss: 4.2734 - val_mean_squared_error: 4.2734\n",
            "Epoch 157/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.4199 - mean_squared_error: 2.4199 - val_loss: 4.0719 - val_mean_squared_error: 4.0719\n",
            "Epoch 158/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.4132 - mean_squared_error: 2.4132 - val_loss: 3.7755 - val_mean_squared_error: 3.7755\n",
            "Epoch 159/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 2.3585 - mean_squared_error: 2.3585 - val_loss: 5.2528 - val_mean_squared_error: 5.2528\n",
            "Epoch 160/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.4490 - mean_squared_error: 2.4490 - val_loss: 4.0305 - val_mean_squared_error: 4.0305\n",
            "Epoch 161/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.3904 - mean_squared_error: 2.3904 - val_loss: 3.9162 - val_mean_squared_error: 3.9162\n",
            "Epoch 162/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.5542 - mean_squared_error: 2.5542 - val_loss: 3.7161 - val_mean_squared_error: 3.7161\n",
            "Epoch 163/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.5112 - mean_squared_error: 2.5112 - val_loss: 3.8276 - val_mean_squared_error: 3.8276\n",
            "Epoch 164/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.4961 - mean_squared_error: 2.4961 - val_loss: 4.0939 - val_mean_squared_error: 4.0939\n",
            "Epoch 165/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.4346 - mean_squared_error: 2.4346 - val_loss: 3.7515 - val_mean_squared_error: 3.7515\n",
            "Epoch 166/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.3453 - mean_squared_error: 2.3453 - val_loss: 4.1106 - val_mean_squared_error: 4.1106\n",
            "Epoch 167/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.4309 - mean_squared_error: 2.4309 - val_loss: 4.0544 - val_mean_squared_error: 4.0544\n",
            "Epoch 168/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.3809 - mean_squared_error: 2.3809 - val_loss: 3.8589 - val_mean_squared_error: 3.8589\n",
            "Epoch 169/300\n",
            "11255/11255 [==============================] - 6s 532us/sample - loss: 2.4479 - mean_squared_error: 2.4479 - val_loss: 4.1394 - val_mean_squared_error: 4.1394\n",
            "Epoch 170/300\n",
            "11255/11255 [==============================] - 6s 532us/sample - loss: 2.3966 - mean_squared_error: 2.3966 - val_loss: 4.3293 - val_mean_squared_error: 4.3293\n",
            "Epoch 171/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.4426 - mean_squared_error: 2.4426 - val_loss: 4.4020 - val_mean_squared_error: 4.4020\n",
            "Epoch 172/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.3104 - mean_squared_error: 2.3104 - val_loss: 3.9965 - val_mean_squared_error: 3.9965\n",
            "Epoch 173/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.3217 - mean_squared_error: 2.3217 - val_loss: 3.8735 - val_mean_squared_error: 3.8735\n",
            "Epoch 174/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.2492 - mean_squared_error: 2.2492 - val_loss: 4.3100 - val_mean_squared_error: 4.3100\n",
            "Epoch 175/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.2538 - mean_squared_error: 2.2538 - val_loss: 4.0912 - val_mean_squared_error: 4.0913\n",
            "Epoch 176/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.3416 - mean_squared_error: 2.3416 - val_loss: 4.1240 - val_mean_squared_error: 4.1240\n",
            "Epoch 177/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.3981 - mean_squared_error: 2.3981 - val_loss: 4.2328 - val_mean_squared_error: 4.2328\n",
            "Epoch 178/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.4179 - mean_squared_error: 2.4179 - val_loss: 4.2249 - val_mean_squared_error: 4.2249\n",
            "Epoch 179/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.3821 - mean_squared_error: 2.3821 - val_loss: 3.8381 - val_mean_squared_error: 3.8381\n",
            "Epoch 180/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.2476 - mean_squared_error: 2.2476 - val_loss: 4.2558 - val_mean_squared_error: 4.2558\n",
            "Epoch 181/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.2408 - mean_squared_error: 2.2408 - val_loss: 3.5797 - val_mean_squared_error: 3.5797\n",
            "Epoch 182/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.3052 - mean_squared_error: 2.3052 - val_loss: 3.6530 - val_mean_squared_error: 3.6530\n",
            "Epoch 183/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.3429 - mean_squared_error: 2.3429 - val_loss: 4.6574 - val_mean_squared_error: 4.6574\n",
            "Epoch 184/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.4109 - mean_squared_error: 2.4109 - val_loss: 3.8511 - val_mean_squared_error: 3.8511\n",
            "Epoch 185/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.2698 - mean_squared_error: 2.2698 - val_loss: 4.3126 - val_mean_squared_error: 4.3126\n",
            "Epoch 186/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.4104 - mean_squared_error: 2.4104 - val_loss: 3.8916 - val_mean_squared_error: 3.8916\n",
            "Epoch 187/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.4161 - mean_squared_error: 2.4161 - val_loss: 4.0222 - val_mean_squared_error: 4.0222\n",
            "Epoch 188/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.3818 - mean_squared_error: 2.3818 - val_loss: 4.1914 - val_mean_squared_error: 4.1914\n",
            "Epoch 189/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.3574 - mean_squared_error: 2.3574 - val_loss: 4.3456 - val_mean_squared_error: 4.3456\n",
            "Epoch 190/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.3316 - mean_squared_error: 2.3316 - val_loss: 4.0272 - val_mean_squared_error: 4.0272\n",
            "Epoch 191/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.3813 - mean_squared_error: 2.3813 - val_loss: 4.0170 - val_mean_squared_error: 4.0170\n",
            "Epoch 192/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.2942 - mean_squared_error: 2.2942 - val_loss: 4.1662 - val_mean_squared_error: 4.1662\n",
            "Epoch 193/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.3660 - mean_squared_error: 2.3660 - val_loss: 4.5209 - val_mean_squared_error: 4.5209\n",
            "Epoch 194/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 2.4208 - mean_squared_error: 2.4208 - val_loss: 3.6007 - val_mean_squared_error: 3.6007\n",
            "Epoch 195/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.2411 - mean_squared_error: 2.2411 - val_loss: 3.9377 - val_mean_squared_error: 3.9377\n",
            "Epoch 196/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.2336 - mean_squared_error: 2.2336 - val_loss: 4.1889 - val_mean_squared_error: 4.1889\n",
            "Epoch 197/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.2846 - mean_squared_error: 2.2846 - val_loss: 3.8373 - val_mean_squared_error: 3.8373\n",
            "Epoch 198/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.2646 - mean_squared_error: 2.2646 - val_loss: 3.6229 - val_mean_squared_error: 3.6229\n",
            "Epoch 199/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.2997 - mean_squared_error: 2.2997 - val_loss: 4.1516 - val_mean_squared_error: 4.1516\n",
            "Epoch 200/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.2945 - mean_squared_error: 2.2945 - val_loss: 4.0148 - val_mean_squared_error: 4.0148\n",
            "Epoch 201/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.2934 - mean_squared_error: 2.2934 - val_loss: 3.7721 - val_mean_squared_error: 3.7721\n",
            "Epoch 202/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.3973 - mean_squared_error: 2.3973 - val_loss: 4.1211 - val_mean_squared_error: 4.1211\n",
            "Epoch 203/300\n",
            "11255/11255 [==============================] - 6s 533us/sample - loss: 2.3559 - mean_squared_error: 2.3559 - val_loss: 4.5141 - val_mean_squared_error: 4.5141\n",
            "Epoch 204/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.2112 - mean_squared_error: 2.2112 - val_loss: 4.0796 - val_mean_squared_error: 4.0796\n",
            "Epoch 205/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.2607 - mean_squared_error: 2.2607 - val_loss: 3.9760 - val_mean_squared_error: 3.9760\n",
            "Epoch 206/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.2213 - mean_squared_error: 2.2213 - val_loss: 4.0615 - val_mean_squared_error: 4.0615\n",
            "Epoch 207/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.3080 - mean_squared_error: 2.3080 - val_loss: 4.0474 - val_mean_squared_error: 4.0474\n",
            "Epoch 208/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.2931 - mean_squared_error: 2.2931 - val_loss: 4.0708 - val_mean_squared_error: 4.0708\n",
            "Epoch 209/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.3264 - mean_squared_error: 2.3264 - val_loss: 4.6529 - val_mean_squared_error: 4.6529\n",
            "Epoch 210/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.3247 - mean_squared_error: 2.3247 - val_loss: 4.2700 - val_mean_squared_error: 4.2700\n",
            "Epoch 211/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.2599 - mean_squared_error: 2.2599 - val_loss: 3.8273 - val_mean_squared_error: 3.8273\n",
            "Epoch 212/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.2452 - mean_squared_error: 2.2452 - val_loss: 4.3433 - val_mean_squared_error: 4.3433\n",
            "Epoch 213/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.2947 - mean_squared_error: 2.2947 - val_loss: 4.0639 - val_mean_squared_error: 4.0639\n",
            "Epoch 214/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.2777 - mean_squared_error: 2.2777 - val_loss: 4.1097 - val_mean_squared_error: 4.1097\n",
            "Epoch 215/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 2.2483 - mean_squared_error: 2.2483 - val_loss: 3.7089 - val_mean_squared_error: 3.7089\n",
            "Epoch 216/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.2243 - mean_squared_error: 2.2243 - val_loss: 3.9694 - val_mean_squared_error: 3.9694\n",
            "Epoch 217/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.2535 - mean_squared_error: 2.2535 - val_loss: 4.1204 - val_mean_squared_error: 4.1204\n",
            "Epoch 218/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.2761 - mean_squared_error: 2.2761 - val_loss: 3.8052 - val_mean_squared_error: 3.8052\n",
            "Epoch 219/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.3152 - mean_squared_error: 2.3152 - val_loss: 3.8239 - val_mean_squared_error: 3.8239\n",
            "Epoch 220/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.2504 - mean_squared_error: 2.2504 - val_loss: 3.5777 - val_mean_squared_error: 3.5777\n",
            "Epoch 221/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.2732 - mean_squared_error: 2.2732 - val_loss: 4.0029 - val_mean_squared_error: 4.0029\n",
            "Epoch 222/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.2424 - mean_squared_error: 2.2424 - val_loss: 3.9227 - val_mean_squared_error: 3.9227\n",
            "Epoch 223/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.3679 - mean_squared_error: 2.3679 - val_loss: 4.0358 - val_mean_squared_error: 4.0358\n",
            "Epoch 224/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.2240 - mean_squared_error: 2.2240 - val_loss: 3.9755 - val_mean_squared_error: 3.9755\n",
            "Epoch 225/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.2903 - mean_squared_error: 2.2903 - val_loss: 3.9534 - val_mean_squared_error: 3.9534\n",
            "Epoch 226/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.2353 - mean_squared_error: 2.2353 - val_loss: 4.0446 - val_mean_squared_error: 4.0446\n",
            "Epoch 227/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.2288 - mean_squared_error: 2.2288 - val_loss: 4.0238 - val_mean_squared_error: 4.0238\n",
            "Epoch 228/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.2011 - mean_squared_error: 2.2011 - val_loss: 3.8417 - val_mean_squared_error: 3.8417\n",
            "Epoch 229/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.3146 - mean_squared_error: 2.3146 - val_loss: 3.9623 - val_mean_squared_error: 3.9623\n",
            "Epoch 230/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.3222 - mean_squared_error: 2.3222 - val_loss: 4.0696 - val_mean_squared_error: 4.0696\n",
            "Epoch 231/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.2320 - mean_squared_error: 2.2320 - val_loss: 3.8531 - val_mean_squared_error: 3.8531\n",
            "Epoch 232/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.1430 - mean_squared_error: 2.1430 - val_loss: 4.0295 - val_mean_squared_error: 4.0295\n",
            "Epoch 233/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.1775 - mean_squared_error: 2.1775 - val_loss: 3.9926 - val_mean_squared_error: 3.9926\n",
            "Epoch 234/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.2737 - mean_squared_error: 2.2737 - val_loss: 3.8510 - val_mean_squared_error: 3.8510\n",
            "Epoch 235/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 2.1765 - mean_squared_error: 2.1765 - val_loss: 3.7103 - val_mean_squared_error: 3.7103\n",
            "Epoch 236/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.1867 - mean_squared_error: 2.1867 - val_loss: 3.8198 - val_mean_squared_error: 3.8198\n",
            "Epoch 237/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.2756 - mean_squared_error: 2.2756 - val_loss: 4.1688 - val_mean_squared_error: 4.1688\n",
            "Epoch 238/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.1816 - mean_squared_error: 2.1816 - val_loss: 3.7136 - val_mean_squared_error: 3.7136\n",
            "Epoch 239/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.1418 - mean_squared_error: 2.1418 - val_loss: 3.7531 - val_mean_squared_error: 3.7531\n",
            "Epoch 240/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 2.2566 - mean_squared_error: 2.2566 - val_loss: 4.1413 - val_mean_squared_error: 4.1413\n",
            "Epoch 241/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.3013 - mean_squared_error: 2.3013 - val_loss: 3.7208 - val_mean_squared_error: 3.7208\n",
            "Epoch 242/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.2814 - mean_squared_error: 2.2814 - val_loss: 3.8641 - val_mean_squared_error: 3.8641\n",
            "Epoch 243/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.2913 - mean_squared_error: 2.2913 - val_loss: 3.7470 - val_mean_squared_error: 3.7470\n",
            "Epoch 244/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.1357 - mean_squared_error: 2.1357 - val_loss: 3.7520 - val_mean_squared_error: 3.7520\n",
            "Epoch 245/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.1484 - mean_squared_error: 2.1484 - val_loss: 4.0690 - val_mean_squared_error: 4.0690\n",
            "Epoch 246/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.2483 - mean_squared_error: 2.2483 - val_loss: 3.5662 - val_mean_squared_error: 3.5662\n",
            "Epoch 247/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.1501 - mean_squared_error: 2.1501 - val_loss: 3.7565 - val_mean_squared_error: 3.7565\n",
            "Epoch 248/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.1249 - mean_squared_error: 2.1249 - val_loss: 3.7043 - val_mean_squared_error: 3.7043\n",
            "Epoch 249/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.2725 - mean_squared_error: 2.2725 - val_loss: 3.8496 - val_mean_squared_error: 3.8496\n",
            "Epoch 250/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.2018 - mean_squared_error: 2.2018 - val_loss: 3.7849 - val_mean_squared_error: 3.7849\n",
            "Epoch 251/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.1968 - mean_squared_error: 2.1968 - val_loss: 3.6478 - val_mean_squared_error: 3.6478\n",
            "Epoch 252/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 2.1864 - mean_squared_error: 2.1864 - val_loss: 3.9898 - val_mean_squared_error: 3.9898\n",
            "Epoch 253/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.2830 - mean_squared_error: 2.2830 - val_loss: 3.8394 - val_mean_squared_error: 3.8394\n",
            "Epoch 254/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.1840 - mean_squared_error: 2.1840 - val_loss: 3.7943 - val_mean_squared_error: 3.7943\n",
            "Epoch 255/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.2958 - mean_squared_error: 2.2958 - val_loss: 3.8859 - val_mean_squared_error: 3.8859\n",
            "Epoch 256/300\n",
            "11255/11255 [==============================] - 6s 535us/sample - loss: 2.2748 - mean_squared_error: 2.2748 - val_loss: 3.9087 - val_mean_squared_error: 3.9087\n",
            "Epoch 257/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.1616 - mean_squared_error: 2.1616 - val_loss: 4.0217 - val_mean_squared_error: 4.0217\n",
            "Epoch 258/300\n",
            "11255/11255 [==============================] - 6s 512us/sample - loss: 2.1202 - mean_squared_error: 2.1202 - val_loss: 3.8737 - val_mean_squared_error: 3.8737\n",
            "Epoch 259/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 2.1036 - mean_squared_error: 2.1036 - val_loss: 3.9986 - val_mean_squared_error: 3.9986\n",
            "Epoch 260/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.0837 - mean_squared_error: 2.0837 - val_loss: 3.8155 - val_mean_squared_error: 3.8155\n",
            "Epoch 261/300\n",
            "11255/11255 [==============================] - 6s 513us/sample - loss: 2.1617 - mean_squared_error: 2.1617 - val_loss: 4.0568 - val_mean_squared_error: 4.0568\n",
            "Epoch 262/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.0412 - mean_squared_error: 2.0412 - val_loss: 4.0513 - val_mean_squared_error: 4.0513\n",
            "Epoch 263/300\n",
            "11255/11255 [==============================] - 6s 513us/sample - loss: 2.2575 - mean_squared_error: 2.2575 - val_loss: 3.8069 - val_mean_squared_error: 3.8069\n",
            "Epoch 264/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.1626 - mean_squared_error: 2.1626 - val_loss: 4.0120 - val_mean_squared_error: 4.0120\n",
            "Epoch 265/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 2.2258 - mean_squared_error: 2.2258 - val_loss: 3.6330 - val_mean_squared_error: 3.6330\n",
            "Epoch 266/300\n",
            "11255/11255 [==============================] - 6s 509us/sample - loss: 2.2272 - mean_squared_error: 2.2272 - val_loss: 4.3292 - val_mean_squared_error: 4.3292\n",
            "Epoch 267/300\n",
            "11255/11255 [==============================] - 6s 507us/sample - loss: 2.1485 - mean_squared_error: 2.1485 - val_loss: 3.5549 - val_mean_squared_error: 3.5549\n",
            "Epoch 268/300\n",
            "11255/11255 [==============================] - 6s 509us/sample - loss: 2.1857 - mean_squared_error: 2.1857 - val_loss: 3.6797 - val_mean_squared_error: 3.6797\n",
            "Epoch 269/300\n",
            "11255/11255 [==============================] - 6s 511us/sample - loss: 2.1618 - mean_squared_error: 2.1618 - val_loss: 3.8243 - val_mean_squared_error: 3.8243\n",
            "Epoch 270/300\n",
            "11255/11255 [==============================] - 6s 508us/sample - loss: 2.1036 - mean_squared_error: 2.1036 - val_loss: 3.9583 - val_mean_squared_error: 3.9583\n",
            "Epoch 271/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.1534 - mean_squared_error: 2.1534 - val_loss: 3.6899 - val_mean_squared_error: 3.6899\n",
            "Epoch 272/300\n",
            "11255/11255 [==============================] - 6s 509us/sample - loss: 2.1297 - mean_squared_error: 2.1297 - val_loss: 4.1665 - val_mean_squared_error: 4.1665\n",
            "Epoch 273/300\n",
            "11255/11255 [==============================] - 6s 506us/sample - loss: 2.1903 - mean_squared_error: 2.1903 - val_loss: 4.1854 - val_mean_squared_error: 4.1854\n",
            "Epoch 274/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.2147 - mean_squared_error: 2.2147 - val_loss: 3.8477 - val_mean_squared_error: 3.8477\n",
            "Epoch 275/300\n",
            "11255/11255 [==============================] - 6s 510us/sample - loss: 2.1409 - mean_squared_error: 2.1409 - val_loss: 4.4374 - val_mean_squared_error: 4.4374\n",
            "Epoch 276/300\n",
            "11255/11255 [==============================] - 6s 509us/sample - loss: 2.0631 - mean_squared_error: 2.0631 - val_loss: 3.9420 - val_mean_squared_error: 3.9420\n",
            "Epoch 277/300\n",
            "11255/11255 [==============================] - 6s 510us/sample - loss: 2.1096 - mean_squared_error: 2.1096 - val_loss: 3.9827 - val_mean_squared_error: 3.9827\n",
            "Epoch 278/300\n",
            "11255/11255 [==============================] - 6s 511us/sample - loss: 2.2585 - mean_squared_error: 2.2585 - val_loss: 3.9206 - val_mean_squared_error: 3.9206\n",
            "Epoch 279/300\n",
            "11255/11255 [==============================] - 6s 512us/sample - loss: 2.0648 - mean_squared_error: 2.0648 - val_loss: 3.6780 - val_mean_squared_error: 3.6780\n",
            "Epoch 280/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.0886 - mean_squared_error: 2.0886 - val_loss: 3.7733 - val_mean_squared_error: 3.7733\n",
            "Epoch 281/300\n",
            "11255/11255 [==============================] - 6s 536us/sample - loss: 2.1805 - mean_squared_error: 2.1805 - val_loss: 3.9237 - val_mean_squared_error: 3.9237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xIdB8ovZASW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_2layer_spec_model(start_filter, d, step, fc1=500, fc2=500):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model \n",
        "    '''\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "#     cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='same', \n",
        "                                         input_shape=(96, 96, 1),activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (5, 5), padding='same', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc1))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc2))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(2, name='Specialist'))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model\n",
        "  \n",
        "def train_improved_2layer_specialists(df, keypoint, sf=16, doi=0.0, dos=0.0, lrf=10, fc1=200, fc2=200):\n",
        "  # Use an early stopping callback and our timing callback\n",
        "  early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                                patience=100, mode='auto', restore_best_weights = True)\n",
        "  time_callback = TimeHistory()\n",
        "  \n",
        "  \n",
        "  # Use the adam optimizer with the default learning rate\n",
        "  adam = optimizers.Adam(lr=0.001*lrf, beta_1=0.9, beta_2=0.999)\n",
        "    \n",
        "  # Create and Compile the model\n",
        "  model = create_2layer_spec_model(sf, doi, dos, fc1=fc1, fc2=fc2)\n",
        "  model.compile(\n",
        "      optimizer=adam,\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['mean_squared_error'])\n",
        "  \n",
        "  \n",
        "  # Subset the image/keypoint dataframe to just have the training locations for \n",
        "  # the keypoints of interest\n",
        "  df_keypoint = df[[keypoint + '_x',keypoint+'_y', 'Image']]\n",
        "  \n",
        "  # Print the remaining columns as a QA check\n",
        "  print(df_keypoint.columns)\n",
        "  \n",
        "  # Drop the NA values\n",
        "  df_keypoint = df_keypoint.dropna().reset_index(drop=True)\n",
        "  \n",
        "  # Grab the last column - that is our image data for X matrix\n",
        "  train_X = df_keypoint.iloc[:, -1]\n",
        "\n",
        "  # Convert from a series of arrays to an NDarray\n",
        "  train_X = np.array([x.reshape(96,96,1) for x in train_X])\n",
        " \n",
        "  \n",
        "  # Get the Y data for just the specified keypoint\n",
        "  train_y = np.array(df_keypoint.iloc[:,:-1])\n",
        " \n",
        "  history = model.fit(\n",
        "      train_X.astype(np.float32), train_y.astype(np.float32),\n",
        "      epochs=300,\n",
        "      validation_split=0.20,\n",
        "      callbacks=[time_callback, early_stop])\n",
        "  times = time_callback.times\n",
        "\n",
        "  # Convert to dataframe\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "  hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "  hist['times'] = times\n",
        "  hist['starting_filter'] = sf\n",
        "  hist['layers'] = 4\n",
        "  hist['pooling'] = 'yes'\n",
        "  hist['fc_layer1'] = fc1\n",
        "  hist['fc_layer2'] = fc2\n",
        "  hist['activation'] = 'relu'\n",
        "  hist['optimizer'] = 'adam'\n",
        "  hist['lrate'] = adam.get_config()['learning_rate']\n",
        "  hist['dropout_initial'] = doi\n",
        "  hist['dropout_step'] = dos\n",
        "  hist['batch_norm'] = 1\n",
        "  hist['bias'] = 0\n",
        "  hist['stride'] = 1\n",
        "  hist['flipped'] = 1.0\n",
        "  hist['keypoint'] = keypoint\n",
        "\n",
        "  # Re-pickle after every model to retain progress\n",
        "  hist.to_pickle(drive_path+\"OutputData/4l_spec_{}_d{}_s{}_sf{}_lrf{}_fc1{}_fc2{}_kern2345.pkl\".format(keypoint, doi, dos, sf, lrf, fc1, fc2))\n",
        "\n",
        "  # Save models.\n",
        "  filename = \"cnn_4l_spec_{}_d{}_s{}_sf{}_lrfactor{}_flipped_100_fc1{}_fc2{}_kern2334\".format(keypoint, doi, dos, sf, lrf, fc1, fc2)\n",
        "  model.save(drive_path+\"Models/\"+filename+\".h5\")\n",
        "  return hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR6dz4FC4MPT",
        "colab_type": "code",
        "outputId": "3c12d146-0531-41f5-9283-ca55b43c2435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "improved_specialist_df = train_improved_2layer_specialists(df, 'mouth_center_bottom_lip', sf=16, doi=0.0, dos=0.0, lrf=10, fc1=200, fc2=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0729 22:39:23.985434 140073071294336 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 96, 96, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 96, 96, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 48, 48, 32)        12800     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               3686600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 200)               40200     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "Specialist (Dense)           (None, 2)                 402       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 3,741,938\n",
            "Trainable params: 3,741,042\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Index(['mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y', 'Image'], dtype='object')\n",
            "Train on 11224 samples, validate on 2806 samples\n",
            "Epoch 1/300\n",
            "11224/11224 [==============================] - 11s 977us/sample - loss: 272.2932 - mean_squared_error: 272.2932 - val_loss: 45.1824 - val_mean_squared_error: 45.1824\n",
            "Epoch 2/300\n",
            "11224/11224 [==============================] - 8s 678us/sample - loss: 24.5768 - mean_squared_error: 24.5768 - val_loss: 26.6834 - val_mean_squared_error: 26.6834\n",
            "Epoch 3/300\n",
            "11224/11224 [==============================] - 8s 675us/sample - loss: 18.2130 - mean_squared_error: 18.2130 - val_loss: 19.8072 - val_mean_squared_error: 19.8072\n",
            "Epoch 4/300\n",
            "11224/11224 [==============================] - 8s 674us/sample - loss: 14.3425 - mean_squared_error: 14.3425 - val_loss: 14.0490 - val_mean_squared_error: 14.0490\n",
            "Epoch 5/300\n",
            "11224/11224 [==============================] - 8s 680us/sample - loss: 11.3517 - mean_squared_error: 11.3517 - val_loss: 12.8728 - val_mean_squared_error: 12.8728\n",
            "Epoch 6/300\n",
            "11224/11224 [==============================] - 8s 690us/sample - loss: 9.5094 - mean_squared_error: 9.5094 - val_loss: 12.4209 - val_mean_squared_error: 12.4209\n",
            "Epoch 7/300\n",
            "11224/11224 [==============================] - 8s 681us/sample - loss: 8.4553 - mean_squared_error: 8.4553 - val_loss: 10.3116 - val_mean_squared_error: 10.3116\n",
            "Epoch 8/300\n",
            "11224/11224 [==============================] - 8s 683us/sample - loss: 7.3966 - mean_squared_error: 7.3966 - val_loss: 12.2237 - val_mean_squared_error: 12.2237\n",
            "Epoch 9/300\n",
            "11224/11224 [==============================] - 8s 674us/sample - loss: 6.4089 - mean_squared_error: 6.4089 - val_loss: 12.0273 - val_mean_squared_error: 12.0273\n",
            "Epoch 10/300\n",
            "11224/11224 [==============================] - 8s 684us/sample - loss: 5.9675 - mean_squared_error: 5.9675 - val_loss: 11.9305 - val_mean_squared_error: 11.9305\n",
            "Epoch 11/300\n",
            "11224/11224 [==============================] - 8s 686us/sample - loss: 5.3327 - mean_squared_error: 5.3327 - val_loss: 11.3076 - val_mean_squared_error: 11.3076\n",
            "Epoch 12/300\n",
            "11224/11224 [==============================] - 8s 681us/sample - loss: 5.0894 - mean_squared_error: 5.0894 - val_loss: 12.5283 - val_mean_squared_error: 12.5283\n",
            "Epoch 13/300\n",
            "11224/11224 [==============================] - 8s 686us/sample - loss: 4.7108 - mean_squared_error: 4.7108 - val_loss: 11.0120 - val_mean_squared_error: 11.0120\n",
            "Epoch 14/300\n",
            "11224/11224 [==============================] - 8s 677us/sample - loss: 4.4098 - mean_squared_error: 4.4098 - val_loss: 10.9980 - val_mean_squared_error: 10.9980\n",
            "Epoch 15/300\n",
            "11224/11224 [==============================] - 8s 681us/sample - loss: 4.0184 - mean_squared_error: 4.0184 - val_loss: 10.3199 - val_mean_squared_error: 10.3199\n",
            "Epoch 16/300\n",
            "11224/11224 [==============================] - 8s 678us/sample - loss: 4.0222 - mean_squared_error: 4.0222 - val_loss: 14.1247 - val_mean_squared_error: 14.1247\n",
            "Epoch 17/300\n",
            "11224/11224 [==============================] - 8s 684us/sample - loss: 3.7834 - mean_squared_error: 3.7834 - val_loss: 12.1841 - val_mean_squared_error: 12.1841\n",
            "Epoch 18/300\n",
            "11224/11224 [==============================] - 8s 679us/sample - loss: 3.5349 - mean_squared_error: 3.5349 - val_loss: 12.5287 - val_mean_squared_error: 12.5287\n",
            "Epoch 19/300\n",
            "11224/11224 [==============================] - 8s 678us/sample - loss: 3.6932 - mean_squared_error: 3.6932 - val_loss: 14.7205 - val_mean_squared_error: 14.7205\n",
            "Epoch 20/300\n",
            "11224/11224 [==============================] - 8s 673us/sample - loss: 3.2639 - mean_squared_error: 3.2639 - val_loss: 11.3323 - val_mean_squared_error: 11.3323\n",
            "Epoch 21/300\n",
            "11224/11224 [==============================] - 8s 675us/sample - loss: 3.1404 - mean_squared_error: 3.1404 - val_loss: 9.8775 - val_mean_squared_error: 9.8775\n",
            "Epoch 22/300\n",
            "11224/11224 [==============================] - 8s 676us/sample - loss: 3.0518 - mean_squared_error: 3.0518 - val_loss: 10.3312 - val_mean_squared_error: 10.3312\n",
            "Epoch 23/300\n",
            "11224/11224 [==============================] - 8s 676us/sample - loss: 3.1271 - mean_squared_error: 3.1271 - val_loss: 10.2236 - val_mean_squared_error: 10.2236\n",
            "Epoch 24/300\n",
            "11224/11224 [==============================] - 8s 679us/sample - loss: 2.9528 - mean_squared_error: 2.9528 - val_loss: 12.4143 - val_mean_squared_error: 12.4143\n",
            "Epoch 25/300\n",
            "11224/11224 [==============================] - 8s 680us/sample - loss: 2.9739 - mean_squared_error: 2.9739 - val_loss: 10.3923 - val_mean_squared_error: 10.3923\n",
            "Epoch 26/300\n",
            "11224/11224 [==============================] - 8s 679us/sample - loss: 2.9920 - mean_squared_error: 2.9920 - val_loss: 12.0433 - val_mean_squared_error: 12.0433\n",
            "Epoch 27/300\n",
            "11224/11224 [==============================] - 8s 678us/sample - loss: 2.7276 - mean_squared_error: 2.7276 - val_loss: 11.1281 - val_mean_squared_error: 11.1281\n",
            "Epoch 28/300\n",
            "11224/11224 [==============================] - 8s 676us/sample - loss: 2.6763 - mean_squared_error: 2.6763 - val_loss: 11.9581 - val_mean_squared_error: 11.9581\n",
            "Epoch 29/300\n",
            "11224/11224 [==============================] - 8s 685us/sample - loss: 2.6961 - mean_squared_error: 2.6961 - val_loss: 11.2653 - val_mean_squared_error: 11.2653\n",
            "Epoch 30/300\n",
            "11224/11224 [==============================] - 8s 676us/sample - loss: 2.5668 - mean_squared_error: 2.5668 - val_loss: 10.8351 - val_mean_squared_error: 10.8351\n",
            "Epoch 31/300\n",
            "11224/11224 [==============================] - 8s 692us/sample - loss: 2.6609 - mean_squared_error: 2.6609 - val_loss: 10.9835 - val_mean_squared_error: 10.9835\n",
            "Epoch 32/300\n",
            "11224/11224 [==============================] - 8s 678us/sample - loss: 2.4246 - mean_squared_error: 2.4246 - val_loss: 10.8630 - val_mean_squared_error: 10.8630\n",
            "Epoch 33/300\n",
            "11224/11224 [==============================] - 8s 690us/sample - loss: 2.3939 - mean_squared_error: 2.3939 - val_loss: 11.4180 - val_mean_squared_error: 11.4180\n",
            "Epoch 34/300\n",
            " 6560/11224 [================>.............] - ETA: 2s - loss: 2.5616 - mean_squared_error: 2.5616"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f15356c92850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimproved_specialist_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_improved_2layer_specialists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mouth_center_bottom_lip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-8616189aa334>\u001b[0m in \u001b[0;36mtrain_improved_2layer_specialists\u001b[0;34m(df, keypoint, sf, doi, dos, lrf, fc1, fc2)\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m       callbacks=[time_callback, early_stop])\n\u001b[0m\u001b[1;32m     76\u001b[0m   \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n13Km-GIYadP",
        "colab_type": "text"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "Our final project consisted of learning how to build, train and improve a convolutional neural network in order to learn to predict the location of facial keypoints such as the tips of noses and the centers of eyes. We compared CNN performance against two baseline models (mean model and KNN) and showed vastly superior performance. This superior performance did not come for free, and did require significant CPU/GPU training time and user time to determine how to optimize hyperparameters such as filter depth, dropout rate, and dense layer size. This project was very effective and providing our team with a empirical framework from which to build on our fundamental understanding of simple neural networks and extend that into the deep learning architecture space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jwr8n_TZ0_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}