{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpu_fkd_cnn_filter.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "18cIN-lKMlPM",
        "oySY-Ro3QlTH",
        "2xLztoT7lsr5",
        "dQBMAFgtaifI",
        "XoHVGSxZQFE7",
        "5WlfGrDiBYGZ",
        "VxQYX1btaBRH",
        "S3-oCZ_ykWUn",
        "iSsk092Pw83H",
        "7yfMbGZhXTrL",
        "KamAqR1D6RSm",
        "eq6_vi6l3juz",
        "GXZrCYW0CdOV",
        "xlFI5h9n3Fpg"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomgoter/w207_finalproject/blob/master/gpu_fkd_cnn_filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44OHv9I6LtZ8",
        "colab_type": "text"
      },
      "source": [
        "# GPU Notebook for Google Colab\n",
        "## Facial Keypoint Detection\n",
        "### W207 Final Project - Summer 2019\n",
        "#### T. P. Goter\n",
        "\n",
        "This notebook is used to run various convolutional neural networks in Google's Colab environment. This is useful as GPUs are available for free use. These GPUs have been shown to increase runtimes for the networks below by 10-20x. The use of Colab has really enable a thorough evaluation of the neural network hyperparameters and resulted in a better model in the end.\n",
        "\n",
        "#### Problem Abstract\n",
        "\n",
        "The goal of our models are to detect facial keypoint locations on input images that are 96 by 96 pixels. In order to do this detection, the modelers choose to evaluate various Convolutional Neural Networks (CNNs) with various architectures, input transformation and hyperparameter settings. These explorations and evaluations result in a final model that is used to evaluate a fixed set of test data that has not been trained on or investigated whatsoever. Through this process the modelers develop a deeper understanding of machine learning (ML), ML applications, deep learning, data preprocessing, hyperparamter tuning and model optimization.\n",
        "\n",
        "#### Data Source\n",
        "The original training and test data is provided from Kaggle, as this was a competition in 2016. This data is explored in a separate Jupyter Notebook (DataExploration) which can be found in this [GitHub repository](https://github.com/tomgoter/w207_finalproject). Through this data exploration it was determined that many of the provided images only have labels for some of the keypoints. This means there is much data that is not useful for training. This data could be labeled manually, but that would be excessively time consuming and prone to error. Additionally, not all of the images even have all of the keypoints. It was determined that initial models would only use the training examples which have labels for all 15 facial keypoints. Unfortunately this reduced the size of the data set from >7000 to ~2100. Additional discussion of the data cleaning is included in the previously linked to Jupyter Notebook. This notebook was used to clean and pickle the data (kept in a Pandas dataframe object). This pkl file was then copied to the modeler's Google Drive account to enable easy reading with Colaboratory. This file was too large to keep in GitHub.\n",
        "\n",
        "#### Scoring Metric\n",
        "Model quality is determined through the Root Mean Squared Error (RMSE) between predictions and actuals. The  equation for this calculation is given below where the sum is over every prediction (i.e., test examples multiplied by keypoints per example.) This scoring metric will also be used as the loss function (actually just he mean squared error) for the neural network training.\n",
        "\n",
        "$$RMSE = \\sqrt{\\frac{1}{n}\\cdot\\sum\\limits_{i=1}^{n}(y-\\hat{y})^2}$$\n",
        "\n",
        "#### Baseline Models\n",
        "The modeler chose to generate three baseline models from which to gage progress for more advanced models. The first of these baseline models was rudimentary. For any new test image, the mean x and y location for each keypoint was used as the keypoint location prediction. In this case the mean keypoint location was able to be determined through an average over all training data (minus those reserved as development data - ~15%) simply by ignoring the NaNs in the dataset. The resultant RMSE from this simple prediction method was 3.16. Thus, a baseline was established with plenty of room for improvement. For reference, the Public Leaderboard for the [Kaggle Competition](https://www.kaggle.com/c/facial-keypoints-detection/leaderboard) had a high score (or low in terms of RMSE) of 1.53 at the time the competition was completed.\n",
        "\n",
        "In addition to this simplistic model, the modelers also constructed a k-nearest regressor and an r-nearest regressor model with tuned hyperparameters to determine how much better than the baseline model one could get with a simple, but slightly more advanced model than the baseline. The k-nearest regressor model was implemented, and a k-value of *five* was determined to be optimal based on testing on ~15% of the data reserved as a development set. This model merely identifies the five closest images in the training set to the target image. Image proximity is determined through the euclidean distance in pixel values for all 9,216 pixels. The keypoint locations of the target image are then estimated to be the average of the k (five in our case) nearest images. Using this model, the validation set RMSE was determined to be 2.49. Thus, a substantial improvement over the baseline model was achieved with relative ease. The r-nearest regressor model follows the same basic concept as the k-nearest regressor model, but instead of predetermining the number of images from which to average keypoint locations, instead a radial \"distance\" is specified. The target image keypoints are then predicted to be the average of all images found within the pre-specified distance. For the model in question, r was determined to be optimized at an approximate distance of 12. This resulted in a RMSE on the validation set of ~2.68 which is worse than the k-nearest regressor model. Due to this depreciation in accuracy and given the additional training time of this model, it was determined that a reasonable baseline+ model was the k-nearest regressor model.\n",
        "\n",
        "#### Single Layer Neural Nets\n",
        "On the modeling path toward the CNNs, the modelers made a pit stop at the single layer neural net. This was done to get more familiarity with different training options and simply building neural nets in tensor flow (with Keras as abstraction) and optimizing neural nets. Several sensitivities to the number of hidden units, optimizer and activation function were evaluated. All of these sensitivities were interesting, but in general the modelers found that the single layer models were underperforming and overfitting. Investigating the differences in gradient descent optimizers was as interesting outcome of this pit stop, and through these evaluations further emphasis was placed mostly on the stochastic gradient descent (SGD) optimizer with Nesterov (look-ahead) momentum and the ADAM optimizer which adapts training rates for each parameter in the model.\n",
        "\n",
        "#### Double Layer Nerual Nets\n",
        "If single layer neural nets was a pit stop, the modelers foray into double layer perceptrons was more of a rolling-stop past a stop sign. Fruitful results were not obtained; overfitting and underperforming continued. The modelers determined perhaps there is a reason people are using CNNs for image detection after all. Thus, emphasis was next placed on building deeper neural nets. This is discussed in the next section.\n",
        "\n",
        "#### Convolutional Neural Nets\n",
        "\n",
        "Convolutional neural nets typically make use of a deep network architecture with limited breadth. The basic principal for image processing is to start with your image pixel values, and then convolve the image through a smaller kernel matrix such as shown in the image below. Each kernel matrix cell is initialized with a random weight, and these weights are learned by training the model. This kernel slides over the input feature matrix as shown by the image below. As this process continues the features are transformed. This transformation continues in layers. In between convolution layers the feature set size is typically decreased through a pooling layer in which another kernel matrix is used (many times 2x2) to identify either the average feature value (average pooling) or the maximum feature (max pooling) from this small grid. This process results in a greatly reduced number of dimensions (e.g., if using a 2x2 pooling layer, the dimensions are halved). This process of convolving and reducing dimensions basically allows the neural net to slowly transform the starting feature space into higher level items it can use to classify (or in this case regress). After several convolutions and poolings, the remaining dimensions are flattened and passed to one or two fully connected layers. These output layers are then used to perform the regression by narrowing down to 30 outputs. Note that for classification it is typically to see a softmax (version of a sigmoid function) as the output activation function, but for this regression problem a floating point output is desired. As such, a linear (identity) activation function is used for the final output layer.\n",
        "\n",
        "\n",
        "![alt text](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides.gif)\n",
        "\n",
        "The basic principles and conventions behind building a CNN can be summarized by the following bullets.\n",
        "\n",
        "\n",
        "\n",
        "*   Small filters are typically used first (i.e., 2x2 to 5x5 or thereabouts) to gather the local information from the base image.\n",
        "*   As we step deeper into the network, the filter width can be expanded. The theory behind this is that as we go deeper into the network we begin to explore more global features as we have already captured the local features.\n",
        "* In terms of filter depth, start with a lowish number (it seems like 8 to 32 is fairly common), and increase as depth is added\n",
        "* Layers are added until overfitting starts to occur. Overfitting can then be addressed through various methods. One can implement dropout rates or regularization fairly easily at different levels throughout the network to help address this.\n",
        "* There are many existing architectures from which to start from, as discussed below.\n",
        "\n",
        "**AlexNet Inspired Models**\n",
        "\n",
        "The modelers chose to begin with a simple AlexNet-inspired architecture which [has it roots in the early 1980s](https://www.analyticsvidhya.com/blog/2017/08/10-advanced-deep-learning-architectures-data-scientists/).  This architecture, in the grand scheme of different CNN architectures, is quite simple and easy to understand. It was the natural starting point for this regression analysis and is quite quick to train on the Colaboratory GPUs. This architecture is outlined in the figure below. The basic principle of this architecture that was followed for this analysis is to simply alternate convolution and pooling layers. For the models developed herein, three convolution-pooling layers were used. During each convolution layer, kernel depth is increased. This basically means that several independent kernels, each with unique weights to learn, are used during the training. Through this process, as shown by the image below, the feature space is reduced in the original dimensions (let's call them height and width) but increased in the depth dimension. The original feature space is replaced by these learned transformations. At each layer the user can specify kernel size, filter depth, dropout rate, and a variety of other parameters that all help in the model tuning process. During the model building discussed herein sensitivities were run to many of these hyperparameters, and in the end an optimized AlexNet model was developed. This model scored an RMSE of ~1.25 on the validation data. Unfortunately when scored against the actual Kaggle test data, it only achieved a 2.87 RMSE which would be about 53rd on the leaderboard. This result led to development and testing of additional model architectures as discussed below.\n",
        "\n",
        "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/08/08131757/temp6.png)\n",
        "\n",
        "**VGG Net Inspired Models**\n",
        "\n",
        "As an alternative to starting with something that resembles an AlexNet architecture, the modelers also decided to construct a model inspired by the [VGG Net](https://www.analyticsvidhya.com/blog/2017/08/10-advanced-deep-learning-architectures-data-scientists/). As compared to the AlexNet, the VGG Net is much deeper. Its basic idea is to layer many convolutions prior to pooling and applying dimensionality reduction. This results in more parameters to train and longer model training times. A basic representation of a VGG Net is shown in the image below. Overall the results of the VGG Nets were comparable to the AlexNet-inspired models, but the additional runtime (~factor of 8) from the VGG-inspired nets was basically just wasted GPU resources. This path was investigated for a few different sensitivities, but after non-inspiring results, it was abandoned in favor of additional AlexNet studies. For reference, the modelers' best VGG-inspired net resulted in an RMSE of 2.92 on the Kaggle test data.\n",
        "\n",
        "![alt text](https://tech.showmax.com/2017/10/convnet-architectures/image_0-8fa3b810.png)\n",
        "\n",
        "**Specialist Models**\n",
        "\n",
        "The AlexNet results both in terms of efficiency and accuracy led the modelers to continue building more specialized model. Instead of building one model that tries to accurately locate the x- and y-coordinates of all 15 facial keypoints, the decision was made to instead build 15 models. There are two significant benefits and one significant deleterious effect of this decision.  First, the good parts:\n",
        "\n",
        "1. Specialized models will allow the modelers to use all the training data. As previously noted, there were only about 2100 examples with all locations labeled. However some keypoints had > 7000 labels. Thus, the dataset for the combined models was much smaller than it really could be if all data were used. There should be a benefit to using all of the training data for the creation of these specialized models.\n",
        "2. Specialized models were expected to be more accurate, even without additional training data, because the point of the entire network is only to identify a single keypoint. Thus, all of the training and learning done by the model is hyper-focused on the individual output. This also allows the modeler to tune hyperparameters for different keypoints, as necessary.\n",
        "\n",
        "But of course the downside must also be discussed.\n",
        "\n",
        "The specialized models will increase our model training time by at least a factor of 15. In addition to this, if the modelers are to tune each model, the amount of hands-on time is greatly increased. This is not an insignificant challenge to overcome. The modelers tried to balance this downside by first running all 15 models, starting with the combined model weights and hyperparameters. This method by itself resulted in very significant improvements in generalization of the model. The predictions from this model were evaluated against the Kaggle test set and determined to have an RMSE of 2.18 (public kaggle score) - an improvement of ~0.7 from the combined AlexNet model!\n",
        "\n",
        "The accuracies of the specialized models were then inspected and ranked and focused effort was placed on improving the worst performing models, as shown below. This ranking allowed the modelers to determine which keypoints to prioritize. From the list below, it appears that significant benefit could come just from improving the predictions of the first two keypoints (i.e., mouth_center_bottom_lip and nose_tip).\n",
        "\n",
        "1.   mouth_center_bottom_lip 2.90\n",
        "2.   nose_tip  2.65\n",
        "3.   left_eye_center 2.04\n",
        "4.   right_eye_center           1.94\n",
        "5.   left_eyebrow_outer_end     1.86\n",
        "6.   right_eyebrow_outer_end    1.72\n",
        "7.   right_eyebrow_inner_end    1.64\n",
        "8.   mouth_right_corner         1.54\n",
        "9. left_eyebrow_inner_end     1.51\n",
        "10. mouth_center_top_lip       1.48\n",
        "11. mouth_left_corner          1.42\n",
        "12. right_eye_outer_corner     1.40\n",
        "13. left_eye_outer_corner      1.30\n",
        "14. right_eye_inner_corner     1.10\n",
        "15. left_eye_inner_corner      1.05\n",
        "\n",
        "**Improved Specialist Models**\n",
        "\n",
        "The process of improving just the two worst specialist models was quite time consuming. The team found that increasing the number of layers and the starting filter depth led to improved results, but in order to reduce overfitting this needed to be coupled with increased dropout rate. The original specialist models implemented a dropout strategy in which the first layer had no dropout, but after every additional layer an additional 10% dropout was added. This strategy was useful in reducing overfitting for the AlexNet which was training to predict all 15 keypoints. For our improved specialist models, our dropout rate strategy was modified to start at 20% and ramping to 65% after the fourth layer. Additionally, in order to reduce the overall number of parameters to train, the improved specialist models for the nose_tip and mouth_center_bottom_lip made use of only 200 hidden units in the fully connected layers. After optimization, the final specialized models for the nose_tip and mouth_center_bottom_lip had a little over a million parameters in each model, relative to ~3.2 in the original specialist models.\n",
        "\n",
        "Using this method we were able to reduce our validation RMSE scores for our worst two keypoints (i.e., mouth_center_bottom_lip and nose_tip).  The resulting validation scores for our improved models were:\n",
        "\n",
        "1.   mouth_center_bottom_lip 2.90  -->  **2.54**\n",
        "2.   nose_tip  2.65 --> **2.41**\n",
        "\n",
        "With improving only our two worst performing specialist models, we scored against our test set and showed public/private Kaggle scores of 2.09/1.71. In fact the private score was good enough to knock us into 9th place on the private leaderboard.  Not bad!\n",
        "\n",
        "#### Conclusions\n",
        "\n",
        "Our final project consisted of learning how to build, train and improve a convolutional neural network in order to learn to predict the location of facial keypoints such as the tips of noses and the centers of eyes. We compared CNN performance against two baseline models (mean model and KNN) and showed vastly superior performance. This superior performance did not come for free, and did require significant CPU/GPU training time and user time to determine how to optimize hyperparameters such as filter depth, dropout rate, and dense layer size. This project was very effective and providing our team with a empirical framework from which to build on our fundamental understanding of simple neural networks and extend that into the deep learning architecture space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko5fAr9dK0bh",
        "colab_type": "code",
        "outputId": "45c21e1b-4f95-4d35-a0ea-b7899b02e3de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Import tensorflow and check the version - we will be using version 1.14\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# Import needed packages\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow.keras import models, layers, callbacks\n",
        "from tensorflow.keras import optimizers, metrics\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import pprint\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G8PiDIEMewA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean up your session\n",
        "tf.keras.backend.clear_session()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8FlEtR_MG1u",
        "colab_type": "text"
      },
      "source": [
        "## Link to Google Drive Account to get access to our dataset\n",
        "\n",
        "Colaboratory allows users to link directly to their Google Drive. So that makes it relatively easy to access our training data and save models as we run our studies. We separately manage our data between GitHub and Google Drive to stay synchronized and maintain version control."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4iBlsB3MGkV",
        "colab_type": "code",
        "outputId": "2fa0730f-07a0-4b65-ba9c-f9954a283630",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Mounting the drive is straightforward but required authentication each time \n",
        "# we reset the session\n",
        "drive.mount('/content/drive')\n",
        "drive_path = '/content/drive/My Drive/FacialKeypointDetection/'\n",
        "! ls /content/drive/My\\ Drive/FacialKeypointDetection/OutputData\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "4l_spec_left_eye_center_d0.2_s0.15_sf20_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_left_eye_center_d0.2_s0.2_sf20_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.05_s0.1_sf8_lrf10_fc1200_fc250_kern2334.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.05_s0.1_sf8_lrf10_fc150_fc250_kern2334.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.0_sf4_lrf1_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.0_sf4_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.1_sf16_lrf10_fc1100_fc250_kern2334.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.1_sf16_lrf10_fc1100_fc250.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.1_sf16_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.1_sf4_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.15_s0.15_sf12_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.15_s0.15_sf16_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.12_sf12_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.15_sf12_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf12_lrf10_fc1100_fc250.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf12_lrf10_fc1200_fc250.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf16_lrf10_fc1200_fc250.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf2_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf10_fc1200_fc2200_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1100_fc2100_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1100_fc2100_kern5432.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1150_fc2150_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1200_fc2200_kern2222.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1200_fc2200_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1200_fc2200_kern5432.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1300_fc2300_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf30_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf6_lrf10_fc1100_fc2100_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf8_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.25_s0.15_sf24_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.05_sf16_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.15_sf20_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.17_sf24_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf6_lrf10_fc1100_fc2100_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf6_lrf10_fc1100_fc2100_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf6_lrf10_fc1100_fc250_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf6_lrf10_fc150_fc250_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf8_lrf10_fc1100_fc250_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf8_lrf10_fc150_fc250_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip.pkl\n",
            "4l_spec_nose_tip_d0.0_s0.1_sf16_lrf10_fc1100_fc250_kern2334.pkl\n",
            "4l_spec_nose_tip_d0.2_s0.15_sf20_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.05_s0.15_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.0_s0.15_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.0_s0.17_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.15_s0.1_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.1_s0.15_sf12_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.1_s0.1_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "cnn_flipped_df2.pkl\n",
            "cnn_flipped_df3.pkl\n",
            "cnn_flipped_df4.pkl\n",
            "cnn_flipped_df5.pkl\n",
            "cnn_flipped_df6.pkl\n",
            "cnn_flipped_df.pkl\n",
            "cnn_lr_df.pkl\n",
            "cnn_stride_df.pkl\n",
            "cnn_vgg_df.pkl\n",
            "cnn_vgg_flipped2_df.pkl\n",
            "cnn_vgg_flipped3_df.pkl\n",
            "cnn_vgg_flipped4_df.pkl\n",
            "cnn_vgg_flipped_df.pkl\n",
            "cnn_vgg_lr_df.pkl\n",
            "single_layer_df.pkl\n",
            "spec_01.pkl\n",
            "spec_left_eyebrow_inner_end.pkl\n",
            "spec_left_eyebrow_outer_end.pkl\n",
            "spec_left_eye_center.pkl\n",
            "spec_left_eye_inner_corner.pkl\n",
            "spec_left_eye_outer_corner.pkl\n",
            "spec_mouth_center_bottom_lip.pkl\n",
            "spec_mouth_center_top_lip.pkl\n",
            "spec_mouth_left_corner.pkl\n",
            "spec_mouth_right_corner.pkl\n",
            "spec_nose_tip.pkl\n",
            "spec_right_eyebrow_inner_end.pkl\n",
            "spec_right_eyebrow_outer_end.pkl\n",
            "spec_right_eye_center.pkl\n",
            "spec_right_eye_inner_corner.pkl\n",
            "spec_right_eye_outer_corner.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18cIN-lKMlPM",
        "colab_type": "text"
      },
      "source": [
        "## Import our data from our drive\n",
        "- Load in the pickle file that was created as part of the EDA in DataExploration.ipynb.\n",
        "- This dataset has the NaNs removed and a few mislabeled images removed as well.\n",
        "- As such there is only limited training and development data to use.\n",
        "- The image data has already been normalized to [0,1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LBGEvacw1ge",
        "colab_type": "code",
        "outputId": "6484127b-1df0-4a81-9e9a-234dc7584d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "! ls /content/drive/My\\ Drive/FacialKeypointDetection/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_nostache_nonan.pkl\t      df_nostache_w_flip.pkl  OutputData\n",
            "df_nostache_nonan_w_flip.pkl  Models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyZy0sLnMlYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize Random Seed for reproducibility\n",
        "np.random.seed(13)\n",
        "\n",
        "# Load the dataframe from the pickle file\n",
        "df_nostache_nonan = pd.read_pickle(drive_path + \"df_nostache_nonan.pkl\")\n",
        "\n",
        "# Grab the last column - that is our image data for X matrix\n",
        "X = df_nostache_nonan.iloc[:, -1]\n",
        "\n",
        "# Convert from a series of arrays to an NDarray\n",
        "X = np.array([x.reshape(96,96,1) for x in X])\n",
        "\n",
        "# Grab the keypoints and stick into our y-variable\n",
        "y = np.array(df_nostache_nonan.iloc[:,:-1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oySY-Ro3QlTH",
        "colab_type": "text"
      },
      "source": [
        "## Setup our Optimizer\n",
        "\n",
        "Based on our previous studies (see cnn_notebook_tpg for details), we will focus efforts with the stochastic gradient descent (SGD - epoch based updates) and the ADAM optimizer (parameter specific gradients with decaying learning rate based on average of past gradients and squared gradients). See the image below for the results of an early optimizer study. The image shows that the Adam and SGD optimizer perform similarly. The adagrad optimizer also converges to about the same accuracy as the other two but appears to take a bit longer to get there. It was also shown to be unstable in terms of epoch training time (note the steep increase in cumulative training time for this optimizer in the third subplot). For these reasons (and to whittle down the number of parameters we continue to evaluate) we did not further consider the adagrad optimizer.\n",
        "\n",
        "![alt text](https://github.com/tomgoter/w207_finalproject/raw/master/Images/optimizer.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJx2btJLQleM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the adam optimizer with the default learning rate\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "# Set up an sgd optimizer as well\n",
        "sgd = optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xLztoT7lsr5",
        "colab_type": "text"
      },
      "source": [
        "## Create a Epoch Timing Callback\n",
        "\n",
        "In addition to using RMSE as a metric to evaluate our models against, we also want to track model training time. If a factor of 10 increase in training time leads to a small accuracy benefit we will tend to gravitate toward the more efficient model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7wupCExb6Y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TimeHistory(callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, batch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQBMAFgtaifI",
        "colab_type": "text"
      },
      "source": [
        "## Create a CNN Model\n",
        "\n",
        "We are going to change the filter strategy from our previous CNN (see cnn_notebook_tpg.ipynb [GitHub Repo (https://github.com/tomgoter/w207_finalproject)) which used the following:\n",
        "\n",
        "32 filters --> 64 filters --> 128 filters\n",
        "\n",
        "Let's halve the number of filters and determine the effects on accuracy and run time for our model.\n",
        "\n",
        "The function below is simple and creates a fixed CNN with three convolute/pool layers and two fully connected layers before the output layer. This model is really just to get our feet wet with building a \"deep\" neural net using tensor flow and keras. Later we will parameterize our function to enable easy, straightforward sensitivity evaluations. But for this first go around, let's keep it simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMjVozNKBX24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a function that will automate the CNN model generation\n",
        "def create_cnn_model():\n",
        "  '''\n",
        "  Simple function that retruns a keras cnn model\n",
        "  Alternate between convolution and pooling\n",
        "  No dropout or regularization\n",
        "  Valid padding during convolution\n",
        "  500 hidden units in fully connected layers\n",
        "  '''\n",
        "  \n",
        "  # Instantiate our model as a Sequential model\n",
        "  cnn_model = tf.keras.models.Sequential()\n",
        "  \n",
        "  # First layer has 96,96,1 dimensions - our image is 96 by 96 pixels and we omit color channels (greyscale)\n",
        "  cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "  \n",
        "  # After this layer we will have 94,94,16 - the valid (or no) padding will reduce the size of our input\n",
        "  # dimensions\n",
        "  cnn_model.add(tf.keras.layers.Conv2D(16, (3, 3), padding='valid', activation='relu'))\n",
        "  \n",
        "  # After this pooling layer we will have 47,47,16 - pooling with a 2,2 kernel will halve our dimensions\n",
        "  cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # After this convolution layer we will have 46,46,32\n",
        "  # Again the convolution layer is not padded so we reduce in dimensionality\n",
        "  cnn_model.add(tf.keras.layers.Conv2D(32, (2, 2), padding='valid', activation='relu'))\n",
        "  \n",
        "  # After this pooling layer we will have 23,23,32 - max pooling with 2x2 kernel\n",
        "  cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # After this convolution layer we will have 22,22,64\n",
        "  # Valid padding - reduce dimensionality\n",
        "  cnn_model.add(tf.keras.layers.Conv2D(64, (2, 2), padding='valid', activation='relu'))\n",
        "  \n",
        "  # After this pooling layer we will have 23,23,64\n",
        "  # Max pooling 2x2 - halve dimension space\n",
        "  cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # When we flatten we have 33,856 nodes\n",
        "  cnn_model.add(tf.keras.layers.Flatten())\n",
        "  \n",
        "  # Reduce to 500 hidden units\n",
        "  cnn_model.add(tf.keras.layers.Dense(500))\n",
        "  cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "  \n",
        "  # Second fully connected layer\n",
        "  cnn_model.add(tf.keras.layers.Dense(500))\n",
        "  cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "  \n",
        "  # Output layer is size 30 - equal to the number of keypoint coordinates we are predicting\n",
        "  cnn_model.add(tf.keras.layers.Dense(30))\n",
        "  cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "  \n",
        "  print(50*\"=\")\n",
        "  print(cnn_model.summary())\n",
        "  print(50*\"=\")\n",
        "  \n",
        "  return cnn_model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoHVGSxZQFE7",
        "colab_type": "text"
      },
      "source": [
        "##Construct the model on the TPU\n",
        "\n",
        "Attempts were made to run our models on the TPU, and the following code is required to do so. Although the models were able to run on the TPU, there appeared to be no performance benefit relative to the GPU which was unexpected. However, as we were getting reasonable performance out of the GPU we did not further troubleshoot the TPU application. The cell below was retained in case we did want to do further evaluations on the TPU.\n",
        "\n",
        "The following information is useful to know when setting up our model for running on a TPU:\n",
        "\n",
        "**batch_size**\n",
        "Determines the number of samples in each mini batch. Its maximum is the number of all samples, which makes gradient descent accurate, the loss will decrease towards the minimum if the learning rate is small enough, but iterations are slower. Its minimum is 1, resulting in stochastic gradient descent: Fast but the direction of the gradient step is based only on one example, the loss may jump around. batch_size allows to adjust between the two extremes: accurate gradient direction and fast iteration. Also, the maximum value for batch_size may be limited if your model + data set does not fit into the available (GPU) memory.\n",
        "\n",
        "**steps_per_epoch**\n",
        "The number of batch iterations before a training epoch is considered finished. If you have a training set of fixed size you can ignore it but it may be useful if you have a huge data set or if you are generating random data augmentations on the fly, i.e. if your training set has a (generated) infinite size. If you have the time to go through your whole training data set I recommend to skip this parameter.\n",
        "\n",
        "**validation_steps**\n",
        "similar to steps_per_epoch but on the validation data set instead on the training data. If you have the time to go through your whole validation data set I recommend to skip this parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL9gcVwUMSdR",
        "colab_type": "code",
        "outputId": "06eb286e-a020-46a5-b00c-dca34aaf8d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "# Grab the hardware address of the TPU and get a list of all of the assigned devices\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:' + devices)\n",
        "\n",
        "# Access the tpu and create a distribution strategy\n",
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_address)\n",
        "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "\n",
        "# Compile the model using the TPU distribution strategy\n",
        "with strategy.scope():\n",
        "  model = create_cnn_model()\n",
        "  model.compile(\n",
        "      optimizer=adam,\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['mean_squared_error'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0719 18:13:38.583853 140366703122304 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 94, 94, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 46, 46, 32)        2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 22, 22, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,148,526\n",
            "Trainable params: 4,148,526\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6OnteR3ZZCl",
        "colab_type": "code",
        "outputId": "58adf5b3-d54f-4831-995a-bbc3424aa1d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "model = create_cnn_model()\n",
        "model.compile(\n",
        "      optimizer=adam,\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['mean_squared_error'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 16:38:04.514947 139916523657088 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 94, 94, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 46, 46, 32)        2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 22, 22, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,148,526\n",
            "Trainable params: 4,148,526\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WlfGrDiBYGZ",
        "colab_type": "text"
      },
      "source": [
        "## Try out our first CNN Model\n",
        "\n",
        "The code below is a simple example of how a model is run on a GPU. The output from this model is basically our baseline CNN. Storing the results of the model fitting to the history object allows us to access epoch data such as timing and loss data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jv9cmYfNA9Q",
        "colab_type": "code",
        "outputId": "6e64596f-8136-4914-fd84-f3a2b7e9a21d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fit the basic CNN\n",
        "history = model.fit(\n",
        "    train_X.astype(np.float32), train_y.astype(np.float32),\n",
        "    epochs=200,\n",
        "    validation_split=0.15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1523 samples, validate on 269 samples\n",
            "Epoch 1/200\n",
            "1523/1523 [==============================] - 4s 3ms/sample - loss: 2186.5384 - mean_squared_error: 2186.5391 - val_loss: 85.1574 - val_mean_squared_error: 85.1573\n",
            "Epoch 2/200\n",
            "1523/1523 [==============================] - 0s 270us/sample - loss: 21.9787 - mean_squared_error: 21.9787 - val_loss: 12.5287 - val_mean_squared_error: 12.5287\n",
            "Epoch 3/200\n",
            "1523/1523 [==============================] - 0s 267us/sample - loss: 11.4104 - mean_squared_error: 11.4104 - val_loss: 11.7648 - val_mean_squared_error: 11.7648\n",
            "Epoch 4/200\n",
            "1523/1523 [==============================] - 0s 258us/sample - loss: 10.9332 - mean_squared_error: 10.9332 - val_loss: 11.3901 - val_mean_squared_error: 11.3901\n",
            "Epoch 5/200\n",
            "1523/1523 [==============================] - 0s 260us/sample - loss: 10.8059 - mean_squared_error: 10.8059 - val_loss: 13.0320 - val_mean_squared_error: 13.0320\n",
            "Epoch 6/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 10.9308 - mean_squared_error: 10.9308 - val_loss: 11.2774 - val_mean_squared_error: 11.2774\n",
            "Epoch 7/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 10.9633 - mean_squared_error: 10.9633 - val_loss: 12.3929 - val_mean_squared_error: 12.3929\n",
            "Epoch 8/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 10.8163 - mean_squared_error: 10.8163 - val_loss: 11.5844 - val_mean_squared_error: 11.5844\n",
            "Epoch 9/200\n",
            "1523/1523 [==============================] - 0s 246us/sample - loss: 10.5549 - mean_squared_error: 10.5549 - val_loss: 11.3282 - val_mean_squared_error: 11.3282\n",
            "Epoch 10/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 10.7388 - mean_squared_error: 10.7388 - val_loss: 11.6792 - val_mean_squared_error: 11.6792\n",
            "Epoch 11/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 10.8855 - mean_squared_error: 10.8855 - val_loss: 12.5035 - val_mean_squared_error: 12.5035\n",
            "Epoch 12/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.5909 - mean_squared_error: 11.5909 - val_loss: 14.0177 - val_mean_squared_error: 14.0177\n",
            "Epoch 13/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 11.2929 - mean_squared_error: 11.2929 - val_loss: 11.3373 - val_mean_squared_error: 11.3373\n",
            "Epoch 14/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 11.1431 - mean_squared_error: 11.1431 - val_loss: 11.4211 - val_mean_squared_error: 11.4211\n",
            "Epoch 15/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.2539 - mean_squared_error: 11.2539 - val_loss: 11.2227 - val_mean_squared_error: 11.2227\n",
            "Epoch 16/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.9072 - mean_squared_error: 10.9072 - val_loss: 11.3502 - val_mean_squared_error: 11.3502\n",
            "Epoch 17/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.8393 - mean_squared_error: 10.8393 - val_loss: 11.3428 - val_mean_squared_error: 11.3428\n",
            "Epoch 18/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.1583 - mean_squared_error: 11.1583 - val_loss: 13.3583 - val_mean_squared_error: 13.3583\n",
            "Epoch 19/200\n",
            "1523/1523 [==============================] - 0s 258us/sample - loss: 12.4589 - mean_squared_error: 12.4589 - val_loss: 12.9226 - val_mean_squared_error: 12.9226\n",
            "Epoch 20/200\n",
            "1523/1523 [==============================] - 0s 247us/sample - loss: 11.2134 - mean_squared_error: 11.2134 - val_loss: 12.2681 - val_mean_squared_error: 12.2681\n",
            "Epoch 21/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 10.6060 - mean_squared_error: 10.6060 - val_loss: 11.3355 - val_mean_squared_error: 11.3356\n",
            "Epoch 22/200\n",
            "1523/1523 [==============================] - 0s 247us/sample - loss: 10.7449 - mean_squared_error: 10.7449 - val_loss: 11.9970 - val_mean_squared_error: 11.9970\n",
            "Epoch 23/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 10.8729 - mean_squared_error: 10.8729 - val_loss: 11.3347 - val_mean_squared_error: 11.3347\n",
            "Epoch 24/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 10.5434 - mean_squared_error: 10.5434 - val_loss: 11.2721 - val_mean_squared_error: 11.2721\n",
            "Epoch 25/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.9073 - mean_squared_error: 10.9073 - val_loss: 11.3886 - val_mean_squared_error: 11.3886\n",
            "Epoch 26/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 11.6348 - mean_squared_error: 11.6348 - val_loss: 11.8605 - val_mean_squared_error: 11.8605\n",
            "Epoch 27/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.1554 - mean_squared_error: 11.1554 - val_loss: 12.4512 - val_mean_squared_error: 12.4512\n",
            "Epoch 28/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.2434 - mean_squared_error: 11.2434 - val_loss: 14.1728 - val_mean_squared_error: 14.1728\n",
            "Epoch 29/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 11.2802 - mean_squared_error: 11.2802 - val_loss: 11.4020 - val_mean_squared_error: 11.4020\n",
            "Epoch 30/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.9479 - mean_squared_error: 10.9479 - val_loss: 11.7527 - val_mean_squared_error: 11.7527\n",
            "Epoch 31/200\n",
            "1523/1523 [==============================] - 0s 246us/sample - loss: 11.5825 - mean_squared_error: 11.5825 - val_loss: 11.0840 - val_mean_squared_error: 11.0840\n",
            "Epoch 32/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 10.5958 - mean_squared_error: 10.5958 - val_loss: 11.1692 - val_mean_squared_error: 11.1692\n",
            "Epoch 33/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 11.1690 - mean_squared_error: 11.1690 - val_loss: 11.5418 - val_mean_squared_error: 11.5418\n",
            "Epoch 34/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 10.8846 - mean_squared_error: 10.8846 - val_loss: 11.6986 - val_mean_squared_error: 11.6986\n",
            "Epoch 35/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 11.7534 - mean_squared_error: 11.7534 - val_loss: 13.9421 - val_mean_squared_error: 13.9421\n",
            "Epoch 36/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.3234 - mean_squared_error: 11.3234 - val_loss: 11.2780 - val_mean_squared_error: 11.2780\n",
            "Epoch 37/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 11.4605 - mean_squared_error: 11.4605 - val_loss: 10.9001 - val_mean_squared_error: 10.9001\n",
            "Epoch 38/200\n",
            "1523/1523 [==============================] - 0s 244us/sample - loss: 12.0751 - mean_squared_error: 12.0751 - val_loss: 16.1726 - val_mean_squared_error: 16.1726\n",
            "Epoch 39/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 11.8547 - mean_squared_error: 11.8547 - val_loss: 10.8940 - val_mean_squared_error: 10.8940\n",
            "Epoch 40/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 10.8181 - mean_squared_error: 10.8181 - val_loss: 12.0226 - val_mean_squared_error: 12.0226\n",
            "Epoch 41/200\n",
            "1523/1523 [==============================] - 0s 247us/sample - loss: 11.7239 - mean_squared_error: 11.7239 - val_loss: 11.3769 - val_mean_squared_error: 11.3769\n",
            "Epoch 42/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 11.2483 - mean_squared_error: 11.2483 - val_loss: 10.6776 - val_mean_squared_error: 10.6776\n",
            "Epoch 43/200\n",
            "1523/1523 [==============================] - 0s 246us/sample - loss: 11.9855 - mean_squared_error: 11.9855 - val_loss: 11.9220 - val_mean_squared_error: 11.9220\n",
            "Epoch 44/200\n",
            "1523/1523 [==============================] - 0s 247us/sample - loss: 10.5701 - mean_squared_error: 10.5701 - val_loss: 11.0283 - val_mean_squared_error: 11.0283\n",
            "Epoch 45/200\n",
            "1523/1523 [==============================] - 0s 260us/sample - loss: 10.6288 - mean_squared_error: 10.6288 - val_loss: 10.6101 - val_mean_squared_error: 10.6101\n",
            "Epoch 46/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.3491 - mean_squared_error: 10.3491 - val_loss: 11.7852 - val_mean_squared_error: 11.7852\n",
            "Epoch 47/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 10.7313 - mean_squared_error: 10.7313 - val_loss: 12.0018 - val_mean_squared_error: 12.0018\n",
            "Epoch 48/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 10.9584 - mean_squared_error: 10.9584 - val_loss: 17.6078 - val_mean_squared_error: 17.6078\n",
            "Epoch 49/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 10.5421 - mean_squared_error: 10.5421 - val_loss: 10.2922 - val_mean_squared_error: 10.2922\n",
            "Epoch 50/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.6156 - mean_squared_error: 10.6156 - val_loss: 10.8067 - val_mean_squared_error: 10.8067\n",
            "Epoch 51/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 10.6448 - mean_squared_error: 10.6448 - val_loss: 10.5169 - val_mean_squared_error: 10.5169\n",
            "Epoch 52/200\n",
            "1523/1523 [==============================] - 0s 246us/sample - loss: 9.6362 - mean_squared_error: 9.6362 - val_loss: 10.2055 - val_mean_squared_error: 10.2055\n",
            "Epoch 53/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 10.2991 - mean_squared_error: 10.2991 - val_loss: 10.6341 - val_mean_squared_error: 10.6341\n",
            "Epoch 54/200\n",
            "1523/1523 [==============================] - 0s 247us/sample - loss: 10.0194 - mean_squared_error: 10.0194 - val_loss: 11.9481 - val_mean_squared_error: 11.9481\n",
            "Epoch 55/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 10.4444 - mean_squared_error: 10.4444 - val_loss: 9.8573 - val_mean_squared_error: 9.8573\n",
            "Epoch 56/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 9.8739 - mean_squared_error: 9.8739 - val_loss: 10.1485 - val_mean_squared_error: 10.1485\n",
            "Epoch 57/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 9.8409 - mean_squared_error: 9.8409 - val_loss: 10.1568 - val_mean_squared_error: 10.1568\n",
            "Epoch 58/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 11.7345 - mean_squared_error: 11.7345 - val_loss: 11.1454 - val_mean_squared_error: 11.1454\n",
            "Epoch 59/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 9.1059 - mean_squared_error: 9.1059 - val_loss: 9.4817 - val_mean_squared_error: 9.4817\n",
            "Epoch 60/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 9.7425 - mean_squared_error: 9.7425 - val_loss: 9.3625 - val_mean_squared_error: 9.3625\n",
            "Epoch 61/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 9.1704 - mean_squared_error: 9.1704 - val_loss: 9.6178 - val_mean_squared_error: 9.6178\n",
            "Epoch 62/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 9.4638 - mean_squared_error: 9.4638 - val_loss: 9.0279 - val_mean_squared_error: 9.0279\n",
            "Epoch 63/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 8.9360 - mean_squared_error: 8.9360 - val_loss: 9.3182 - val_mean_squared_error: 9.3182\n",
            "Epoch 64/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 8.5014 - mean_squared_error: 8.5014 - val_loss: 9.5323 - val_mean_squared_error: 9.5323\n",
            "Epoch 65/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 8.5894 - mean_squared_error: 8.5894 - val_loss: 9.0202 - val_mean_squared_error: 9.0202\n",
            "Epoch 66/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 8.7035 - mean_squared_error: 8.7035 - val_loss: 11.0113 - val_mean_squared_error: 11.0113\n",
            "Epoch 67/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 8.6125 - mean_squared_error: 8.6125 - val_loss: 9.6424 - val_mean_squared_error: 9.6424\n",
            "Epoch 68/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 8.7846 - mean_squared_error: 8.7846 - val_loss: 9.7801 - val_mean_squared_error: 9.7801\n",
            "Epoch 69/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 9.1472 - mean_squared_error: 9.1472 - val_loss: 10.3220 - val_mean_squared_error: 10.3220\n",
            "Epoch 70/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 9.6579 - mean_squared_error: 9.6579 - val_loss: 9.1459 - val_mean_squared_error: 9.1459\n",
            "Epoch 71/200\n",
            "1523/1523 [==============================] - 0s 259us/sample - loss: 7.9400 - mean_squared_error: 7.9400 - val_loss: 12.5291 - val_mean_squared_error: 12.5291\n",
            "Epoch 72/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 8.7742 - mean_squared_error: 8.7742 - val_loss: 10.1852 - val_mean_squared_error: 10.1852\n",
            "Epoch 73/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 11.9332 - mean_squared_error: 11.9332 - val_loss: 8.5253 - val_mean_squared_error: 8.5253\n",
            "Epoch 74/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 8.0548 - mean_squared_error: 8.0548 - val_loss: 8.3496 - val_mean_squared_error: 8.3496\n",
            "Epoch 75/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 7.6513 - mean_squared_error: 7.6513 - val_loss: 8.3298 - val_mean_squared_error: 8.3298\n",
            "Epoch 76/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 7.6619 - mean_squared_error: 7.6619 - val_loss: 8.1450 - val_mean_squared_error: 8.1450\n",
            "Epoch 77/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 7.6146 - mean_squared_error: 7.6146 - val_loss: 8.0492 - val_mean_squared_error: 8.0492\n",
            "Epoch 78/200\n",
            "1523/1523 [==============================] - 0s 247us/sample - loss: 7.5626 - mean_squared_error: 7.5626 - val_loss: 11.0281 - val_mean_squared_error: 11.0281\n",
            "Epoch 79/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 7.3256 - mean_squared_error: 7.3256 - val_loss: 7.7061 - val_mean_squared_error: 7.7061\n",
            "Epoch 80/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 6.9129 - mean_squared_error: 6.9129 - val_loss: 7.3109 - val_mean_squared_error: 7.3109\n",
            "Epoch 81/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 6.5433 - mean_squared_error: 6.5433 - val_loss: 7.1938 - val_mean_squared_error: 7.1938\n",
            "Epoch 82/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 6.1104 - mean_squared_error: 6.1104 - val_loss: 7.0612 - val_mean_squared_error: 7.0612\n",
            "Epoch 83/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 6.4696 - mean_squared_error: 6.4696 - val_loss: 6.8015 - val_mean_squared_error: 6.8015\n",
            "Epoch 84/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 6.1186 - mean_squared_error: 6.1186 - val_loss: 7.0542 - val_mean_squared_error: 7.0542\n",
            "Epoch 85/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 6.0993 - mean_squared_error: 6.0993 - val_loss: 7.1176 - val_mean_squared_error: 7.1176\n",
            "Epoch 86/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 6.4485 - mean_squared_error: 6.4485 - val_loss: 6.5254 - val_mean_squared_error: 6.5254\n",
            "Epoch 87/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 6.0172 - mean_squared_error: 6.0172 - val_loss: 6.3240 - val_mean_squared_error: 6.3240\n",
            "Epoch 88/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 5.6972 - mean_squared_error: 5.6972 - val_loss: 6.2878 - val_mean_squared_error: 6.2878\n",
            "Epoch 89/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 5.8656 - mean_squared_error: 5.8656 - val_loss: 6.1162 - val_mean_squared_error: 6.1162\n",
            "Epoch 90/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 5.8477 - mean_squared_error: 5.8477 - val_loss: 5.9035 - val_mean_squared_error: 5.9035\n",
            "Epoch 91/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 5.1237 - mean_squared_error: 5.1237 - val_loss: 6.0044 - val_mean_squared_error: 6.0044\n",
            "Epoch 92/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 5.8330 - mean_squared_error: 5.8330 - val_loss: 5.8483 - val_mean_squared_error: 5.8483\n",
            "Epoch 93/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 5.8411 - mean_squared_error: 5.8411 - val_loss: 6.2803 - val_mean_squared_error: 6.2803\n",
            "Epoch 94/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 5.0654 - mean_squared_error: 5.0654 - val_loss: 5.7353 - val_mean_squared_error: 5.7353\n",
            "Epoch 95/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 5.6155 - mean_squared_error: 5.6155 - val_loss: 6.0768 - val_mean_squared_error: 6.0768\n",
            "Epoch 96/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 5.3107 - mean_squared_error: 5.3107 - val_loss: 5.4001 - val_mean_squared_error: 5.4001\n",
            "Epoch 97/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 4.6326 - mean_squared_error: 4.6326 - val_loss: 5.3812 - val_mean_squared_error: 5.3812\n",
            "Epoch 98/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 5.0380 - mean_squared_error: 5.0380 - val_loss: 5.6562 - val_mean_squared_error: 5.6562\n",
            "Epoch 99/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 4.9512 - mean_squared_error: 4.9512 - val_loss: 5.3829 - val_mean_squared_error: 5.3829\n",
            "Epoch 100/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 4.7277 - mean_squared_error: 4.7277 - val_loss: 5.9268 - val_mean_squared_error: 5.9268\n",
            "Epoch 101/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 5.5963 - mean_squared_error: 5.5963 - val_loss: 5.3027 - val_mean_squared_error: 5.3027\n",
            "Epoch 102/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 4.9725 - mean_squared_error: 4.9725 - val_loss: 5.7850 - val_mean_squared_error: 5.7850\n",
            "Epoch 103/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 4.6928 - mean_squared_error: 4.6928 - val_loss: 5.3264 - val_mean_squared_error: 5.3264\n",
            "Epoch 104/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 4.3827 - mean_squared_error: 4.3827 - val_loss: 5.8130 - val_mean_squared_error: 5.8130\n",
            "Epoch 105/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 4.0511 - mean_squared_error: 4.0511 - val_loss: 5.8714 - val_mean_squared_error: 5.8714\n",
            "Epoch 106/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 4.2892 - mean_squared_error: 4.2892 - val_loss: 4.9869 - val_mean_squared_error: 4.9869\n",
            "Epoch 107/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 4.1399 - mean_squared_error: 4.1399 - val_loss: 5.3128 - val_mean_squared_error: 5.3128\n",
            "Epoch 108/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 3.9579 - mean_squared_error: 3.9579 - val_loss: 4.9781 - val_mean_squared_error: 4.9781\n",
            "Epoch 109/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 4.1932 - mean_squared_error: 4.1932 - val_loss: 5.5343 - val_mean_squared_error: 5.5343\n",
            "Epoch 110/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 4.1194 - mean_squared_error: 4.1194 - val_loss: 4.5430 - val_mean_squared_error: 4.5430\n",
            "Epoch 111/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 3.6807 - mean_squared_error: 3.6807 - val_loss: 4.4029 - val_mean_squared_error: 4.4029\n",
            "Epoch 112/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 3.4742 - mean_squared_error: 3.4742 - val_loss: 4.8416 - val_mean_squared_error: 4.8416\n",
            "Epoch 113/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 4.1988 - mean_squared_error: 4.1988 - val_loss: 6.0491 - val_mean_squared_error: 6.0491\n",
            "Epoch 114/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 3.7302 - mean_squared_error: 3.7302 - val_loss: 4.2168 - val_mean_squared_error: 4.2168\n",
            "Epoch 115/200\n",
            "1523/1523 [==============================] - 0s 257us/sample - loss: 3.9120 - mean_squared_error: 3.9120 - val_loss: 4.2305 - val_mean_squared_error: 4.2305\n",
            "Epoch 116/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 3.5498 - mean_squared_error: 3.5498 - val_loss: 4.8664 - val_mean_squared_error: 4.8664\n",
            "Epoch 117/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 3.8020 - mean_squared_error: 3.8020 - val_loss: 4.2361 - val_mean_squared_error: 4.2361\n",
            "Epoch 118/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 3.3981 - mean_squared_error: 3.3981 - val_loss: 4.3540 - val_mean_squared_error: 4.3540\n",
            "Epoch 119/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 3.3746 - mean_squared_error: 3.3746 - val_loss: 4.2422 - val_mean_squared_error: 4.2422\n",
            "Epoch 120/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 3.0419 - mean_squared_error: 3.0419 - val_loss: 4.1896 - val_mean_squared_error: 4.1896\n",
            "Epoch 121/200\n",
            "1523/1523 [==============================] - 0s 259us/sample - loss: 3.0100 - mean_squared_error: 3.0100 - val_loss: 4.4095 - val_mean_squared_error: 4.4095\n",
            "Epoch 122/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 3.1114 - mean_squared_error: 3.1114 - val_loss: 4.2619 - val_mean_squared_error: 4.2619\n",
            "Epoch 123/200\n",
            "1523/1523 [==============================] - 0s 260us/sample - loss: 2.9106 - mean_squared_error: 2.9106 - val_loss: 4.1942 - val_mean_squared_error: 4.1942\n",
            "Epoch 124/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 3.0467 - mean_squared_error: 3.0467 - val_loss: 4.6457 - val_mean_squared_error: 4.6457\n",
            "Epoch 125/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 3.2866 - mean_squared_error: 3.2866 - val_loss: 5.4107 - val_mean_squared_error: 5.4107\n",
            "Epoch 126/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 2.9748 - mean_squared_error: 2.9748 - val_loss: 3.8916 - val_mean_squared_error: 3.8916\n",
            "Epoch 127/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 2.7590 - mean_squared_error: 2.7590 - val_loss: 3.8440 - val_mean_squared_error: 3.8440\n",
            "Epoch 128/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 2.7930 - mean_squared_error: 2.7930 - val_loss: 3.8961 - val_mean_squared_error: 3.8961\n",
            "Epoch 129/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 2.5477 - mean_squared_error: 2.5477 - val_loss: 3.9098 - val_mean_squared_error: 3.9098\n",
            "Epoch 130/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 2.5119 - mean_squared_error: 2.5119 - val_loss: 4.0160 - val_mean_squared_error: 4.0160\n",
            "Epoch 131/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 2.7305 - mean_squared_error: 2.7305 - val_loss: 4.6461 - val_mean_squared_error: 4.6461\n",
            "Epoch 132/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 2.5016 - mean_squared_error: 2.5016 - val_loss: 4.2591 - val_mean_squared_error: 4.2591\n",
            "Epoch 133/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 2.5971 - mean_squared_error: 2.5971 - val_loss: 4.5622 - val_mean_squared_error: 4.5622\n",
            "Epoch 134/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 2.4486 - mean_squared_error: 2.4486 - val_loss: 3.9677 - val_mean_squared_error: 3.9677\n",
            "Epoch 135/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 2.3128 - mean_squared_error: 2.3128 - val_loss: 3.6781 - val_mean_squared_error: 3.6781\n",
            "Epoch 136/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 2.1649 - mean_squared_error: 2.1649 - val_loss: 3.5597 - val_mean_squared_error: 3.5597\n",
            "Epoch 137/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 2.1406 - mean_squared_error: 2.1406 - val_loss: 4.2139 - val_mean_squared_error: 4.2139\n",
            "Epoch 138/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 2.3293 - mean_squared_error: 2.3293 - val_loss: 3.8239 - val_mean_squared_error: 3.8239\n",
            "Epoch 139/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 2.5016 - mean_squared_error: 2.5016 - val_loss: 4.7531 - val_mean_squared_error: 4.7531\n",
            "Epoch 140/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 2.2518 - mean_squared_error: 2.2518 - val_loss: 4.2025 - val_mean_squared_error: 4.2025\n",
            "Epoch 141/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 2.1436 - mean_squared_error: 2.1436 - val_loss: 4.3515 - val_mean_squared_error: 4.3515\n",
            "Epoch 142/200\n",
            "1523/1523 [==============================] - 0s 257us/sample - loss: 2.2351 - mean_squared_error: 2.2351 - val_loss: 3.8939 - val_mean_squared_error: 3.8939\n",
            "Epoch 143/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 2.0506 - mean_squared_error: 2.0506 - val_loss: 3.7393 - val_mean_squared_error: 3.7393\n",
            "Epoch 144/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 2.1602 - mean_squared_error: 2.1602 - val_loss: 4.2231 - val_mean_squared_error: 4.2231\n",
            "Epoch 145/200\n",
            "1523/1523 [==============================] - 0s 260us/sample - loss: 2.1634 - mean_squared_error: 2.1634 - val_loss: 3.7979 - val_mean_squared_error: 3.7979\n",
            "Epoch 146/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.9778 - mean_squared_error: 1.9778 - val_loss: 4.0015 - val_mean_squared_error: 4.0015\n",
            "Epoch 147/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.8597 - mean_squared_error: 1.8597 - val_loss: 3.5484 - val_mean_squared_error: 3.5484\n",
            "Epoch 148/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 1.7534 - mean_squared_error: 1.7534 - val_loss: 3.9050 - val_mean_squared_error: 3.9050\n",
            "Epoch 149/200\n",
            "1523/1523 [==============================] - 0s 259us/sample - loss: 1.8591 - mean_squared_error: 1.8591 - val_loss: 4.0597 - val_mean_squared_error: 4.0597\n",
            "Epoch 150/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 2.1221 - mean_squared_error: 2.1221 - val_loss: 4.9068 - val_mean_squared_error: 4.9068\n",
            "Epoch 151/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 2.0297 - mean_squared_error: 2.0297 - val_loss: 3.6319 - val_mean_squared_error: 3.6319\n",
            "Epoch 152/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 1.7058 - mean_squared_error: 1.7058 - val_loss: 3.6505 - val_mean_squared_error: 3.6505\n",
            "Epoch 153/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 1.6978 - mean_squared_error: 1.6978 - val_loss: 3.4419 - val_mean_squared_error: 3.4419\n",
            "Epoch 154/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.9036 - mean_squared_error: 1.9036 - val_loss: 4.2756 - val_mean_squared_error: 4.2756\n",
            "Epoch 155/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 1.8122 - mean_squared_error: 1.8122 - val_loss: 3.6204 - val_mean_squared_error: 3.6204\n",
            "Epoch 156/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.6471 - mean_squared_error: 1.6471 - val_loss: 3.6115 - val_mean_squared_error: 3.6115\n",
            "Epoch 157/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.5383 - mean_squared_error: 1.5383 - val_loss: 3.5560 - val_mean_squared_error: 3.5560\n",
            "Epoch 158/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 1.4638 - mean_squared_error: 1.4638 - val_loss: 3.5228 - val_mean_squared_error: 3.5228\n",
            "Epoch 159/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.5674 - mean_squared_error: 1.5674 - val_loss: 4.1173 - val_mean_squared_error: 4.1173\n",
            "Epoch 160/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 1.9161 - mean_squared_error: 1.9161 - val_loss: 3.9769 - val_mean_squared_error: 3.9769\n",
            "Epoch 161/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 1.6398 - mean_squared_error: 1.6398 - val_loss: 3.7663 - val_mean_squared_error: 3.7663\n",
            "Epoch 162/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 1.5981 - mean_squared_error: 1.5981 - val_loss: 3.5267 - val_mean_squared_error: 3.5267\n",
            "Epoch 163/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.5102 - mean_squared_error: 1.5102 - val_loss: 3.6904 - val_mean_squared_error: 3.6904\n",
            "Epoch 164/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 1.4467 - mean_squared_error: 1.4467 - val_loss: 3.5398 - val_mean_squared_error: 3.5398\n",
            "Epoch 165/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.3855 - mean_squared_error: 1.3855 - val_loss: 3.6115 - val_mean_squared_error: 3.6115\n",
            "Epoch 166/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 1.4055 - mean_squared_error: 1.4055 - val_loss: 4.0637 - val_mean_squared_error: 4.0637\n",
            "Epoch 167/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.6381 - mean_squared_error: 1.6381 - val_loss: 3.5681 - val_mean_squared_error: 3.5681\n",
            "Epoch 168/200\n",
            "1523/1523 [==============================] - 0s 249us/sample - loss: 1.3605 - mean_squared_error: 1.3605 - val_loss: 3.5610 - val_mean_squared_error: 3.5610\n",
            "Epoch 169/200\n",
            "1523/1523 [==============================] - 0s 258us/sample - loss: 1.3482 - mean_squared_error: 1.3482 - val_loss: 3.6626 - val_mean_squared_error: 3.6626\n",
            "Epoch 170/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 1.3517 - mean_squared_error: 1.3517 - val_loss: 3.6798 - val_mean_squared_error: 3.6798\n",
            "Epoch 171/200\n",
            "1523/1523 [==============================] - 0s 258us/sample - loss: 1.4873 - mean_squared_error: 1.4873 - val_loss: 3.6441 - val_mean_squared_error: 3.6441\n",
            "Epoch 172/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.3080 - mean_squared_error: 1.3080 - val_loss: 3.6187 - val_mean_squared_error: 3.6187\n",
            "Epoch 173/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.1645 - mean_squared_error: 1.1645 - val_loss: 3.5470 - val_mean_squared_error: 3.5470\n",
            "Epoch 174/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.2152 - mean_squared_error: 1.2152 - val_loss: 3.5694 - val_mean_squared_error: 3.5694\n",
            "Epoch 175/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 1.2807 - mean_squared_error: 1.2807 - val_loss: 3.6412 - val_mean_squared_error: 3.6412\n",
            "Epoch 176/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.4923 - mean_squared_error: 1.4923 - val_loss: 3.8647 - val_mean_squared_error: 3.8647\n",
            "Epoch 177/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.5791 - mean_squared_error: 1.5791 - val_loss: 4.3232 - val_mean_squared_error: 4.3232\n",
            "Epoch 178/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.6231 - mean_squared_error: 1.6231 - val_loss: 3.4380 - val_mean_squared_error: 3.4380\n",
            "Epoch 179/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.2866 - mean_squared_error: 1.2866 - val_loss: 3.4734 - val_mean_squared_error: 3.4734\n",
            "Epoch 180/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 1.1311 - mean_squared_error: 1.1311 - val_loss: 3.4142 - val_mean_squared_error: 3.4142\n",
            "Epoch 181/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 1.0921 - mean_squared_error: 1.0921 - val_loss: 3.5626 - val_mean_squared_error: 3.5626\n",
            "Epoch 182/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 1.3488 - mean_squared_error: 1.3488 - val_loss: 4.4737 - val_mean_squared_error: 4.4737\n",
            "Epoch 183/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 1.7049 - mean_squared_error: 1.7049 - val_loss: 3.7555 - val_mean_squared_error: 3.7555\n",
            "Epoch 184/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.2116 - mean_squared_error: 1.2116 - val_loss: 3.4356 - val_mean_squared_error: 3.4356\n",
            "Epoch 185/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.1320 - mean_squared_error: 1.1320 - val_loss: 3.6670 - val_mean_squared_error: 3.6670\n",
            "Epoch 186/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 1.1757 - mean_squared_error: 1.1757 - val_loss: 3.6879 - val_mean_squared_error: 3.6878\n",
            "Epoch 187/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 1.2657 - mean_squared_error: 1.2657 - val_loss: 3.5455 - val_mean_squared_error: 3.5455\n",
            "Epoch 188/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 1.0694 - mean_squared_error: 1.0694 - val_loss: 3.7443 - val_mean_squared_error: 3.7443\n",
            "Epoch 189/200\n",
            "1523/1523 [==============================] - 0s 252us/sample - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 3.4032 - val_mean_squared_error: 3.4032\n",
            "Epoch 190/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 1.0183 - mean_squared_error: 1.0183 - val_loss: 3.7105 - val_mean_squared_error: 3.7105\n",
            "Epoch 191/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 3.4146 - val_mean_squared_error: 3.4146\n",
            "Epoch 192/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 1.0172 - mean_squared_error: 1.0172 - val_loss: 3.5050 - val_mean_squared_error: 3.5050\n",
            "Epoch 193/200\n",
            "1523/1523 [==============================] - 0s 248us/sample - loss: 1.0255 - mean_squared_error: 1.0255 - val_loss: 3.5907 - val_mean_squared_error: 3.5907\n",
            "Epoch 194/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 1.0169 - mean_squared_error: 1.0169 - val_loss: 3.5567 - val_mean_squared_error: 3.5567\n",
            "Epoch 195/200\n",
            "1523/1523 [==============================] - 0s 255us/sample - loss: 0.9713 - mean_squared_error: 0.9713 - val_loss: 3.6207 - val_mean_squared_error: 3.6207\n",
            "Epoch 196/200\n",
            "1523/1523 [==============================] - 0s 251us/sample - loss: 0.9154 - mean_squared_error: 0.9154 - val_loss: 3.6059 - val_mean_squared_error: 3.6059\n",
            "Epoch 197/200\n",
            "1523/1523 [==============================] - 0s 253us/sample - loss: 0.9677 - mean_squared_error: 0.9677 - val_loss: 3.6231 - val_mean_squared_error: 3.6231\n",
            "Epoch 198/200\n",
            "1523/1523 [==============================] - 0s 254us/sample - loss: 1.3251 - mean_squared_error: 1.3251 - val_loss: 3.9969 - val_mean_squared_error: 3.9969\n",
            "Epoch 199/200\n",
            "1523/1523 [==============================] - 0s 250us/sample - loss: 1.1739 - mean_squared_error: 1.1739 - val_loss: 3.6905 - val_mean_squared_error: 3.6905\n",
            "Epoch 200/200\n",
            "1523/1523 [==============================] - 0s 256us/sample - loss: 0.9650 - mean_squared_error: 0.9650 - val_loss: 3.7492 - val_mean_squared_error: 3.7492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxQYX1btaBRH",
        "colab_type": "text"
      },
      "source": [
        "## Stride versus Pool Sensitivity\n",
        "\n",
        "The following code section will look at using a stride of 2 instead of using a pooling layer in order to determine which performs better for our model. Both cases perform the same task of reducing our input matrix size by a factor of 2. Based on some reading, it was indicated that the use of a stride greater than one might be a good alternative to adding a pooling layer.\n",
        "\n",
        "However the results of our limited study indicated that max pooling actually performed better. From this point on all studies maintained a stride of one. In other words when performing the convolution, our kernel slides one feature or matrix cell at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuhUYt20RRFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_stride_cnn_model(start_filter, d, step):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model \n",
        "    '''\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "    cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), strides=(2,2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (2, 2), strides=(2,2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (2, 2), strides=(2,2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+2*step))\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(500))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(500))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(30))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb-0PZfjahBg",
        "colab_type": "code",
        "outputId": "5455601c-76f8-402b-e8ec-db51cea8e150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Redefine optimizer list to just focus on adam and sgd\n",
        "opt_list = {'adam':adam, 'sgd':sgd}\n",
        "\n",
        "# Use an early stopping callback and our timing callback\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                              patience=100, mode='auto')\n",
        "time_callback = TimeHistory()\n",
        "\n",
        "# Initialize a new data frame to hold our output data\n",
        "cnn_stride_df = pd.DataFrame()\n",
        "\n",
        "# Start filter list\n",
        "start_filters = [16]\n",
        "\n",
        "# Create a list of initial dropout values and steps to increase\n",
        "dropouts = [(0.00,0.00), (0.0,0.01), (0.00,0.02)]\n",
        "\n",
        "for opt_name, opt in opt_list.items():\n",
        "    for start_filter in start_filters:\n",
        "        for d in dropouts:\n",
        "            model = create_stride_cnn_model(start_filter,d[0], d[1])\n",
        "            model.compile(\n",
        "                  optimizer=opt,\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_squared_error'])\n",
        "            history = model.fit(\n",
        "                X.astype(np.float32), y.astype(np.float32),\n",
        "                epochs=200,\n",
        "                validation_split=0.15, callbacks=[time_callback, early_stop])\n",
        "            times = time_callback.times\n",
        "\n",
        "            # Convert to dataframe\n",
        "            hist = pd.DataFrame(history.history)\n",
        "            hist['epoch'] = history.epoch\n",
        "            hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "            hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "            hist['times'] = times\n",
        "            hist['starting_filter'] = start_filter\n",
        "            hist['layers'] = 3\n",
        "            hist['pooling'] = 'yes'\n",
        "            hist['fc_layer'] = 500\n",
        "            hist['activation'] = 'relu'\n",
        "            hist['optimizer'] = opt_name\n",
        "            hist['lrate'] = opt.get_config()['learning_rate']\n",
        "            hist['dropout_initial'] = d[0]\n",
        "            hist['dropout_step'] = d[1]\n",
        "            hist['batch_norm'] = 1\n",
        "            hist['bias'] = 0\n",
        "            hist['stride'] = 2\n",
        "\n",
        "            # Keep concatenating to dataframe\n",
        "            cnn_stride_df = pd.concat([cnn_stride_df,hist])\n",
        "\n",
        "            # Re-pickle after every model to retain progress\n",
        "            cnn_stride_df.to_pickle(drive_path+\"OutputData/cnn_stride_df.pkl\")\n",
        "\n",
        "            # Save models.\n",
        "            filename = \"cnn_stride_model_{}_d{}_s{}_sf{}_stride2\".format(opt_name, d[0], d[1], start_filter)\n",
        "            model.save(drive_path+\"Models/\"+filename+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 47, 47, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 47, 47, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 23, 23, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 23, 23, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 11, 11, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 636us/sample - loss: 2049.1691 - mean_squared_error: 2049.1689 - val_loss: 2008.9767 - val_mean_squared_error: 2008.9767\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 971.5344 - mean_squared_error: 971.5344 - val_loss: 982.7558 - val_mean_squared_error: 982.7559\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 340.4282 - mean_squared_error: 340.4282 - val_loss: 181.6316 - val_mean_squared_error: 181.6317\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 106.4754 - mean_squared_error: 106.4754 - val_loss: 59.0815 - val_mean_squared_error: 59.0815\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 43.1735 - mean_squared_error: 43.1735 - val_loss: 64.9920 - val_mean_squared_error: 64.9920\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 21.8872 - mean_squared_error: 21.8872 - val_loss: 25.0446 - val_mean_squared_error: 25.0446\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 19.1502 - mean_squared_error: 19.1502 - val_loss: 15.0909 - val_mean_squared_error: 15.0909\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 17.6137 - mean_squared_error: 17.6137 - val_loss: 17.1161 - val_mean_squared_error: 17.1161\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 12.8488 - mean_squared_error: 12.8488 - val_loss: 16.3345 - val_mean_squared_error: 16.3345\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 13.0733 - mean_squared_error: 13.0733 - val_loss: 12.6076 - val_mean_squared_error: 12.6076\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 11.4412 - mean_squared_error: 11.4412 - val_loss: 12.4693 - val_mean_squared_error: 12.4693\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 11.5235 - mean_squared_error: 11.5235 - val_loss: 11.6923 - val_mean_squared_error: 11.6923\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 10.3447 - mean_squared_error: 10.3446 - val_loss: 11.7521 - val_mean_squared_error: 11.7521\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 9.8211 - mean_squared_error: 9.8211 - val_loss: 10.7797 - val_mean_squared_error: 10.7797\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 10.1403 - mean_squared_error: 10.1403 - val_loss: 16.1372 - val_mean_squared_error: 16.1372\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 9.4024 - mean_squared_error: 9.4024 - val_loss: 12.8495 - val_mean_squared_error: 12.8495\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 8.6638 - mean_squared_error: 8.6638 - val_loss: 8.5557 - val_mean_squared_error: 8.5557\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 7.7820 - mean_squared_error: 7.7820 - val_loss: 8.0109 - val_mean_squared_error: 8.0109\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 7.2886 - mean_squared_error: 7.2886 - val_loss: 7.5669 - val_mean_squared_error: 7.5669\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 7.1572 - mean_squared_error: 7.1572 - val_loss: 7.9953 - val_mean_squared_error: 7.9953\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 6.8958 - mean_squared_error: 6.8958 - val_loss: 9.2246 - val_mean_squared_error: 9.2246\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 7.2268 - mean_squared_error: 7.2268 - val_loss: 7.6976 - val_mean_squared_error: 7.6976\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 6.3183 - mean_squared_error: 6.3183 - val_loss: 6.3677 - val_mean_squared_error: 6.3677\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 6.2205 - mean_squared_error: 6.2205 - val_loss: 6.3799 - val_mean_squared_error: 6.3799\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 5.9314 - mean_squared_error: 5.9314 - val_loss: 6.3843 - val_mean_squared_error: 6.3843\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 5.4099 - mean_squared_error: 5.4099 - val_loss: 6.1325 - val_mean_squared_error: 6.1325\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 5.2052 - mean_squared_error: 5.2052 - val_loss: 6.4031 - val_mean_squared_error: 6.4031\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 5.7661 - mean_squared_error: 5.7661 - val_loss: 5.3838 - val_mean_squared_error: 5.3838\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 5.0298 - mean_squared_error: 5.0298 - val_loss: 6.7613 - val_mean_squared_error: 6.7613\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 4.6147 - mean_squared_error: 4.6147 - val_loss: 7.4356 - val_mean_squared_error: 7.4357\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 4.5930 - mean_squared_error: 4.5930 - val_loss: 7.3788 - val_mean_squared_error: 7.3788\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 4.8447 - mean_squared_error: 4.8447 - val_loss: 5.6912 - val_mean_squared_error: 5.6912\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 4.3010 - mean_squared_error: 4.3010 - val_loss: 5.2225 - val_mean_squared_error: 5.2225\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 3.9815 - mean_squared_error: 3.9815 - val_loss: 5.3625 - val_mean_squared_error: 5.3625\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 3.6900 - mean_squared_error: 3.6900 - val_loss: 5.2586 - val_mean_squared_error: 5.2586\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 3.5207 - mean_squared_error: 3.5207 - val_loss: 4.7961 - val_mean_squared_error: 4.7961\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 3.6323 - mean_squared_error: 3.6323 - val_loss: 4.6863 - val_mean_squared_error: 4.6863\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 3.4422 - mean_squared_error: 3.4422 - val_loss: 4.3749 - val_mean_squared_error: 4.3749\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 3.5371 - mean_squared_error: 3.5371 - val_loss: 4.6471 - val_mean_squared_error: 4.6471\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 3.5114 - mean_squared_error: 3.5114 - val_loss: 5.0990 - val_mean_squared_error: 5.0990\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 3.3855 - mean_squared_error: 3.3855 - val_loss: 4.0196 - val_mean_squared_error: 4.0196\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 3.0621 - mean_squared_error: 3.0621 - val_loss: 4.6513 - val_mean_squared_error: 4.6513\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 3.2321 - mean_squared_error: 3.2321 - val_loss: 5.0443 - val_mean_squared_error: 5.0443\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 3.0244 - mean_squared_error: 3.0244 - val_loss: 4.7235 - val_mean_squared_error: 4.7235\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.8338 - mean_squared_error: 2.8338 - val_loss: 4.7589 - val_mean_squared_error: 4.7589\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.9092 - mean_squared_error: 2.9092 - val_loss: 4.2882 - val_mean_squared_error: 4.2882\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 2.9760 - mean_squared_error: 2.9760 - val_loss: 4.3009 - val_mean_squared_error: 4.3009\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.8549 - mean_squared_error: 2.8549 - val_loss: 3.9676 - val_mean_squared_error: 3.9676\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 2.7016 - mean_squared_error: 2.7016 - val_loss: 4.0619 - val_mean_squared_error: 4.0619\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.7550 - mean_squared_error: 2.7550 - val_loss: 5.2858 - val_mean_squared_error: 5.2858\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.9871 - mean_squared_error: 2.9871 - val_loss: 3.7768 - val_mean_squared_error: 3.7768\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.7841 - mean_squared_error: 2.7841 - val_loss: 3.9382 - val_mean_squared_error: 3.9382\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.7132 - mean_squared_error: 2.7132 - val_loss: 4.2698 - val_mean_squared_error: 4.2698\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 2.6212 - mean_squared_error: 2.6212 - val_loss: 3.5228 - val_mean_squared_error: 3.5228\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 2.7717 - mean_squared_error: 2.7717 - val_loss: 3.6308 - val_mean_squared_error: 3.6308\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.4362 - mean_squared_error: 2.4362 - val_loss: 3.6323 - val_mean_squared_error: 3.6323\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 0s 259us/sample - loss: 2.4964 - mean_squared_error: 2.4964 - val_loss: 4.1736 - val_mean_squared_error: 4.1736\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.5899 - mean_squared_error: 2.5899 - val_loss: 3.5068 - val_mean_squared_error: 3.5068\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 2.3613 - mean_squared_error: 2.3613 - val_loss: 3.5913 - val_mean_squared_error: 3.5913\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.1814 - mean_squared_error: 2.1814 - val_loss: 3.3884 - val_mean_squared_error: 3.3884\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.2443 - mean_squared_error: 2.2443 - val_loss: 4.7800 - val_mean_squared_error: 4.7800\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.2582 - mean_squared_error: 2.2582 - val_loss: 3.4790 - val_mean_squared_error: 3.4790\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 2.3073 - mean_squared_error: 2.3073 - val_loss: 3.5033 - val_mean_squared_error: 3.5033\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 2.1433 - mean_squared_error: 2.1433 - val_loss: 3.6507 - val_mean_squared_error: 3.6507\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.0835 - mean_squared_error: 2.0835 - val_loss: 3.6283 - val_mean_squared_error: 3.6283\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 2.2265 - mean_squared_error: 2.2265 - val_loss: 3.5066 - val_mean_squared_error: 3.5066\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.3094 - mean_squared_error: 2.3094 - val_loss: 3.4184 - val_mean_squared_error: 3.4184\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 2.2691 - mean_squared_error: 2.2691 - val_loss: 5.1261 - val_mean_squared_error: 5.1261\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.1888 - mean_squared_error: 2.1888 - val_loss: 3.2838 - val_mean_squared_error: 3.2838\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 2.2583 - mean_squared_error: 2.2583 - val_loss: 4.1016 - val_mean_squared_error: 4.1016\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.1493 - mean_squared_error: 2.1493 - val_loss: 4.5967 - val_mean_squared_error: 4.5967\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 2.3242 - mean_squared_error: 2.3242 - val_loss: 3.5842 - val_mean_squared_error: 3.5842\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.1695 - mean_squared_error: 2.1695 - val_loss: 3.8381 - val_mean_squared_error: 3.8381\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 2.1535 - mean_squared_error: 2.1535 - val_loss: 3.9764 - val_mean_squared_error: 3.9764\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.0386 - mean_squared_error: 2.0386 - val_loss: 5.0688 - val_mean_squared_error: 5.0688\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 2.1396 - mean_squared_error: 2.1396 - val_loss: 4.1027 - val_mean_squared_error: 4.1027\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.8949 - mean_squared_error: 1.8949 - val_loss: 3.4398 - val_mean_squared_error: 3.4398\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.9256 - mean_squared_error: 1.9256 - val_loss: 4.2186 - val_mean_squared_error: 4.2186\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.8847 - mean_squared_error: 1.8847 - val_loss: 3.5578 - val_mean_squared_error: 3.5578\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 1.8467 - mean_squared_error: 1.8467 - val_loss: 3.4126 - val_mean_squared_error: 3.4126\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.8597 - mean_squared_error: 1.8597 - val_loss: 3.4995 - val_mean_squared_error: 3.4995\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.9790 - mean_squared_error: 1.9790 - val_loss: 3.4614 - val_mean_squared_error: 3.4614\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.8450 - mean_squared_error: 1.8450 - val_loss: 3.8350 - val_mean_squared_error: 3.8350\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.9170 - mean_squared_error: 1.9170 - val_loss: 3.5201 - val_mean_squared_error: 3.5201\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.7526 - mean_squared_error: 1.7526 - val_loss: 3.6452 - val_mean_squared_error: 3.6452\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.9153 - mean_squared_error: 1.9153 - val_loss: 3.3258 - val_mean_squared_error: 3.3258\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.7535 - mean_squared_error: 1.7535 - val_loss: 3.1821 - val_mean_squared_error: 3.1821\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.8133 - mean_squared_error: 1.8133 - val_loss: 3.3732 - val_mean_squared_error: 3.3732\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 2.1274 - mean_squared_error: 2.1274 - val_loss: 3.2372 - val_mean_squared_error: 3.2372\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.9386 - mean_squared_error: 1.9386 - val_loss: 3.5564 - val_mean_squared_error: 3.5564\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.8841 - mean_squared_error: 1.8841 - val_loss: 3.5756 - val_mean_squared_error: 3.5756\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.7348 - mean_squared_error: 1.7348 - val_loss: 3.5380 - val_mean_squared_error: 3.5380\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.7349 - mean_squared_error: 1.7349 - val_loss: 3.6041 - val_mean_squared_error: 3.6041\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.8005 - mean_squared_error: 1.8005 - val_loss: 3.5942 - val_mean_squared_error: 3.5942\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 1.7077 - mean_squared_error: 1.7077 - val_loss: 4.9999 - val_mean_squared_error: 4.9999\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.7467 - mean_squared_error: 1.7467 - val_loss: 3.6253 - val_mean_squared_error: 3.6253\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.5296 - mean_squared_error: 1.5296 - val_loss: 3.0965 - val_mean_squared_error: 3.0965\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.5226 - mean_squared_error: 1.5226 - val_loss: 3.1056 - val_mean_squared_error: 3.1056\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.5178 - mean_squared_error: 1.5178 - val_loss: 4.8810 - val_mean_squared_error: 4.8810\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.6850 - mean_squared_error: 1.6850 - val_loss: 3.2978 - val_mean_squared_error: 3.2978\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.5879 - mean_squared_error: 1.5879 - val_loss: 3.9906 - val_mean_squared_error: 3.9906\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.5147 - mean_squared_error: 1.5147 - val_loss: 3.2957 - val_mean_squared_error: 3.2957\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 0s 259us/sample - loss: 1.6706 - mean_squared_error: 1.6706 - val_loss: 3.4307 - val_mean_squared_error: 3.4307\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.5482 - mean_squared_error: 1.5482 - val_loss: 3.4853 - val_mean_squared_error: 3.4853\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.5028 - mean_squared_error: 1.5028 - val_loss: 3.0381 - val_mean_squared_error: 3.0381\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.5800 - mean_squared_error: 1.5800 - val_loss: 3.1995 - val_mean_squared_error: 3.1995\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 1.5430 - mean_squared_error: 1.5430 - val_loss: 3.1540 - val_mean_squared_error: 3.1540\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.4933 - mean_squared_error: 1.4933 - val_loss: 3.7560 - val_mean_squared_error: 3.7560\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.4160 - mean_squared_error: 1.4160 - val_loss: 3.7233 - val_mean_squared_error: 3.7233\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.3385 - mean_squared_error: 1.3385 - val_loss: 3.5470 - val_mean_squared_error: 3.5470\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 1.4341 - mean_squared_error: 1.4341 - val_loss: 3.3458 - val_mean_squared_error: 3.3458\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.4379 - mean_squared_error: 1.4379 - val_loss: 3.4171 - val_mean_squared_error: 3.4171\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.3854 - mean_squared_error: 1.3854 - val_loss: 3.4555 - val_mean_squared_error: 3.4555\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.4648 - mean_squared_error: 1.4648 - val_loss: 3.5208 - val_mean_squared_error: 3.5208\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.4023 - mean_squared_error: 1.4023 - val_loss: 3.0822 - val_mean_squared_error: 3.0822\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.4930 - mean_squared_error: 1.4930 - val_loss: 3.4225 - val_mean_squared_error: 3.4225\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.4775 - mean_squared_error: 1.4775 - val_loss: 3.1812 - val_mean_squared_error: 3.1812\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.3829 - mean_squared_error: 1.3829 - val_loss: 4.1106 - val_mean_squared_error: 4.1106\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.3309 - mean_squared_error: 1.3309 - val_loss: 3.1292 - val_mean_squared_error: 3.1292\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.3482 - mean_squared_error: 1.3482 - val_loss: 3.3282 - val_mean_squared_error: 3.3282\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.2774 - mean_squared_error: 1.2774 - val_loss: 3.3332 - val_mean_squared_error: 3.3332\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.3409 - mean_squared_error: 1.3409 - val_loss: 3.2846 - val_mean_squared_error: 3.2846\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.3253 - mean_squared_error: 1.3253 - val_loss: 3.6148 - val_mean_squared_error: 3.6148\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.3960 - mean_squared_error: 1.3960 - val_loss: 3.0988 - val_mean_squared_error: 3.0988\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.3048 - mean_squared_error: 1.3048 - val_loss: 3.2159 - val_mean_squared_error: 3.2159\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.2614 - mean_squared_error: 1.2614 - val_loss: 3.7238 - val_mean_squared_error: 3.7238\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.2661 - mean_squared_error: 1.2661 - val_loss: 3.2757 - val_mean_squared_error: 3.2757\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.2135 - mean_squared_error: 1.2135 - val_loss: 3.2404 - val_mean_squared_error: 3.2404\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2448 - mean_squared_error: 1.2448 - val_loss: 3.5274 - val_mean_squared_error: 3.5274\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.3672 - mean_squared_error: 1.3672 - val_loss: 3.2678 - val_mean_squared_error: 3.2678\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.2277 - mean_squared_error: 1.2277 - val_loss: 3.2395 - val_mean_squared_error: 3.2395\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.2607 - mean_squared_error: 1.2607 - val_loss: 3.1941 - val_mean_squared_error: 3.1941\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.2605 - mean_squared_error: 1.2605 - val_loss: 3.4215 - val_mean_squared_error: 3.4215\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.3181 - mean_squared_error: 1.3181 - val_loss: 3.0600 - val_mean_squared_error: 3.0600\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.4322 - mean_squared_error: 1.4322 - val_loss: 3.2017 - val_mean_squared_error: 3.2017\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 0s 259us/sample - loss: 1.2901 - mean_squared_error: 1.2901 - val_loss: 3.2578 - val_mean_squared_error: 3.2578\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.2496 - mean_squared_error: 1.2496 - val_loss: 3.3711 - val_mean_squared_error: 3.3711\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 1.2330 - mean_squared_error: 1.2330 - val_loss: 3.1952 - val_mean_squared_error: 3.1952\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.2125 - mean_squared_error: 1.2125 - val_loss: 3.2523 - val_mean_squared_error: 3.2523\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.2546 - mean_squared_error: 1.2546 - val_loss: 3.0099 - val_mean_squared_error: 3.0099\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.1554 - mean_squared_error: 1.1554 - val_loss: 3.2956 - val_mean_squared_error: 3.2956\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.1697 - mean_squared_error: 1.1697 - val_loss: 3.0540 - val_mean_squared_error: 3.0540\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.4842 - mean_squared_error: 1.4842 - val_loss: 3.9968 - val_mean_squared_error: 3.9968\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.3077 - mean_squared_error: 1.3077 - val_loss: 3.2600 - val_mean_squared_error: 3.2600\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.2051 - mean_squared_error: 1.2051 - val_loss: 3.0099 - val_mean_squared_error: 3.0099\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.2337 - mean_squared_error: 1.2337 - val_loss: 3.8897 - val_mean_squared_error: 3.8897\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.5919 - mean_squared_error: 1.5919 - val_loss: 4.5350 - val_mean_squared_error: 4.5350\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.3623 - mean_squared_error: 1.3623 - val_loss: 3.2515 - val_mean_squared_error: 3.2515\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 1.3013 - mean_squared_error: 1.3013 - val_loss: 3.1506 - val_mean_squared_error: 3.1506\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.1692 - mean_squared_error: 1.1692 - val_loss: 3.1673 - val_mean_squared_error: 3.1673\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.1539 - mean_squared_error: 1.1539 - val_loss: 3.2208 - val_mean_squared_error: 3.2208\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.0251 - mean_squared_error: 1.0251 - val_loss: 3.3957 - val_mean_squared_error: 3.3957\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 1.1529 - mean_squared_error: 1.1529 - val_loss: 3.2374 - val_mean_squared_error: 3.2374\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 1.1617 - mean_squared_error: 1.1617 - val_loss: 3.3251 - val_mean_squared_error: 3.3251\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 0s 259us/sample - loss: 1.1803 - mean_squared_error: 1.1803 - val_loss: 3.3907 - val_mean_squared_error: 3.3907\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.0735 - mean_squared_error: 1.0735 - val_loss: 3.4379 - val_mean_squared_error: 3.4379\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.0068 - mean_squared_error: 1.0068 - val_loss: 3.2955 - val_mean_squared_error: 3.2955\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.0901 - mean_squared_error: 1.0901 - val_loss: 3.1763 - val_mean_squared_error: 3.1763\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.1201 - mean_squared_error: 1.1201 - val_loss: 3.2283 - val_mean_squared_error: 3.2283\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.1995 - mean_squared_error: 1.1995 - val_loss: 3.3825 - val_mean_squared_error: 3.3825\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.2389 - mean_squared_error: 1.2389 - val_loss: 3.1573 - val_mean_squared_error: 3.1573\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 1.0922 - mean_squared_error: 1.0922 - val_loss: 3.0806 - val_mean_squared_error: 3.0806\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.0818 - mean_squared_error: 1.0818 - val_loss: 2.8946 - val_mean_squared_error: 2.8946\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.0211 - mean_squared_error: 1.0211 - val_loss: 3.1678 - val_mean_squared_error: 3.1678\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.0494 - mean_squared_error: 1.0494 - val_loss: 3.3109 - val_mean_squared_error: 3.3109\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 1.1266 - mean_squared_error: 1.1266 - val_loss: 3.1444 - val_mean_squared_error: 3.1444\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 0.9707 - mean_squared_error: 0.9707 - val_loss: 3.1765 - val_mean_squared_error: 3.1765\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.0909 - mean_squared_error: 1.0909 - val_loss: 3.1025 - val_mean_squared_error: 3.1025\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 0.9708 - mean_squared_error: 0.9708 - val_loss: 3.4532 - val_mean_squared_error: 3.4532\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.0565 - mean_squared_error: 1.0565 - val_loss: 4.1342 - val_mean_squared_error: 4.1342\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.0611 - mean_squared_error: 1.0611 - val_loss: 3.5898 - val_mean_squared_error: 3.5898\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.0596 - mean_squared_error: 1.0596 - val_loss: 2.9996 - val_mean_squared_error: 2.9996\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.0575 - mean_squared_error: 1.0575 - val_loss: 3.1887 - val_mean_squared_error: 3.1887\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 1.0649 - mean_squared_error: 1.0649 - val_loss: 3.1812 - val_mean_squared_error: 3.1812\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.0287 - mean_squared_error: 1.0287 - val_loss: 3.0099 - val_mean_squared_error: 3.0099\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 3.5218 - val_mean_squared_error: 3.5218\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 0.9838 - mean_squared_error: 0.9838 - val_loss: 3.5429 - val_mean_squared_error: 3.5429\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 1.1833 - mean_squared_error: 1.1833 - val_loss: 3.4616 - val_mean_squared_error: 3.4616\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.0547 - mean_squared_error: 1.0547 - val_loss: 3.2239 - val_mean_squared_error: 3.2239\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.0271 - mean_squared_error: 1.0271 - val_loss: 2.9289 - val_mean_squared_error: 2.9289\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 0.9961 - mean_squared_error: 0.9961 - val_loss: 3.0439 - val_mean_squared_error: 3.0439\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.0517 - mean_squared_error: 1.0517 - val_loss: 2.8955 - val_mean_squared_error: 2.8955\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 1.0030 - mean_squared_error: 1.0030 - val_loss: 3.0473 - val_mean_squared_error: 3.0473\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 0.9863 - mean_squared_error: 0.9863 - val_loss: 3.0948 - val_mean_squared_error: 3.0948\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 0.9844 - mean_squared_error: 0.9844 - val_loss: 3.0820 - val_mean_squared_error: 3.0820\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 0.9577 - mean_squared_error: 0.9577 - val_loss: 3.0971 - val_mean_squared_error: 3.0971\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 0.9179 - mean_squared_error: 0.9179 - val_loss: 3.2698 - val_mean_squared_error: 3.2698\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 0.9672 - mean_squared_error: 0.9672 - val_loss: 3.0824 - val_mean_squared_error: 3.0824\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 0.9448 - mean_squared_error: 0.9448 - val_loss: 2.9945 - val_mean_squared_error: 2.9945\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 0.9570 - mean_squared_error: 0.9570 - val_loss: 2.8524 - val_mean_squared_error: 2.8524\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 0.9737 - mean_squared_error: 0.9737 - val_loss: 3.2529 - val_mean_squared_error: 3.2529\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 0.9648 - mean_squared_error: 0.9648 - val_loss: 3.2654 - val_mean_squared_error: 3.2654\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 0s 262us/sample - loss: 0.9296 - mean_squared_error: 0.9296 - val_loss: 3.1039 - val_mean_squared_error: 3.1039\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 0.9700 - mean_squared_error: 0.9700 - val_loss: 3.0817 - val_mean_squared_error: 3.0817\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 0.9452 - mean_squared_error: 0.9452 - val_loss: 2.9810 - val_mean_squared_error: 2.9810\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 0.9052 - mean_squared_error: 0.9052 - val_loss: 2.9796 - val_mean_squared_error: 2.9796\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 0.8669 - mean_squared_error: 0.8669 - val_loss: 2.9057 - val_mean_squared_error: 2.9057\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 0.9495 - mean_squared_error: 0.9495 - val_loss: 2.9174 - val_mean_squared_error: 2.9174\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 0s 263us/sample - loss: 0.9711 - mean_squared_error: 0.9711 - val_loss: 2.9775 - val_mean_squared_error: 2.9775\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 0.8725 - mean_squared_error: 0.8725 - val_loss: 3.2153 - val_mean_squared_error: 3.2153\n",
            "==================================================\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 47, 47, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 47, 47, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 23, 23, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 23, 23, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 11, 11, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 680us/sample - loss: 659.3698 - mean_squared_error: 659.3699 - val_loss: 513.1532 - val_mean_squared_error: 513.1533\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 25.8767 - mean_squared_error: 25.8767 - val_loss: 187.0349 - val_mean_squared_error: 187.0349\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 19.1004 - mean_squared_error: 19.1004 - val_loss: 73.3706 - val_mean_squared_error: 73.3706\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 19.1033 - mean_squared_error: 19.1033 - val_loss: 36.2824 - val_mean_squared_error: 36.2824\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 286us/sample - loss: 17.5863 - mean_squared_error: 17.5863 - val_loss: 22.4220 - val_mean_squared_error: 22.4220\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 296us/sample - loss: 16.3921 - mean_squared_error: 16.3921 - val_loss: 21.1731 - val_mean_squared_error: 21.1731\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 313us/sample - loss: 14.2596 - mean_squared_error: 14.2596 - val_loss: 12.0758 - val_mean_squared_error: 12.0757\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 14.6091 - mean_squared_error: 14.6091 - val_loss: 10.8359 - val_mean_squared_error: 10.8359\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 14.0447 - mean_squared_error: 14.0447 - val_loss: 20.4389 - val_mean_squared_error: 20.4389\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 14.6168 - mean_squared_error: 14.6168 - val_loss: 10.7891 - val_mean_squared_error: 10.7891\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 12.1657 - mean_squared_error: 12.1657 - val_loss: 9.9050 - val_mean_squared_error: 9.9050\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 10.9459 - mean_squared_error: 10.9459 - val_loss: 9.4184 - val_mean_squared_error: 9.4184\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 11.7551 - mean_squared_error: 11.7551 - val_loss: 10.0355 - val_mean_squared_error: 10.0355\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 10.6724 - mean_squared_error: 10.6724 - val_loss: 10.9424 - val_mean_squared_error: 10.9424\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 10.3946 - mean_squared_error: 10.3946 - val_loss: 12.9927 - val_mean_squared_error: 12.9927\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 10.2394 - mean_squared_error: 10.2394 - val_loss: 10.1253 - val_mean_squared_error: 10.1253\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 10.3514 - mean_squared_error: 10.3514 - val_loss: 9.8423 - val_mean_squared_error: 9.8423\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 9.6846 - mean_squared_error: 9.6846 - val_loss: 13.8745 - val_mean_squared_error: 13.8745\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 9.4305 - mean_squared_error: 9.4305 - val_loss: 8.3052 - val_mean_squared_error: 8.3052\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 291us/sample - loss: 8.7847 - mean_squared_error: 8.7847 - val_loss: 8.8239 - val_mean_squared_error: 8.8239\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 286us/sample - loss: 8.7242 - mean_squared_error: 8.7242 - val_loss: 11.2603 - val_mean_squared_error: 11.2603\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 8.7765 - mean_squared_error: 8.7765 - val_loss: 8.9575 - val_mean_squared_error: 8.9575\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 290us/sample - loss: 7.8278 - mean_squared_error: 7.8278 - val_loss: 9.1722 - val_mean_squared_error: 9.1722\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 7.5385 - mean_squared_error: 7.5385 - val_loss: 7.5951 - val_mean_squared_error: 7.5951\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 7.3524 - mean_squared_error: 7.3524 - val_loss: 7.9848 - val_mean_squared_error: 7.9848\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 7.2953 - mean_squared_error: 7.2953 - val_loss: 8.9933 - val_mean_squared_error: 8.9933\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 285us/sample - loss: 6.6967 - mean_squared_error: 6.6967 - val_loss: 6.8073 - val_mean_squared_error: 6.8073\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 6.2704 - mean_squared_error: 6.2704 - val_loss: 6.9493 - val_mean_squared_error: 6.9493\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 293us/sample - loss: 6.1721 - mean_squared_error: 6.1721 - val_loss: 8.3320 - val_mean_squared_error: 8.3320\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 6.3073 - mean_squared_error: 6.3073 - val_loss: 7.5080 - val_mean_squared_error: 7.5080\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 5.4580 - mean_squared_error: 5.4580 - val_loss: 6.2087 - val_mean_squared_error: 6.2087\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 5.5133 - mean_squared_error: 5.5133 - val_loss: 5.8132 - val_mean_squared_error: 5.8132\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 285us/sample - loss: 5.1548 - mean_squared_error: 5.1548 - val_loss: 5.9204 - val_mean_squared_error: 5.9204\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 285us/sample - loss: 5.1581 - mean_squared_error: 5.1581 - val_loss: 6.2786 - val_mean_squared_error: 6.2786\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 4.8736 - mean_squared_error: 4.8736 - val_loss: 6.6903 - val_mean_squared_error: 6.6903\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 4.7530 - mean_squared_error: 4.7530 - val_loss: 5.2568 - val_mean_squared_error: 5.2568\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 286us/sample - loss: 4.8858 - mean_squared_error: 4.8858 - val_loss: 6.2578 - val_mean_squared_error: 6.2578\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 4.5399 - mean_squared_error: 4.5399 - val_loss: 6.1132 - val_mean_squared_error: 6.1132\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 284us/sample - loss: 4.7753 - mean_squared_error: 4.7753 - val_loss: 4.8856 - val_mean_squared_error: 4.8856\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 4.1750 - mean_squared_error: 4.1750 - val_loss: 5.4377 - val_mean_squared_error: 5.4377\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 4.3842 - mean_squared_error: 4.3842 - val_loss: 4.9493 - val_mean_squared_error: 4.9493\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 4.2187 - mean_squared_error: 4.2187 - val_loss: 5.4753 - val_mean_squared_error: 5.4753\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 3.9629 - mean_squared_error: 3.9629 - val_loss: 5.7318 - val_mean_squared_error: 5.7318\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 3.8955 - mean_squared_error: 3.8955 - val_loss: 4.9175 - val_mean_squared_error: 4.9175\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 4.0900 - mean_squared_error: 4.0900 - val_loss: 4.9306 - val_mean_squared_error: 4.9306\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 3.4866 - mean_squared_error: 3.4866 - val_loss: 4.6905 - val_mean_squared_error: 4.6905\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 3.4576 - mean_squared_error: 3.4576 - val_loss: 4.3268 - val_mean_squared_error: 4.3268\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 3.6554 - mean_squared_error: 3.6554 - val_loss: 4.3148 - val_mean_squared_error: 4.3148\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 285us/sample - loss: 3.6045 - mean_squared_error: 3.6045 - val_loss: 4.5818 - val_mean_squared_error: 4.5818\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 3.3937 - mean_squared_error: 3.3937 - val_loss: 4.2896 - val_mean_squared_error: 4.2896\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 3.3881 - mean_squared_error: 3.3881 - val_loss: 4.3314 - val_mean_squared_error: 4.3314\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 3.1576 - mean_squared_error: 3.1576 - val_loss: 4.1368 - val_mean_squared_error: 4.1368\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 3.0365 - mean_squared_error: 3.0365 - val_loss: 4.0561 - val_mean_squared_error: 4.0561\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 3.3679 - mean_squared_error: 3.3679 - val_loss: 4.6871 - val_mean_squared_error: 4.6871\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 3.3145 - mean_squared_error: 3.3145 - val_loss: 4.1231 - val_mean_squared_error: 4.1231\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 3.3628 - mean_squared_error: 3.3628 - val_loss: 3.9424 - val_mean_squared_error: 3.9424\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 2.9687 - mean_squared_error: 2.9687 - val_loss: 3.9180 - val_mean_squared_error: 3.9180\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.9448 - mean_squared_error: 2.9448 - val_loss: 3.7895 - val_mean_squared_error: 3.7895\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 3.2714 - mean_squared_error: 3.2714 - val_loss: 6.6526 - val_mean_squared_error: 6.6526\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.9130 - mean_squared_error: 2.9130 - val_loss: 3.9143 - val_mean_squared_error: 3.9143\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.8531 - mean_squared_error: 2.8531 - val_loss: 4.4284 - val_mean_squared_error: 4.4284\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.8156 - mean_squared_error: 2.8156 - val_loss: 3.5911 - val_mean_squared_error: 3.5911\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 2.7073 - mean_squared_error: 2.7073 - val_loss: 3.9628 - val_mean_squared_error: 3.9628\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.7188 - mean_squared_error: 2.7188 - val_loss: 3.7007 - val_mean_squared_error: 3.7007\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 2.7969 - mean_squared_error: 2.7969 - val_loss: 3.4469 - val_mean_squared_error: 3.4469\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.4883 - mean_squared_error: 2.4883 - val_loss: 4.2252 - val_mean_squared_error: 4.2252\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.3896 - mean_squared_error: 2.3896 - val_loss: 3.4976 - val_mean_squared_error: 3.4976\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.4826 - mean_squared_error: 2.4826 - val_loss: 3.7043 - val_mean_squared_error: 3.7043\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 2.5175 - mean_squared_error: 2.5175 - val_loss: 3.9103 - val_mean_squared_error: 3.9103\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.4320 - mean_squared_error: 2.4320 - val_loss: 3.4495 - val_mean_squared_error: 3.4495\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.4296 - mean_squared_error: 2.4296 - val_loss: 3.5664 - val_mean_squared_error: 3.5664\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.5832 - mean_squared_error: 2.5832 - val_loss: 3.6877 - val_mean_squared_error: 3.6877\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.5340 - mean_squared_error: 2.5340 - val_loss: 3.6902 - val_mean_squared_error: 3.6902\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.4158 - mean_squared_error: 2.4158 - val_loss: 3.7894 - val_mean_squared_error: 3.7894\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.2845 - mean_squared_error: 2.2845 - val_loss: 3.1815 - val_mean_squared_error: 3.1815\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.3072 - mean_squared_error: 2.3072 - val_loss: 3.3901 - val_mean_squared_error: 3.3901\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 2.4883 - mean_squared_error: 2.4883 - val_loss: 3.5686 - val_mean_squared_error: 3.5686\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.2621 - mean_squared_error: 2.2621 - val_loss: 3.2592 - val_mean_squared_error: 3.2592\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.1030 - mean_squared_error: 2.1030 - val_loss: 3.3292 - val_mean_squared_error: 3.3292\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 2.1467 - mean_squared_error: 2.1467 - val_loss: 3.6425 - val_mean_squared_error: 3.6425\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.1393 - mean_squared_error: 2.1393 - val_loss: 3.2512 - val_mean_squared_error: 3.2512\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 2.1127 - mean_squared_error: 2.1127 - val_loss: 3.2395 - val_mean_squared_error: 3.2395\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.2944 - mean_squared_error: 2.2944 - val_loss: 3.9942 - val_mean_squared_error: 3.9942\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 2.2783 - mean_squared_error: 2.2783 - val_loss: 3.8218 - val_mean_squared_error: 3.8218\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.2037 - mean_squared_error: 2.2037 - val_loss: 3.3707 - val_mean_squared_error: 3.3707\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 2.1968 - mean_squared_error: 2.1968 - val_loss: 3.3114 - val_mean_squared_error: 3.3114\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 2.1694 - mean_squared_error: 2.1694 - val_loss: 3.2300 - val_mean_squared_error: 3.2300\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 2.2674 - mean_squared_error: 2.2674 - val_loss: 3.2778 - val_mean_squared_error: 3.2778\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 2.0229 - mean_squared_error: 2.0229 - val_loss: 3.4083 - val_mean_squared_error: 3.4083\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 2.1458 - mean_squared_error: 2.1458 - val_loss: 3.8107 - val_mean_squared_error: 3.8107\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.0327 - mean_squared_error: 2.0327 - val_loss: 3.6203 - val_mean_squared_error: 3.6203\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.9426 - mean_squared_error: 1.9426 - val_loss: 3.1981 - val_mean_squared_error: 3.1981\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.9561 - mean_squared_error: 1.9561 - val_loss: 3.2556 - val_mean_squared_error: 3.2556\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.9560 - mean_squared_error: 1.9560 - val_loss: 3.1729 - val_mean_squared_error: 3.1729\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.8612 - mean_squared_error: 1.8612 - val_loss: 3.3626 - val_mean_squared_error: 3.3626\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.8681 - mean_squared_error: 1.8681 - val_loss: 3.3389 - val_mean_squared_error: 3.3389\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.9120 - mean_squared_error: 1.9120 - val_loss: 3.0698 - val_mean_squared_error: 3.0698\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.8447 - mean_squared_error: 1.8447 - val_loss: 3.5020 - val_mean_squared_error: 3.5020\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.8290 - mean_squared_error: 1.8290 - val_loss: 3.1300 - val_mean_squared_error: 3.1300\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.9110 - mean_squared_error: 1.9110 - val_loss: 3.0400 - val_mean_squared_error: 3.0400\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.8228 - mean_squared_error: 1.8228 - val_loss: 3.0054 - val_mean_squared_error: 3.0054\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.8431 - mean_squared_error: 1.8431 - val_loss: 3.2053 - val_mean_squared_error: 3.2053\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.8483 - mean_squared_error: 1.8483 - val_loss: 3.3704 - val_mean_squared_error: 3.3704\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.7949 - mean_squared_error: 1.7949 - val_loss: 3.2085 - val_mean_squared_error: 3.2085\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.8352 - mean_squared_error: 1.8352 - val_loss: 3.4324 - val_mean_squared_error: 3.4324\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.9036 - mean_squared_error: 1.9036 - val_loss: 2.9613 - val_mean_squared_error: 2.9613\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.7133 - mean_squared_error: 1.7133 - val_loss: 3.1131 - val_mean_squared_error: 3.1131\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.7007 - mean_squared_error: 1.7007 - val_loss: 3.2731 - val_mean_squared_error: 3.2731\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.7247 - mean_squared_error: 1.7247 - val_loss: 2.9587 - val_mean_squared_error: 2.9587\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 285us/sample - loss: 1.6305 - mean_squared_error: 1.6305 - val_loss: 3.1655 - val_mean_squared_error: 3.1655\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.7948 - mean_squared_error: 1.7948 - val_loss: 3.6298 - val_mean_squared_error: 3.6298\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.7986 - mean_squared_error: 1.7986 - val_loss: 3.6616 - val_mean_squared_error: 3.6616\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.7916 - mean_squared_error: 1.7916 - val_loss: 3.2063 - val_mean_squared_error: 3.2063\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 1.7883 - mean_squared_error: 1.7883 - val_loss: 3.5868 - val_mean_squared_error: 3.5868\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.6178 - mean_squared_error: 1.6178 - val_loss: 3.1612 - val_mean_squared_error: 3.1612\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.8174 - mean_squared_error: 1.8174 - val_loss: 3.0607 - val_mean_squared_error: 3.0607\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.6723 - mean_squared_error: 1.6723 - val_loss: 3.0601 - val_mean_squared_error: 3.0601\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.5816 - mean_squared_error: 1.5816 - val_loss: 3.0963 - val_mean_squared_error: 3.0963\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.6449 - mean_squared_error: 1.6449 - val_loss: 3.1157 - val_mean_squared_error: 3.1157\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.7288 - mean_squared_error: 1.7288 - val_loss: 2.9552 - val_mean_squared_error: 2.9552\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.6435 - mean_squared_error: 1.6435 - val_loss: 2.9209 - val_mean_squared_error: 2.9209\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.5276 - mean_squared_error: 1.5276 - val_loss: 3.0290 - val_mean_squared_error: 3.0290\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.6371 - mean_squared_error: 1.6371 - val_loss: 3.2716 - val_mean_squared_error: 3.2716\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.5307 - mean_squared_error: 1.5307 - val_loss: 2.9299 - val_mean_squared_error: 2.9299\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.4634 - mean_squared_error: 1.4634 - val_loss: 2.9627 - val_mean_squared_error: 2.9627\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.5141 - mean_squared_error: 1.5141 - val_loss: 2.9261 - val_mean_squared_error: 2.9261\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.5220 - mean_squared_error: 1.5220 - val_loss: 3.0705 - val_mean_squared_error: 3.0705\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.6131 - mean_squared_error: 1.6131 - val_loss: 3.1765 - val_mean_squared_error: 3.1765\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.5022 - mean_squared_error: 1.5022 - val_loss: 2.8382 - val_mean_squared_error: 2.8382\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 287us/sample - loss: 1.5017 - mean_squared_error: 1.5017 - val_loss: 3.0735 - val_mean_squared_error: 3.0735\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.5243 - mean_squared_error: 1.5243 - val_loss: 2.9996 - val_mean_squared_error: 2.9996\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.6970 - mean_squared_error: 1.6970 - val_loss: 3.1996 - val_mean_squared_error: 3.1996\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.5044 - mean_squared_error: 1.5044 - val_loss: 2.9407 - val_mean_squared_error: 2.9407\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.4585 - mean_squared_error: 1.4585 - val_loss: 2.9264 - val_mean_squared_error: 2.9264\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.5093 - mean_squared_error: 1.5093 - val_loss: 2.9994 - val_mean_squared_error: 2.9994\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.3524 - mean_squared_error: 1.3524 - val_loss: 2.9062 - val_mean_squared_error: 2.9062\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.4117 - mean_squared_error: 1.4117 - val_loss: 2.8826 - val_mean_squared_error: 2.8826\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 1.3736 - mean_squared_error: 1.3736 - val_loss: 3.0447 - val_mean_squared_error: 3.0447\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.4140 - mean_squared_error: 1.4140 - val_loss: 3.1324 - val_mean_squared_error: 3.1324\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.4548 - mean_squared_error: 1.4548 - val_loss: 2.9798 - val_mean_squared_error: 2.9798\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.4319 - mean_squared_error: 1.4319 - val_loss: 2.9281 - val_mean_squared_error: 2.9281\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.3698 - mean_squared_error: 1.3698 - val_loss: 3.0548 - val_mean_squared_error: 3.0548\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.3504 - mean_squared_error: 1.3504 - val_loss: 2.7826 - val_mean_squared_error: 2.7826\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.4328 - mean_squared_error: 1.4328 - val_loss: 3.1919 - val_mean_squared_error: 3.1919\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.3433 - mean_squared_error: 1.3433 - val_loss: 3.0759 - val_mean_squared_error: 3.0759\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.5905 - mean_squared_error: 1.5905 - val_loss: 3.0916 - val_mean_squared_error: 3.0916\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.3057 - mean_squared_error: 1.3057 - val_loss: 2.8730 - val_mean_squared_error: 2.8730\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.3301 - mean_squared_error: 1.3301 - val_loss: 2.9188 - val_mean_squared_error: 2.9188\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.4177 - mean_squared_error: 1.4177 - val_loss: 3.0622 - val_mean_squared_error: 3.0622\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.4214 - mean_squared_error: 1.4214 - val_loss: 2.9185 - val_mean_squared_error: 2.9185\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.3392 - mean_squared_error: 1.3392 - val_loss: 2.9177 - val_mean_squared_error: 2.9177\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.3535 - mean_squared_error: 1.3535 - val_loss: 2.7693 - val_mean_squared_error: 2.7693\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.3440 - mean_squared_error: 1.3440 - val_loss: 3.2663 - val_mean_squared_error: 3.2663\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.2981 - mean_squared_error: 1.2981 - val_loss: 2.9512 - val_mean_squared_error: 2.9512\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.2605 - mean_squared_error: 1.2605 - val_loss: 3.1548 - val_mean_squared_error: 3.1548\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.2529 - mean_squared_error: 1.2529 - val_loss: 2.7875 - val_mean_squared_error: 2.7875\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.2510 - mean_squared_error: 1.2510 - val_loss: 2.9835 - val_mean_squared_error: 2.9835\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.2847 - mean_squared_error: 1.2847 - val_loss: 2.8218 - val_mean_squared_error: 2.8218\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2667 - mean_squared_error: 1.2667 - val_loss: 2.9213 - val_mean_squared_error: 2.9213\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2640 - mean_squared_error: 1.2640 - val_loss: 2.9127 - val_mean_squared_error: 2.9127\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.3770 - mean_squared_error: 1.3770 - val_loss: 3.1925 - val_mean_squared_error: 3.1925\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.3102 - mean_squared_error: 1.3102 - val_loss: 2.8848 - val_mean_squared_error: 2.8848\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.2588 - mean_squared_error: 1.2588 - val_loss: 3.0142 - val_mean_squared_error: 3.0142\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.2288 - mean_squared_error: 1.2288 - val_loss: 2.8055 - val_mean_squared_error: 2.8055\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2613 - mean_squared_error: 1.2613 - val_loss: 2.7578 - val_mean_squared_error: 2.7578\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.3006 - mean_squared_error: 1.3006 - val_loss: 2.8286 - val_mean_squared_error: 2.8286\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.2871 - mean_squared_error: 1.2871 - val_loss: 2.7641 - val_mean_squared_error: 2.7641\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.2887 - mean_squared_error: 1.2887 - val_loss: 2.9298 - val_mean_squared_error: 2.9298\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.2336 - mean_squared_error: 1.2336 - val_loss: 3.0422 - val_mean_squared_error: 3.0422\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.4152 - mean_squared_error: 1.4152 - val_loss: 3.3211 - val_mean_squared_error: 3.3211\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.2072 - mean_squared_error: 1.2072 - val_loss: 3.2424 - val_mean_squared_error: 3.2424\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2155 - mean_squared_error: 1.2155 - val_loss: 3.2794 - val_mean_squared_error: 3.2794\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.2750 - mean_squared_error: 1.2750 - val_loss: 2.8636 - val_mean_squared_error: 2.8636\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.2374 - mean_squared_error: 1.2374 - val_loss: 2.9672 - val_mean_squared_error: 2.9672\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.2020 - mean_squared_error: 1.2020 - val_loss: 2.8320 - val_mean_squared_error: 2.8320\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.1526 - mean_squared_error: 1.1526 - val_loss: 2.7560 - val_mean_squared_error: 2.7560\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.1845 - mean_squared_error: 1.1845 - val_loss: 2.9294 - val_mean_squared_error: 2.9294\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.1637 - mean_squared_error: 1.1637 - val_loss: 2.8677 - val_mean_squared_error: 2.8677\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.1693 - mean_squared_error: 1.1693 - val_loss: 2.8893 - val_mean_squared_error: 2.8893\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.1735 - mean_squared_error: 1.1735 - val_loss: 3.1143 - val_mean_squared_error: 3.1143\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.2084 - mean_squared_error: 1.2084 - val_loss: 3.0420 - val_mean_squared_error: 3.0420\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.2613 - mean_squared_error: 1.2613 - val_loss: 2.7555 - val_mean_squared_error: 2.7555\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.2047 - mean_squared_error: 1.2047 - val_loss: 3.4434 - val_mean_squared_error: 3.4434\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.1834 - mean_squared_error: 1.1834 - val_loss: 2.8813 - val_mean_squared_error: 2.8813\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.2295 - mean_squared_error: 1.2295 - val_loss: 2.7468 - val_mean_squared_error: 2.7468\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.2155 - mean_squared_error: 1.2155 - val_loss: 2.8637 - val_mean_squared_error: 2.8637\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.2146 - mean_squared_error: 1.2146 - val_loss: 2.9788 - val_mean_squared_error: 2.9788\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.2282 - mean_squared_error: 1.2282 - val_loss: 3.1662 - val_mean_squared_error: 3.1662\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.0426 - mean_squared_error: 1.0426 - val_loss: 2.7944 - val_mean_squared_error: 2.7944\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.1812 - mean_squared_error: 1.1812 - val_loss: 2.9256 - val_mean_squared_error: 2.9256\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.1862 - mean_squared_error: 1.1862 - val_loss: 2.8209 - val_mean_squared_error: 2.8209\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 1.1030 - mean_squared_error: 1.1030 - val_loss: 2.7328 - val_mean_squared_error: 2.7328\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.1938 - mean_squared_error: 1.1938 - val_loss: 2.8251 - val_mean_squared_error: 2.8251\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.1224 - mean_squared_error: 1.1224 - val_loss: 2.7264 - val_mean_squared_error: 2.7264\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 289us/sample - loss: 1.1866 - mean_squared_error: 1.1866 - val_loss: 2.9803 - val_mean_squared_error: 2.9803\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.0709 - mean_squared_error: 1.0709 - val_loss: 2.9021 - val_mean_squared_error: 2.9021\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 1.1714 - mean_squared_error: 1.1714 - val_loss: 2.8735 - val_mean_squared_error: 2.8735\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.0955 - mean_squared_error: 1.0955 - val_loss: 2.8618 - val_mean_squared_error: 2.8618\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 1.0924 - mean_squared_error: 1.0924 - val_loss: 2.8886 - val_mean_squared_error: 2.8886\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.0044 - mean_squared_error: 1.0044 - val_loss: 2.7458 - val_mean_squared_error: 2.7458\n",
            "==================================================\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 47, 47, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 47, 47, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 23, 23, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 23, 23, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 11, 11, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 719us/sample - loss: 658.2473 - mean_squared_error: 658.2473 - val_loss: 290.2615 - val_mean_squared_error: 290.2615\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 291us/sample - loss: 24.7004 - mean_squared_error: 24.7004 - val_loss: 165.3872 - val_mean_squared_error: 165.3872\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 19.1928 - mean_squared_error: 19.1928 - val_loss: 67.7393 - val_mean_squared_error: 67.7393\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 15.7305 - mean_squared_error: 15.7305 - val_loss: 29.8510 - val_mean_squared_error: 29.8510\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 312us/sample - loss: 15.1602 - mean_squared_error: 15.1602 - val_loss: 16.7579 - val_mean_squared_error: 16.7579\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 323us/sample - loss: 13.5012 - mean_squared_error: 13.5012 - val_loss: 18.0298 - val_mean_squared_error: 18.0298\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 295us/sample - loss: 13.5276 - mean_squared_error: 13.5276 - val_loss: 14.3137 - val_mean_squared_error: 14.3137\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 12.2554 - mean_squared_error: 12.2554 - val_loss: 10.6344 - val_mean_squared_error: 10.6344\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 10.8583 - mean_squared_error: 10.8583 - val_loss: 9.3793 - val_mean_squared_error: 9.3793\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 11.1603 - mean_squared_error: 11.1603 - val_loss: 11.1263 - val_mean_squared_error: 11.1263\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 11.2308 - mean_squared_error: 11.2308 - val_loss: 14.6836 - val_mean_squared_error: 14.6836\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 290us/sample - loss: 10.7420 - mean_squared_error: 10.7420 - val_loss: 9.8294 - val_mean_squared_error: 9.8294\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 10.6394 - mean_squared_error: 10.6394 - val_loss: 12.7667 - val_mean_squared_error: 12.7667\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 10.4115 - mean_squared_error: 10.4115 - val_loss: 9.8517 - val_mean_squared_error: 9.8517\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 10.1806 - mean_squared_error: 10.1806 - val_loss: 10.8530 - val_mean_squared_error: 10.8530\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 9.4574 - mean_squared_error: 9.4574 - val_loss: 8.7040 - val_mean_squared_error: 8.7040\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 9.0333 - mean_squared_error: 9.0333 - val_loss: 8.5232 - val_mean_squared_error: 8.5232\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 9.2281 - mean_squared_error: 9.2281 - val_loss: 8.9112 - val_mean_squared_error: 8.9112\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 8.5386 - mean_squared_error: 8.5386 - val_loss: 7.9539 - val_mean_squared_error: 7.9539\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 8.0634 - mean_squared_error: 8.0634 - val_loss: 7.9245 - val_mean_squared_error: 7.9245\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 7.6665 - mean_squared_error: 7.6665 - val_loss: 8.4996 - val_mean_squared_error: 8.4996\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 7.6312 - mean_squared_error: 7.6312 - val_loss: 7.7865 - val_mean_squared_error: 7.7865\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 7.5598 - mean_squared_error: 7.5598 - val_loss: 12.1380 - val_mean_squared_error: 12.1380\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 7.1799 - mean_squared_error: 7.1799 - val_loss: 7.2031 - val_mean_squared_error: 7.2031\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 6.9570 - mean_squared_error: 6.9570 - val_loss: 7.4392 - val_mean_squared_error: 7.4392\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 6.4042 - mean_squared_error: 6.4042 - val_loss: 7.0091 - val_mean_squared_error: 7.0091\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 5.9344 - mean_squared_error: 5.9344 - val_loss: 6.6878 - val_mean_squared_error: 6.6878\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 5.8264 - mean_squared_error: 5.8264 - val_loss: 6.6282 - val_mean_squared_error: 6.6282\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 5.8336 - mean_squared_error: 5.8336 - val_loss: 6.9478 - val_mean_squared_error: 6.9478\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 5.9867 - mean_squared_error: 5.9867 - val_loss: 6.2487 - val_mean_squared_error: 6.2487\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 5.3375 - mean_squared_error: 5.3375 - val_loss: 5.9082 - val_mean_squared_error: 5.9082\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 5.3917 - mean_squared_error: 5.3917 - val_loss: 5.5851 - val_mean_squared_error: 5.5851\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 5.0428 - mean_squared_error: 5.0428 - val_loss: 5.8880 - val_mean_squared_error: 5.8880\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 4.7819 - mean_squared_error: 4.7819 - val_loss: 5.3983 - val_mean_squared_error: 5.3983\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 4.7949 - mean_squared_error: 4.7949 - val_loss: 5.8007 - val_mean_squared_error: 5.8007\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 4.8737 - mean_squared_error: 4.8737 - val_loss: 6.0340 - val_mean_squared_error: 6.0340\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 4.5782 - mean_squared_error: 4.5782 - val_loss: 5.9766 - val_mean_squared_error: 5.9766\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 4.8295 - mean_squared_error: 4.8295 - val_loss: 6.1286 - val_mean_squared_error: 6.1286\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 4.2083 - mean_squared_error: 4.2083 - val_loss: 4.8243 - val_mean_squared_error: 4.8243\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 4.4910 - mean_squared_error: 4.4910 - val_loss: 5.7710 - val_mean_squared_error: 5.7710\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 4.3516 - mean_squared_error: 4.3516 - val_loss: 4.9558 - val_mean_squared_error: 4.9558\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 4.2062 - mean_squared_error: 4.2062 - val_loss: 5.6800 - val_mean_squared_error: 5.6800\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 4.0245 - mean_squared_error: 4.0245 - val_loss: 4.8134 - val_mean_squared_error: 4.8134\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 4.1142 - mean_squared_error: 4.1142 - val_loss: 5.1719 - val_mean_squared_error: 5.1719\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 3.8275 - mean_squared_error: 3.8275 - val_loss: 4.5545 - val_mean_squared_error: 4.5545\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 3.7186 - mean_squared_error: 3.7186 - val_loss: 4.4705 - val_mean_squared_error: 4.4705\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 3.7386 - mean_squared_error: 3.7386 - val_loss: 4.8782 - val_mean_squared_error: 4.8782\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 3.6879 - mean_squared_error: 3.6879 - val_loss: 5.1754 - val_mean_squared_error: 5.1754\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 3.7001 - mean_squared_error: 3.7001 - val_loss: 4.2629 - val_mean_squared_error: 4.2629\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 282us/sample - loss: 3.8649 - mean_squared_error: 3.8649 - val_loss: 4.5981 - val_mean_squared_error: 4.5981\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 3.3927 - mean_squared_error: 3.3927 - val_loss: 4.2730 - val_mean_squared_error: 4.2730\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 3.4769 - mean_squared_error: 3.4769 - val_loss: 4.3010 - val_mean_squared_error: 4.3010\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 3.3282 - mean_squared_error: 3.3282 - val_loss: 4.4291 - val_mean_squared_error: 4.4291\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 3.3409 - mean_squared_error: 3.3409 - val_loss: 4.1623 - val_mean_squared_error: 4.1623\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 3.1618 - mean_squared_error: 3.1618 - val_loss: 3.8821 - val_mean_squared_error: 3.8821\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 3.1948 - mean_squared_error: 3.1948 - val_loss: 5.3485 - val_mean_squared_error: 5.3485\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 3.0832 - mean_squared_error: 3.0832 - val_loss: 4.1945 - val_mean_squared_error: 4.1945\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 3.1883 - mean_squared_error: 3.1883 - val_loss: 3.8582 - val_mean_squared_error: 3.8582\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 3.0475 - mean_squared_error: 3.0475 - val_loss: 4.1328 - val_mean_squared_error: 4.1328\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.9834 - mean_squared_error: 2.9834 - val_loss: 4.9528 - val_mean_squared_error: 4.9528\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 2.9890 - mean_squared_error: 2.9890 - val_loss: 3.5659 - val_mean_squared_error: 3.5659\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 3.2585 - mean_squared_error: 3.2585 - val_loss: 4.4982 - val_mean_squared_error: 4.4982\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.7720 - mean_squared_error: 2.7720 - val_loss: 3.6995 - val_mean_squared_error: 3.6995\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 3.2163 - mean_squared_error: 3.2163 - val_loss: 5.7120 - val_mean_squared_error: 5.7120\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 3.0197 - mean_squared_error: 3.0197 - val_loss: 3.7812 - val_mean_squared_error: 3.7812\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 2.8850 - mean_squared_error: 2.8850 - val_loss: 4.8447 - val_mean_squared_error: 4.8447\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 2.6042 - mean_squared_error: 2.6042 - val_loss: 3.4615 - val_mean_squared_error: 3.4615\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 2.7618 - mean_squared_error: 2.7618 - val_loss: 4.1844 - val_mean_squared_error: 4.1844\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 2.8514 - mean_squared_error: 2.8514 - val_loss: 3.8925 - val_mean_squared_error: 3.8925\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 3.1474 - mean_squared_error: 3.1474 - val_loss: 4.7078 - val_mean_squared_error: 4.7078\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.8264 - mean_squared_error: 2.8264 - val_loss: 3.5852 - val_mean_squared_error: 3.5852\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 2.6835 - mean_squared_error: 2.6835 - val_loss: 4.4014 - val_mean_squared_error: 4.4014\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 2.3817 - mean_squared_error: 2.3817 - val_loss: 3.5277 - val_mean_squared_error: 3.5277\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 2.5358 - mean_squared_error: 2.5358 - val_loss: 4.0008 - val_mean_squared_error: 4.0008\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 2.4694 - mean_squared_error: 2.4694 - val_loss: 3.6489 - val_mean_squared_error: 3.6489\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.4563 - mean_squared_error: 2.4563 - val_loss: 3.5015 - val_mean_squared_error: 3.5015\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.3972 - mean_squared_error: 2.3972 - val_loss: 3.7888 - val_mean_squared_error: 3.7888\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.4052 - mean_squared_error: 2.4052 - val_loss: 3.3531 - val_mean_squared_error: 3.3531\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 2.3837 - mean_squared_error: 2.3837 - val_loss: 3.5083 - val_mean_squared_error: 3.5083\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.2429 - mean_squared_error: 2.2429 - val_loss: 3.4156 - val_mean_squared_error: 3.4156\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.1363 - mean_squared_error: 2.1363 - val_loss: 3.3198 - val_mean_squared_error: 3.3198\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.3832 - mean_squared_error: 2.3832 - val_loss: 3.5755 - val_mean_squared_error: 3.5755\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.3378 - mean_squared_error: 2.3378 - val_loss: 3.2669 - val_mean_squared_error: 3.2669\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 2.2816 - mean_squared_error: 2.2816 - val_loss: 3.4298 - val_mean_squared_error: 3.4298\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.4229 - mean_squared_error: 2.4229 - val_loss: 3.6307 - val_mean_squared_error: 3.6307\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.2964 - mean_squared_error: 2.2964 - val_loss: 3.3799 - val_mean_squared_error: 3.3799\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.1407 - mean_squared_error: 2.1407 - val_loss: 3.3825 - val_mean_squared_error: 3.3825\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.3484 - mean_squared_error: 2.3484 - val_loss: 3.2427 - val_mean_squared_error: 3.2427\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 2.0671 - mean_squared_error: 2.0671 - val_loss: 3.3138 - val_mean_squared_error: 3.3138\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 2.0718 - mean_squared_error: 2.0718 - val_loss: 3.5415 - val_mean_squared_error: 3.5415\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 2.1978 - mean_squared_error: 2.1978 - val_loss: 3.3167 - val_mean_squared_error: 3.3167\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 2.1241 - mean_squared_error: 2.1241 - val_loss: 3.3545 - val_mean_squared_error: 3.3545\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.9897 - mean_squared_error: 1.9897 - val_loss: 3.4338 - val_mean_squared_error: 3.4338\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 2.0363 - mean_squared_error: 2.0363 - val_loss: 3.0422 - val_mean_squared_error: 3.0422\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 2.1968 - mean_squared_error: 2.1968 - val_loss: 3.5604 - val_mean_squared_error: 3.5604\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 2.2268 - mean_squared_error: 2.2268 - val_loss: 3.2434 - val_mean_squared_error: 3.2434\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.0695 - mean_squared_error: 2.0695 - val_loss: 3.2518 - val_mean_squared_error: 3.2518\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.0286 - mean_squared_error: 2.0286 - val_loss: 3.3284 - val_mean_squared_error: 3.3284\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.8944 - mean_squared_error: 1.8944 - val_loss: 3.6493 - val_mean_squared_error: 3.6493\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.9773 - mean_squared_error: 1.9773 - val_loss: 3.3334 - val_mean_squared_error: 3.3334\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.9458 - mean_squared_error: 1.9458 - val_loss: 3.0350 - val_mean_squared_error: 3.0350\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.9169 - mean_squared_error: 1.9169 - val_loss: 3.3636 - val_mean_squared_error: 3.3636\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 2.0373 - mean_squared_error: 2.0373 - val_loss: 3.4078 - val_mean_squared_error: 3.4078\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 2.1152 - mean_squared_error: 2.1152 - val_loss: 3.5602 - val_mean_squared_error: 3.5602\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.8443 - mean_squared_error: 1.8443 - val_loss: 3.3380 - val_mean_squared_error: 3.3380\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.8649 - mean_squared_error: 1.8649 - val_loss: 3.0509 - val_mean_squared_error: 3.0509\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.8560 - mean_squared_error: 1.8560 - val_loss: 3.0519 - val_mean_squared_error: 3.0519\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.8285 - mean_squared_error: 1.8285 - val_loss: 3.1977 - val_mean_squared_error: 3.1977\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.8605 - mean_squared_error: 1.8605 - val_loss: 2.9555 - val_mean_squared_error: 2.9555\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.9110 - mean_squared_error: 1.9110 - val_loss: 3.3939 - val_mean_squared_error: 3.3939\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.8035 - mean_squared_error: 1.8035 - val_loss: 3.1150 - val_mean_squared_error: 3.1150\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.7669 - mean_squared_error: 1.7669 - val_loss: 3.1013 - val_mean_squared_error: 3.1013\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.7423 - mean_squared_error: 1.7423 - val_loss: 3.0163 - val_mean_squared_error: 3.0163\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.8027 - mean_squared_error: 1.8027 - val_loss: 3.5013 - val_mean_squared_error: 3.5013\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.7590 - mean_squared_error: 1.7590 - val_loss: 3.2760 - val_mean_squared_error: 3.2760\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.7923 - mean_squared_error: 1.7923 - val_loss: 3.1092 - val_mean_squared_error: 3.1092\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.8076 - mean_squared_error: 1.8076 - val_loss: 2.9420 - val_mean_squared_error: 2.9420\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.7093 - mean_squared_error: 1.7093 - val_loss: 3.2427 - val_mean_squared_error: 3.2427\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.6871 - mean_squared_error: 1.6871 - val_loss: 3.5771 - val_mean_squared_error: 3.5771\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.6640 - mean_squared_error: 1.6640 - val_loss: 3.4010 - val_mean_squared_error: 3.4010\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.7552 - mean_squared_error: 1.7552 - val_loss: 2.9477 - val_mean_squared_error: 2.9477\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.6491 - mean_squared_error: 1.6491 - val_loss: 2.9121 - val_mean_squared_error: 2.9121\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.7596 - mean_squared_error: 1.7596 - val_loss: 3.4841 - val_mean_squared_error: 3.4841\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.7406 - mean_squared_error: 1.7406 - val_loss: 3.3041 - val_mean_squared_error: 3.3041\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.8101 - mean_squared_error: 1.8101 - val_loss: 3.5848 - val_mean_squared_error: 3.5848\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.6377 - mean_squared_error: 1.6377 - val_loss: 3.1560 - val_mean_squared_error: 3.1560\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.6973 - mean_squared_error: 1.6973 - val_loss: 2.9123 - val_mean_squared_error: 2.9123\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.5947 - mean_squared_error: 1.5947 - val_loss: 2.9298 - val_mean_squared_error: 2.9298\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.7590 - mean_squared_error: 1.7590 - val_loss: 3.2344 - val_mean_squared_error: 3.2344\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.6389 - mean_squared_error: 1.6389 - val_loss: 3.6390 - val_mean_squared_error: 3.6390\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.7180 - mean_squared_error: 1.7180 - val_loss: 3.0059 - val_mean_squared_error: 3.0059\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.6800 - mean_squared_error: 1.6800 - val_loss: 3.0557 - val_mean_squared_error: 3.0557\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.6518 - mean_squared_error: 1.6518 - val_loss: 3.3344 - val_mean_squared_error: 3.3344\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.6061 - mean_squared_error: 1.6061 - val_loss: 2.9767 - val_mean_squared_error: 2.9767\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.6251 - mean_squared_error: 1.6251 - val_loss: 3.3506 - val_mean_squared_error: 3.3506\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 0s 267us/sample - loss: 1.5767 - mean_squared_error: 1.5767 - val_loss: 3.3153 - val_mean_squared_error: 3.3153\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.7178 - mean_squared_error: 1.7178 - val_loss: 3.8299 - val_mean_squared_error: 3.8299\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 0s 265us/sample - loss: 1.5316 - mean_squared_error: 1.5316 - val_loss: 3.1496 - val_mean_squared_error: 3.1496\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.6263 - mean_squared_error: 1.6263 - val_loss: 3.4267 - val_mean_squared_error: 3.4267\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.4975 - mean_squared_error: 1.4975 - val_loss: 3.0484 - val_mean_squared_error: 3.0484\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.6077 - mean_squared_error: 1.6077 - val_loss: 3.3202 - val_mean_squared_error: 3.3202\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.4394 - mean_squared_error: 1.4394 - val_loss: 2.8525 - val_mean_squared_error: 2.8525\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.4372 - mean_squared_error: 1.4372 - val_loss: 2.8951 - val_mean_squared_error: 2.8951\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.4580 - mean_squared_error: 1.4580 - val_loss: 3.1049 - val_mean_squared_error: 3.1049\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.4775 - mean_squared_error: 1.4775 - val_loss: 2.9317 - val_mean_squared_error: 2.9317\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.4852 - mean_squared_error: 1.4852 - val_loss: 2.8955 - val_mean_squared_error: 2.8955\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.4594 - mean_squared_error: 1.4594 - val_loss: 3.3986 - val_mean_squared_error: 3.3986\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.4128 - mean_squared_error: 1.4128 - val_loss: 2.8683 - val_mean_squared_error: 2.8683\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.4854 - mean_squared_error: 1.4854 - val_loss: 3.3116 - val_mean_squared_error: 3.3116\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 1.5002 - mean_squared_error: 1.5002 - val_loss: 3.1849 - val_mean_squared_error: 3.1849\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.4840 - mean_squared_error: 1.4840 - val_loss: 3.0787 - val_mean_squared_error: 3.0787\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.4583 - mean_squared_error: 1.4583 - val_loss: 3.1143 - val_mean_squared_error: 3.1143\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 1.3758 - mean_squared_error: 1.3758 - val_loss: 3.4813 - val_mean_squared_error: 3.4813\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.4895 - mean_squared_error: 1.4895 - val_loss: 3.1469 - val_mean_squared_error: 3.1469\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.4954 - mean_squared_error: 1.4954 - val_loss: 3.0018 - val_mean_squared_error: 3.0018\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.4181 - mean_squared_error: 1.4181 - val_loss: 3.0652 - val_mean_squared_error: 3.0652\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.5544 - mean_squared_error: 1.5544 - val_loss: 3.1618 - val_mean_squared_error: 3.1618\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.3553 - mean_squared_error: 1.3553 - val_loss: 3.3041 - val_mean_squared_error: 3.3041\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.4052 - mean_squared_error: 1.4052 - val_loss: 3.0844 - val_mean_squared_error: 3.0844\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.3666 - mean_squared_error: 1.3666 - val_loss: 2.9906 - val_mean_squared_error: 2.9906\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.5024 - mean_squared_error: 1.5024 - val_loss: 3.0908 - val_mean_squared_error: 3.0908\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.3949 - mean_squared_error: 1.3949 - val_loss: 3.3345 - val_mean_squared_error: 3.3345\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.4993 - mean_squared_error: 1.4993 - val_loss: 3.0950 - val_mean_squared_error: 3.0950\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.3377 - mean_squared_error: 1.3377 - val_loss: 2.9253 - val_mean_squared_error: 2.9253\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.4229 - mean_squared_error: 1.4229 - val_loss: 2.8192 - val_mean_squared_error: 2.8192\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.4685 - mean_squared_error: 1.4685 - val_loss: 2.9352 - val_mean_squared_error: 2.9352\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.3648 - mean_squared_error: 1.3648 - val_loss: 2.9422 - val_mean_squared_error: 2.9422\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.4084 - mean_squared_error: 1.4084 - val_loss: 2.9218 - val_mean_squared_error: 2.9218\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.3251 - mean_squared_error: 1.3251 - val_loss: 2.8779 - val_mean_squared_error: 2.8779\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.3546 - mean_squared_error: 1.3546 - val_loss: 3.1600 - val_mean_squared_error: 3.1600\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.3461 - mean_squared_error: 1.3461 - val_loss: 2.9680 - val_mean_squared_error: 2.9680\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.3642 - mean_squared_error: 1.3642 - val_loss: 2.9219 - val_mean_squared_error: 2.9219\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.3619 - mean_squared_error: 1.3619 - val_loss: 2.7976 - val_mean_squared_error: 2.7976\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2728 - mean_squared_error: 1.2728 - val_loss: 2.9788 - val_mean_squared_error: 2.9788\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.2778 - mean_squared_error: 1.2778 - val_loss: 2.8290 - val_mean_squared_error: 2.8290\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 0s 268us/sample - loss: 1.3978 - mean_squared_error: 1.3978 - val_loss: 3.0273 - val_mean_squared_error: 3.0273\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.2691 - mean_squared_error: 1.2691 - val_loss: 3.1371 - val_mean_squared_error: 3.1371\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.3536 - mean_squared_error: 1.3536 - val_loss: 3.2754 - val_mean_squared_error: 3.2754\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.3093 - mean_squared_error: 1.3093 - val_loss: 2.9544 - val_mean_squared_error: 2.9544\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 1.3510 - mean_squared_error: 1.3510 - val_loss: 2.9378 - val_mean_squared_error: 2.9378\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 0s 266us/sample - loss: 1.2352 - mean_squared_error: 1.2352 - val_loss: 3.1165 - val_mean_squared_error: 3.1165\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 1.2923 - mean_squared_error: 1.2923 - val_loss: 2.8035 - val_mean_squared_error: 2.8035\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.3194 - mean_squared_error: 1.3194 - val_loss: 3.1109 - val_mean_squared_error: 3.1109\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2897 - mean_squared_error: 1.2897 - val_loss: 2.9016 - val_mean_squared_error: 2.9016\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 0s 269us/sample - loss: 1.2551 - mean_squared_error: 1.2551 - val_loss: 3.0160 - val_mean_squared_error: 3.0160\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 0s 275us/sample - loss: 1.2286 - mean_squared_error: 1.2286 - val_loss: 2.9512 - val_mean_squared_error: 2.9512\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 1.1834 - mean_squared_error: 1.1834 - val_loss: 3.2849 - val_mean_squared_error: 3.2849\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 0s 274us/sample - loss: 1.2188 - mean_squared_error: 1.2188 - val_loss: 2.9106 - val_mean_squared_error: 2.9106\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 1.1956 - mean_squared_error: 1.1956 - val_loss: 3.0473 - val_mean_squared_error: 3.0473\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.2654 - mean_squared_error: 1.2654 - val_loss: 3.1335 - val_mean_squared_error: 3.1335\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.2006 - mean_squared_error: 1.2006 - val_loss: 2.7628 - val_mean_squared_error: 2.7628\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 279us/sample - loss: 1.2510 - mean_squared_error: 1.2510 - val_loss: 2.8453 - val_mean_squared_error: 2.8453\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.2205 - mean_squared_error: 1.2205 - val_loss: 2.9722 - val_mean_squared_error: 2.9722\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 281us/sample - loss: 1.1973 - mean_squared_error: 1.1973 - val_loss: 2.9052 - val_mean_squared_error: 2.9052\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 277us/sample - loss: 1.1711 - mean_squared_error: 1.1711 - val_loss: 3.0387 - val_mean_squared_error: 3.0387\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 280us/sample - loss: 1.1837 - mean_squared_error: 1.1837 - val_loss: 3.3280 - val_mean_squared_error: 3.3280\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 275us/sample - loss: 1.2116 - mean_squared_error: 1.2116 - val_loss: 2.9807 - val_mean_squared_error: 2.9807\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 1.2192 - mean_squared_error: 1.2192 - val_loss: 2.9459 - val_mean_squared_error: 2.9459\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 1.2151 - mean_squared_error: 1.2151 - val_loss: 3.0766 - val_mean_squared_error: 3.0766\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 276us/sample - loss: 1.1565 - mean_squared_error: 1.1565 - val_loss: 2.9416 - val_mean_squared_error: 2.9416\n",
            "==================================================\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_15 (Conv2D)           (None, 47, 47, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 47, 47, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 23, 23, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 23, 23, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 11, 11, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 649us/sample - loss: 179.4249 - mean_squared_error: 179.4249 - val_loss: 17926.0858 - val_mean_squared_error: 17926.0879\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 14.1257 - mean_squared_error: 14.1257 - val_loss: 102.8252 - val_mean_squared_error: 102.8252\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 11.7555 - mean_squared_error: 11.7555 - val_loss: 28.7266 - val_mean_squared_error: 28.7266\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 0s 254us/sample - loss: 11.2626 - mean_squared_error: 11.2626 - val_loss: 12.1707 - val_mean_squared_error: 12.1707\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 10.2960 - mean_squared_error: 10.2960 - val_loss: 10.3585 - val_mean_squared_error: 10.3585\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 9.8333 - mean_squared_error: 9.8333 - val_loss: 9.3637 - val_mean_squared_error: 9.3637\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 0s 231us/sample - loss: 9.7206 - mean_squared_error: 9.7206 - val_loss: 9.5083 - val_mean_squared_error: 9.5083\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 9.5473 - mean_squared_error: 9.5473 - val_loss: 8.8492 - val_mean_squared_error: 8.8492\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 9.2938 - mean_squared_error: 9.2938 - val_loss: 8.8506 - val_mean_squared_error: 8.8506\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 9.1435 - mean_squared_error: 9.1435 - val_loss: 8.7753 - val_mean_squared_error: 8.7753\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 283us/sample - loss: 8.8466 - mean_squared_error: 8.8466 - val_loss: 8.7503 - val_mean_squared_error: 8.7503\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 0s 272us/sample - loss: 8.5122 - mean_squared_error: 8.5122 - val_loss: 8.3786 - val_mean_squared_error: 8.3786\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 0s 261us/sample - loss: 8.3589 - mean_squared_error: 8.3589 - val_loss: 8.2799 - val_mean_squared_error: 8.2799\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 7.9806 - mean_squared_error: 7.9806 - val_loss: 8.2974 - val_mean_squared_error: 8.2974\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 7.8225 - mean_squared_error: 7.8225 - val_loss: 7.7742 - val_mean_squared_error: 7.7742\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 7.5502 - mean_squared_error: 7.5502 - val_loss: 7.8660 - val_mean_squared_error: 7.8660\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 7.2534 - mean_squared_error: 7.2534 - val_loss: 7.7319 - val_mean_squared_error: 7.7319\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 7.0930 - mean_squared_error: 7.0930 - val_loss: 7.2594 - val_mean_squared_error: 7.2594\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 6.8527 - mean_squared_error: 6.8527 - val_loss: 7.3855 - val_mean_squared_error: 7.3855\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 6.4943 - mean_squared_error: 6.4943 - val_loss: 7.1198 - val_mean_squared_error: 7.1198\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 6.3526 - mean_squared_error: 6.3526 - val_loss: 6.8479 - val_mean_squared_error: 6.8479\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 5.9916 - mean_squared_error: 5.9916 - val_loss: 6.6541 - val_mean_squared_error: 6.6541\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 0s 229us/sample - loss: 5.7174 - mean_squared_error: 5.7174 - val_loss: 6.4733 - val_mean_squared_error: 6.4733\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 5.4282 - mean_squared_error: 5.4281 - val_loss: 6.3782 - val_mean_squared_error: 6.3782\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 5.1734 - mean_squared_error: 5.1734 - val_loss: 6.0480 - val_mean_squared_error: 6.0480\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 4.8901 - mean_squared_error: 4.8901 - val_loss: 5.4586 - val_mean_squared_error: 5.4586\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 4.7259 - mean_squared_error: 4.7259 - val_loss: 5.3376 - val_mean_squared_error: 5.3376\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 4.5995 - mean_squared_error: 4.5995 - val_loss: 5.4281 - val_mean_squared_error: 5.4281\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 4.4890 - mean_squared_error: 4.4890 - val_loss: 5.2161 - val_mean_squared_error: 5.2161\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 4.4597 - mean_squared_error: 4.4597 - val_loss: 5.2042 - val_mean_squared_error: 5.2042\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 4.1684 - mean_squared_error: 4.1684 - val_loss: 4.9728 - val_mean_squared_error: 4.9728\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 4.1223 - mean_squared_error: 4.1223 - val_loss: 5.0366 - val_mean_squared_error: 5.0366\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 4.1088 - mean_squared_error: 4.1088 - val_loss: 4.7529 - val_mean_squared_error: 4.7529\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 3.8363 - mean_squared_error: 3.8363 - val_loss: 4.6140 - val_mean_squared_error: 4.6140\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 3.7866 - mean_squared_error: 3.7866 - val_loss: 4.7448 - val_mean_squared_error: 4.7448\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 3.6902 - mean_squared_error: 3.6902 - val_loss: 4.5134 - val_mean_squared_error: 4.5134\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 3.6435 - mean_squared_error: 3.6435 - val_loss: 4.6755 - val_mean_squared_error: 4.6755\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 3.5734 - mean_squared_error: 3.5734 - val_loss: 4.6800 - val_mean_squared_error: 4.6800\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 0s 231us/sample - loss: 3.4868 - mean_squared_error: 3.4868 - val_loss: 4.6703 - val_mean_squared_error: 4.6703\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 3.3892 - mean_squared_error: 3.3892 - val_loss: 4.7840 - val_mean_squared_error: 4.7840\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 3.2650 - mean_squared_error: 3.2650 - val_loss: 4.4893 - val_mean_squared_error: 4.4893\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 3.3334 - mean_squared_error: 3.3334 - val_loss: 4.2488 - val_mean_squared_error: 4.2488\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 3.1442 - mean_squared_error: 3.1442 - val_loss: 4.7698 - val_mean_squared_error: 4.7698\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 3.0749 - mean_squared_error: 3.0749 - val_loss: 4.4396 - val_mean_squared_error: 4.4396\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 3.0371 - mean_squared_error: 3.0371 - val_loss: 4.4485 - val_mean_squared_error: 4.4485\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 2.9892 - mean_squared_error: 2.9892 - val_loss: 4.1086 - val_mean_squared_error: 4.1086\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 2.9881 - mean_squared_error: 2.9881 - val_loss: 4.1764 - val_mean_squared_error: 4.1764\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 2.8752 - mean_squared_error: 2.8752 - val_loss: 4.1598 - val_mean_squared_error: 4.1598\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 2.8059 - mean_squared_error: 2.8059 - val_loss: 4.2483 - val_mean_squared_error: 4.2483\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 2.7630 - mean_squared_error: 2.7630 - val_loss: 4.1353 - val_mean_squared_error: 4.1353\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 2.8002 - mean_squared_error: 2.8002 - val_loss: 4.0119 - val_mean_squared_error: 4.0119\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 2.7147 - mean_squared_error: 2.7147 - val_loss: 4.0884 - val_mean_squared_error: 4.0884\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 0s 231us/sample - loss: 2.6642 - mean_squared_error: 2.6642 - val_loss: 3.9968 - val_mean_squared_error: 3.9968\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 2.5082 - mean_squared_error: 2.5082 - val_loss: 4.0375 - val_mean_squared_error: 4.0375\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 2.5549 - mean_squared_error: 2.5549 - val_loss: 4.1073 - val_mean_squared_error: 4.1073\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 2.4734 - mean_squared_error: 2.4734 - val_loss: 4.0535 - val_mean_squared_error: 4.0535\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 2.5513 - mean_squared_error: 2.5513 - val_loss: 3.9440 - val_mean_squared_error: 3.9440\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 2.4503 - mean_squared_error: 2.4503 - val_loss: 3.9907 - val_mean_squared_error: 3.9907\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 2.3567 - mean_squared_error: 2.3567 - val_loss: 3.9294 - val_mean_squared_error: 3.9294\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 2.3449 - mean_squared_error: 2.3449 - val_loss: 4.2700 - val_mean_squared_error: 4.2700\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 2.2824 - mean_squared_error: 2.2824 - val_loss: 3.9318 - val_mean_squared_error: 3.9318\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 2.3598 - mean_squared_error: 2.3598 - val_loss: 3.7987 - val_mean_squared_error: 3.7987\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 2.2885 - mean_squared_error: 2.2885 - val_loss: 3.8139 - val_mean_squared_error: 3.8139\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 2.2803 - mean_squared_error: 2.2803 - val_loss: 3.8525 - val_mean_squared_error: 3.8525\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 2.2440 - mean_squared_error: 2.2440 - val_loss: 3.8870 - val_mean_squared_error: 3.8870\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 2.1734 - mean_squared_error: 2.1734 - val_loss: 4.2321 - val_mean_squared_error: 4.2321\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 0s 231us/sample - loss: 2.2340 - mean_squared_error: 2.2340 - val_loss: 3.6870 - val_mean_squared_error: 3.6870\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 2.2501 - mean_squared_error: 2.2501 - val_loss: 3.7973 - val_mean_squared_error: 3.7973\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 2.1454 - mean_squared_error: 2.1454 - val_loss: 3.9022 - val_mean_squared_error: 3.9022\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 2.1042 - mean_squared_error: 2.1042 - val_loss: 3.6460 - val_mean_squared_error: 3.6460\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 2.0712 - mean_squared_error: 2.0712 - val_loss: 3.7299 - val_mean_squared_error: 3.7299\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 2.0601 - mean_squared_error: 2.0601 - val_loss: 3.9832 - val_mean_squared_error: 3.9832\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 2.0383 - mean_squared_error: 2.0383 - val_loss: 3.8797 - val_mean_squared_error: 3.8797\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.9850 - mean_squared_error: 1.9850 - val_loss: 3.6526 - val_mean_squared_error: 3.6526\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 2.0026 - mean_squared_error: 2.0026 - val_loss: 3.9009 - val_mean_squared_error: 3.9009\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.8960 - mean_squared_error: 1.8960 - val_loss: 3.7444 - val_mean_squared_error: 3.7444\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.8861 - mean_squared_error: 1.8861 - val_loss: 3.7514 - val_mean_squared_error: 3.7514\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.8812 - mean_squared_error: 1.8812 - val_loss: 3.9597 - val_mean_squared_error: 3.9597\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.8315 - mean_squared_error: 1.8315 - val_loss: 4.1159 - val_mean_squared_error: 4.1159\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.9060 - mean_squared_error: 1.9060 - val_loss: 3.6476 - val_mean_squared_error: 3.6476\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.8636 - mean_squared_error: 1.8636 - val_loss: 3.7725 - val_mean_squared_error: 3.7725\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.7809 - mean_squared_error: 1.7809 - val_loss: 3.8643 - val_mean_squared_error: 3.8643\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.7801 - mean_squared_error: 1.7801 - val_loss: 3.6653 - val_mean_squared_error: 3.6653\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.8353 - mean_squared_error: 1.8353 - val_loss: 3.7458 - val_mean_squared_error: 3.7458\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.7727 - mean_squared_error: 1.7727 - val_loss: 3.8620 - val_mean_squared_error: 3.8620\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 1.7186 - mean_squared_error: 1.7186 - val_loss: 3.6131 - val_mean_squared_error: 3.6131\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.7034 - mean_squared_error: 1.7034 - val_loss: 4.0721 - val_mean_squared_error: 4.0721\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.7376 - mean_squared_error: 1.7376 - val_loss: 3.6413 - val_mean_squared_error: 3.6413\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.7889 - mean_squared_error: 1.7889 - val_loss: 3.5704 - val_mean_squared_error: 3.5704\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.7674 - mean_squared_error: 1.7674 - val_loss: 4.0798 - val_mean_squared_error: 4.0798\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 1.6779 - mean_squared_error: 1.6779 - val_loss: 3.6446 - val_mean_squared_error: 3.6446\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.7004 - mean_squared_error: 1.7004 - val_loss: 3.5126 - val_mean_squared_error: 3.5126\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 1.7293 - mean_squared_error: 1.7293 - val_loss: 3.6849 - val_mean_squared_error: 3.6849\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.7478 - mean_squared_error: 1.7478 - val_loss: 3.6119 - val_mean_squared_error: 3.6119\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.6260 - mean_squared_error: 1.6260 - val_loss: 3.6794 - val_mean_squared_error: 3.6794\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.6095 - mean_squared_error: 1.6095 - val_loss: 3.7372 - val_mean_squared_error: 3.7372\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.6248 - mean_squared_error: 1.6248 - val_loss: 3.6614 - val_mean_squared_error: 3.6614\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 1.6155 - mean_squared_error: 1.6155 - val_loss: 3.7116 - val_mean_squared_error: 3.7116\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.5324 - mean_squared_error: 1.5324 - val_loss: 3.6616 - val_mean_squared_error: 3.6616\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.5780 - mean_squared_error: 1.5780 - val_loss: 3.5554 - val_mean_squared_error: 3.5554\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.6460 - mean_squared_error: 1.6460 - val_loss: 3.6298 - val_mean_squared_error: 3.6298\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.5613 - mean_squared_error: 1.5613 - val_loss: 3.6263 - val_mean_squared_error: 3.6263\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.5581 - mean_squared_error: 1.5581 - val_loss: 3.6889 - val_mean_squared_error: 3.6889\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.5674 - mean_squared_error: 1.5674 - val_loss: 3.7916 - val_mean_squared_error: 3.7916\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.4412 - mean_squared_error: 1.4412 - val_loss: 3.7156 - val_mean_squared_error: 3.7156\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.5113 - mean_squared_error: 1.5113 - val_loss: 3.6500 - val_mean_squared_error: 3.6500\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.4987 - mean_squared_error: 1.4987 - val_loss: 3.9866 - val_mean_squared_error: 3.9866\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.5735 - mean_squared_error: 1.5735 - val_loss: 3.5458 - val_mean_squared_error: 3.5458\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.4139 - mean_squared_error: 1.4139 - val_loss: 3.5835 - val_mean_squared_error: 3.5835\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.4759 - mean_squared_error: 1.4759 - val_loss: 3.4966 - val_mean_squared_error: 3.4966\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.4767 - mean_squared_error: 1.4767 - val_loss: 3.5987 - val_mean_squared_error: 3.5987\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.4724 - mean_squared_error: 1.4724 - val_loss: 3.6030 - val_mean_squared_error: 3.6030\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.4855 - mean_squared_error: 1.4855 - val_loss: 3.6866 - val_mean_squared_error: 3.6866\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.4852 - mean_squared_error: 1.4852 - val_loss: 3.9422 - val_mean_squared_error: 3.9422\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.4474 - mean_squared_error: 1.4474 - val_loss: 3.6296 - val_mean_squared_error: 3.6296\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.4612 - mean_squared_error: 1.4612 - val_loss: 3.6329 - val_mean_squared_error: 3.6329\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.3796 - mean_squared_error: 1.3796 - val_loss: 3.6672 - val_mean_squared_error: 3.6672\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.4327 - mean_squared_error: 1.4327 - val_loss: 3.6312 - val_mean_squared_error: 3.6312\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.4538 - mean_squared_error: 1.4538 - val_loss: 3.5960 - val_mean_squared_error: 3.5960\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.4018 - mean_squared_error: 1.4018 - val_loss: 3.6535 - val_mean_squared_error: 3.6535\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.4606 - mean_squared_error: 1.4606 - val_loss: 3.8179 - val_mean_squared_error: 3.8179\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.4127 - mean_squared_error: 1.4127 - val_loss: 3.6113 - val_mean_squared_error: 3.6113\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.3179 - mean_squared_error: 1.3179 - val_loss: 3.5388 - val_mean_squared_error: 3.5388\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.3228 - mean_squared_error: 1.3228 - val_loss: 3.5148 - val_mean_squared_error: 3.5148\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.3389 - mean_squared_error: 1.3389 - val_loss: 3.6876 - val_mean_squared_error: 3.6876\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 1.3194 - mean_squared_error: 1.3194 - val_loss: 3.5055 - val_mean_squared_error: 3.5055\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.3893 - mean_squared_error: 1.3893 - val_loss: 3.5540 - val_mean_squared_error: 3.5540\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.4302 - mean_squared_error: 1.4302 - val_loss: 3.6229 - val_mean_squared_error: 3.6229\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.3562 - mean_squared_error: 1.3562 - val_loss: 3.7298 - val_mean_squared_error: 3.7298\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.3171 - mean_squared_error: 1.3171 - val_loss: 3.6553 - val_mean_squared_error: 3.6553\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.2876 - mean_squared_error: 1.2876 - val_loss: 3.5040 - val_mean_squared_error: 3.5040\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.3079 - mean_squared_error: 1.3079 - val_loss: 3.5632 - val_mean_squared_error: 3.5632\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.3270 - mean_squared_error: 1.3270 - val_loss: 3.6363 - val_mean_squared_error: 3.6363\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.2780 - mean_squared_error: 1.2780 - val_loss: 3.9181 - val_mean_squared_error: 3.9181\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.3329 - mean_squared_error: 1.3329 - val_loss: 3.4720 - val_mean_squared_error: 3.4720\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.2645 - mean_squared_error: 1.2645 - val_loss: 3.6574 - val_mean_squared_error: 3.6574\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.3168 - mean_squared_error: 1.3168 - val_loss: 3.5610 - val_mean_squared_error: 3.5610\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.3612 - mean_squared_error: 1.3612 - val_loss: 3.4930 - val_mean_squared_error: 3.4930\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 0s 229us/sample - loss: 1.3065 - mean_squared_error: 1.3065 - val_loss: 3.5787 - val_mean_squared_error: 3.5787\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.2993 - mean_squared_error: 1.2993 - val_loss: 3.6622 - val_mean_squared_error: 3.6622\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.2280 - mean_squared_error: 1.2280 - val_loss: 3.6376 - val_mean_squared_error: 3.6376\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.2860 - mean_squared_error: 1.2860 - val_loss: 3.5799 - val_mean_squared_error: 3.5799\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 1.2358 - mean_squared_error: 1.2358 - val_loss: 3.6595 - val_mean_squared_error: 3.6595\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.2604 - mean_squared_error: 1.2604 - val_loss: 3.5853 - val_mean_squared_error: 3.5853\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.1727 - mean_squared_error: 1.1727 - val_loss: 3.6009 - val_mean_squared_error: 3.6009\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 1.2115 - mean_squared_error: 1.2115 - val_loss: 3.8180 - val_mean_squared_error: 3.8180\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2004 - mean_squared_error: 1.2004 - val_loss: 3.5566 - val_mean_squared_error: 3.5566\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 1.1777 - mean_squared_error: 1.1777 - val_loss: 3.5072 - val_mean_squared_error: 3.5072\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.2322 - mean_squared_error: 1.2322 - val_loss: 3.5788 - val_mean_squared_error: 3.5788\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.2487 - mean_squared_error: 1.2487 - val_loss: 3.6734 - val_mean_squared_error: 3.6734\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 0s 232us/sample - loss: 1.2449 - mean_squared_error: 1.2449 - val_loss: 3.5635 - val_mean_squared_error: 3.5635\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 1.2311 - mean_squared_error: 1.2311 - val_loss: 3.5368 - val_mean_squared_error: 3.5368\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.3288 - mean_squared_error: 1.3288 - val_loss: 3.9318 - val_mean_squared_error: 3.9318\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.2588 - mean_squared_error: 1.2588 - val_loss: 3.4809 - val_mean_squared_error: 3.4809\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.1693 - mean_squared_error: 1.1693 - val_loss: 3.6026 - val_mean_squared_error: 3.6026\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.1530 - mean_squared_error: 1.1530 - val_loss: 3.7899 - val_mean_squared_error: 3.7899\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.2685 - mean_squared_error: 1.2685 - val_loss: 3.7008 - val_mean_squared_error: 3.7008\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.1636 - mean_squared_error: 1.1636 - val_loss: 3.4564 - val_mean_squared_error: 3.4564\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.0910 - mean_squared_error: 1.0910 - val_loss: 3.4437 - val_mean_squared_error: 3.4437\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.2016 - mean_squared_error: 1.2016 - val_loss: 3.5778 - val_mean_squared_error: 3.5778\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 0s 231us/sample - loss: 1.1744 - mean_squared_error: 1.1744 - val_loss: 3.6280 - val_mean_squared_error: 3.6280\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.1245 - mean_squared_error: 1.1245 - val_loss: 3.5793 - val_mean_squared_error: 3.5793\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.1291 - mean_squared_error: 1.1291 - val_loss: 3.5813 - val_mean_squared_error: 3.5813\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.1701 - mean_squared_error: 1.1701 - val_loss: 3.6168 - val_mean_squared_error: 3.6168\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.1397 - mean_squared_error: 1.1397 - val_loss: 3.6349 - val_mean_squared_error: 3.6349\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.1518 - mean_squared_error: 1.1518 - val_loss: 3.8907 - val_mean_squared_error: 3.8907\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.1375 - mean_squared_error: 1.1375 - val_loss: 3.5185 - val_mean_squared_error: 3.5185\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.1537 - mean_squared_error: 1.1537 - val_loss: 3.6644 - val_mean_squared_error: 3.6644\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.2005 - mean_squared_error: 1.2005 - val_loss: 3.5393 - val_mean_squared_error: 3.5393\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.2067 - mean_squared_error: 1.2067 - val_loss: 3.5069 - val_mean_squared_error: 3.5069\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.1164 - mean_squared_error: 1.1164 - val_loss: 3.4991 - val_mean_squared_error: 3.4991\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.1372 - mean_squared_error: 1.1372 - val_loss: 3.5558 - val_mean_squared_error: 3.5558\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.1631 - mean_squared_error: 1.1631 - val_loss: 3.4968 - val_mean_squared_error: 3.4968\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.1311 - mean_squared_error: 1.1311 - val_loss: 4.2234 - val_mean_squared_error: 4.2234\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.1051 - mean_squared_error: 1.1051 - val_loss: 3.5135 - val_mean_squared_error: 3.5135\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.0471 - mean_squared_error: 1.0471 - val_loss: 3.5774 - val_mean_squared_error: 3.5774\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.1399 - mean_squared_error: 1.1399 - val_loss: 3.6318 - val_mean_squared_error: 3.6318\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.0513 - mean_squared_error: 1.0513 - val_loss: 3.7294 - val_mean_squared_error: 3.7294\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.1117 - mean_squared_error: 1.1117 - val_loss: 3.5940 - val_mean_squared_error: 3.5940\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.0769 - mean_squared_error: 1.0769 - val_loss: 3.4549 - val_mean_squared_error: 3.4549\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.0703 - mean_squared_error: 1.0703 - val_loss: 3.5168 - val_mean_squared_error: 3.5168\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.0773 - mean_squared_error: 1.0773 - val_loss: 3.4992 - val_mean_squared_error: 3.4992\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.0885 - mean_squared_error: 1.0885 - val_loss: 3.5842 - val_mean_squared_error: 3.5842\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 0s 231us/sample - loss: 1.0735 - mean_squared_error: 1.0735 - val_loss: 3.5242 - val_mean_squared_error: 3.5242\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.0535 - mean_squared_error: 1.0535 - val_loss: 3.5929 - val_mean_squared_error: 3.5929\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.0732 - mean_squared_error: 1.0732 - val_loss: 3.5915 - val_mean_squared_error: 3.5915\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.0771 - mean_squared_error: 1.0771 - val_loss: 3.5046 - val_mean_squared_error: 3.5046\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.0599 - mean_squared_error: 1.0599 - val_loss: 3.5294 - val_mean_squared_error: 3.5294\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.0418 - mean_squared_error: 1.0418 - val_loss: 3.5124 - val_mean_squared_error: 3.5124\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.0530 - mean_squared_error: 1.0530 - val_loss: 3.6881 - val_mean_squared_error: 3.6881\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 0s 236us/sample - loss: 1.0584 - mean_squared_error: 1.0584 - val_loss: 3.5945 - val_mean_squared_error: 3.5945\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.0530 - mean_squared_error: 1.0530 - val_loss: 3.5418 - val_mean_squared_error: 3.5418\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 0s 237us/sample - loss: 1.0265 - mean_squared_error: 1.0265 - val_loss: 3.6008 - val_mean_squared_error: 3.6008\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.0855 - mean_squared_error: 1.0855 - val_loss: 3.5549 - val_mean_squared_error: 3.5549\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.0833 - mean_squared_error: 1.0833 - val_loss: 3.6988 - val_mean_squared_error: 3.6988\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 0s 233us/sample - loss: 1.0280 - mean_squared_error: 1.0280 - val_loss: 3.6301 - val_mean_squared_error: 3.6301\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 0s 238us/sample - loss: 1.1070 - mean_squared_error: 1.1070 - val_loss: 3.5379 - val_mean_squared_error: 3.5379\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 0s 234us/sample - loss: 1.0323 - mean_squared_error: 1.0323 - val_loss: 3.4751 - val_mean_squared_error: 3.4751\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.0050 - mean_squared_error: 1.0050 - val_loss: 3.5065 - val_mean_squared_error: 3.5065\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 0s 235us/sample - loss: 1.0440 - mean_squared_error: 1.0440 - val_loss: 3.6437 - val_mean_squared_error: 3.6437\n",
            "==================================================\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 47, 47, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 47, 47, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 23, 23, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 23, 23, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 11, 11, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 728us/sample - loss: 184.6800 - mean_squared_error: 184.6800 - val_loss: 130311.5926 - val_mean_squared_error: 130311.5859\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 14.3958 - mean_squared_error: 14.3958 - val_loss: 908.1665 - val_mean_squared_error: 908.1664\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 12.6987 - mean_squared_error: 12.6987 - val_loss: 40.1760 - val_mean_squared_error: 40.1760\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 10.7887 - mean_squared_error: 10.7887 - val_loss: 12.0602 - val_mean_squared_error: 12.0602\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 10.7258 - mean_squared_error: 10.7258 - val_loss: 9.5127 - val_mean_squared_error: 9.5127\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 9.8158 - mean_squared_error: 9.8158 - val_loss: 9.3846 - val_mean_squared_error: 9.3846\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 0s 270us/sample - loss: 9.8203 - mean_squared_error: 9.8203 - val_loss: 9.1056 - val_mean_squared_error: 9.1056\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 9.6421 - mean_squared_error: 9.6421 - val_loss: 9.0155 - val_mean_squared_error: 9.0155\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 9.3581 - mean_squared_error: 9.3581 - val_loss: 8.9525 - val_mean_squared_error: 8.9525\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 9.1277 - mean_squared_error: 9.1277 - val_loss: 8.9577 - val_mean_squared_error: 8.9577\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 8.9481 - mean_squared_error: 8.9481 - val_loss: 8.6335 - val_mean_squared_error: 8.6335\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 8.7904 - mean_squared_error: 8.7904 - val_loss: 8.6487 - val_mean_squared_error: 8.6487\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 8.4898 - mean_squared_error: 8.4898 - val_loss: 8.2964 - val_mean_squared_error: 8.2964\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 8.2842 - mean_squared_error: 8.2842 - val_loss: 8.1457 - val_mean_squared_error: 8.1457\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 7.9665 - mean_squared_error: 7.9665 - val_loss: 8.2282 - val_mean_squared_error: 8.2282\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 7.8931 - mean_squared_error: 7.8931 - val_loss: 7.8479 - val_mean_squared_error: 7.8479\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 7.6534 - mean_squared_error: 7.6534 - val_loss: 7.4943 - val_mean_squared_error: 7.4943\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 7.5451 - mean_squared_error: 7.5451 - val_loss: 7.4564 - val_mean_squared_error: 7.4564\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 7.3636 - mean_squared_error: 7.3636 - val_loss: 7.6419 - val_mean_squared_error: 7.6419\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 7.2509 - mean_squared_error: 7.2509 - val_loss: 7.9272 - val_mean_squared_error: 7.9272\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 7.0128 - mean_squared_error: 7.0128 - val_loss: 7.3677 - val_mean_squared_error: 7.3677\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 6.8834 - mean_squared_error: 6.8834 - val_loss: 7.0133 - val_mean_squared_error: 7.0133\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 6.6326 - mean_squared_error: 6.6326 - val_loss: 6.9059 - val_mean_squared_error: 6.9059\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 6.3620 - mean_squared_error: 6.3620 - val_loss: 6.5187 - val_mean_squared_error: 6.5187\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 6.1150 - mean_squared_error: 6.1150 - val_loss: 6.0454 - val_mean_squared_error: 6.0454\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 0s 256us/sample - loss: 5.9012 - mean_squared_error: 5.9012 - val_loss: 5.9152 - val_mean_squared_error: 5.9152\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 5.7433 - mean_squared_error: 5.7433 - val_loss: 5.6882 - val_mean_squared_error: 5.6882\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 5.4387 - mean_squared_error: 5.4387 - val_loss: 5.9573 - val_mean_squared_error: 5.9573\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 5.3453 - mean_squared_error: 5.3453 - val_loss: 5.3839 - val_mean_squared_error: 5.3839\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 5.0615 - mean_squared_error: 5.0615 - val_loss: 5.1864 - val_mean_squared_error: 5.1864\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 4.9790 - mean_squared_error: 4.9790 - val_loss: 5.1477 - val_mean_squared_error: 5.1477\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 4.8249 - mean_squared_error: 4.8249 - val_loss: 5.0489 - val_mean_squared_error: 5.0489\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 4.6788 - mean_squared_error: 4.6788 - val_loss: 4.9817 - val_mean_squared_error: 4.9817\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 4.4604 - mean_squared_error: 4.4604 - val_loss: 4.7619 - val_mean_squared_error: 4.7619\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 4.4167 - mean_squared_error: 4.4167 - val_loss: 4.6722 - val_mean_squared_error: 4.6722\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 4.3563 - mean_squared_error: 4.3563 - val_loss: 4.8965 - val_mean_squared_error: 4.8965\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 4.2227 - mean_squared_error: 4.2227 - val_loss: 4.4644 - val_mean_squared_error: 4.4644\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 4.1086 - mean_squared_error: 4.1086 - val_loss: 4.3650 - val_mean_squared_error: 4.3650\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 4.1164 - mean_squared_error: 4.1164 - val_loss: 4.4162 - val_mean_squared_error: 4.4162\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 3.8665 - mean_squared_error: 3.8665 - val_loss: 4.4579 - val_mean_squared_error: 4.4579\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 3.8745 - mean_squared_error: 3.8745 - val_loss: 4.4044 - val_mean_squared_error: 4.4044\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 3.8050 - mean_squared_error: 3.8050 - val_loss: 4.2309 - val_mean_squared_error: 4.2309\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 3.6565 - mean_squared_error: 3.6565 - val_loss: 4.0979 - val_mean_squared_error: 4.0979\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 0s 271us/sample - loss: 3.6626 - mean_squared_error: 3.6626 - val_loss: 4.2451 - val_mean_squared_error: 4.2451\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 3.6533 - mean_squared_error: 3.6533 - val_loss: 4.1671 - val_mean_squared_error: 4.1671\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 0s 257us/sample - loss: 3.5159 - mean_squared_error: 3.5159 - val_loss: 4.1329 - val_mean_squared_error: 4.1329\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 3.4262 - mean_squared_error: 3.4262 - val_loss: 4.1868 - val_mean_squared_error: 4.1868\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 3.2886 - mean_squared_error: 3.2886 - val_loss: 3.9676 - val_mean_squared_error: 3.9676\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 3.2307 - mean_squared_error: 3.2307 - val_loss: 3.8724 - val_mean_squared_error: 3.8724\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 3.1751 - mean_squared_error: 3.1751 - val_loss: 3.9490 - val_mean_squared_error: 3.9490\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 3.0851 - mean_squared_error: 3.0851 - val_loss: 3.8199 - val_mean_squared_error: 3.8199\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 3.0827 - mean_squared_error: 3.0827 - val_loss: 3.9954 - val_mean_squared_error: 3.9954\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 3.0051 - mean_squared_error: 3.0051 - val_loss: 3.8253 - val_mean_squared_error: 3.8253\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 2.9953 - mean_squared_error: 2.9953 - val_loss: 3.6891 - val_mean_squared_error: 3.6891\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 2.8607 - mean_squared_error: 2.8607 - val_loss: 3.8354 - val_mean_squared_error: 3.8354\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 2.8401 - mean_squared_error: 2.8401 - val_loss: 4.3935 - val_mean_squared_error: 4.3935\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 0s 254us/sample - loss: 2.8286 - mean_squared_error: 2.8286 - val_loss: 3.6326 - val_mean_squared_error: 3.6326\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 2.7612 - mean_squared_error: 2.7612 - val_loss: 3.7490 - val_mean_squared_error: 3.7490\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 0s 257us/sample - loss: 2.7465 - mean_squared_error: 2.7465 - val_loss: 3.5127 - val_mean_squared_error: 3.5127\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 2.6542 - mean_squared_error: 2.6542 - val_loss: 3.4971 - val_mean_squared_error: 3.4971\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 0s 256us/sample - loss: 2.6685 - mean_squared_error: 2.6685 - val_loss: 3.5069 - val_mean_squared_error: 3.5069\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 2.6291 - mean_squared_error: 2.6291 - val_loss: 4.2185 - val_mean_squared_error: 4.2185\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 0s 257us/sample - loss: 2.5918 - mean_squared_error: 2.5918 - val_loss: 3.5135 - val_mean_squared_error: 3.5135\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 0s 259us/sample - loss: 2.6125 - mean_squared_error: 2.6125 - val_loss: 3.5170 - val_mean_squared_error: 3.5170\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 0s 254us/sample - loss: 2.5359 - mean_squared_error: 2.5359 - val_loss: 3.5723 - val_mean_squared_error: 3.5723\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 2.5516 - mean_squared_error: 2.5516 - val_loss: 3.5874 - val_mean_squared_error: 3.5874\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.4701 - mean_squared_error: 2.4701 - val_loss: 3.3965 - val_mean_squared_error: 3.3965\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 2.4712 - mean_squared_error: 2.4712 - val_loss: 3.4370 - val_mean_squared_error: 3.4370\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.5166 - mean_squared_error: 2.5166 - val_loss: 3.4998 - val_mean_squared_error: 3.4998\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.4709 - mean_squared_error: 2.4709 - val_loss: 3.4769 - val_mean_squared_error: 3.4769\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 2.4223 - mean_squared_error: 2.4223 - val_loss: 3.3280 - val_mean_squared_error: 3.3280\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 2.3753 - mean_squared_error: 2.3753 - val_loss: 3.2924 - val_mean_squared_error: 3.2924\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 2.3064 - mean_squared_error: 2.3064 - val_loss: 3.6519 - val_mean_squared_error: 3.6519\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.3204 - mean_squared_error: 2.3204 - val_loss: 3.4275 - val_mean_squared_error: 3.4275\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.3563 - mean_squared_error: 2.3563 - val_loss: 3.3551 - val_mean_squared_error: 3.3551\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 2.2838 - mean_squared_error: 2.2838 - val_loss: 3.6899 - val_mean_squared_error: 3.6899\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.2797 - mean_squared_error: 2.2797 - val_loss: 3.4288 - val_mean_squared_error: 3.4288\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 2.3255 - mean_squared_error: 2.3255 - val_loss: 3.4104 - val_mean_squared_error: 3.4104\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 2.2334 - mean_squared_error: 2.2334 - val_loss: 3.2567 - val_mean_squared_error: 3.2567\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.2483 - mean_squared_error: 2.2483 - val_loss: 3.4566 - val_mean_squared_error: 3.4566\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 2.2363 - mean_squared_error: 2.2363 - val_loss: 3.3900 - val_mean_squared_error: 3.3900\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.1780 - mean_squared_error: 2.1780 - val_loss: 3.2744 - val_mean_squared_error: 3.2744\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 2.2169 - mean_squared_error: 2.2169 - val_loss: 3.3430 - val_mean_squared_error: 3.3430\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.1219 - mean_squared_error: 2.1219 - val_loss: 3.6573 - val_mean_squared_error: 3.6573\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.0733 - mean_squared_error: 2.0733 - val_loss: 3.2621 - val_mean_squared_error: 3.2621\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.1551 - mean_squared_error: 2.1551 - val_loss: 3.2259 - val_mean_squared_error: 3.2259\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.9842 - mean_squared_error: 1.9842 - val_loss: 3.2532 - val_mean_squared_error: 3.2532\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 2.0700 - mean_squared_error: 2.0700 - val_loss: 3.5407 - val_mean_squared_error: 3.5407\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 2.1606 - mean_squared_error: 2.1606 - val_loss: 3.3231 - val_mean_squared_error: 3.3231\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.9938 - mean_squared_error: 1.9938 - val_loss: 3.2685 - val_mean_squared_error: 3.2685\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.9975 - mean_squared_error: 1.9975 - val_loss: 3.1902 - val_mean_squared_error: 3.1902\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.9900 - mean_squared_error: 1.9900 - val_loss: 3.3776 - val_mean_squared_error: 3.3776\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 2.0203 - mean_squared_error: 2.0203 - val_loss: 3.3749 - val_mean_squared_error: 3.3749\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.9436 - mean_squared_error: 1.9436 - val_loss: 3.1598 - val_mean_squared_error: 3.1598\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.8890 - mean_squared_error: 1.8890 - val_loss: 3.2655 - val_mean_squared_error: 3.2655\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.9394 - mean_squared_error: 1.9394 - val_loss: 3.2935 - val_mean_squared_error: 3.2935\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.8948 - mean_squared_error: 1.8948 - val_loss: 3.1628 - val_mean_squared_error: 3.1628\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.8746 - mean_squared_error: 1.8746 - val_loss: 3.4524 - val_mean_squared_error: 3.4524\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.9194 - mean_squared_error: 1.9194 - val_loss: 3.2582 - val_mean_squared_error: 3.2582\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.8580 - mean_squared_error: 1.8580 - val_loss: 3.2199 - val_mean_squared_error: 3.2199\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.8085 - mean_squared_error: 1.8085 - val_loss: 3.2243 - val_mean_squared_error: 3.2243\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.7322 - mean_squared_error: 1.7322 - val_loss: 3.2036 - val_mean_squared_error: 3.2036\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.8217 - mean_squared_error: 1.8217 - val_loss: 3.2824 - val_mean_squared_error: 3.2824\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.8050 - mean_squared_error: 1.8050 - val_loss: 3.2566 - val_mean_squared_error: 3.2566\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.7747 - mean_squared_error: 1.7747 - val_loss: 3.3022 - val_mean_squared_error: 3.3022\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.7641 - mean_squared_error: 1.7641 - val_loss: 3.2110 - val_mean_squared_error: 3.2110\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.8239 - mean_squared_error: 1.8239 - val_loss: 3.2069 - val_mean_squared_error: 3.2069\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.7546 - mean_squared_error: 1.7546 - val_loss: 3.5654 - val_mean_squared_error: 3.5654\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.7245 - mean_squared_error: 1.7245 - val_loss: 3.1192 - val_mean_squared_error: 3.1192\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.7356 - mean_squared_error: 1.7356 - val_loss: 3.1932 - val_mean_squared_error: 3.1932\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.7497 - mean_squared_error: 1.7497 - val_loss: 3.1684 - val_mean_squared_error: 3.1684\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.7053 - mean_squared_error: 1.7053 - val_loss: 3.3306 - val_mean_squared_error: 3.3306\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.7403 - mean_squared_error: 1.7403 - val_loss: 3.1768 - val_mean_squared_error: 3.1768\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.6370 - mean_squared_error: 1.6370 - val_loss: 3.1350 - val_mean_squared_error: 3.1350\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.7115 - mean_squared_error: 1.7115 - val_loss: 3.2109 - val_mean_squared_error: 3.2109\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.7362 - mean_squared_error: 1.7362 - val_loss: 3.2580 - val_mean_squared_error: 3.2580\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.7234 - mean_squared_error: 1.7234 - val_loss: 3.1597 - val_mean_squared_error: 3.1597\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.6358 - mean_squared_error: 1.6358 - val_loss: 3.1722 - val_mean_squared_error: 3.1722\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.6003 - mean_squared_error: 1.6003 - val_loss: 3.0443 - val_mean_squared_error: 3.0443\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.6582 - mean_squared_error: 1.6582 - val_loss: 3.2313 - val_mean_squared_error: 3.2313\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.5837 - mean_squared_error: 1.5837 - val_loss: 3.0261 - val_mean_squared_error: 3.0261\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.5500 - mean_squared_error: 1.5500 - val_loss: 3.1039 - val_mean_squared_error: 3.1039\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.6619 - mean_squared_error: 1.6619 - val_loss: 3.1228 - val_mean_squared_error: 3.1228\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.5418 - mean_squared_error: 1.5418 - val_loss: 3.2079 - val_mean_squared_error: 3.2079\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.5042 - mean_squared_error: 1.5042 - val_loss: 3.2104 - val_mean_squared_error: 3.2104\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.6590 - mean_squared_error: 1.6590 - val_loss: 3.1210 - val_mean_squared_error: 3.1210\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.5771 - mean_squared_error: 1.5771 - val_loss: 3.0919 - val_mean_squared_error: 3.0919\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.4988 - mean_squared_error: 1.4988 - val_loss: 3.1473 - val_mean_squared_error: 3.1472\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.5225 - mean_squared_error: 1.5225 - val_loss: 3.1442 - val_mean_squared_error: 3.1442\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.5373 - mean_squared_error: 1.5373 - val_loss: 3.1028 - val_mean_squared_error: 3.1028\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.5280 - mean_squared_error: 1.5280 - val_loss: 3.0905 - val_mean_squared_error: 3.0905\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.4900 - mean_squared_error: 1.4900 - val_loss: 3.1245 - val_mean_squared_error: 3.1245\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.5121 - mean_squared_error: 1.5121 - val_loss: 3.1712 - val_mean_squared_error: 3.1712\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.4541 - mean_squared_error: 1.4541 - val_loss: 3.2433 - val_mean_squared_error: 3.2433\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.5326 - mean_squared_error: 1.5326 - val_loss: 3.1197 - val_mean_squared_error: 3.1197\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.4729 - mean_squared_error: 1.4729 - val_loss: 3.0506 - val_mean_squared_error: 3.0506\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.5770 - mean_squared_error: 1.5770 - val_loss: 3.1727 - val_mean_squared_error: 3.1727\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.4620 - mean_squared_error: 1.4620 - val_loss: 2.9923 - val_mean_squared_error: 2.9923\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.5423 - mean_squared_error: 1.5423 - val_loss: 3.3125 - val_mean_squared_error: 3.3125\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.4925 - mean_squared_error: 1.4925 - val_loss: 3.1428 - val_mean_squared_error: 3.1428\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 1.4238 - mean_squared_error: 1.4238 - val_loss: 3.1511 - val_mean_squared_error: 3.1511\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.4733 - mean_squared_error: 1.4733 - val_loss: 3.0507 - val_mean_squared_error: 3.0507\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.5279 - mean_squared_error: 1.5279 - val_loss: 3.1598 - val_mean_squared_error: 3.1598\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.4576 - mean_squared_error: 1.4576 - val_loss: 3.0687 - val_mean_squared_error: 3.0687\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.4446 - mean_squared_error: 1.4446 - val_loss: 3.4184 - val_mean_squared_error: 3.4184\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.4467 - mean_squared_error: 1.4467 - val_loss: 3.0220 - val_mean_squared_error: 3.0220\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.4436 - mean_squared_error: 1.4436 - val_loss: 3.1413 - val_mean_squared_error: 3.1413\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.4577 - mean_squared_error: 1.4577 - val_loss: 3.1035 - val_mean_squared_error: 3.1035\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.4099 - mean_squared_error: 1.4099 - val_loss: 3.0102 - val_mean_squared_error: 3.0102\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.4845 - mean_squared_error: 1.4845 - val_loss: 3.1713 - val_mean_squared_error: 3.1713\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.4285 - mean_squared_error: 1.4285 - val_loss: 3.1296 - val_mean_squared_error: 3.1296\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.4204 - mean_squared_error: 1.4204 - val_loss: 3.0091 - val_mean_squared_error: 3.0091\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 1.4195 - mean_squared_error: 1.4195 - val_loss: 3.1773 - val_mean_squared_error: 3.1773\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.4329 - mean_squared_error: 1.4329 - val_loss: 2.9335 - val_mean_squared_error: 2.9335\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.4359 - mean_squared_error: 1.4359 - val_loss: 2.9756 - val_mean_squared_error: 2.9756\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.3533 - mean_squared_error: 1.3533 - val_loss: 2.9801 - val_mean_squared_error: 2.9801\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.3761 - mean_squared_error: 1.3761 - val_loss: 3.1093 - val_mean_squared_error: 3.1093\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.3533 - mean_squared_error: 1.3533 - val_loss: 3.0269 - val_mean_squared_error: 3.0269\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3735 - mean_squared_error: 1.3735 - val_loss: 3.0150 - val_mean_squared_error: 3.0150\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.3328 - mean_squared_error: 1.3328 - val_loss: 3.0153 - val_mean_squared_error: 3.0153\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3982 - mean_squared_error: 1.3982 - val_loss: 3.2284 - val_mean_squared_error: 3.2284\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.3379 - mean_squared_error: 1.3379 - val_loss: 3.0317 - val_mean_squared_error: 3.0317\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3557 - mean_squared_error: 1.3557 - val_loss: 3.1731 - val_mean_squared_error: 3.1731\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.3301 - mean_squared_error: 1.3301 - val_loss: 3.0711 - val_mean_squared_error: 3.0711\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3366 - mean_squared_error: 1.3366 - val_loss: 3.0067 - val_mean_squared_error: 3.0067\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.2788 - mean_squared_error: 1.2788 - val_loss: 3.0031 - val_mean_squared_error: 3.0031\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.3260 - mean_squared_error: 1.3260 - val_loss: 3.0444 - val_mean_squared_error: 3.0444\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.3546 - mean_squared_error: 1.3546 - val_loss: 3.3490 - val_mean_squared_error: 3.3490\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.3266 - mean_squared_error: 1.3266 - val_loss: 3.0130 - val_mean_squared_error: 3.0130\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.2783 - mean_squared_error: 1.2783 - val_loss: 2.9777 - val_mean_squared_error: 2.9777\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.3000 - mean_squared_error: 1.3000 - val_loss: 3.1665 - val_mean_squared_error: 3.1665\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3174 - mean_squared_error: 1.3174 - val_loss: 3.1293 - val_mean_squared_error: 3.1293\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.2591 - mean_squared_error: 1.2591 - val_loss: 3.0372 - val_mean_squared_error: 3.0372\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.3488 - mean_squared_error: 1.3488 - val_loss: 3.1624 - val_mean_squared_error: 3.1624\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.3242 - mean_squared_error: 1.3242 - val_loss: 3.1311 - val_mean_squared_error: 3.1311\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.3275 - mean_squared_error: 1.3275 - val_loss: 3.1667 - val_mean_squared_error: 3.1667\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.2656 - mean_squared_error: 1.2656 - val_loss: 2.9988 - val_mean_squared_error: 2.9988\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2521 - mean_squared_error: 1.2521 - val_loss: 3.3468 - val_mean_squared_error: 3.3468\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3064 - mean_squared_error: 1.3064 - val_loss: 3.0172 - val_mean_squared_error: 3.0172\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.2828 - mean_squared_error: 1.2828 - val_loss: 3.1095 - val_mean_squared_error: 3.1095\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.2365 - mean_squared_error: 1.2365 - val_loss: 3.1000 - val_mean_squared_error: 3.1000\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2923 - mean_squared_error: 1.2923 - val_loss: 3.0214 - val_mean_squared_error: 3.0214\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2804 - mean_squared_error: 1.2804 - val_loss: 3.0876 - val_mean_squared_error: 3.0876\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.2249 - mean_squared_error: 1.2249 - val_loss: 3.0449 - val_mean_squared_error: 3.0449\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.2209 - mean_squared_error: 1.2209 - val_loss: 3.0539 - val_mean_squared_error: 3.0539\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.2483 - mean_squared_error: 1.2483 - val_loss: 3.1226 - val_mean_squared_error: 3.1226\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.2448 - mean_squared_error: 1.2448 - val_loss: 2.9525 - val_mean_squared_error: 2.9525\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.2666 - mean_squared_error: 1.2666 - val_loss: 2.9692 - val_mean_squared_error: 2.9692\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.2192 - mean_squared_error: 1.2192 - val_loss: 2.9035 - val_mean_squared_error: 2.9035\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2639 - mean_squared_error: 1.2639 - val_loss: 2.9348 - val_mean_squared_error: 2.9348\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.2237 - mean_squared_error: 1.2237 - val_loss: 2.9725 - val_mean_squared_error: 2.9725\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.1842 - mean_squared_error: 1.1842 - val_loss: 3.1095 - val_mean_squared_error: 3.1095\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.2469 - mean_squared_error: 1.2469 - val_loss: 3.0464 - val_mean_squared_error: 3.0464\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.2302 - mean_squared_error: 1.2302 - val_loss: 3.0344 - val_mean_squared_error: 3.0344\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.2516 - mean_squared_error: 1.2516 - val_loss: 3.0627 - val_mean_squared_error: 3.0627\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.3128 - mean_squared_error: 1.3128 - val_loss: 3.0456 - val_mean_squared_error: 3.0456\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.2395 - mean_squared_error: 1.2395 - val_loss: 2.9453 - val_mean_squared_error: 2.9453\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 0s 240us/sample - loss: 1.1628 - mean_squared_error: 1.1628 - val_loss: 2.9139 - val_mean_squared_error: 2.9139\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.1650 - mean_squared_error: 1.1650 - val_loss: 2.9294 - val_mean_squared_error: 2.9294\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.1196 - mean_squared_error: 1.1196 - val_loss: 3.0638 - val_mean_squared_error: 3.0638\n",
            "==================================================\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 47, 47, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 47, 47, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 23, 23, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 23, 23, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 11, 11, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 742us/sample - loss: 185.0017 - mean_squared_error: 185.0017 - val_loss: 128974.6745 - val_mean_squared_error: 128974.6797\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 0s 259us/sample - loss: 12.4920 - mean_squared_error: 12.4920 - val_loss: 886.7932 - val_mean_squared_error: 886.7932\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 11.0854 - mean_squared_error: 11.0854 - val_loss: 42.0045 - val_mean_squared_error: 42.0045\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 10.4580 - mean_squared_error: 10.4580 - val_loss: 10.8134 - val_mean_squared_error: 10.8134\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 10.1914 - mean_squared_error: 10.1914 - val_loss: 10.0731 - val_mean_squared_error: 10.0731\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 9.6159 - mean_squared_error: 9.6159 - val_loss: 10.3317 - val_mean_squared_error: 10.3317\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 0s 264us/sample - loss: 9.5706 - mean_squared_error: 9.5706 - val_loss: 9.0666 - val_mean_squared_error: 9.0666\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 9.2587 - mean_squared_error: 9.2587 - val_loss: 9.1672 - val_mean_squared_error: 9.1672\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 9.3279 - mean_squared_error: 9.3279 - val_loss: 9.2992 - val_mean_squared_error: 9.2992\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 8.9914 - mean_squared_error: 8.9914 - val_loss: 8.8214 - val_mean_squared_error: 8.8214\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 8.9942 - mean_squared_error: 8.9942 - val_loss: 8.7586 - val_mean_squared_error: 8.7586\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 0s 256us/sample - loss: 8.7529 - mean_squared_error: 8.7529 - val_loss: 8.7933 - val_mean_squared_error: 8.7933\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 0s 273us/sample - loss: 8.5962 - mean_squared_error: 8.5962 - val_loss: 8.4378 - val_mean_squared_error: 8.4378\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 278us/sample - loss: 8.3463 - mean_squared_error: 8.3463 - val_loss: 8.3310 - val_mean_squared_error: 8.3310\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 8.0841 - mean_squared_error: 8.0841 - val_loss: 8.2121 - val_mean_squared_error: 8.2121\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 7.9839 - mean_squared_error: 7.9839 - val_loss: 8.3990 - val_mean_squared_error: 8.3990\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 7.8222 - mean_squared_error: 7.8222 - val_loss: 7.9972 - val_mean_squared_error: 7.9972\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 7.6555 - mean_squared_error: 7.6555 - val_loss: 7.8877 - val_mean_squared_error: 7.8877\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 7.6186 - mean_squared_error: 7.6186 - val_loss: 7.6407 - val_mean_squared_error: 7.6407\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 7.2094 - mean_squared_error: 7.2094 - val_loss: 7.2891 - val_mean_squared_error: 7.2891\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 6.9369 - mean_squared_error: 6.9369 - val_loss: 6.7540 - val_mean_squared_error: 6.7540\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 6.7177 - mean_squared_error: 6.7177 - val_loss: 6.3974 - val_mean_squared_error: 6.3974\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 6.5039 - mean_squared_error: 6.5039 - val_loss: 6.1806 - val_mean_squared_error: 6.1806\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 6.2344 - mean_squared_error: 6.2344 - val_loss: 5.9341 - val_mean_squared_error: 5.9341\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 6.0388 - mean_squared_error: 6.0388 - val_loss: 6.5853 - val_mean_squared_error: 6.5853\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 5.8173 - mean_squared_error: 5.8173 - val_loss: 5.8543 - val_mean_squared_error: 5.8543\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 5.6988 - mean_squared_error: 5.6988 - val_loss: 5.5736 - val_mean_squared_error: 5.5736\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 5.5324 - mean_squared_error: 5.5324 - val_loss: 5.3968 - val_mean_squared_error: 5.3968\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 5.2114 - mean_squared_error: 5.2114 - val_loss: 5.1489 - val_mean_squared_error: 5.1489\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 5.1708 - mean_squared_error: 5.1708 - val_loss: 5.0488 - val_mean_squared_error: 5.0488\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 4.9215 - mean_squared_error: 4.9215 - val_loss: 5.1005 - val_mean_squared_error: 5.1005\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 4.9261 - mean_squared_error: 4.9261 - val_loss: 5.1947 - val_mean_squared_error: 5.1947\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 4.7901 - mean_squared_error: 4.7901 - val_loss: 5.4684 - val_mean_squared_error: 5.4684\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 4.7415 - mean_squared_error: 4.7415 - val_loss: 4.9472 - val_mean_squared_error: 4.9472\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 4.5804 - mean_squared_error: 4.5804 - val_loss: 4.8919 - val_mean_squared_error: 4.8919\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 4.5847 - mean_squared_error: 4.5847 - val_loss: 4.8110 - val_mean_squared_error: 4.8110\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 4.4240 - mean_squared_error: 4.4240 - val_loss: 4.7459 - val_mean_squared_error: 4.7459\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 4.3788 - mean_squared_error: 4.3788 - val_loss: 4.4390 - val_mean_squared_error: 4.4390\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 4.2530 - mean_squared_error: 4.2530 - val_loss: 4.6247 - val_mean_squared_error: 4.6247\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 4.2368 - mean_squared_error: 4.2368 - val_loss: 4.6526 - val_mean_squared_error: 4.6526\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 4.1543 - mean_squared_error: 4.1543 - val_loss: 4.7660 - val_mean_squared_error: 4.7660\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 4.0550 - mean_squared_error: 4.0550 - val_loss: 4.6045 - val_mean_squared_error: 4.6045\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 3.9576 - mean_squared_error: 3.9576 - val_loss: 4.3755 - val_mean_squared_error: 4.3755\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 3.9044 - mean_squared_error: 3.9044 - val_loss: 4.6612 - val_mean_squared_error: 4.6612\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 3.7206 - mean_squared_error: 3.7206 - val_loss: 4.2184 - val_mean_squared_error: 4.2184\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 3.7408 - mean_squared_error: 3.7408 - val_loss: 4.0310 - val_mean_squared_error: 4.0310\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 3.6390 - mean_squared_error: 3.6390 - val_loss: 4.1144 - val_mean_squared_error: 4.1144\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 3.5778 - mean_squared_error: 3.5778 - val_loss: 4.0533 - val_mean_squared_error: 4.0533\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 3.5435 - mean_squared_error: 3.5435 - val_loss: 4.2856 - val_mean_squared_error: 4.2856\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 3.5151 - mean_squared_error: 3.5151 - val_loss: 3.7982 - val_mean_squared_error: 3.7982\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 3.4011 - mean_squared_error: 3.4011 - val_loss: 3.9859 - val_mean_squared_error: 3.9859\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 3.3907 - mean_squared_error: 3.3907 - val_loss: 3.8104 - val_mean_squared_error: 3.8104\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 3.2740 - mean_squared_error: 3.2740 - val_loss: 4.0880 - val_mean_squared_error: 4.0880\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 3.2780 - mean_squared_error: 3.2781 - val_loss: 3.8251 - val_mean_squared_error: 3.8251\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 3.2631 - mean_squared_error: 3.2631 - val_loss: 3.7723 - val_mean_squared_error: 3.7723\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 3.1051 - mean_squared_error: 3.1051 - val_loss: 3.6090 - val_mean_squared_error: 3.6090\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 3.0534 - mean_squared_error: 3.0534 - val_loss: 3.4661 - val_mean_squared_error: 3.4661\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 2.9956 - mean_squared_error: 2.9956 - val_loss: 3.5424 - val_mean_squared_error: 3.5424\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.8938 - mean_squared_error: 2.8938 - val_loss: 3.5383 - val_mean_squared_error: 3.5383\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 0s 260us/sample - loss: 2.8464 - mean_squared_error: 2.8464 - val_loss: 3.3869 - val_mean_squared_error: 3.3869\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 2.8506 - mean_squared_error: 2.8506 - val_loss: 3.4559 - val_mean_squared_error: 3.4559\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.8254 - mean_squared_error: 2.8254 - val_loss: 3.3757 - val_mean_squared_error: 3.3757\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.7997 - mean_squared_error: 2.7997 - val_loss: 3.3783 - val_mean_squared_error: 3.3783\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.7090 - mean_squared_error: 2.7090 - val_loss: 3.5480 - val_mean_squared_error: 3.5480\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.7061 - mean_squared_error: 2.7061 - val_loss: 3.5242 - val_mean_squared_error: 3.5242\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 2.6554 - mean_squared_error: 2.6554 - val_loss: 3.4113 - val_mean_squared_error: 3.4113\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.5590 - mean_squared_error: 2.5590 - val_loss: 3.4027 - val_mean_squared_error: 3.4027\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.6103 - mean_squared_error: 2.6103 - val_loss: 3.2488 - val_mean_squared_error: 3.2488\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.5148 - mean_squared_error: 2.5148 - val_loss: 3.3270 - val_mean_squared_error: 3.3270\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.4649 - mean_squared_error: 2.4649 - val_loss: 3.3231 - val_mean_squared_error: 3.3231\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 2.4640 - mean_squared_error: 2.4640 - val_loss: 3.3972 - val_mean_squared_error: 3.3972\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 2.5117 - mean_squared_error: 2.5117 - val_loss: 3.3609 - val_mean_squared_error: 3.3609\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 2.4153 - mean_squared_error: 2.4153 - val_loss: 3.2700 - val_mean_squared_error: 3.2700\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 2.3968 - mean_squared_error: 2.3968 - val_loss: 3.3015 - val_mean_squared_error: 3.3015\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 0s 256us/sample - loss: 2.3683 - mean_squared_error: 2.3683 - val_loss: 3.3628 - val_mean_squared_error: 3.3628\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 2.3520 - mean_squared_error: 2.3520 - val_loss: 3.2279 - val_mean_squared_error: 3.2279\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 2.2230 - mean_squared_error: 2.2230 - val_loss: 3.2133 - val_mean_squared_error: 3.2133\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 2.2479 - mean_squared_error: 2.2479 - val_loss: 3.1909 - val_mean_squared_error: 3.1909\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 2.2713 - mean_squared_error: 2.2713 - val_loss: 3.4556 - val_mean_squared_error: 3.4556\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 2.1972 - mean_squared_error: 2.1972 - val_loss: 3.0919 - val_mean_squared_error: 3.0919\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.2534 - mean_squared_error: 2.2534 - val_loss: 3.1911 - val_mean_squared_error: 3.1911\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.1621 - mean_squared_error: 2.1621 - val_loss: 3.1788 - val_mean_squared_error: 3.1788\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 2.1952 - mean_squared_error: 2.1952 - val_loss: 3.2974 - val_mean_squared_error: 3.2974\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.1161 - mean_squared_error: 2.1161 - val_loss: 3.1389 - val_mean_squared_error: 3.1389\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 2.1949 - mean_squared_error: 2.1949 - val_loss: 3.0914 - val_mean_squared_error: 3.0914\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 2.1575 - mean_squared_error: 2.1575 - val_loss: 3.1712 - val_mean_squared_error: 3.1712\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.0907 - mean_squared_error: 2.0907 - val_loss: 3.2652 - val_mean_squared_error: 3.2652\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 2.1208 - mean_squared_error: 2.1208 - val_loss: 3.1792 - val_mean_squared_error: 3.1792\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 2.0270 - mean_squared_error: 2.0270 - val_loss: 3.7164 - val_mean_squared_error: 3.7164\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 2.0803 - mean_squared_error: 2.0803 - val_loss: 3.2227 - val_mean_squared_error: 3.2227\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.9949 - mean_squared_error: 1.9949 - val_loss: 3.4948 - val_mean_squared_error: 3.4948\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 2.0446 - mean_squared_error: 2.0446 - val_loss: 3.0117 - val_mean_squared_error: 3.0117\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.9387 - mean_squared_error: 1.9387 - val_loss: 3.1031 - val_mean_squared_error: 3.1031\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.9801 - mean_squared_error: 1.9801 - val_loss: 3.0680 - val_mean_squared_error: 3.0680\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 2.0291 - mean_squared_error: 2.0291 - val_loss: 3.1766 - val_mean_squared_error: 3.1766\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.9175 - mean_squared_error: 1.9175 - val_loss: 3.2654 - val_mean_squared_error: 3.2654\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.9097 - mean_squared_error: 1.9097 - val_loss: 3.0803 - val_mean_squared_error: 3.0803\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 1.8831 - mean_squared_error: 1.8831 - val_loss: 3.1477 - val_mean_squared_error: 3.1477\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.8305 - mean_squared_error: 1.8305 - val_loss: 3.0300 - val_mean_squared_error: 3.0300\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.9702 - mean_squared_error: 1.9702 - val_loss: 3.1040 - val_mean_squared_error: 3.1040\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.9068 - mean_squared_error: 1.9068 - val_loss: 3.4078 - val_mean_squared_error: 3.4078\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.8926 - mean_squared_error: 1.8926 - val_loss: 3.1557 - val_mean_squared_error: 3.1557\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.8738 - mean_squared_error: 1.8738 - val_loss: 3.1613 - val_mean_squared_error: 3.1613\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.8374 - mean_squared_error: 1.8374 - val_loss: 3.0157 - val_mean_squared_error: 3.0157\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.8956 - mean_squared_error: 1.8956 - val_loss: 3.0607 - val_mean_squared_error: 3.0607\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.7401 - mean_squared_error: 1.7401 - val_loss: 3.0566 - val_mean_squared_error: 3.0566\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.7885 - mean_squared_error: 1.7885 - val_loss: 2.9655 - val_mean_squared_error: 2.9655\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.8242 - mean_squared_error: 1.8242 - val_loss: 2.9626 - val_mean_squared_error: 2.9626\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.7472 - mean_squared_error: 1.7472 - val_loss: 2.9911 - val_mean_squared_error: 2.9911\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.7660 - mean_squared_error: 1.7660 - val_loss: 3.0308 - val_mean_squared_error: 3.0308\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 0s 254us/sample - loss: 1.6931 - mean_squared_error: 1.6931 - val_loss: 2.9444 - val_mean_squared_error: 2.9444\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.7462 - mean_squared_error: 1.7462 - val_loss: 2.9909 - val_mean_squared_error: 2.9909\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.6732 - mean_squared_error: 1.6732 - val_loss: 3.1940 - val_mean_squared_error: 3.1940\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.7709 - mean_squared_error: 1.7709 - val_loss: 3.0695 - val_mean_squared_error: 3.0695\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.7392 - mean_squared_error: 1.7392 - val_loss: 2.9767 - val_mean_squared_error: 2.9767\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.6305 - mean_squared_error: 1.6305 - val_loss: 3.0739 - val_mean_squared_error: 3.0739\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.7025 - mean_squared_error: 1.7025 - val_loss: 3.0003 - val_mean_squared_error: 3.0003\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.6472 - mean_squared_error: 1.6472 - val_loss: 2.9682 - val_mean_squared_error: 2.9682\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.6608 - mean_squared_error: 1.6608 - val_loss: 2.9574 - val_mean_squared_error: 2.9574\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.6839 - mean_squared_error: 1.6839 - val_loss: 2.9119 - val_mean_squared_error: 2.9119\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.6633 - mean_squared_error: 1.6633 - val_loss: 3.0347 - val_mean_squared_error: 3.0347\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.5940 - mean_squared_error: 1.5940 - val_loss: 3.1993 - val_mean_squared_error: 3.1993\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.6121 - mean_squared_error: 1.6121 - val_loss: 3.0132 - val_mean_squared_error: 3.0132\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.5997 - mean_squared_error: 1.5997 - val_loss: 2.8996 - val_mean_squared_error: 2.8996\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 1.6251 - mean_squared_error: 1.6251 - val_loss: 2.9677 - val_mean_squared_error: 2.9677\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.6017 - mean_squared_error: 1.6017 - val_loss: 2.9839 - val_mean_squared_error: 2.9839\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.6345 - mean_squared_error: 1.6345 - val_loss: 2.9141 - val_mean_squared_error: 2.9141\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.5733 - mean_squared_error: 1.5733 - val_loss: 3.0973 - val_mean_squared_error: 3.0973\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.6140 - mean_squared_error: 1.6140 - val_loss: 2.9016 - val_mean_squared_error: 2.9016\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.6278 - mean_squared_error: 1.6278 - val_loss: 3.0033 - val_mean_squared_error: 3.0033\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.5386 - mean_squared_error: 1.5386 - val_loss: 3.0411 - val_mean_squared_error: 3.0411\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.5364 - mean_squared_error: 1.5364 - val_loss: 3.0294 - val_mean_squared_error: 3.0294\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 0s 242us/sample - loss: 1.5834 - mean_squared_error: 1.5834 - val_loss: 2.9871 - val_mean_squared_error: 2.9871\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.4756 - mean_squared_error: 1.4756 - val_loss: 3.1378 - val_mean_squared_error: 3.1378\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.5420 - mean_squared_error: 1.5420 - val_loss: 3.3723 - val_mean_squared_error: 3.3723\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 1.4889 - mean_squared_error: 1.4889 - val_loss: 2.9693 - val_mean_squared_error: 2.9693\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.5282 - mean_squared_error: 1.5282 - val_loss: 2.8948 - val_mean_squared_error: 2.8948\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.4955 - mean_squared_error: 1.4955 - val_loss: 3.0516 - val_mean_squared_error: 3.0516\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.5381 - mean_squared_error: 1.5381 - val_loss: 2.9975 - val_mean_squared_error: 2.9975\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.4793 - mean_squared_error: 1.4793 - val_loss: 2.8340 - val_mean_squared_error: 2.8340\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.4191 - mean_squared_error: 1.4191 - val_loss: 2.8483 - val_mean_squared_error: 2.8483\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.4341 - mean_squared_error: 1.4341 - val_loss: 2.9209 - val_mean_squared_error: 2.9209\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 1.4700 - mean_squared_error: 1.4700 - val_loss: 2.8768 - val_mean_squared_error: 2.8768\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.5660 - mean_squared_error: 1.5660 - val_loss: 2.9278 - val_mean_squared_error: 2.9278\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.4333 - mean_squared_error: 1.4333 - val_loss: 2.9374 - val_mean_squared_error: 2.9374\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.4175 - mean_squared_error: 1.4175 - val_loss: 2.8870 - val_mean_squared_error: 2.8870\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.4648 - mean_squared_error: 1.4648 - val_loss: 2.9155 - val_mean_squared_error: 2.9155\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.4032 - mean_squared_error: 1.4032 - val_loss: 2.9366 - val_mean_squared_error: 2.9366\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3743 - mean_squared_error: 1.3743 - val_loss: 2.8834 - val_mean_squared_error: 2.8834\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.4724 - mean_squared_error: 1.4724 - val_loss: 2.9535 - val_mean_squared_error: 2.9535\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 0s 241us/sample - loss: 1.4683 - mean_squared_error: 1.4683 - val_loss: 3.0854 - val_mean_squared_error: 3.0854\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.4197 - mean_squared_error: 1.4197 - val_loss: 2.8131 - val_mean_squared_error: 2.8131\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.4791 - mean_squared_error: 1.4791 - val_loss: 2.8855 - val_mean_squared_error: 2.8855\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.3887 - mean_squared_error: 1.3887 - val_loss: 2.8530 - val_mean_squared_error: 2.8530\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3880 - mean_squared_error: 1.3880 - val_loss: 2.8249 - val_mean_squared_error: 2.8249\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3849 - mean_squared_error: 1.3849 - val_loss: 2.8981 - val_mean_squared_error: 2.8981\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3849 - mean_squared_error: 1.3849 - val_loss: 2.8677 - val_mean_squared_error: 2.8677\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3716 - mean_squared_error: 1.3716 - val_loss: 2.9392 - val_mean_squared_error: 2.9392\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.4306 - mean_squared_error: 1.4306 - val_loss: 2.9191 - val_mean_squared_error: 2.9191\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.3557 - mean_squared_error: 1.3557 - val_loss: 2.9242 - val_mean_squared_error: 2.9242\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.3465 - mean_squared_error: 1.3465 - val_loss: 2.8556 - val_mean_squared_error: 2.8556\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.3830 - mean_squared_error: 1.3830 - val_loss: 2.8869 - val_mean_squared_error: 2.8869\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 0s 256us/sample - loss: 1.3628 - mean_squared_error: 1.3628 - val_loss: 2.8611 - val_mean_squared_error: 2.8611\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.3506 - mean_squared_error: 1.3506 - val_loss: 2.8707 - val_mean_squared_error: 2.8707\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.3514 - mean_squared_error: 1.3514 - val_loss: 2.8450 - val_mean_squared_error: 2.8450\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.4015 - mean_squared_error: 1.4015 - val_loss: 2.8240 - val_mean_squared_error: 2.8240\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.3950 - mean_squared_error: 1.3950 - val_loss: 2.8208 - val_mean_squared_error: 2.8208\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 1.2900 - mean_squared_error: 1.2900 - val_loss: 2.9131 - val_mean_squared_error: 2.9131\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.3562 - mean_squared_error: 1.3562 - val_loss: 2.8153 - val_mean_squared_error: 2.8153\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.2967 - mean_squared_error: 1.2967 - val_loss: 3.1156 - val_mean_squared_error: 3.1156\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.3339 - mean_squared_error: 1.3339 - val_loss: 2.8334 - val_mean_squared_error: 2.8334\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.2791 - mean_squared_error: 1.2791 - val_loss: 2.7635 - val_mean_squared_error: 2.7635\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.3037 - mean_squared_error: 1.3037 - val_loss: 2.8884 - val_mean_squared_error: 2.8884\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3276 - mean_squared_error: 1.3276 - val_loss: 2.8670 - val_mean_squared_error: 2.8670\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 0s 247us/sample - loss: 1.3024 - mean_squared_error: 1.3024 - val_loss: 2.8316 - val_mean_squared_error: 2.8316\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.2925 - mean_squared_error: 1.2925 - val_loss: 3.0112 - val_mean_squared_error: 3.0112\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 0s 249us/sample - loss: 1.3403 - mean_squared_error: 1.3403 - val_loss: 2.9519 - val_mean_squared_error: 2.9519\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.2931 - mean_squared_error: 1.2931 - val_loss: 2.7914 - val_mean_squared_error: 2.7914\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 0s 255us/sample - loss: 1.3377 - mean_squared_error: 1.3377 - val_loss: 3.2896 - val_mean_squared_error: 3.2896\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2996 - mean_squared_error: 1.2996 - val_loss: 2.8254 - val_mean_squared_error: 2.8254\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.2350 - mean_squared_error: 1.2350 - val_loss: 2.8031 - val_mean_squared_error: 2.8031\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.3255 - mean_squared_error: 1.3255 - val_loss: 2.8916 - val_mean_squared_error: 2.8916\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 1.3620 - mean_squared_error: 1.3620 - val_loss: 2.8686 - val_mean_squared_error: 2.8686\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.3049 - mean_squared_error: 1.3049 - val_loss: 2.8289 - val_mean_squared_error: 2.8289\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.2031 - mean_squared_error: 1.2031 - val_loss: 2.8556 - val_mean_squared_error: 2.8556\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.2214 - mean_squared_error: 1.2214 - val_loss: 2.8357 - val_mean_squared_error: 2.8357\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 0s 243us/sample - loss: 1.2665 - mean_squared_error: 1.2665 - val_loss: 2.8237 - val_mean_squared_error: 2.8237\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 0s 248us/sample - loss: 1.2997 - mean_squared_error: 1.2997 - val_loss: 2.8469 - val_mean_squared_error: 2.8469\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.2728 - mean_squared_error: 1.2728 - val_loss: 2.8688 - val_mean_squared_error: 2.8688\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 1.2729 - mean_squared_error: 1.2729 - val_loss: 2.8773 - val_mean_squared_error: 2.8773\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 0s 246us/sample - loss: 1.2215 - mean_squared_error: 1.2215 - val_loss: 2.8229 - val_mean_squared_error: 2.8229\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 0s 250us/sample - loss: 1.2637 - mean_squared_error: 1.2637 - val_loss: 2.8726 - val_mean_squared_error: 2.8726\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 0s 244us/sample - loss: 1.2754 - mean_squared_error: 1.2754 - val_loss: 2.8890 - val_mean_squared_error: 2.8890\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 0s 251us/sample - loss: 1.2041 - mean_squared_error: 1.2041 - val_loss: 2.8202 - val_mean_squared_error: 2.8202\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 0s 254us/sample - loss: 1.2843 - mean_squared_error: 1.2843 - val_loss: 2.8543 - val_mean_squared_error: 2.8543\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 0s 239us/sample - loss: 1.2426 - mean_squared_error: 1.2426 - val_loss: 2.8896 - val_mean_squared_error: 2.8896\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 0s 253us/sample - loss: 1.2819 - mean_squared_error: 1.2819 - val_loss: 2.8194 - val_mean_squared_error: 2.8194\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.2933 - mean_squared_error: 1.2933 - val_loss: 3.0076 - val_mean_squared_error: 3.0076\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 0s 252us/sample - loss: 1.1984 - mean_squared_error: 1.1984 - val_loss: 2.7585 - val_mean_squared_error: 2.7585\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 0s 245us/sample - loss: 1.2721 - mean_squared_error: 1.2721 - val_loss: 2.7961 - val_mean_squared_error: 2.7961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3-oCZ_ykWUn",
        "colab_type": "text"
      },
      "source": [
        "## Learning Rate Sensitivity\n",
        "\n",
        "Several rounds of sensitivity studies were performed outside on the Colaboratory environment on the team's personal computers. Many of these studies can be found in the cnn_notebook_tpg.ipynb file located in the team's [GitHub repository](https://github.com/tomgoter/w207_finalproject).\n",
        "\n",
        "To summarize what was learned from those studies:\n",
        "\n",
        "1.   Starting filter depth of 12 to 16 was a better balance to runtime and accuracy than starting with the initial assumption of 32. This is clearly shown below. The starting filter size of 32 is unnecessary for accuracy and runs much more slowly than a reduced starting filter depth.\n",
        "![alt text](https://github.com/tomgoter/w207_finalproject/raw/master/Images/starting_filter.png?raw=true)\n",
        "2.   It was also determined (as previously mentioned in this document) that the Adam and SGD optimizers performed the best for these models. \n",
        "3.  The use of batch normalization (without a bias term) allows us to increase the learning rate and achieve better accuracies. Batch normalization basically standardizes the feature set after every convolution/pooling layer. The image below shows that the use of batch normalization does lead to faster convergence, but comes at the cost of model runtime. \n",
        "\n",
        "![alt text](https://github.com/tomgoter/w207_finalproject/raw/master/Images/batch_norm.png?raw=true)\n",
        "\n",
        "4. Gradual dropouts starting at 0 and increasing by small amounts appeared to give the best final accuracies (but this is further explored below).\n",
        "\n",
        "This cell blocks below were used to pick-up where the aforementioned notebook left off. From here on out all models were built and run through the Colaboratory environment to make the progress easier to follow. We start with a learning rate sensitivity, which also explore different dropout rates and starting filter sizes. \n",
        "\n",
        "The results of the experiment below seemed to indicate that learning rates between 5x to 10x the default rate of 0.001 seem to perform better than the default rate, and that this is true for both starting filter depths of 12 and 16. See the plot below which was generated with the neural_net_analysis.ipynb notebook located at the GitHub repository (link above)\n",
        "\n",
        "![image](https://github.com/tomgoter/w207_finalproject/raw/master/Images/LR_Sense_16SF.png \"Learning Rate Sensitivity\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF2rTYqJkrre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_bn_cnn_model(start_filter, d, step, fc1=500, fc2=500):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model that implements\n",
        "    convolute/pool framework (similar to AlexNet). It makes use of valid padding during the convolution\n",
        "    layers with rectified linear unit activation functions. Kernels are typically 2x2. Starting filter \n",
        "    depth can be varied, but it is assumed the filter depth increases by a factor of two during every \n",
        "    convolution (inspired by AlexNet architecture)\n",
        "    '''\n",
        "    # Instantiate Sequential Model\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "    \n",
        "    # Define Input structure\n",
        "    cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    \n",
        "    # Define convolution layer - takes the start_filter parameter as the filter depth\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='valid', activation='relu', use_bias=False))\n",
        "    \n",
        "    # Standardize the convoluted features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Max Pool - reduce the feature space\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add a form of regularization through dropout - dropout rate is parameterized\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    \n",
        "    # Rinse and repeat for two more full layers - increasing filter depth and dropout rate\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (2, 2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (2, 2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+2*step))\n",
        "    \n",
        "    # Flatten the feature space and transition to fully connected layers\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc1))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc2))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    \n",
        "    # Output layer of 30 keypoint coordinates with linear activation\n",
        "    cnn_model.add(tf.keras.layers.Dense(30))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FK2GEwckcAi",
        "colab_type": "code",
        "outputId": "d0e318ee-1d67-4ac0-f8bb-92a10f9487ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Use an early stopping callback and our timing callback\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                              patience=100, mode='auto')\n",
        "time_callback = TimeHistory()\n",
        "\n",
        "# Initialize a new data frame to hold our output data\n",
        "cnn_lr_df = pd.DataFrame()\n",
        "\n",
        "# Start filter list\n",
        "start_filters = [12, 16]\n",
        "\n",
        "# Create a list of initial dropout values and steps to increase\n",
        "dropouts = [(0.00,0.00), (0.0,0.01), (0.00,0.02), (0.00,0.03)]\n",
        "\n",
        "\n",
        "# Run a parameteric set of studies\n",
        "for lr_factor in [2, 5, 10]: \n",
        "    for start_filter in start_filters:\n",
        "        for d in dropouts:\n",
        "            model = create_bn_cnn_model(start_filter, d[0], d[1])\n",
        "            # Use the adam optimizer with the default learning rate\n",
        "            adam = optimizers.Adam(lr=0.001*lr_factor, beta_1=0.9, beta_2=0.999)\n",
        "            model.compile(\n",
        "                  optimizer=adam,\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_squared_error'])\n",
        "            history = model.fit(\n",
        "                X.astype(np.float32), y.astype(np.float32),\n",
        "                epochs=200,\n",
        "                validation_split=0.15, callbacks=[time_callback, early_stop])\n",
        "            times = time_callback.times\n",
        "\n",
        "            # Convert model output to dataframe\n",
        "            hist = pd.DataFrame(history.history)\n",
        "            hist['epoch'] = history.epoch\n",
        "            hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "            hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "            hist['times'] = times\n",
        "            \n",
        "            # Add model specific metadata to differentiate between models\n",
        "            hist['starting_filter'] = start_filter\n",
        "            hist['layers'] = 3\n",
        "            hist['pooling'] = 'yes'\n",
        "            hist['fc_layer1'] = 500\n",
        "            hist['fc_layer2'] = 500\n",
        "            hist['activation'] = 'relu'\n",
        "            hist['optimizer'] = 'adam'\n",
        "            hist['lrate'] = adam.get_config()['learning_rate']\n",
        "            hist['dropout_initial'] = d[0]\n",
        "            hist['dropout_step'] = d[1]\n",
        "            hist['batch_norm'] = 1\n",
        "            hist['bias'] = 0\n",
        "            hist['stride'] = 1\n",
        "\n",
        "            # Keep concatenating to dataframe\n",
        "            cnn_lr_df = pd.concat([cnn_lr_df,hist])\n",
        "\n",
        "            # Re-pickle after every model to retain progress\n",
        "            cnn_lr_df.to_pickle(drive_path+\"OutputData/cnn_lr_df.pkl\")\n",
        "\n",
        "            # Save models.\n",
        "            filename = \"cnn_lr_{}_d{}_s{}_sf{}_lrfactor{}\".format('adam', d[0], d[1], start_filter, lr_factor)\n",
        "            model.save(drive_path+\"Models/\"+filename+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 691us/sample - loss: 1524.5925 - mean_squared_error: 1524.5928 - val_loss: 1340.4921 - val_mean_squared_error: 1340.4921\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 229.2972 - mean_squared_error: 229.2972 - val_loss: 333.2501 - val_mean_squared_error: 333.2502\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 39.2172 - mean_squared_error: 39.2172 - val_loss: 103.8055 - val_mean_squared_error: 103.8055\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 21.5871 - mean_squared_error: 21.5871 - val_loss: 47.4265 - val_mean_squared_error: 47.4265\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 15.4881 - mean_squared_error: 15.4881 - val_loss: 25.3412 - val_mean_squared_error: 25.3412\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 16.4591 - mean_squared_error: 16.4591 - val_loss: 18.1627 - val_mean_squared_error: 18.1627\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 14.7848 - mean_squared_error: 14.7848 - val_loss: 15.6381 - val_mean_squared_error: 15.6381\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 14.6200 - mean_squared_error: 14.6200 - val_loss: 10.7751 - val_mean_squared_error: 10.7751\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 12.8585 - mean_squared_error: 12.8585 - val_loss: 14.3254 - val_mean_squared_error: 14.3254\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 12.8056 - mean_squared_error: 12.8056 - val_loss: 16.0035 - val_mean_squared_error: 16.0035\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 11.9934 - mean_squared_error: 11.9934 - val_loss: 12.3756 - val_mean_squared_error: 12.3756\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 10.5664 - mean_squared_error: 10.5664 - val_loss: 11.3312 - val_mean_squared_error: 11.3312\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 11.5691 - mean_squared_error: 11.5691 - val_loss: 10.4683 - val_mean_squared_error: 10.4683\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 10.4908 - mean_squared_error: 10.4908 - val_loss: 11.0794 - val_mean_squared_error: 11.0794\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 10.7546 - mean_squared_error: 10.7546 - val_loss: 13.9518 - val_mean_squared_error: 13.9518\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 9.2093 - mean_squared_error: 9.2093 - val_loss: 10.7245 - val_mean_squared_error: 10.7245\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 8.6495 - mean_squared_error: 8.6495 - val_loss: 10.8164 - val_mean_squared_error: 10.8164\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 8.8125 - mean_squared_error: 8.8125 - val_loss: 11.7447 - val_mean_squared_error: 11.7447\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 8.0065 - mean_squared_error: 8.0065 - val_loss: 9.3142 - val_mean_squared_error: 9.3142\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 7.3384 - mean_squared_error: 7.3384 - val_loss: 8.7957 - val_mean_squared_error: 8.7957\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 6.8864 - mean_squared_error: 6.8864 - val_loss: 9.0984 - val_mean_squared_error: 9.0984\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 6.4859 - mean_squared_error: 6.4859 - val_loss: 7.9767 - val_mean_squared_error: 7.9767\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 5.8136 - mean_squared_error: 5.8136 - val_loss: 6.2399 - val_mean_squared_error: 6.2399\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 5.4800 - mean_squared_error: 5.4800 - val_loss: 7.0085 - val_mean_squared_error: 7.0085\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 5.9928 - mean_squared_error: 5.9928 - val_loss: 6.8727 - val_mean_squared_error: 6.8727\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 5.7755 - mean_squared_error: 5.7755 - val_loss: 7.4414 - val_mean_squared_error: 7.4414\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 5.2558 - mean_squared_error: 5.2558 - val_loss: 5.8596 - val_mean_squared_error: 5.8596\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 327us/sample - loss: 5.0365 - mean_squared_error: 5.0365 - val_loss: 6.0949 - val_mean_squared_error: 6.0949\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 4.4527 - mean_squared_error: 4.4527 - val_loss: 5.2767 - val_mean_squared_error: 5.2767\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 4.1954 - mean_squared_error: 4.1954 - val_loss: 5.6757 - val_mean_squared_error: 5.6757\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 4.2053 - mean_squared_error: 4.2053 - val_loss: 5.0579 - val_mean_squared_error: 5.0579\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 4.1148 - mean_squared_error: 4.1148 - val_loss: 4.5708 - val_mean_squared_error: 4.5708\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 3.9779 - mean_squared_error: 3.9779 - val_loss: 5.7409 - val_mean_squared_error: 5.7409\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 3.8312 - mean_squared_error: 3.8312 - val_loss: 4.8623 - val_mean_squared_error: 4.8623\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 3.7779 - mean_squared_error: 3.7779 - val_loss: 4.5047 - val_mean_squared_error: 4.5047\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 3.6256 - mean_squared_error: 3.6256 - val_loss: 4.7511 - val_mean_squared_error: 4.7511\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 3.5155 - mean_squared_error: 3.5155 - val_loss: 4.5789 - val_mean_squared_error: 4.5789\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 327us/sample - loss: 3.4155 - mean_squared_error: 3.4155 - val_loss: 5.0810 - val_mean_squared_error: 5.0810\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 3.4059 - mean_squared_error: 3.4059 - val_loss: 5.4563 - val_mean_squared_error: 5.4563\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 3.1935 - mean_squared_error: 3.1935 - val_loss: 3.9975 - val_mean_squared_error: 3.9975\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 3.1112 - mean_squared_error: 3.1112 - val_loss: 4.4076 - val_mean_squared_error: 4.4076\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 3.2834 - mean_squared_error: 3.2834 - val_loss: 4.8234 - val_mean_squared_error: 4.8234\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 3.4395 - mean_squared_error: 3.4395 - val_loss: 4.5921 - val_mean_squared_error: 4.5921\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 3.1076 - mean_squared_error: 3.1076 - val_loss: 3.7299 - val_mean_squared_error: 3.7299\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 3.1186 - mean_squared_error: 3.1186 - val_loss: 3.9973 - val_mean_squared_error: 3.9973\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 2.9553 - mean_squared_error: 2.9553 - val_loss: 3.6556 - val_mean_squared_error: 3.6556\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 2.7850 - mean_squared_error: 2.7850 - val_loss: 4.2310 - val_mean_squared_error: 4.2310\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 2.6972 - mean_squared_error: 2.6972 - val_loss: 3.3696 - val_mean_squared_error: 3.3696\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 2.9707 - mean_squared_error: 2.9707 - val_loss: 3.3160 - val_mean_squared_error: 3.3160\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 2.6943 - mean_squared_error: 2.6943 - val_loss: 5.7513 - val_mean_squared_error: 5.7513\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 3.0725 - mean_squared_error: 3.0725 - val_loss: 5.1434 - val_mean_squared_error: 5.1434\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 2.7004 - mean_squared_error: 2.7004 - val_loss: 4.9225 - val_mean_squared_error: 4.9225\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 2.7124 - mean_squared_error: 2.7124 - val_loss: 3.3402 - val_mean_squared_error: 3.3402\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 2.7610 - mean_squared_error: 2.7610 - val_loss: 3.6415 - val_mean_squared_error: 3.6415\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 2.7432 - mean_squared_error: 2.7432 - val_loss: 3.8502 - val_mean_squared_error: 3.8502\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 2.4309 - mean_squared_error: 2.4309 - val_loss: 4.1760 - val_mean_squared_error: 4.1760\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 2.2484 - mean_squared_error: 2.2484 - val_loss: 3.5271 - val_mean_squared_error: 3.5271\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 2.4465 - mean_squared_error: 2.4465 - val_loss: 3.6380 - val_mean_squared_error: 3.6380\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.3687 - mean_squared_error: 2.3687 - val_loss: 3.8697 - val_mean_squared_error: 3.8697\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 2.2982 - mean_squared_error: 2.2982 - val_loss: 3.5946 - val_mean_squared_error: 3.5946\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 2.1950 - mean_squared_error: 2.1950 - val_loss: 3.5468 - val_mean_squared_error: 3.5468\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 2.1717 - mean_squared_error: 2.1717 - val_loss: 4.3792 - val_mean_squared_error: 4.3792\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 2.1563 - mean_squared_error: 2.1563 - val_loss: 3.3375 - val_mean_squared_error: 3.3375\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 2.0412 - mean_squared_error: 2.0412 - val_loss: 3.4253 - val_mean_squared_error: 3.4253\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 2.2236 - mean_squared_error: 2.2236 - val_loss: 4.0542 - val_mean_squared_error: 4.0542\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 2.1583 - mean_squared_error: 2.1583 - val_loss: 4.4447 - val_mean_squared_error: 4.4447\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 2.0917 - mean_squared_error: 2.0917 - val_loss: 4.2642 - val_mean_squared_error: 4.2642\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 2.1918 - mean_squared_error: 2.1918 - val_loss: 3.1926 - val_mean_squared_error: 3.1926\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 2.0283 - mean_squared_error: 2.0283 - val_loss: 2.8176 - val_mean_squared_error: 2.8176\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.9635 - mean_squared_error: 1.9635 - val_loss: 3.3382 - val_mean_squared_error: 3.3382\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.0154 - mean_squared_error: 2.0154 - val_loss: 3.9843 - val_mean_squared_error: 3.9843\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 2.0209 - mean_squared_error: 2.0209 - val_loss: 2.9425 - val_mean_squared_error: 2.9425\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.9137 - mean_squared_error: 1.9137 - val_loss: 3.3100 - val_mean_squared_error: 3.3100\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.7703 - mean_squared_error: 1.7703 - val_loss: 2.9951 - val_mean_squared_error: 2.9951\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.9670 - mean_squared_error: 1.9670 - val_loss: 3.6787 - val_mean_squared_error: 3.6787\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.9712 - mean_squared_error: 1.9712 - val_loss: 3.4390 - val_mean_squared_error: 3.4390\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.1967 - mean_squared_error: 2.1967 - val_loss: 3.3732 - val_mean_squared_error: 3.3732\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 1.9861 - mean_squared_error: 1.9861 - val_loss: 2.9091 - val_mean_squared_error: 2.9091\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.8522 - mean_squared_error: 1.8522 - val_loss: 3.0388 - val_mean_squared_error: 3.0388\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.7615 - mean_squared_error: 1.7615 - val_loss: 2.8752 - val_mean_squared_error: 2.8752\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.7972 - mean_squared_error: 1.7972 - val_loss: 3.1531 - val_mean_squared_error: 3.1531\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.6786 - mean_squared_error: 1.6786 - val_loss: 2.8551 - val_mean_squared_error: 2.8551\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.8268 - mean_squared_error: 1.8268 - val_loss: 3.5829 - val_mean_squared_error: 3.5829\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.9200 - mean_squared_error: 1.9200 - val_loss: 4.4824 - val_mean_squared_error: 4.4824\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 1.9233 - mean_squared_error: 1.9233 - val_loss: 3.1965 - val_mean_squared_error: 3.1965\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.6678 - mean_squared_error: 1.6678 - val_loss: 2.7654 - val_mean_squared_error: 2.7654\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.6699 - mean_squared_error: 1.6699 - val_loss: 3.4868 - val_mean_squared_error: 3.4868\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.7726 - mean_squared_error: 1.7726 - val_loss: 3.0454 - val_mean_squared_error: 3.0454\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.5334 - mean_squared_error: 1.5334 - val_loss: 3.0022 - val_mean_squared_error: 3.0022\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.9744 - mean_squared_error: 1.9744 - val_loss: 2.8378 - val_mean_squared_error: 2.8378\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 1.8268 - mean_squared_error: 1.8268 - val_loss: 3.1858 - val_mean_squared_error: 3.1858\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 1.7038 - mean_squared_error: 1.7038 - val_loss: 3.6120 - val_mean_squared_error: 3.6120\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 1.5929 - mean_squared_error: 1.5929 - val_loss: 2.8858 - val_mean_squared_error: 2.8858\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.6499 - mean_squared_error: 1.6499 - val_loss: 3.1895 - val_mean_squared_error: 3.1896\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.5100 - mean_squared_error: 1.5100 - val_loss: 3.3787 - val_mean_squared_error: 3.3787\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.5992 - mean_squared_error: 1.5992 - val_loss: 2.7657 - val_mean_squared_error: 2.7657\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.5502 - mean_squared_error: 1.5502 - val_loss: 2.8314 - val_mean_squared_error: 2.8314\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.4696 - mean_squared_error: 1.4696 - val_loss: 3.1666 - val_mean_squared_error: 3.1666\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.3915 - mean_squared_error: 1.3915 - val_loss: 2.8946 - val_mean_squared_error: 2.8946\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.5075 - mean_squared_error: 1.5075 - val_loss: 3.0499 - val_mean_squared_error: 3.0499\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.5333 - mean_squared_error: 1.5333 - val_loss: 3.3448 - val_mean_squared_error: 3.3448\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.6521 - mean_squared_error: 1.6521 - val_loss: 3.6577 - val_mean_squared_error: 3.6577\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 1.5652 - mean_squared_error: 1.5652 - val_loss: 2.6486 - val_mean_squared_error: 2.6486\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.4510 - mean_squared_error: 1.4510 - val_loss: 3.0655 - val_mean_squared_error: 3.0655\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.4722 - mean_squared_error: 1.4722 - val_loss: 3.0636 - val_mean_squared_error: 3.0636\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.5761 - mean_squared_error: 1.5761 - val_loss: 3.2183 - val_mean_squared_error: 3.2183\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.6860 - mean_squared_error: 1.6860 - val_loss: 3.0852 - val_mean_squared_error: 3.0852\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 1.6515 - mean_squared_error: 1.6515 - val_loss: 3.0542 - val_mean_squared_error: 3.0542\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.5147 - mean_squared_error: 1.5147 - val_loss: 3.2559 - val_mean_squared_error: 3.2559\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.5189 - mean_squared_error: 1.5189 - val_loss: 3.0748 - val_mean_squared_error: 3.0748\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.5855 - mean_squared_error: 1.5855 - val_loss: 3.1558 - val_mean_squared_error: 3.1558\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.3288 - mean_squared_error: 1.3288 - val_loss: 3.1580 - val_mean_squared_error: 3.1580\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.3528 - mean_squared_error: 1.3528 - val_loss: 2.9205 - val_mean_squared_error: 2.9205\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.3930 - mean_squared_error: 1.3930 - val_loss: 3.4026 - val_mean_squared_error: 3.4026\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.4290 - mean_squared_error: 1.4290 - val_loss: 3.0492 - val_mean_squared_error: 3.0492\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.2499 - mean_squared_error: 1.2499 - val_loss: 2.9560 - val_mean_squared_error: 2.9560\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.3829 - mean_squared_error: 1.3829 - val_loss: 2.6087 - val_mean_squared_error: 2.6087\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.2697 - mean_squared_error: 1.2697 - val_loss: 2.8086 - val_mean_squared_error: 2.8086\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.3186 - mean_squared_error: 1.3186 - val_loss: 3.2032 - val_mean_squared_error: 3.2032\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.3838 - mean_squared_error: 1.3838 - val_loss: 2.9809 - val_mean_squared_error: 2.9809\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.3503 - mean_squared_error: 1.3503 - val_loss: 2.9820 - val_mean_squared_error: 2.9820\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 1.3109 - mean_squared_error: 1.3109 - val_loss: 2.8117 - val_mean_squared_error: 2.8117\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.4020 - mean_squared_error: 1.4020 - val_loss: 4.1778 - val_mean_squared_error: 4.1778\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.4137 - mean_squared_error: 1.4137 - val_loss: 3.2657 - val_mean_squared_error: 3.2657\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.3992 - mean_squared_error: 1.3992 - val_loss: 2.7730 - val_mean_squared_error: 2.7730\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.2853 - mean_squared_error: 1.2853 - val_loss: 3.1351 - val_mean_squared_error: 3.1351\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.2874 - mean_squared_error: 1.2874 - val_loss: 2.8040 - val_mean_squared_error: 2.8040\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.2224 - mean_squared_error: 1.2224 - val_loss: 2.9431 - val_mean_squared_error: 2.9431\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.2373 - mean_squared_error: 1.2373 - val_loss: 2.7026 - val_mean_squared_error: 2.7026\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 1.2033 - mean_squared_error: 1.2033 - val_loss: 2.6911 - val_mean_squared_error: 2.6911\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.2562 - mean_squared_error: 1.2562 - val_loss: 3.6669 - val_mean_squared_error: 3.6669\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.3466 - mean_squared_error: 1.3466 - val_loss: 3.0587 - val_mean_squared_error: 3.0587\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.2520 - mean_squared_error: 1.2520 - val_loss: 3.3521 - val_mean_squared_error: 3.3521\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.2556 - mean_squared_error: 1.2556 - val_loss: 3.1148 - val_mean_squared_error: 3.1148\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.2919 - mean_squared_error: 1.2919 - val_loss: 3.5194 - val_mean_squared_error: 3.5194\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.1034 - mean_squared_error: 1.1034 - val_loss: 2.6932 - val_mean_squared_error: 2.6932\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.3507 - mean_squared_error: 1.3507 - val_loss: 2.7983 - val_mean_squared_error: 2.7983\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 1.1841 - mean_squared_error: 1.1841 - val_loss: 2.7635 - val_mean_squared_error: 2.7635\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.2228 - mean_squared_error: 1.2228 - val_loss: 2.9916 - val_mean_squared_error: 2.9916\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.2590 - mean_squared_error: 1.2590 - val_loss: 2.9638 - val_mean_squared_error: 2.9638\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.1848 - mean_squared_error: 1.1848 - val_loss: 3.2261 - val_mean_squared_error: 3.2261\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.3669 - mean_squared_error: 1.3669 - val_loss: 2.6746 - val_mean_squared_error: 2.6746\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.2367 - mean_squared_error: 1.2367 - val_loss: 2.6942 - val_mean_squared_error: 2.6942\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 1.2440 - mean_squared_error: 1.2440 - val_loss: 2.8893 - val_mean_squared_error: 2.8893\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 1.1372 - mean_squared_error: 1.1372 - val_loss: 3.4356 - val_mean_squared_error: 3.4356\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.1430 - mean_squared_error: 1.1430 - val_loss: 2.7737 - val_mean_squared_error: 2.7737\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.1281 - mean_squared_error: 1.1281 - val_loss: 2.7658 - val_mean_squared_error: 2.7658\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 1.2148 - mean_squared_error: 1.2148 - val_loss: 2.6524 - val_mean_squared_error: 2.6524\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.1351 - mean_squared_error: 1.1351 - val_loss: 2.7441 - val_mean_squared_error: 2.7441\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.1403 - mean_squared_error: 1.1403 - val_loss: 2.5393 - val_mean_squared_error: 2.5393\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.1392 - mean_squared_error: 1.1392 - val_loss: 3.0913 - val_mean_squared_error: 3.0913\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.1939 - mean_squared_error: 1.1939 - val_loss: 2.5334 - val_mean_squared_error: 2.5334\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.0369 - mean_squared_error: 1.0369 - val_loss: 2.8408 - val_mean_squared_error: 2.8408\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.0257 - mean_squared_error: 1.0257 - val_loss: 3.0412 - val_mean_squared_error: 3.0412\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 1.1442 - mean_squared_error: 1.1442 - val_loss: 3.0601 - val_mean_squared_error: 3.0601\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.1605 - mean_squared_error: 1.1605 - val_loss: 2.8725 - val_mean_squared_error: 2.8725\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.2256 - mean_squared_error: 1.2256 - val_loss: 2.7384 - val_mean_squared_error: 2.7384\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.0667 - mean_squared_error: 1.0667 - val_loss: 2.8791 - val_mean_squared_error: 2.8791\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.2401 - mean_squared_error: 1.2401 - val_loss: 2.8068 - val_mean_squared_error: 2.8068\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 1.0490 - mean_squared_error: 1.0490 - val_loss: 2.8483 - val_mean_squared_error: 2.8483\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.0272 - mean_squared_error: 1.0272 - val_loss: 2.5878 - val_mean_squared_error: 2.5878\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 1.0097 - mean_squared_error: 1.0097 - val_loss: 2.4772 - val_mean_squared_error: 2.4772\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 0.9846 - mean_squared_error: 0.9846 - val_loss: 2.7602 - val_mean_squared_error: 2.7602\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 0.9718 - mean_squared_error: 0.9718 - val_loss: 2.5430 - val_mean_squared_error: 2.5430\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.0677 - mean_squared_error: 1.0677 - val_loss: 2.5938 - val_mean_squared_error: 2.5938\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.0692 - mean_squared_error: 1.0692 - val_loss: 2.5925 - val_mean_squared_error: 2.5925\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 328us/sample - loss: 0.9344 - mean_squared_error: 0.9344 - val_loss: 2.7897 - val_mean_squared_error: 2.7897\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 0.9988 - mean_squared_error: 0.9988 - val_loss: 3.4614 - val_mean_squared_error: 3.4614\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 1.0472 - mean_squared_error: 1.0472 - val_loss: 3.3565 - val_mean_squared_error: 3.3565\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 1.1021 - mean_squared_error: 1.1021 - val_loss: 2.4928 - val_mean_squared_error: 2.4928\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.0261 - mean_squared_error: 1.0261 - val_loss: 2.5720 - val_mean_squared_error: 2.5720\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 0.9979 - mean_squared_error: 0.9979 - val_loss: 2.6140 - val_mean_squared_error: 2.6140\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.0132 - mean_squared_error: 1.0132 - val_loss: 2.4827 - val_mean_squared_error: 2.4827\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.0035 - mean_squared_error: 1.0035 - val_loss: 2.7606 - val_mean_squared_error: 2.7606\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 2.9191 - val_mean_squared_error: 2.9191\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 0.9622 - mean_squared_error: 0.9622 - val_loss: 2.9331 - val_mean_squared_error: 2.9331\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 1.0396 - mean_squared_error: 1.0396 - val_loss: 2.7388 - val_mean_squared_error: 2.7388\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 330us/sample - loss: 1.1263 - mean_squared_error: 1.1263 - val_loss: 2.7988 - val_mean_squared_error: 2.7988\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 1.0204 - mean_squared_error: 1.0204 - val_loss: 2.4584 - val_mean_squared_error: 2.4584\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 324us/sample - loss: 0.9972 - mean_squared_error: 0.9972 - val_loss: 2.4462 - val_mean_squared_error: 2.4462\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 1.0616 - mean_squared_error: 1.0616 - val_loss: 3.0488 - val_mean_squared_error: 3.0488\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.1434 - mean_squared_error: 1.1434 - val_loss: 2.8065 - val_mean_squared_error: 2.8065\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.0747 - mean_squared_error: 1.0747 - val_loss: 2.8111 - val_mean_squared_error: 2.8111\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 0.9592 - mean_squared_error: 0.9592 - val_loss: 2.4511 - val_mean_squared_error: 2.4511\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 0.9603 - mean_squared_error: 0.9603 - val_loss: 3.1299 - val_mean_squared_error: 3.1299\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.0444 - mean_squared_error: 1.0444 - val_loss: 2.6308 - val_mean_squared_error: 2.6308\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 0.8967 - mean_squared_error: 0.8967 - val_loss: 2.3878 - val_mean_squared_error: 2.3878\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 0.9619 - mean_squared_error: 0.9619 - val_loss: 2.7498 - val_mean_squared_error: 2.7498\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 0.8410 - mean_squared_error: 0.8410 - val_loss: 2.4158 - val_mean_squared_error: 2.4158\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 333us/sample - loss: 0.9353 - mean_squared_error: 0.9353 - val_loss: 2.5129 - val_mean_squared_error: 2.5129\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 0.8498 - mean_squared_error: 0.8498 - val_loss: 2.5320 - val_mean_squared_error: 2.5320\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 329us/sample - loss: 0.8835 - mean_squared_error: 0.8835 - val_loss: 2.5566 - val_mean_squared_error: 2.5566\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 0.8192 - mean_squared_error: 0.8192 - val_loss: 2.5539 - val_mean_squared_error: 2.5539\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 0.9380 - mean_squared_error: 0.9380 - val_loss: 2.6786 - val_mean_squared_error: 2.6786\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 0.9260 - mean_squared_error: 0.9260 - val_loss: 2.9203 - val_mean_squared_error: 2.9203\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 0.8660 - mean_squared_error: 0.8660 - val_loss: 2.5545 - val_mean_squared_error: 2.5545\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 335us/sample - loss: 0.8759 - mean_squared_error: 0.8759 - val_loss: 2.5923 - val_mean_squared_error: 2.5923\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 0.9031 - mean_squared_error: 0.9031 - val_loss: 2.8666 - val_mean_squared_error: 2.8666\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 0.9407 - mean_squared_error: 0.9407 - val_loss: 2.9140 - val_mean_squared_error: 2.9140\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 2.7811 - val_mean_squared_error: 2.7811\n",
            "==================================================\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 746us/sample - loss: 1531.7508 - mean_squared_error: 1531.7510 - val_loss: 1833.5960 - val_mean_squared_error: 1833.5958\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 227.3205 - mean_squared_error: 227.3205 - val_loss: 636.7475 - val_mean_squared_error: 636.7476\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 34.3137 - mean_squared_error: 34.3137 - val_loss: 129.6406 - val_mean_squared_error: 129.6406\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 20.7294 - mean_squared_error: 20.7294 - val_loss: 36.6403 - val_mean_squared_error: 36.6403\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 17.7408 - mean_squared_error: 17.7408 - val_loss: 31.2636 - val_mean_squared_error: 31.2636\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 15.8775 - mean_squared_error: 15.8775 - val_loss: 17.1555 - val_mean_squared_error: 17.1555\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 15.7749 - mean_squared_error: 15.7749 - val_loss: 12.8060 - val_mean_squared_error: 12.8060\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 13.3198 - mean_squared_error: 13.3198 - val_loss: 13.6501 - val_mean_squared_error: 13.6501\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 14.8992 - mean_squared_error: 14.8992 - val_loss: 13.7562 - val_mean_squared_error: 13.7562\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 13.5944 - mean_squared_error: 13.5944 - val_loss: 11.2093 - val_mean_squared_error: 11.2093\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 10.8711 - mean_squared_error: 10.8711 - val_loss: 10.1716 - val_mean_squared_error: 10.1716\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 10.9808 - mean_squared_error: 10.9808 - val_loss: 12.4758 - val_mean_squared_error: 12.4758\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 10.7591 - mean_squared_error: 10.7591 - val_loss: 11.0797 - val_mean_squared_error: 11.0797\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 9.9483 - mean_squared_error: 9.9483 - val_loss: 14.1063 - val_mean_squared_error: 14.1063\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 9.0034 - mean_squared_error: 9.0034 - val_loss: 12.7847 - val_mean_squared_error: 12.7847\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 8.5551 - mean_squared_error: 8.5551 - val_loss: 9.5947 - val_mean_squared_error: 9.5947\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 8.8986 - mean_squared_error: 8.8986 - val_loss: 8.7610 - val_mean_squared_error: 8.7610\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 7.7325 - mean_squared_error: 7.7325 - val_loss: 7.1777 - val_mean_squared_error: 7.1777\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 7.1611 - mean_squared_error: 7.1611 - val_loss: 7.4120 - val_mean_squared_error: 7.4120\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 6.9081 - mean_squared_error: 6.9081 - val_loss: 6.9830 - val_mean_squared_error: 6.9830\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 7.3473 - mean_squared_error: 7.3473 - val_loss: 9.4985 - val_mean_squared_error: 9.4985\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 6.2172 - mean_squared_error: 6.2172 - val_loss: 8.0752 - val_mean_squared_error: 8.0752\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 6.1281 - mean_squared_error: 6.1281 - val_loss: 7.2919 - val_mean_squared_error: 7.2919\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 5.3750 - mean_squared_error: 5.3750 - val_loss: 6.7647 - val_mean_squared_error: 6.7647\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 5.6228 - mean_squared_error: 5.6228 - val_loss: 5.7332 - val_mean_squared_error: 5.7332\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 5.1281 - mean_squared_error: 5.1281 - val_loss: 5.5741 - val_mean_squared_error: 5.5741\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 4.9573 - mean_squared_error: 4.9573 - val_loss: 6.4072 - val_mean_squared_error: 6.4072\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 5.0629 - mean_squared_error: 5.0629 - val_loss: 5.0215 - val_mean_squared_error: 5.0215\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 4.8806 - mean_squared_error: 4.8806 - val_loss: 5.6619 - val_mean_squared_error: 5.6619\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 4.1556 - mean_squared_error: 4.1556 - val_loss: 6.4258 - val_mean_squared_error: 6.4258\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 4.1947 - mean_squared_error: 4.1947 - val_loss: 6.2721 - val_mean_squared_error: 6.2721\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 3.9906 - mean_squared_error: 3.9906 - val_loss: 5.6023 - val_mean_squared_error: 5.6023\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 4.0203 - mean_squared_error: 4.0203 - val_loss: 4.9937 - val_mean_squared_error: 4.9937\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 3.6927 - mean_squared_error: 3.6927 - val_loss: 4.2784 - val_mean_squared_error: 4.2784\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 3.6724 - mean_squared_error: 3.6724 - val_loss: 4.5915 - val_mean_squared_error: 4.5915\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 4.0681 - mean_squared_error: 4.0681 - val_loss: 3.9745 - val_mean_squared_error: 3.9745\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 3.7794 - mean_squared_error: 3.7794 - val_loss: 4.0804 - val_mean_squared_error: 4.0804\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 3.7756 - mean_squared_error: 3.7756 - val_loss: 4.9983 - val_mean_squared_error: 4.9983\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 332us/sample - loss: 3.6434 - mean_squared_error: 3.6434 - val_loss: 4.1991 - val_mean_squared_error: 4.1991\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 3.3269 - mean_squared_error: 3.3269 - val_loss: 5.1763 - val_mean_squared_error: 5.1763\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 331us/sample - loss: 3.5580 - mean_squared_error: 3.5580 - val_loss: 4.1809 - val_mean_squared_error: 4.1809\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 3.6196 - mean_squared_error: 3.6196 - val_loss: 4.2060 - val_mean_squared_error: 4.2060\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 3.5793 - mean_squared_error: 3.5793 - val_loss: 4.2517 - val_mean_squared_error: 4.2517\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 3.2795 - mean_squared_error: 3.2795 - val_loss: 3.8887 - val_mean_squared_error: 3.8887\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.9832 - mean_squared_error: 2.9832 - val_loss: 4.0836 - val_mean_squared_error: 4.0836\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 3.1435 - mean_squared_error: 3.1435 - val_loss: 3.5348 - val_mean_squared_error: 3.5348\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 2.8675 - mean_squared_error: 2.8675 - val_loss: 3.4064 - val_mean_squared_error: 3.4064\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 2.7854 - mean_squared_error: 2.7854 - val_loss: 4.0605 - val_mean_squared_error: 4.0605\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 2.7528 - mean_squared_error: 2.7528 - val_loss: 4.0674 - val_mean_squared_error: 4.0674\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.6106 - mean_squared_error: 2.6106 - val_loss: 3.2367 - val_mean_squared_error: 3.2367\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.6025 - mean_squared_error: 2.6025 - val_loss: 3.2029 - val_mean_squared_error: 3.2029\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 3.0152 - mean_squared_error: 3.0152 - val_loss: 3.5455 - val_mean_squared_error: 3.5455\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 2.6341 - mean_squared_error: 2.6341 - val_loss: 3.2474 - val_mean_squared_error: 3.2474\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.8424 - mean_squared_error: 2.8424 - val_loss: 4.9503 - val_mean_squared_error: 4.9503\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.6412 - mean_squared_error: 2.6412 - val_loss: 3.3565 - val_mean_squared_error: 3.3565\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.6999 - mean_squared_error: 2.6999 - val_loss: 3.5115 - val_mean_squared_error: 3.5115\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.5216 - mean_squared_error: 2.5216 - val_loss: 3.8212 - val_mean_squared_error: 3.8212\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 2.5677 - mean_squared_error: 2.5677 - val_loss: 3.7734 - val_mean_squared_error: 3.7734\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 2.7973 - mean_squared_error: 2.7973 - val_loss: 3.3221 - val_mean_squared_error: 3.3221\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.5895 - mean_squared_error: 2.5895 - val_loss: 3.4111 - val_mean_squared_error: 3.4111\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.5085 - mean_squared_error: 2.5085 - val_loss: 4.0086 - val_mean_squared_error: 4.0086\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 2.3536 - mean_squared_error: 2.3536 - val_loss: 3.1289 - val_mean_squared_error: 3.1289\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 2.2753 - mean_squared_error: 2.2753 - val_loss: 3.7759 - val_mean_squared_error: 3.7759\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.3875 - mean_squared_error: 2.3875 - val_loss: 3.8786 - val_mean_squared_error: 3.8786\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 2.3129 - mean_squared_error: 2.3129 - val_loss: 4.0096 - val_mean_squared_error: 4.0096\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.3944 - mean_squared_error: 2.3944 - val_loss: 4.1020 - val_mean_squared_error: 4.1020\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.1674 - mean_squared_error: 2.1674 - val_loss: 3.2697 - val_mean_squared_error: 3.2697\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.3325 - mean_squared_error: 2.3325 - val_loss: 3.1854 - val_mean_squared_error: 3.1854\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 2.1679 - mean_squared_error: 2.1679 - val_loss: 3.4384 - val_mean_squared_error: 3.4384\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 2.2473 - mean_squared_error: 2.2473 - val_loss: 3.1735 - val_mean_squared_error: 3.1735\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.2524 - mean_squared_error: 2.2524 - val_loss: 3.6364 - val_mean_squared_error: 3.6364\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 2.0567 - mean_squared_error: 2.0567 - val_loss: 3.1127 - val_mean_squared_error: 3.1127\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 2.1227 - mean_squared_error: 2.1227 - val_loss: 3.1082 - val_mean_squared_error: 3.1082\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 2.0594 - mean_squared_error: 2.0594 - val_loss: 2.9600 - val_mean_squared_error: 2.9600\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.9603 - mean_squared_error: 1.9603 - val_loss: 3.2447 - val_mean_squared_error: 3.2447\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 2.1427 - mean_squared_error: 2.1427 - val_loss: 3.1463 - val_mean_squared_error: 3.1463\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 2.1888 - mean_squared_error: 2.1888 - val_loss: 2.9011 - val_mean_squared_error: 2.9011\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.9067 - mean_squared_error: 1.9067 - val_loss: 2.7594 - val_mean_squared_error: 2.7594\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.7529 - mean_squared_error: 1.7529 - val_loss: 3.0492 - val_mean_squared_error: 3.0492\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.8576 - mean_squared_error: 1.8576 - val_loss: 3.6980 - val_mean_squared_error: 3.6980\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.9385 - mean_squared_error: 1.9385 - val_loss: 3.2157 - val_mean_squared_error: 3.2157\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.9608 - mean_squared_error: 1.9608 - val_loss: 2.9709 - val_mean_squared_error: 2.9709\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.8596 - mean_squared_error: 1.8596 - val_loss: 2.9752 - val_mean_squared_error: 2.9752\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.9284 - mean_squared_error: 1.9284 - val_loss: 2.8694 - val_mean_squared_error: 2.8694\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.8748 - mean_squared_error: 1.8748 - val_loss: 3.0896 - val_mean_squared_error: 3.0896\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.9133 - mean_squared_error: 1.9133 - val_loss: 2.8866 - val_mean_squared_error: 2.8866\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.8594 - mean_squared_error: 1.8594 - val_loss: 3.0264 - val_mean_squared_error: 3.0264\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.7998 - mean_squared_error: 1.7998 - val_loss: 3.6406 - val_mean_squared_error: 3.6406\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.8177 - mean_squared_error: 1.8177 - val_loss: 3.2685 - val_mean_squared_error: 3.2685\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.8832 - mean_squared_error: 1.8832 - val_loss: 3.1390 - val_mean_squared_error: 3.1390\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.8344 - mean_squared_error: 1.8344 - val_loss: 3.2387 - val_mean_squared_error: 3.2387\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.7053 - mean_squared_error: 1.7053 - val_loss: 3.0298 - val_mean_squared_error: 3.0298\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.7629 - mean_squared_error: 1.7629 - val_loss: 2.8983 - val_mean_squared_error: 2.8983\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.7862 - mean_squared_error: 1.7862 - val_loss: 2.8197 - val_mean_squared_error: 2.8197\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.7655 - mean_squared_error: 1.7655 - val_loss: 2.7140 - val_mean_squared_error: 2.7140\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.6807 - mean_squared_error: 1.6807 - val_loss: 2.8759 - val_mean_squared_error: 2.8759\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.7159 - mean_squared_error: 1.7159 - val_loss: 2.9226 - val_mean_squared_error: 2.9226\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.6338 - mean_squared_error: 1.6338 - val_loss: 2.8084 - val_mean_squared_error: 2.8084\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.6550 - mean_squared_error: 1.6550 - val_loss: 3.0734 - val_mean_squared_error: 3.0734\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.7801 - mean_squared_error: 1.7801 - val_loss: 3.1237 - val_mean_squared_error: 3.1237\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.7050 - mean_squared_error: 1.7050 - val_loss: 3.1416 - val_mean_squared_error: 3.1416\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.6976 - mean_squared_error: 1.6976 - val_loss: 2.9897 - val_mean_squared_error: 2.9897\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.6797 - mean_squared_error: 1.6797 - val_loss: 2.8713 - val_mean_squared_error: 2.8713\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.7926 - mean_squared_error: 1.7926 - val_loss: 3.2137 - val_mean_squared_error: 3.2137\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.9228 - mean_squared_error: 1.9228 - val_loss: 3.1132 - val_mean_squared_error: 3.1132\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.7678 - mean_squared_error: 1.7678 - val_loss: 3.0888 - val_mean_squared_error: 3.0888\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.7468 - mean_squared_error: 1.7468 - val_loss: 3.4390 - val_mean_squared_error: 3.4390\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.6636 - mean_squared_error: 1.6636 - val_loss: 2.9194 - val_mean_squared_error: 2.9194\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.5578 - mean_squared_error: 1.5578 - val_loss: 2.8632 - val_mean_squared_error: 2.8632\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.5727 - mean_squared_error: 1.5727 - val_loss: 2.5893 - val_mean_squared_error: 2.5893\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.5924 - mean_squared_error: 1.5924 - val_loss: 3.1592 - val_mean_squared_error: 3.1592\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.5921 - mean_squared_error: 1.5921 - val_loss: 2.7343 - val_mean_squared_error: 2.7343\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.8415 - mean_squared_error: 1.8415 - val_loss: 2.8866 - val_mean_squared_error: 2.8866\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.5994 - mean_squared_error: 1.5994 - val_loss: 3.3479 - val_mean_squared_error: 3.3479\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.5063 - mean_squared_error: 1.5063 - val_loss: 2.7707 - val_mean_squared_error: 2.7707\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.6251 - mean_squared_error: 1.6251 - val_loss: 2.8279 - val_mean_squared_error: 2.8279\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.9298 - mean_squared_error: 1.9298 - val_loss: 3.3635 - val_mean_squared_error: 3.3635\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.6606 - mean_squared_error: 1.6606 - val_loss: 2.7797 - val_mean_squared_error: 2.7797\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.6366 - mean_squared_error: 1.6366 - val_loss: 2.8445 - val_mean_squared_error: 2.8445\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.5016 - mean_squared_error: 1.5016 - val_loss: 2.8955 - val_mean_squared_error: 2.8955\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.4426 - mean_squared_error: 1.4426 - val_loss: 2.6126 - val_mean_squared_error: 2.6126\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.3267 - mean_squared_error: 1.3267 - val_loss: 2.9418 - val_mean_squared_error: 2.9418\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.5202 - mean_squared_error: 1.5202 - val_loss: 2.4002 - val_mean_squared_error: 2.4002\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.5574 - mean_squared_error: 1.5574 - val_loss: 3.2675 - val_mean_squared_error: 3.2675\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.5757 - mean_squared_error: 1.5757 - val_loss: 2.7796 - val_mean_squared_error: 2.7796\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.3995 - mean_squared_error: 1.3995 - val_loss: 2.5042 - val_mean_squared_error: 2.5042\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.2981 - mean_squared_error: 1.2981 - val_loss: 2.5214 - val_mean_squared_error: 2.5214\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.4248 - mean_squared_error: 1.4248 - val_loss: 2.8333 - val_mean_squared_error: 2.8333\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.3885 - mean_squared_error: 1.3885 - val_loss: 2.4288 - val_mean_squared_error: 2.4288\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4475 - mean_squared_error: 1.4475 - val_loss: 2.6834 - val_mean_squared_error: 2.6834\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.4824 - mean_squared_error: 1.4824 - val_loss: 2.9444 - val_mean_squared_error: 2.9444\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4742 - mean_squared_error: 1.4742 - val_loss: 2.6404 - val_mean_squared_error: 2.6404\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.3760 - mean_squared_error: 1.3760 - val_loss: 2.5610 - val_mean_squared_error: 2.5610\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3043 - mean_squared_error: 1.3043 - val_loss: 2.8319 - val_mean_squared_error: 2.8319\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.2822 - mean_squared_error: 1.2822 - val_loss: 2.4961 - val_mean_squared_error: 2.4961\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.4975 - mean_squared_error: 1.4975 - val_loss: 2.9093 - val_mean_squared_error: 2.9093\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.2920 - mean_squared_error: 1.2920 - val_loss: 2.8443 - val_mean_squared_error: 2.8443\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.3635 - mean_squared_error: 1.3635 - val_loss: 2.5012 - val_mean_squared_error: 2.5012\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.2478 - mean_squared_error: 1.2478 - val_loss: 2.8683 - val_mean_squared_error: 2.8683\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 334us/sample - loss: 1.3766 - mean_squared_error: 1.3766 - val_loss: 3.7227 - val_mean_squared_error: 3.7227\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.4704 - mean_squared_error: 1.4704 - val_loss: 2.6212 - val_mean_squared_error: 2.6212\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 337us/sample - loss: 1.3666 - mean_squared_error: 1.3666 - val_loss: 2.7865 - val_mean_squared_error: 2.7865\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.2630 - mean_squared_error: 1.2630 - val_loss: 2.8477 - val_mean_squared_error: 2.8477\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3704 - mean_squared_error: 1.3704 - val_loss: 2.8854 - val_mean_squared_error: 2.8854\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.3192 - mean_squared_error: 1.3192 - val_loss: 2.6678 - val_mean_squared_error: 2.6678\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.2514 - mean_squared_error: 1.2514 - val_loss: 2.6758 - val_mean_squared_error: 2.6758\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.1823 - mean_squared_error: 1.1823 - val_loss: 2.4725 - val_mean_squared_error: 2.4725\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3750 - mean_squared_error: 1.3750 - val_loss: 2.6369 - val_mean_squared_error: 2.6369\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3311 - mean_squared_error: 1.3311 - val_loss: 2.8039 - val_mean_squared_error: 2.8039\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.3235 - mean_squared_error: 1.3235 - val_loss: 2.8059 - val_mean_squared_error: 2.8059\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.2958 - mean_squared_error: 1.2958 - val_loss: 2.3819 - val_mean_squared_error: 2.3819\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.1764 - mean_squared_error: 1.1764 - val_loss: 2.4208 - val_mean_squared_error: 2.4208\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.2341 - mean_squared_error: 1.2341 - val_loss: 2.5228 - val_mean_squared_error: 2.5228\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.2022 - mean_squared_error: 1.2022 - val_loss: 2.3769 - val_mean_squared_error: 2.3769\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 1.2511 - mean_squared_error: 1.2511 - val_loss: 2.8382 - val_mean_squared_error: 2.8382\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.1425 - mean_squared_error: 1.1425 - val_loss: 2.7583 - val_mean_squared_error: 2.7583\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.2313 - mean_squared_error: 1.2313 - val_loss: 2.7232 - val_mean_squared_error: 2.7232\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.2363 - mean_squared_error: 1.2363 - val_loss: 2.6317 - val_mean_squared_error: 2.6317\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.1710 - mean_squared_error: 1.1710 - val_loss: 2.4829 - val_mean_squared_error: 2.4829\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.2638 - mean_squared_error: 1.2638 - val_loss: 2.7688 - val_mean_squared_error: 2.7688\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4417 - mean_squared_error: 1.4417 - val_loss: 2.5435 - val_mean_squared_error: 2.5435\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.4134 - mean_squared_error: 1.4134 - val_loss: 2.9569 - val_mean_squared_error: 2.9569\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.2296 - mean_squared_error: 1.2296 - val_loss: 2.4083 - val_mean_squared_error: 2.4083\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.1556 - mean_squared_error: 1.1556 - val_loss: 2.6134 - val_mean_squared_error: 2.6134\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.1098 - mean_squared_error: 1.1098 - val_loss: 2.8618 - val_mean_squared_error: 2.8618\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.2037 - mean_squared_error: 1.2037 - val_loss: 2.7587 - val_mean_squared_error: 2.7587\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.1781 - mean_squared_error: 1.1781 - val_loss: 2.4546 - val_mean_squared_error: 2.4546\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.2038 - mean_squared_error: 1.2038 - val_loss: 2.5379 - val_mean_squared_error: 2.5379\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.2559 - mean_squared_error: 1.2559 - val_loss: 2.9326 - val_mean_squared_error: 2.9326\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.1161 - mean_squared_error: 1.1161 - val_loss: 2.3828 - val_mean_squared_error: 2.3828\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.0972 - mean_squared_error: 1.0972 - val_loss: 2.7361 - val_mean_squared_error: 2.7361\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.0726 - mean_squared_error: 1.0726 - val_loss: 2.3746 - val_mean_squared_error: 2.3746\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.1085 - mean_squared_error: 1.1085 - val_loss: 3.0910 - val_mean_squared_error: 3.0910\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.1364 - mean_squared_error: 1.1364 - val_loss: 3.2407 - val_mean_squared_error: 3.2407\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.1556 - mean_squared_error: 1.1556 - val_loss: 2.5414 - val_mean_squared_error: 2.5414\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.0648 - mean_squared_error: 1.0648 - val_loss: 3.0395 - val_mean_squared_error: 3.0395\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.1127 - mean_squared_error: 1.1127 - val_loss: 2.4982 - val_mean_squared_error: 2.4982\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.1607 - mean_squared_error: 1.1607 - val_loss: 2.6727 - val_mean_squared_error: 2.6727\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.1442 - mean_squared_error: 1.1442 - val_loss: 2.5081 - val_mean_squared_error: 2.5081\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.2102 - mean_squared_error: 1.2102 - val_loss: 2.6670 - val_mean_squared_error: 2.6670\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.1277 - mean_squared_error: 1.1277 - val_loss: 2.5181 - val_mean_squared_error: 2.5181\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.1226 - mean_squared_error: 1.1226 - val_loss: 2.5793 - val_mean_squared_error: 2.5793\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.1583 - mean_squared_error: 1.1583 - val_loss: 2.7873 - val_mean_squared_error: 2.7873\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.2088 - mean_squared_error: 1.2088 - val_loss: 2.3078 - val_mean_squared_error: 2.3078\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.1859 - mean_squared_error: 1.1859 - val_loss: 2.5505 - val_mean_squared_error: 2.5505\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.1410 - mean_squared_error: 1.1410 - val_loss: 2.4683 - val_mean_squared_error: 2.4683\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.0798 - mean_squared_error: 1.0798 - val_loss: 2.5065 - val_mean_squared_error: 2.5065\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 2.4983 - val_mean_squared_error: 2.4983\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0418 - mean_squared_error: 1.0418 - val_loss: 2.6166 - val_mean_squared_error: 2.6166\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0597 - mean_squared_error: 1.0597 - val_loss: 2.7839 - val_mean_squared_error: 2.7839\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.0239 - mean_squared_error: 1.0239 - val_loss: 2.3438 - val_mean_squared_error: 2.3438\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.1178 - mean_squared_error: 1.1178 - val_loss: 2.5696 - val_mean_squared_error: 2.5696\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.1536 - mean_squared_error: 1.1536 - val_loss: 2.5550 - val_mean_squared_error: 2.5550\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0307 - mean_squared_error: 1.0307 - val_loss: 2.3767 - val_mean_squared_error: 2.3767\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.0822 - mean_squared_error: 1.0822 - val_loss: 2.4491 - val_mean_squared_error: 2.4491\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.0530 - mean_squared_error: 1.0530 - val_loss: 2.4692 - val_mean_squared_error: 2.4692\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.1326 - mean_squared_error: 1.1326 - val_loss: 2.4393 - val_mean_squared_error: 2.4393\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.1159 - mean_squared_error: 1.1159 - val_loss: 2.5482 - val_mean_squared_error: 2.5482\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.0698 - mean_squared_error: 1.0698 - val_loss: 2.2544 - val_mean_squared_error: 2.2544\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.1206 - mean_squared_error: 1.1206 - val_loss: 2.5271 - val_mean_squared_error: 2.5271\n",
            "==================================================\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1542.6482 - mean_squared_error: 1542.6478 - val_loss: 1595.8480 - val_mean_squared_error: 1595.8479\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 236.2562 - mean_squared_error: 236.2562 - val_loss: 501.9699 - val_mean_squared_error: 501.9699\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 28.7331 - mean_squared_error: 28.7331 - val_loss: 126.5701 - val_mean_squared_error: 126.5701\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 23.9120 - mean_squared_error: 23.9120 - val_loss: 56.7752 - val_mean_squared_error: 56.7752\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 19.2990 - mean_squared_error: 19.2990 - val_loss: 36.4718 - val_mean_squared_error: 36.4718\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 17.0369 - mean_squared_error: 17.0369 - val_loss: 21.4212 - val_mean_squared_error: 21.4212\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 17.2341 - mean_squared_error: 17.2341 - val_loss: 20.9821 - val_mean_squared_error: 20.9821\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 12.9664 - mean_squared_error: 12.9664 - val_loss: 10.8653 - val_mean_squared_error: 10.8653\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 12.7966 - mean_squared_error: 12.7966 - val_loss: 12.7495 - val_mean_squared_error: 12.7495\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 11.9209 - mean_squared_error: 11.9209 - val_loss: 12.5547 - val_mean_squared_error: 12.5547\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 12.8122 - mean_squared_error: 12.8122 - val_loss: 10.7831 - val_mean_squared_error: 10.7831\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 11.3683 - mean_squared_error: 11.3683 - val_loss: 10.2853 - val_mean_squared_error: 10.2853\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 10.7981 - mean_squared_error: 10.7981 - val_loss: 11.0279 - val_mean_squared_error: 11.0279\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 10.1402 - mean_squared_error: 10.1402 - val_loss: 10.2144 - val_mean_squared_error: 10.2144\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 10.6085 - mean_squared_error: 10.6085 - val_loss: 10.5334 - val_mean_squared_error: 10.5334\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 8.7892 - mean_squared_error: 8.7892 - val_loss: 10.7382 - val_mean_squared_error: 10.7382\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 8.4159 - mean_squared_error: 8.4159 - val_loss: 8.2618 - val_mean_squared_error: 8.2618\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 8.7013 - mean_squared_error: 8.7013 - val_loss: 7.6274 - val_mean_squared_error: 7.6274\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 8.1637 - mean_squared_error: 8.1637 - val_loss: 9.6942 - val_mean_squared_error: 9.6942\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 7.6025 - mean_squared_error: 7.6025 - val_loss: 7.0039 - val_mean_squared_error: 7.0039\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 7.0122 - mean_squared_error: 7.0122 - val_loss: 7.0823 - val_mean_squared_error: 7.0823\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 6.6777 - mean_squared_error: 6.6777 - val_loss: 9.0166 - val_mean_squared_error: 9.0166\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 6.0895 - mean_squared_error: 6.0895 - val_loss: 6.3072 - val_mean_squared_error: 6.3072\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 6.9554 - mean_squared_error: 6.9554 - val_loss: 8.5893 - val_mean_squared_error: 8.5893\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 6.1056 - mean_squared_error: 6.1056 - val_loss: 5.8660 - val_mean_squared_error: 5.8660\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 5.8550 - mean_squared_error: 5.8550 - val_loss: 8.9422 - val_mean_squared_error: 8.9422\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 5.1085 - mean_squared_error: 5.1085 - val_loss: 5.8311 - val_mean_squared_error: 5.8311\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 5.2098 - mean_squared_error: 5.2098 - val_loss: 5.9749 - val_mean_squared_error: 5.9749\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 5.7929 - mean_squared_error: 5.7929 - val_loss: 7.9519 - val_mean_squared_error: 7.9519\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 5.2112 - mean_squared_error: 5.2112 - val_loss: 5.9666 - val_mean_squared_error: 5.9666\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 4.6705 - mean_squared_error: 4.6705 - val_loss: 5.1367 - val_mean_squared_error: 5.1367\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 4.5308 - mean_squared_error: 4.5308 - val_loss: 4.2603 - val_mean_squared_error: 4.2603\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 4.3667 - mean_squared_error: 4.3667 - val_loss: 4.3887 - val_mean_squared_error: 4.3887\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 4.1120 - mean_squared_error: 4.1120 - val_loss: 4.7155 - val_mean_squared_error: 4.7155\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 4.5517 - mean_squared_error: 4.5517 - val_loss: 4.5787 - val_mean_squared_error: 4.5787\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 3.9382 - mean_squared_error: 3.9382 - val_loss: 4.8112 - val_mean_squared_error: 4.8112\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 4.0561 - mean_squared_error: 4.0561 - val_loss: 4.4159 - val_mean_squared_error: 4.4159\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 3.7561 - mean_squared_error: 3.7561 - val_loss: 4.8827 - val_mean_squared_error: 4.8827\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 3.7039 - mean_squared_error: 3.7039 - val_loss: 8.5294 - val_mean_squared_error: 8.5294\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 3.5960 - mean_squared_error: 3.5960 - val_loss: 4.5265 - val_mean_squared_error: 4.5265\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 3.5867 - mean_squared_error: 3.5867 - val_loss: 3.7064 - val_mean_squared_error: 3.7064\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 3.5989 - mean_squared_error: 3.5989 - val_loss: 3.7031 - val_mean_squared_error: 3.7031\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 3.4821 - mean_squared_error: 3.4821 - val_loss: 4.2741 - val_mean_squared_error: 4.2741\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 3.4676 - mean_squared_error: 3.4676 - val_loss: 4.3028 - val_mean_squared_error: 4.3028\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 3.3237 - mean_squared_error: 3.3237 - val_loss: 3.8111 - val_mean_squared_error: 3.8111\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 3.2385 - mean_squared_error: 3.2385 - val_loss: 3.6701 - val_mean_squared_error: 3.6701\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 3.6148 - mean_squared_error: 3.6148 - val_loss: 3.4275 - val_mean_squared_error: 3.4275\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 3.2436 - mean_squared_error: 3.2436 - val_loss: 3.5922 - val_mean_squared_error: 3.5922\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 3.0842 - mean_squared_error: 3.0842 - val_loss: 3.7136 - val_mean_squared_error: 3.7136\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 2.9751 - mean_squared_error: 2.9751 - val_loss: 3.7579 - val_mean_squared_error: 3.7579\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.7787 - mean_squared_error: 2.7787 - val_loss: 3.5467 - val_mean_squared_error: 3.5467\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.8986 - mean_squared_error: 2.8986 - val_loss: 3.4574 - val_mean_squared_error: 3.4574\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 3.1836 - mean_squared_error: 3.1836 - val_loss: 4.4540 - val_mean_squared_error: 4.4540\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 3.1433 - mean_squared_error: 3.1433 - val_loss: 4.1540 - val_mean_squared_error: 4.1540\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.8627 - mean_squared_error: 2.8627 - val_loss: 3.7064 - val_mean_squared_error: 3.7064\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.9369 - mean_squared_error: 2.9369 - val_loss: 5.1505 - val_mean_squared_error: 5.1505\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.7003 - mean_squared_error: 2.7003 - val_loss: 3.2724 - val_mean_squared_error: 3.2724\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 2.8117 - mean_squared_error: 2.8117 - val_loss: 3.2321 - val_mean_squared_error: 3.2321\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 2.5327 - mean_squared_error: 2.5327 - val_loss: 3.3423 - val_mean_squared_error: 3.3423\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.8317 - mean_squared_error: 2.8317 - val_loss: 4.0122 - val_mean_squared_error: 4.0122\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.7584 - mean_squared_error: 2.7584 - val_loss: 4.1921 - val_mean_squared_error: 4.1921\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.6050 - mean_squared_error: 2.6050 - val_loss: 4.5777 - val_mean_squared_error: 4.5777\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 2.5572 - mean_squared_error: 2.5572 - val_loss: 3.1960 - val_mean_squared_error: 3.1960\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.4946 - mean_squared_error: 2.4946 - val_loss: 3.1531 - val_mean_squared_error: 3.1531\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.3324 - mean_squared_error: 2.3324 - val_loss: 3.0898 - val_mean_squared_error: 3.0898\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.4651 - mean_squared_error: 2.4651 - val_loss: 3.5752 - val_mean_squared_error: 3.5752\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 3.0569 - mean_squared_error: 3.0569 - val_loss: 3.6045 - val_mean_squared_error: 3.6045\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 2.5633 - mean_squared_error: 2.5633 - val_loss: 3.4937 - val_mean_squared_error: 3.4937\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 2.4474 - mean_squared_error: 2.4474 - val_loss: 2.9549 - val_mean_squared_error: 2.9549\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.3934 - mean_squared_error: 2.3934 - val_loss: 3.3352 - val_mean_squared_error: 3.3352\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 2.2869 - mean_squared_error: 2.2869 - val_loss: 3.0342 - val_mean_squared_error: 3.0342\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 2.3549 - mean_squared_error: 2.3549 - val_loss: 3.3616 - val_mean_squared_error: 3.3616\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.2709 - mean_squared_error: 2.2709 - val_loss: 3.1181 - val_mean_squared_error: 3.1181\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 2.1954 - mean_squared_error: 2.1954 - val_loss: 3.1733 - val_mean_squared_error: 3.1733\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.2518 - mean_squared_error: 2.2518 - val_loss: 3.4403 - val_mean_squared_error: 3.4403\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.4013 - mean_squared_error: 2.4013 - val_loss: 3.1547 - val_mean_squared_error: 3.1547\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.2832 - mean_squared_error: 2.2832 - val_loss: 3.4134 - val_mean_squared_error: 3.4134\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.2198 - mean_squared_error: 2.2198 - val_loss: 2.7337 - val_mean_squared_error: 2.7337\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 2.3358 - mean_squared_error: 2.3358 - val_loss: 3.8921 - val_mean_squared_error: 3.8921\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.2349 - mean_squared_error: 2.2349 - val_loss: 3.1271 - val_mean_squared_error: 3.1271\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.4054 - mean_squared_error: 2.4054 - val_loss: 3.4849 - val_mean_squared_error: 3.4849\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.4378 - mean_squared_error: 2.4378 - val_loss: 2.8600 - val_mean_squared_error: 2.8600\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 2.3998 - mean_squared_error: 2.3998 - val_loss: 2.9210 - val_mean_squared_error: 2.9210\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.2751 - mean_squared_error: 2.2751 - val_loss: 4.2610 - val_mean_squared_error: 4.2610\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.9923 - mean_squared_error: 1.9923 - val_loss: 2.8520 - val_mean_squared_error: 2.8520\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.2022 - mean_squared_error: 2.2022 - val_loss: 3.1824 - val_mean_squared_error: 3.1824\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.1490 - mean_squared_error: 2.1490 - val_loss: 2.8225 - val_mean_squared_error: 2.8225\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 336us/sample - loss: 2.0674 - mean_squared_error: 2.0674 - val_loss: 3.0899 - val_mean_squared_error: 3.0899\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 2.0128 - mean_squared_error: 2.0128 - val_loss: 3.3872 - val_mean_squared_error: 3.3872\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.1897 - mean_squared_error: 2.1897 - val_loss: 2.8824 - val_mean_squared_error: 2.8824\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 2.0123 - mean_squared_error: 2.0123 - val_loss: 2.6829 - val_mean_squared_error: 2.6829\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.8738 - mean_squared_error: 1.8738 - val_loss: 2.8663 - val_mean_squared_error: 2.8663\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.0037 - mean_squared_error: 2.0037 - val_loss: 3.0091 - val_mean_squared_error: 3.0091\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.9785 - mean_squared_error: 1.9785 - val_loss: 2.8517 - val_mean_squared_error: 2.8517\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.9409 - mean_squared_error: 1.9409 - val_loss: 2.8816 - val_mean_squared_error: 2.8816\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.9561 - mean_squared_error: 1.9561 - val_loss: 2.8009 - val_mean_squared_error: 2.8009\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.9203 - mean_squared_error: 1.9203 - val_loss: 3.4112 - val_mean_squared_error: 3.4112\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.9829 - mean_squared_error: 1.9829 - val_loss: 2.8969 - val_mean_squared_error: 2.8969\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.7671 - mean_squared_error: 1.7671 - val_loss: 2.6110 - val_mean_squared_error: 2.6110\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.9052 - mean_squared_error: 1.9052 - val_loss: 2.5860 - val_mean_squared_error: 2.5860\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.8579 - mean_squared_error: 1.8579 - val_loss: 2.7161 - val_mean_squared_error: 2.7161\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 2.2779 - mean_squared_error: 2.2779 - val_loss: 3.6312 - val_mean_squared_error: 3.6312\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.8899 - mean_squared_error: 1.8899 - val_loss: 3.0711 - val_mean_squared_error: 3.0711\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.7707 - mean_squared_error: 1.7707 - val_loss: 2.7938 - val_mean_squared_error: 2.7938\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 2.0288 - mean_squared_error: 2.0288 - val_loss: 2.8966 - val_mean_squared_error: 2.8966\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.7112 - mean_squared_error: 1.7112 - val_loss: 2.7409 - val_mean_squared_error: 2.7409\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 2.0220 - mean_squared_error: 2.0220 - val_loss: 2.4151 - val_mean_squared_error: 2.4151\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.7363 - mean_squared_error: 1.7363 - val_loss: 2.5251 - val_mean_squared_error: 2.5251\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.7872 - mean_squared_error: 1.7872 - val_loss: 2.6397 - val_mean_squared_error: 2.6397\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.8206 - mean_squared_error: 1.8206 - val_loss: 2.9949 - val_mean_squared_error: 2.9949\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.7679 - mean_squared_error: 1.7679 - val_loss: 2.5709 - val_mean_squared_error: 2.5709\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.6141 - mean_squared_error: 1.6141 - val_loss: 2.6423 - val_mean_squared_error: 2.6423\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.5621 - mean_squared_error: 1.5621 - val_loss: 2.5731 - val_mean_squared_error: 2.5731\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.6771 - mean_squared_error: 1.6771 - val_loss: 2.5609 - val_mean_squared_error: 2.5609\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.5521 - mean_squared_error: 1.5521 - val_loss: 3.0669 - val_mean_squared_error: 3.0669\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.7863 - mean_squared_error: 1.7863 - val_loss: 2.7035 - val_mean_squared_error: 2.7035\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.6196 - mean_squared_error: 1.6196 - val_loss: 3.1030 - val_mean_squared_error: 3.1030\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.7252 - mean_squared_error: 1.7252 - val_loss: 2.8772 - val_mean_squared_error: 2.8772\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.7637 - mean_squared_error: 1.7637 - val_loss: 3.9922 - val_mean_squared_error: 3.9922\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.7699 - mean_squared_error: 1.7699 - val_loss: 2.8543 - val_mean_squared_error: 2.8543\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.5903 - mean_squared_error: 1.5903 - val_loss: 2.7017 - val_mean_squared_error: 2.7017\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.5002 - mean_squared_error: 1.5002 - val_loss: 2.8467 - val_mean_squared_error: 2.8467\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.6565 - mean_squared_error: 1.6565 - val_loss: 2.5334 - val_mean_squared_error: 2.5334\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.5648 - mean_squared_error: 1.5648 - val_loss: 2.8196 - val_mean_squared_error: 2.8196\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 338us/sample - loss: 1.6101 - mean_squared_error: 1.6101 - val_loss: 2.4455 - val_mean_squared_error: 2.4455\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.4955 - mean_squared_error: 1.4955 - val_loss: 2.6431 - val_mean_squared_error: 2.6431\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.5051 - mean_squared_error: 1.5051 - val_loss: 2.7922 - val_mean_squared_error: 2.7922\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.4664 - mean_squared_error: 1.4664 - val_loss: 2.4943 - val_mean_squared_error: 2.4943\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.4392 - mean_squared_error: 1.4392 - val_loss: 2.6077 - val_mean_squared_error: 2.6077\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.5827 - mean_squared_error: 1.5827 - val_loss: 2.8065 - val_mean_squared_error: 2.8065\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.5864 - mean_squared_error: 1.5864 - val_loss: 2.6773 - val_mean_squared_error: 2.6773\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4677 - mean_squared_error: 1.4677 - val_loss: 2.8950 - val_mean_squared_error: 2.8950\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.4936 - mean_squared_error: 1.4936 - val_loss: 2.8028 - val_mean_squared_error: 2.8028\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.4274 - mean_squared_error: 1.4274 - val_loss: 2.5161 - val_mean_squared_error: 2.5161\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.5562 - mean_squared_error: 1.5562 - val_loss: 2.4193 - val_mean_squared_error: 2.4193\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.6154 - mean_squared_error: 1.6154 - val_loss: 3.3194 - val_mean_squared_error: 3.3194\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.5342 - mean_squared_error: 1.5342 - val_loss: 2.5376 - val_mean_squared_error: 2.5376\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.5774 - mean_squared_error: 1.5774 - val_loss: 2.9324 - val_mean_squared_error: 2.9324\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.5847 - mean_squared_error: 1.5847 - val_loss: 3.2837 - val_mean_squared_error: 3.2837\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.6731 - mean_squared_error: 1.6731 - val_loss: 3.3260 - val_mean_squared_error: 3.3260\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.5581 - mean_squared_error: 1.5581 - val_loss: 2.6942 - val_mean_squared_error: 2.6942\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.4677 - mean_squared_error: 1.4677 - val_loss: 2.5559 - val_mean_squared_error: 2.5559\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.5138 - mean_squared_error: 1.5138 - val_loss: 2.6891 - val_mean_squared_error: 2.6891\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.4484 - mean_squared_error: 1.4484 - val_loss: 2.6222 - val_mean_squared_error: 2.6222\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.4582 - mean_squared_error: 1.4582 - val_loss: 2.9324 - val_mean_squared_error: 2.9324\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.4416 - mean_squared_error: 1.4416 - val_loss: 2.5222 - val_mean_squared_error: 2.5222\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.4356 - mean_squared_error: 1.4356 - val_loss: 2.5261 - val_mean_squared_error: 2.5261\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4348 - mean_squared_error: 1.4348 - val_loss: 2.8564 - val_mean_squared_error: 2.8564\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.5065 - mean_squared_error: 1.5065 - val_loss: 2.6221 - val_mean_squared_error: 2.6221\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3385 - mean_squared_error: 1.3385 - val_loss: 2.9355 - val_mean_squared_error: 2.9355\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.3834 - mean_squared_error: 1.3834 - val_loss: 2.5723 - val_mean_squared_error: 2.5723\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.4293 - mean_squared_error: 1.4293 - val_loss: 2.6209 - val_mean_squared_error: 2.6209\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.5332 - mean_squared_error: 1.5332 - val_loss: 2.9660 - val_mean_squared_error: 2.9660\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.5402 - mean_squared_error: 1.5402 - val_loss: 3.3313 - val_mean_squared_error: 3.3313\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.4326 - mean_squared_error: 1.4326 - val_loss: 2.8200 - val_mean_squared_error: 2.8200\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.4914 - mean_squared_error: 1.4914 - val_loss: 3.4441 - val_mean_squared_error: 3.4441\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.5477 - mean_squared_error: 1.5477 - val_loss: 2.7196 - val_mean_squared_error: 2.7196\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.4350 - mean_squared_error: 1.4350 - val_loss: 2.4621 - val_mean_squared_error: 2.4621\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.2573 - mean_squared_error: 1.2573 - val_loss: 2.6056 - val_mean_squared_error: 2.6056\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.4298 - mean_squared_error: 1.4298 - val_loss: 2.6677 - val_mean_squared_error: 2.6677\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.4482 - mean_squared_error: 1.4482 - val_loss: 2.8753 - val_mean_squared_error: 2.8753\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.4071 - mean_squared_error: 1.4071 - val_loss: 2.5214 - val_mean_squared_error: 2.5214\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3710 - mean_squared_error: 1.3710 - val_loss: 2.6164 - val_mean_squared_error: 2.6164\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.3524 - mean_squared_error: 1.3524 - val_loss: 2.5349 - val_mean_squared_error: 2.5349\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3249 - mean_squared_error: 1.3249 - val_loss: 2.7735 - val_mean_squared_error: 2.7735\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3583 - mean_squared_error: 1.3583 - val_loss: 2.4534 - val_mean_squared_error: 2.4534\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.3470 - mean_squared_error: 1.3470 - val_loss: 2.4335 - val_mean_squared_error: 2.4335\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.1967 - mean_squared_error: 1.1967 - val_loss: 2.6099 - val_mean_squared_error: 2.6099\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.3637 - mean_squared_error: 1.3637 - val_loss: 2.5884 - val_mean_squared_error: 2.5884\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.3245 - mean_squared_error: 1.3245 - val_loss: 2.6564 - val_mean_squared_error: 2.6564\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.4723 - mean_squared_error: 1.4723 - val_loss: 3.0289 - val_mean_squared_error: 3.0289\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.3126 - mean_squared_error: 1.3126 - val_loss: 2.4132 - val_mean_squared_error: 2.4132\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.4132 - mean_squared_error: 1.4132 - val_loss: 2.8929 - val_mean_squared_error: 2.8929\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.2793 - mean_squared_error: 1.2793 - val_loss: 2.4196 - val_mean_squared_error: 2.4196\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.2790 - mean_squared_error: 1.2790 - val_loss: 2.4231 - val_mean_squared_error: 2.4231\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.2619 - mean_squared_error: 1.2619 - val_loss: 2.4584 - val_mean_squared_error: 2.4584\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.3153 - mean_squared_error: 1.3153 - val_loss: 2.8401 - val_mean_squared_error: 2.8401\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3874 - mean_squared_error: 1.3874 - val_loss: 2.8626 - val_mean_squared_error: 2.8626\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.3254 - mean_squared_error: 1.3254 - val_loss: 2.5281 - val_mean_squared_error: 2.5281\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.2882 - mean_squared_error: 1.2882 - val_loss: 2.5168 - val_mean_squared_error: 2.5168\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.2418 - mean_squared_error: 1.2418 - val_loss: 2.4297 - val_mean_squared_error: 2.4297\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.2042 - mean_squared_error: 1.2042 - val_loss: 2.3443 - val_mean_squared_error: 2.3443\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.1960 - mean_squared_error: 1.1960 - val_loss: 2.7529 - val_mean_squared_error: 2.7529\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.2984 - mean_squared_error: 1.2984 - val_loss: 2.7948 - val_mean_squared_error: 2.7948\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.2985 - mean_squared_error: 1.2985 - val_loss: 2.4300 - val_mean_squared_error: 2.4300\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3228 - mean_squared_error: 1.3228 - val_loss: 2.4404 - val_mean_squared_error: 2.4404\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.2366 - mean_squared_error: 1.2366 - val_loss: 3.1691 - val_mean_squared_error: 3.1691\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 339us/sample - loss: 1.2210 - mean_squared_error: 1.2210 - val_loss: 2.6451 - val_mean_squared_error: 2.6451\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.2469 - mean_squared_error: 1.2469 - val_loss: 2.7089 - val_mean_squared_error: 2.7089\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.2105 - mean_squared_error: 1.2105 - val_loss: 2.7190 - val_mean_squared_error: 2.7190\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.2076 - mean_squared_error: 1.2076 - val_loss: 2.5527 - val_mean_squared_error: 2.5527\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.1963 - mean_squared_error: 1.1963 - val_loss: 2.4928 - val_mean_squared_error: 2.4928\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.1872 - mean_squared_error: 1.1872 - val_loss: 2.2246 - val_mean_squared_error: 2.2246\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 1.2806 - mean_squared_error: 1.2806 - val_loss: 2.4246 - val_mean_squared_error: 2.4246\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.1299 - mean_squared_error: 1.1299 - val_loss: 2.6571 - val_mean_squared_error: 2.6571\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 342us/sample - loss: 1.1948 - mean_squared_error: 1.1948 - val_loss: 2.6697 - val_mean_squared_error: 2.6697\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.1968 - mean_squared_error: 1.1968 - val_loss: 2.5676 - val_mean_squared_error: 2.5676\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.0704 - mean_squared_error: 1.0704 - val_loss: 2.4225 - val_mean_squared_error: 2.4225\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.0745 - mean_squared_error: 1.0745 - val_loss: 2.4638 - val_mean_squared_error: 2.4638\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.1380 - mean_squared_error: 1.1380 - val_loss: 2.6232 - val_mean_squared_error: 2.6232\n",
            "==================================================\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 860us/sample - loss: 1555.1615 - mean_squared_error: 1555.1613 - val_loss: 1614.6392 - val_mean_squared_error: 1614.6392\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 232.7503 - mean_squared_error: 232.7503 - val_loss: 384.8420 - val_mean_squared_error: 384.8420\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 38.0865 - mean_squared_error: 38.0865 - val_loss: 138.1138 - val_mean_squared_error: 138.1138\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 19.6731 - mean_squared_error: 19.6731 - val_loss: 77.7957 - val_mean_squared_error: 77.7957\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 16.3299 - mean_squared_error: 16.3299 - val_loss: 23.3999 - val_mean_squared_error: 23.3999\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 19.2439 - mean_squared_error: 19.2439 - val_loss: 26.4006 - val_mean_squared_error: 26.4006\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 13.4431 - mean_squared_error: 13.4431 - val_loss: 13.7353 - val_mean_squared_error: 13.7353\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 13.5774 - mean_squared_error: 13.5774 - val_loss: 12.3626 - val_mean_squared_error: 12.3626\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 12.1562 - mean_squared_error: 12.1562 - val_loss: 10.3349 - val_mean_squared_error: 10.3349\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 12.4768 - mean_squared_error: 12.4768 - val_loss: 11.3044 - val_mean_squared_error: 11.3044\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 11.6733 - mean_squared_error: 11.6733 - val_loss: 11.3043 - val_mean_squared_error: 11.3043\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 11.3115 - mean_squared_error: 11.3115 - val_loss: 12.4261 - val_mean_squared_error: 12.4261\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 10.0064 - mean_squared_error: 10.0064 - val_loss: 9.6214 - val_mean_squared_error: 9.6214\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 10.7705 - mean_squared_error: 10.7705 - val_loss: 11.3102 - val_mean_squared_error: 11.3102\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 9.4085 - mean_squared_error: 9.4085 - val_loss: 8.7827 - val_mean_squared_error: 8.7827\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 343us/sample - loss: 9.3029 - mean_squared_error: 9.3029 - val_loss: 9.3318 - val_mean_squared_error: 9.3318\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 9.1082 - mean_squared_error: 9.1082 - val_loss: 10.2413 - val_mean_squared_error: 10.2413\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 8.2137 - mean_squared_error: 8.2137 - val_loss: 8.8956 - val_mean_squared_error: 8.8956\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 8.4855 - mean_squared_error: 8.4855 - val_loss: 7.4718 - val_mean_squared_error: 7.4718\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 7.9313 - mean_squared_error: 7.9313 - val_loss: 8.3051 - val_mean_squared_error: 8.3051\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 7.0562 - mean_squared_error: 7.0562 - val_loss: 7.9368 - val_mean_squared_error: 7.9368\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 6.7548 - mean_squared_error: 6.7548 - val_loss: 7.7180 - val_mean_squared_error: 7.7180\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 6.5925 - mean_squared_error: 6.5925 - val_loss: 6.1244 - val_mean_squared_error: 6.1244\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 6.2164 - mean_squared_error: 6.2164 - val_loss: 6.8401 - val_mean_squared_error: 6.8401\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 5.8638 - mean_squared_error: 5.8638 - val_loss: 5.8653 - val_mean_squared_error: 5.8653\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 5.4193 - mean_squared_error: 5.4193 - val_loss: 5.1049 - val_mean_squared_error: 5.1049\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 5.3379 - mean_squared_error: 5.3379 - val_loss: 5.5164 - val_mean_squared_error: 5.5164\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 5.5173 - mean_squared_error: 5.5173 - val_loss: 6.6790 - val_mean_squared_error: 6.6790\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 5.2289 - mean_squared_error: 5.2289 - val_loss: 8.7638 - val_mean_squared_error: 8.7638\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 4.8368 - mean_squared_error: 4.8368 - val_loss: 4.5763 - val_mean_squared_error: 4.5763\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 4.4792 - mean_squared_error: 4.4792 - val_loss: 4.4763 - val_mean_squared_error: 4.4763\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 4.9099 - mean_squared_error: 4.9099 - val_loss: 4.5058 - val_mean_squared_error: 4.5058\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 4.1847 - mean_squared_error: 4.1847 - val_loss: 4.5126 - val_mean_squared_error: 4.5126\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 4.7974 - mean_squared_error: 4.7974 - val_loss: 6.2929 - val_mean_squared_error: 6.2929\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 4.7159 - mean_squared_error: 4.7159 - val_loss: 4.9556 - val_mean_squared_error: 4.9556\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 4.1803 - mean_squared_error: 4.1803 - val_loss: 4.6357 - val_mean_squared_error: 4.6357\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 3.7098 - mean_squared_error: 3.7098 - val_loss: 4.0260 - val_mean_squared_error: 4.0260\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 3.7178 - mean_squared_error: 3.7178 - val_loss: 4.5959 - val_mean_squared_error: 4.5959\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 3.8129 - mean_squared_error: 3.8129 - val_loss: 4.3705 - val_mean_squared_error: 4.3705\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 4.0502 - mean_squared_error: 4.0502 - val_loss: 5.5506 - val_mean_squared_error: 5.5506\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 3.6621 - mean_squared_error: 3.6621 - val_loss: 3.6704 - val_mean_squared_error: 3.6704\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 3.5267 - mean_squared_error: 3.5267 - val_loss: 4.2984 - val_mean_squared_error: 4.2984\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 3.3358 - mean_squared_error: 3.3358 - val_loss: 3.5702 - val_mean_squared_error: 3.5702\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 3.8814 - mean_squared_error: 3.8814 - val_loss: 3.3111 - val_mean_squared_error: 3.3111\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 3.5931 - mean_squared_error: 3.5931 - val_loss: 4.3222 - val_mean_squared_error: 4.3222\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 3.2695 - mean_squared_error: 3.2695 - val_loss: 4.9654 - val_mean_squared_error: 4.9654\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 3.2189 - mean_squared_error: 3.2189 - val_loss: 3.9661 - val_mean_squared_error: 3.9661\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 3.0978 - mean_squared_error: 3.0978 - val_loss: 3.7265 - val_mean_squared_error: 3.7265\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 3.0518 - mean_squared_error: 3.0518 - val_loss: 3.2623 - val_mean_squared_error: 3.2623\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.8630 - mean_squared_error: 2.8630 - val_loss: 4.0057 - val_mean_squared_error: 4.0057\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 3.0924 - mean_squared_error: 3.0924 - val_loss: 3.2962 - val_mean_squared_error: 3.2962\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.9974 - mean_squared_error: 2.9974 - val_loss: 3.9773 - val_mean_squared_error: 3.9773\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 3.0943 - mean_squared_error: 3.0943 - val_loss: 3.9900 - val_mean_squared_error: 3.9900\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 3.1164 - mean_squared_error: 3.1164 - val_loss: 3.3836 - val_mean_squared_error: 3.3836\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 3.1654 - mean_squared_error: 3.1654 - val_loss: 3.4465 - val_mean_squared_error: 3.4465\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 3.1882 - mean_squared_error: 3.1882 - val_loss: 3.2689 - val_mean_squared_error: 3.2689\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.8097 - mean_squared_error: 2.8097 - val_loss: 3.0727 - val_mean_squared_error: 3.0727\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 3.0704 - mean_squared_error: 3.0704 - val_loss: 3.5507 - val_mean_squared_error: 3.5507\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 2.6766 - mean_squared_error: 2.6766 - val_loss: 3.3365 - val_mean_squared_error: 3.3365\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.6841 - mean_squared_error: 2.6841 - val_loss: 3.3368 - val_mean_squared_error: 3.3368\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.8678 - mean_squared_error: 2.8678 - val_loss: 3.8722 - val_mean_squared_error: 3.8722\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.6998 - mean_squared_error: 2.6998 - val_loss: 3.0093 - val_mean_squared_error: 3.0093\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 2.4737 - mean_squared_error: 2.4737 - val_loss: 2.8838 - val_mean_squared_error: 2.8838\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 2.5252 - mean_squared_error: 2.5252 - val_loss: 2.8406 - val_mean_squared_error: 2.8406\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.6198 - mean_squared_error: 2.6198 - val_loss: 3.1922 - val_mean_squared_error: 3.1922\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.7698 - mean_squared_error: 2.7698 - val_loss: 3.3188 - val_mean_squared_error: 3.3188\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 2.5000 - mean_squared_error: 2.5000 - val_loss: 3.2382 - val_mean_squared_error: 3.2382\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 2.5684 - mean_squared_error: 2.5684 - val_loss: 2.8096 - val_mean_squared_error: 2.8096\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.4858 - mean_squared_error: 2.4858 - val_loss: 2.8358 - val_mean_squared_error: 2.8358\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.5076 - mean_squared_error: 2.5076 - val_loss: 3.0881 - val_mean_squared_error: 3.0881\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.5047 - mean_squared_error: 2.5047 - val_loss: 2.8844 - val_mean_squared_error: 2.8844\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.4886 - mean_squared_error: 2.4886 - val_loss: 2.7873 - val_mean_squared_error: 2.7873\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.5693 - mean_squared_error: 2.5693 - val_loss: 3.0706 - val_mean_squared_error: 3.0706\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.6606 - mean_squared_error: 2.6606 - val_loss: 3.5300 - val_mean_squared_error: 3.5300\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 2.3147 - mean_squared_error: 2.3147 - val_loss: 2.9865 - val_mean_squared_error: 2.9865\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.3020 - mean_squared_error: 2.3020 - val_loss: 2.9822 - val_mean_squared_error: 2.9822\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.1473 - mean_squared_error: 2.1473 - val_loss: 2.6397 - val_mean_squared_error: 2.6397\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.2817 - mean_squared_error: 2.2817 - val_loss: 3.0052 - val_mean_squared_error: 3.0052\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.1956 - mean_squared_error: 2.1956 - val_loss: 2.9701 - val_mean_squared_error: 2.9701\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 2.1546 - mean_squared_error: 2.1546 - val_loss: 3.0716 - val_mean_squared_error: 3.0716\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.2535 - mean_squared_error: 2.2535 - val_loss: 3.0918 - val_mean_squared_error: 3.0918\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.1399 - mean_squared_error: 2.1399 - val_loss: 3.2695 - val_mean_squared_error: 3.2695\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.0815 - mean_squared_error: 2.0815 - val_loss: 2.9978 - val_mean_squared_error: 2.9978\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.1982 - mean_squared_error: 2.1982 - val_loss: 2.8056 - val_mean_squared_error: 2.8056\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.0524 - mean_squared_error: 2.0524 - val_loss: 2.8245 - val_mean_squared_error: 2.8245\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.3002 - mean_squared_error: 2.3002 - val_loss: 2.5705 - val_mean_squared_error: 2.5705\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.0832 - mean_squared_error: 2.0832 - val_loss: 2.6438 - val_mean_squared_error: 2.6438\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 2.1236 - mean_squared_error: 2.1236 - val_loss: 3.2392 - val_mean_squared_error: 3.2392\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.0109 - mean_squared_error: 2.0109 - val_loss: 2.5588 - val_mean_squared_error: 2.5588\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.0002 - mean_squared_error: 2.0002 - val_loss: 2.8633 - val_mean_squared_error: 2.8633\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.1372 - mean_squared_error: 2.1372 - val_loss: 3.0272 - val_mean_squared_error: 3.0272\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 2.0061 - mean_squared_error: 2.0061 - val_loss: 2.5486 - val_mean_squared_error: 2.5486\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.8893 - mean_squared_error: 1.8893 - val_loss: 2.7336 - val_mean_squared_error: 2.7336\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 2.0647 - mean_squared_error: 2.0647 - val_loss: 3.0724 - val_mean_squared_error: 3.0724\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 2.2851 - mean_squared_error: 2.2851 - val_loss: 3.6278 - val_mean_squared_error: 3.6278\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.0462 - mean_squared_error: 2.0462 - val_loss: 2.6992 - val_mean_squared_error: 2.6992\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 2.0581 - mean_squared_error: 2.0581 - val_loss: 2.4761 - val_mean_squared_error: 2.4761\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.9929 - mean_squared_error: 1.9929 - val_loss: 2.8908 - val_mean_squared_error: 2.8908\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 2.0616 - mean_squared_error: 2.0616 - val_loss: 3.6275 - val_mean_squared_error: 3.6275\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.0475 - mean_squared_error: 2.0475 - val_loss: 3.5683 - val_mean_squared_error: 3.5683\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.0730 - mean_squared_error: 2.0730 - val_loss: 2.9986 - val_mean_squared_error: 2.9986\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.9209 - mean_squared_error: 1.9209 - val_loss: 2.4947 - val_mean_squared_error: 2.4947\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.9612 - mean_squared_error: 1.9612 - val_loss: 2.4391 - val_mean_squared_error: 2.4391\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.8609 - mean_squared_error: 1.8609 - val_loss: 2.8457 - val_mean_squared_error: 2.8457\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.9416 - mean_squared_error: 1.9416 - val_loss: 2.7366 - val_mean_squared_error: 2.7366\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.8992 - mean_squared_error: 1.8992 - val_loss: 2.9952 - val_mean_squared_error: 2.9952\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 340us/sample - loss: 1.9747 - mean_squared_error: 1.9747 - val_loss: 2.9776 - val_mean_squared_error: 2.9776\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.9786 - mean_squared_error: 1.9786 - val_loss: 3.2061 - val_mean_squared_error: 3.2061\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.8566 - mean_squared_error: 1.8566 - val_loss: 2.9172 - val_mean_squared_error: 2.9172\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.6894 - mean_squared_error: 1.6894 - val_loss: 2.7207 - val_mean_squared_error: 2.7207\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.8249 - mean_squared_error: 1.8249 - val_loss: 2.6695 - val_mean_squared_error: 2.6695\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 341us/sample - loss: 1.8207 - mean_squared_error: 1.8207 - val_loss: 2.6100 - val_mean_squared_error: 2.6100\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.8116 - mean_squared_error: 1.8116 - val_loss: 2.5400 - val_mean_squared_error: 2.5400\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.7772 - mean_squared_error: 1.7772 - val_loss: 2.6740 - val_mean_squared_error: 2.6740\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.7616 - mean_squared_error: 1.7616 - val_loss: 2.4197 - val_mean_squared_error: 2.4197\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.7229 - mean_squared_error: 1.7229 - val_loss: 2.6452 - val_mean_squared_error: 2.6452\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.6778 - mean_squared_error: 1.6778 - val_loss: 2.8509 - val_mean_squared_error: 2.8509\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.6609 - mean_squared_error: 1.6609 - val_loss: 2.4339 - val_mean_squared_error: 2.4339\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.7646 - mean_squared_error: 1.7646 - val_loss: 2.7914 - val_mean_squared_error: 2.7914\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.7160 - mean_squared_error: 1.7160 - val_loss: 2.3871 - val_mean_squared_error: 2.3871\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.7223 - mean_squared_error: 1.7223 - val_loss: 2.5659 - val_mean_squared_error: 2.5659\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.7179 - mean_squared_error: 1.7179 - val_loss: 2.4932 - val_mean_squared_error: 2.4932\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.6088 - mean_squared_error: 1.6088 - val_loss: 2.2575 - val_mean_squared_error: 2.2575\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.7150 - mean_squared_error: 1.7150 - val_loss: 2.4668 - val_mean_squared_error: 2.4668\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.8164 - mean_squared_error: 1.8164 - val_loss: 2.7185 - val_mean_squared_error: 2.7185\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.6799 - mean_squared_error: 1.6799 - val_loss: 2.5581 - val_mean_squared_error: 2.5581\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.8075 - mean_squared_error: 1.8075 - val_loss: 3.3600 - val_mean_squared_error: 3.3600\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.6065 - mean_squared_error: 1.6065 - val_loss: 2.6061 - val_mean_squared_error: 2.6061\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.6692 - mean_squared_error: 1.6692 - val_loss: 2.9635 - val_mean_squared_error: 2.9635\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.6234 - mean_squared_error: 1.6234 - val_loss: 2.6387 - val_mean_squared_error: 2.6387\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.7801 - mean_squared_error: 1.7801 - val_loss: 2.6070 - val_mean_squared_error: 2.6070\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.5179 - mean_squared_error: 1.5179 - val_loss: 2.7603 - val_mean_squared_error: 2.7603\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.8949 - mean_squared_error: 1.8949 - val_loss: 2.7629 - val_mean_squared_error: 2.7629\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.6578 - mean_squared_error: 1.6578 - val_loss: 2.6059 - val_mean_squared_error: 2.6059\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.6863 - mean_squared_error: 1.6863 - val_loss: 2.5510 - val_mean_squared_error: 2.5510\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.6579 - mean_squared_error: 1.6579 - val_loss: 2.4628 - val_mean_squared_error: 2.4628\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.6160 - mean_squared_error: 1.6160 - val_loss: 2.4128 - val_mean_squared_error: 2.4128\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.7996 - mean_squared_error: 1.7996 - val_loss: 2.5953 - val_mean_squared_error: 2.5953\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.6917 - mean_squared_error: 1.6917 - val_loss: 2.5613 - val_mean_squared_error: 2.5613\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.5638 - mean_squared_error: 1.5638 - val_loss: 2.6576 - val_mean_squared_error: 2.6576\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.4618 - mean_squared_error: 1.4618 - val_loss: 2.5431 - val_mean_squared_error: 2.5431\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.6200 - mean_squared_error: 1.6200 - val_loss: 2.4235 - val_mean_squared_error: 2.4235\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.6584 - mean_squared_error: 1.6584 - val_loss: 2.2994 - val_mean_squared_error: 2.2994\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.5951 - mean_squared_error: 1.5951 - val_loss: 3.1898 - val_mean_squared_error: 3.1898\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.6432 - mean_squared_error: 1.6432 - val_loss: 2.9670 - val_mean_squared_error: 2.9670\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.5887 - mean_squared_error: 1.5887 - val_loss: 2.4466 - val_mean_squared_error: 2.4466\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.4501 - mean_squared_error: 1.4501 - val_loss: 2.5661 - val_mean_squared_error: 2.5661\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.4705 - mean_squared_error: 1.4705 - val_loss: 2.6547 - val_mean_squared_error: 2.6547\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.4628 - mean_squared_error: 1.4628 - val_loss: 2.4906 - val_mean_squared_error: 2.4906\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.4770 - mean_squared_error: 1.4770 - val_loss: 2.4641 - val_mean_squared_error: 2.4641\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.3521 - mean_squared_error: 1.3521 - val_loss: 2.6487 - val_mean_squared_error: 2.6487\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.5236 - mean_squared_error: 1.5236 - val_loss: 2.5465 - val_mean_squared_error: 2.5465\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4563 - mean_squared_error: 1.4563 - val_loss: 2.2371 - val_mean_squared_error: 2.2371\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4823 - mean_squared_error: 1.4823 - val_loss: 2.2540 - val_mean_squared_error: 2.2540\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.4110 - mean_squared_error: 1.4110 - val_loss: 2.3638 - val_mean_squared_error: 2.3638\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 345us/sample - loss: 1.4077 - mean_squared_error: 1.4077 - val_loss: 2.5580 - val_mean_squared_error: 2.5580\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.4708 - mean_squared_error: 1.4708 - val_loss: 2.8186 - val_mean_squared_error: 2.8186\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.4262 - mean_squared_error: 1.4262 - val_loss: 2.1208 - val_mean_squared_error: 2.1208\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.4133 - mean_squared_error: 1.4133 - val_loss: 2.1902 - val_mean_squared_error: 2.1902\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.5409 - mean_squared_error: 1.5409 - val_loss: 2.4034 - val_mean_squared_error: 2.4034\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.4398 - mean_squared_error: 1.4398 - val_loss: 2.5052 - val_mean_squared_error: 2.5052\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.4429 - mean_squared_error: 1.4429 - val_loss: 2.4107 - val_mean_squared_error: 2.4107\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.4836 - mean_squared_error: 1.4836 - val_loss: 2.5164 - val_mean_squared_error: 2.5164\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.4212 - mean_squared_error: 1.4212 - val_loss: 2.1584 - val_mean_squared_error: 2.1584\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.3700 - mean_squared_error: 1.3700 - val_loss: 2.4373 - val_mean_squared_error: 2.4373\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3839 - mean_squared_error: 1.3839 - val_loss: 2.3354 - val_mean_squared_error: 2.3354\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 346us/sample - loss: 1.3228 - mean_squared_error: 1.3228 - val_loss: 2.4240 - val_mean_squared_error: 2.4240\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.3596 - mean_squared_error: 1.3596 - val_loss: 2.3576 - val_mean_squared_error: 2.3576\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3455 - mean_squared_error: 1.3455 - val_loss: 2.2457 - val_mean_squared_error: 2.2457\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.3061 - mean_squared_error: 1.3061 - val_loss: 2.5040 - val_mean_squared_error: 2.5040\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.3285 - mean_squared_error: 1.3285 - val_loss: 2.4703 - val_mean_squared_error: 2.4703\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.3307 - mean_squared_error: 1.3307 - val_loss: 2.2785 - val_mean_squared_error: 2.2785\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.3068 - mean_squared_error: 1.3068 - val_loss: 2.2008 - val_mean_squared_error: 2.2008\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.4499 - mean_squared_error: 1.4499 - val_loss: 2.4906 - val_mean_squared_error: 2.4906\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.3224 - mean_squared_error: 1.3224 - val_loss: 2.3323 - val_mean_squared_error: 2.3323\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.3432 - mean_squared_error: 1.3432 - val_loss: 2.2810 - val_mean_squared_error: 2.2810\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.3509 - mean_squared_error: 1.3509 - val_loss: 2.5700 - val_mean_squared_error: 2.5700\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.3682 - mean_squared_error: 1.3682 - val_loss: 2.4810 - val_mean_squared_error: 2.4810\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.3692 - mean_squared_error: 1.3692 - val_loss: 2.2864 - val_mean_squared_error: 2.2864\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.2897 - mean_squared_error: 1.2897 - val_loss: 2.2055 - val_mean_squared_error: 2.2055\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.3122 - mean_squared_error: 1.3122 - val_loss: 2.2126 - val_mean_squared_error: 2.2126\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.3404 - mean_squared_error: 1.3404 - val_loss: 2.5120 - val_mean_squared_error: 2.5120\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.3254 - mean_squared_error: 1.3254 - val_loss: 2.4422 - val_mean_squared_error: 2.4422\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.3046 - mean_squared_error: 1.3046 - val_loss: 2.4611 - val_mean_squared_error: 2.4611\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.3621 - mean_squared_error: 1.3621 - val_loss: 2.5299 - val_mean_squared_error: 2.5299\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.4233 - mean_squared_error: 1.4233 - val_loss: 2.3485 - val_mean_squared_error: 2.3485\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.4170 - mean_squared_error: 1.4170 - val_loss: 2.7058 - val_mean_squared_error: 2.7058\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.3077 - mean_squared_error: 1.3077 - val_loss: 2.5342 - val_mean_squared_error: 2.5342\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.3472 - mean_squared_error: 1.3472 - val_loss: 2.6681 - val_mean_squared_error: 2.6681\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.2475 - mean_squared_error: 1.2475 - val_loss: 2.5054 - val_mean_squared_error: 2.5054\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.3304 - mean_squared_error: 1.3304 - val_loss: 2.4046 - val_mean_squared_error: 2.4046\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.4560 - mean_squared_error: 1.4560 - val_loss: 2.1383 - val_mean_squared_error: 2.1383\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.2013 - mean_squared_error: 1.2013 - val_loss: 2.2316 - val_mean_squared_error: 2.2316\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.2825 - mean_squared_error: 1.2825 - val_loss: 2.2018 - val_mean_squared_error: 2.2018\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.3109 - mean_squared_error: 1.3109 - val_loss: 2.9414 - val_mean_squared_error: 2.9414\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.2489 - mean_squared_error: 1.2489 - val_loss: 2.1163 - val_mean_squared_error: 2.1163\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.2009 - mean_squared_error: 1.2009 - val_loss: 2.5886 - val_mean_squared_error: 2.5886\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.2050 - mean_squared_error: 1.2050 - val_loss: 2.3470 - val_mean_squared_error: 2.3470\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.3356 - mean_squared_error: 1.3356 - val_loss: 2.4519 - val_mean_squared_error: 2.4519\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.4238 - mean_squared_error: 1.4238 - val_loss: 2.8586 - val_mean_squared_error: 2.8586\n",
            "==================================================\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_15 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 967us/sample - loss: 1530.3863 - mean_squared_error: 1530.3864 - val_loss: 1192.0035 - val_mean_squared_error: 1192.0035\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 233.8412 - mean_squared_error: 233.8412 - val_loss: 271.1752 - val_mean_squared_error: 271.1752\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 33.4862 - mean_squared_error: 33.4862 - val_loss: 91.7978 - val_mean_squared_error: 91.7978\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 23.8187 - mean_squared_error: 23.8186 - val_loss: 78.7706 - val_mean_squared_error: 78.7706\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 18.3343 - mean_squared_error: 18.3343 - val_loss: 37.6090 - val_mean_squared_error: 37.6090\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 16.5556 - mean_squared_error: 16.5556 - val_loss: 30.7180 - val_mean_squared_error: 30.7180\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 16.2933 - mean_squared_error: 16.2933 - val_loss: 16.1901 - val_mean_squared_error: 16.1901\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 14.9922 - mean_squared_error: 14.9922 - val_loss: 17.4007 - val_mean_squared_error: 17.4007\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 13.9644 - mean_squared_error: 13.9644 - val_loss: 14.8769 - val_mean_squared_error: 14.8769\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 13.4619 - mean_squared_error: 13.4619 - val_loss: 14.1344 - val_mean_squared_error: 14.1344\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 12.6056 - mean_squared_error: 12.6056 - val_loss: 11.1634 - val_mean_squared_error: 11.1634\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 11.9298 - mean_squared_error: 11.9298 - val_loss: 12.1391 - val_mean_squared_error: 12.1391\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 11.3997 - mean_squared_error: 11.3997 - val_loss: 9.9755 - val_mean_squared_error: 9.9755\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 10.2750 - mean_squared_error: 10.2750 - val_loss: 9.4383 - val_mean_squared_error: 9.4383\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 10.1002 - mean_squared_error: 10.1002 - val_loss: 10.9194 - val_mean_squared_error: 10.9194\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 9.6049 - mean_squared_error: 9.6049 - val_loss: 13.5874 - val_mean_squared_error: 13.5874\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 9.2947 - mean_squared_error: 9.2947 - val_loss: 10.8512 - val_mean_squared_error: 10.8512\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 8.4631 - mean_squared_error: 8.4631 - val_loss: 11.4087 - val_mean_squared_error: 11.4087\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 7.8603 - mean_squared_error: 7.8603 - val_loss: 8.9462 - val_mean_squared_error: 8.9462\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 7.8403 - mean_squared_error: 7.8403 - val_loss: 7.6377 - val_mean_squared_error: 7.6376\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 7.2988 - mean_squared_error: 7.2988 - val_loss: 8.6265 - val_mean_squared_error: 8.6265\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 6.3727 - mean_squared_error: 6.3727 - val_loss: 7.4293 - val_mean_squared_error: 7.4293\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 6.4458 - mean_squared_error: 6.4458 - val_loss: 7.2046 - val_mean_squared_error: 7.2046\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 6.9760 - mean_squared_error: 6.9760 - val_loss: 11.1852 - val_mean_squared_error: 11.1852\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 6.9458 - mean_squared_error: 6.9458 - val_loss: 7.1318 - val_mean_squared_error: 7.1318\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 5.4990 - mean_squared_error: 5.4990 - val_loss: 5.3559 - val_mean_squared_error: 5.3559\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 5.7184 - mean_squared_error: 5.7184 - val_loss: 7.4961 - val_mean_squared_error: 7.4961\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 5.5028 - mean_squared_error: 5.5028 - val_loss: 6.8460 - val_mean_squared_error: 6.8460\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 4.8222 - mean_squared_error: 4.8222 - val_loss: 5.7080 - val_mean_squared_error: 5.7080\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 4.2481 - mean_squared_error: 4.2481 - val_loss: 4.8382 - val_mean_squared_error: 4.8382\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 4.2163 - mean_squared_error: 4.2163 - val_loss: 4.9412 - val_mean_squared_error: 4.9412\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 3.9890 - mean_squared_error: 3.9890 - val_loss: 4.6096 - val_mean_squared_error: 4.6096\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 3.8756 - mean_squared_error: 3.8756 - val_loss: 6.0651 - val_mean_squared_error: 6.0651\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 3.7906 - mean_squared_error: 3.7906 - val_loss: 4.6778 - val_mean_squared_error: 4.6778\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 3.6914 - mean_squared_error: 3.6914 - val_loss: 5.4482 - val_mean_squared_error: 5.4482\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 3.8782 - mean_squared_error: 3.8782 - val_loss: 4.2759 - val_mean_squared_error: 4.2759\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 3.5882 - mean_squared_error: 3.5882 - val_loss: 3.7806 - val_mean_squared_error: 3.7806\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 3.4192 - mean_squared_error: 3.4192 - val_loss: 4.8491 - val_mean_squared_error: 4.8491\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 3.4945 - mean_squared_error: 3.4945 - val_loss: 4.0207 - val_mean_squared_error: 4.0207\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 3.2948 - mean_squared_error: 3.2948 - val_loss: 3.6990 - val_mean_squared_error: 3.6990\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 3.0617 - mean_squared_error: 3.0617 - val_loss: 4.2440 - val_mean_squared_error: 4.2440\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 3.3109 - mean_squared_error: 3.3109 - val_loss: 4.6082 - val_mean_squared_error: 4.6082\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 3.0515 - mean_squared_error: 3.0515 - val_loss: 6.6823 - val_mean_squared_error: 6.6823\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 3.4705 - mean_squared_error: 3.4705 - val_loss: 3.8199 - val_mean_squared_error: 3.8199\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 3.1178 - mean_squared_error: 3.1178 - val_loss: 4.9685 - val_mean_squared_error: 4.9685\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 2.9762 - mean_squared_error: 2.9762 - val_loss: 3.5138 - val_mean_squared_error: 3.5138\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 2.5213 - mean_squared_error: 2.5213 - val_loss: 4.3865 - val_mean_squared_error: 4.3865\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.7756 - mean_squared_error: 2.7756 - val_loss: 5.7904 - val_mean_squared_error: 5.7904\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.9538 - mean_squared_error: 2.9538 - val_loss: 3.6822 - val_mean_squared_error: 3.6822\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.5916 - mean_squared_error: 2.5916 - val_loss: 3.4622 - val_mean_squared_error: 3.4622\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.5230 - mean_squared_error: 2.5230 - val_loss: 3.3589 - val_mean_squared_error: 3.3589\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.5552 - mean_squared_error: 2.5552 - val_loss: 4.0321 - val_mean_squared_error: 4.0321\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.5015 - mean_squared_error: 2.5015 - val_loss: 3.8073 - val_mean_squared_error: 3.8073\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.3907 - mean_squared_error: 2.3907 - val_loss: 3.3058 - val_mean_squared_error: 3.3058\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.5351 - mean_squared_error: 2.5351 - val_loss: 4.3235 - val_mean_squared_error: 4.3235\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 2.2766 - mean_squared_error: 2.2766 - val_loss: 3.2634 - val_mean_squared_error: 3.2634\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.4411 - mean_squared_error: 2.4411 - val_loss: 5.3048 - val_mean_squared_error: 5.3048\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.2359 - mean_squared_error: 2.2359 - val_loss: 3.1392 - val_mean_squared_error: 3.1392\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 2.3757 - mean_squared_error: 2.3757 - val_loss: 4.1535 - val_mean_squared_error: 4.1535\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.2879 - mean_squared_error: 2.2879 - val_loss: 3.0112 - val_mean_squared_error: 3.0112\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.2659 - mean_squared_error: 2.2659 - val_loss: 3.8895 - val_mean_squared_error: 3.8895\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.1151 - mean_squared_error: 2.1151 - val_loss: 3.2098 - val_mean_squared_error: 3.2098\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 2.1166 - mean_squared_error: 2.1166 - val_loss: 3.1872 - val_mean_squared_error: 3.1872\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.1582 - mean_squared_error: 2.1582 - val_loss: 3.1148 - val_mean_squared_error: 3.1148\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.9890 - mean_squared_error: 1.9890 - val_loss: 3.0413 - val_mean_squared_error: 3.0413\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 2.0814 - mean_squared_error: 2.0814 - val_loss: 3.0292 - val_mean_squared_error: 3.0292\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.2400 - mean_squared_error: 2.2400 - val_loss: 3.1020 - val_mean_squared_error: 3.1020\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.2517 - mean_squared_error: 2.2517 - val_loss: 3.3288 - val_mean_squared_error: 3.3288\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 2.0646 - mean_squared_error: 2.0646 - val_loss: 3.0405 - val_mean_squared_error: 3.0405\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 2.0242 - mean_squared_error: 2.0242 - val_loss: 2.9005 - val_mean_squared_error: 2.9005\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.0126 - mean_squared_error: 2.0126 - val_loss: 2.9949 - val_mean_squared_error: 2.9949\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.9279 - mean_squared_error: 1.9279 - val_loss: 3.0159 - val_mean_squared_error: 3.0159\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.0131 - mean_squared_error: 2.0131 - val_loss: 3.2709 - val_mean_squared_error: 3.2709\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.9005 - mean_squared_error: 1.9005 - val_loss: 3.1165 - val_mean_squared_error: 3.1165\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.7238 - mean_squared_error: 1.7238 - val_loss: 3.3480 - val_mean_squared_error: 3.3480\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.7196 - mean_squared_error: 1.7196 - val_loss: 2.8657 - val_mean_squared_error: 2.8657\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.8053 - mean_squared_error: 1.8053 - val_loss: 3.7603 - val_mean_squared_error: 3.7603\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.9301 - mean_squared_error: 1.9301 - val_loss: 3.5042 - val_mean_squared_error: 3.5042\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.7627 - mean_squared_error: 1.7627 - val_loss: 2.9826 - val_mean_squared_error: 2.9826\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.8048 - mean_squared_error: 1.8048 - val_loss: 2.9327 - val_mean_squared_error: 2.9327\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.8986 - mean_squared_error: 1.8986 - val_loss: 2.8656 - val_mean_squared_error: 2.8656\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.7858 - mean_squared_error: 1.7858 - val_loss: 3.2334 - val_mean_squared_error: 3.2334\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.7286 - mean_squared_error: 1.7286 - val_loss: 3.2754 - val_mean_squared_error: 3.2754\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 2.0291 - mean_squared_error: 2.0291 - val_loss: 3.2944 - val_mean_squared_error: 3.2944\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.7899 - mean_squared_error: 1.7899 - val_loss: 2.9347 - val_mean_squared_error: 2.9347\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.6929 - mean_squared_error: 1.6929 - val_loss: 2.8672 - val_mean_squared_error: 2.8672\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.8073 - mean_squared_error: 1.8073 - val_loss: 2.7510 - val_mean_squared_error: 2.7510\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.7853 - mean_squared_error: 1.7853 - val_loss: 2.8849 - val_mean_squared_error: 2.8849\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.7786 - mean_squared_error: 1.7786 - val_loss: 2.6559 - val_mean_squared_error: 2.6559\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.6383 - mean_squared_error: 1.6383 - val_loss: 3.4030 - val_mean_squared_error: 3.4030\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.6851 - mean_squared_error: 1.6851 - val_loss: 3.0381 - val_mean_squared_error: 3.0381\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.6127 - mean_squared_error: 1.6127 - val_loss: 2.9481 - val_mean_squared_error: 2.9481\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.6476 - mean_squared_error: 1.6476 - val_loss: 3.0746 - val_mean_squared_error: 3.0746\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.6807 - mean_squared_error: 1.6807 - val_loss: 3.2870 - val_mean_squared_error: 3.2870\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.7635 - mean_squared_error: 1.7635 - val_loss: 3.1899 - val_mean_squared_error: 3.1899\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.7217 - mean_squared_error: 1.7217 - val_loss: 3.8511 - val_mean_squared_error: 3.8511\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.4882 - mean_squared_error: 1.4882 - val_loss: 2.9503 - val_mean_squared_error: 2.9503\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.6844 - mean_squared_error: 1.6844 - val_loss: 2.6730 - val_mean_squared_error: 2.6730\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.5769 - mean_squared_error: 1.5769 - val_loss: 3.3625 - val_mean_squared_error: 3.3625\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.4988 - mean_squared_error: 1.4988 - val_loss: 2.7071 - val_mean_squared_error: 2.7071\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.4405 - mean_squared_error: 1.4405 - val_loss: 2.8514 - val_mean_squared_error: 2.8514\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.4958 - mean_squared_error: 1.4958 - val_loss: 3.0514 - val_mean_squared_error: 3.0514\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.4764 - mean_squared_error: 1.4764 - val_loss: 2.4802 - val_mean_squared_error: 2.4802\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.3256 - mean_squared_error: 1.3256 - val_loss: 3.0789 - val_mean_squared_error: 3.0789\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.4236 - mean_squared_error: 1.4236 - val_loss: 3.3583 - val_mean_squared_error: 3.3583\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.5382 - mean_squared_error: 1.5382 - val_loss: 3.3323 - val_mean_squared_error: 3.3323\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.4815 - mean_squared_error: 1.4815 - val_loss: 3.5163 - val_mean_squared_error: 3.5163\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.5863 - mean_squared_error: 1.5863 - val_loss: 2.6253 - val_mean_squared_error: 2.6253\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.5250 - mean_squared_error: 1.5250 - val_loss: 2.5491 - val_mean_squared_error: 2.5491\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.3926 - mean_squared_error: 1.3926 - val_loss: 2.8529 - val_mean_squared_error: 2.8529\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.3275 - mean_squared_error: 1.3275 - val_loss: 3.0120 - val_mean_squared_error: 3.0120\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.4029 - mean_squared_error: 1.4029 - val_loss: 2.8029 - val_mean_squared_error: 2.8029\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.5801 - mean_squared_error: 1.5801 - val_loss: 3.9030 - val_mean_squared_error: 3.9030\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.3742 - mean_squared_error: 1.3742 - val_loss: 2.5296 - val_mean_squared_error: 2.5296\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.2243 - mean_squared_error: 1.2243 - val_loss: 2.5056 - val_mean_squared_error: 2.5056\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.4369 - mean_squared_error: 1.4369 - val_loss: 3.0220 - val_mean_squared_error: 3.0220\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.3754 - mean_squared_error: 1.3754 - val_loss: 2.9553 - val_mean_squared_error: 2.9553\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.2976 - mean_squared_error: 1.2976 - val_loss: 2.7946 - val_mean_squared_error: 2.7946\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.3475 - mean_squared_error: 1.3475 - val_loss: 2.7409 - val_mean_squared_error: 2.7409\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.3286 - mean_squared_error: 1.3286 - val_loss: 3.7146 - val_mean_squared_error: 3.7146\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.2822 - mean_squared_error: 1.2822 - val_loss: 2.6421 - val_mean_squared_error: 2.6421\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.2876 - mean_squared_error: 1.2876 - val_loss: 2.8078 - val_mean_squared_error: 2.8078\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.3493 - mean_squared_error: 1.3493 - val_loss: 2.7907 - val_mean_squared_error: 2.7907\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.3392 - mean_squared_error: 1.3392 - val_loss: 2.7318 - val_mean_squared_error: 2.7318\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.2059 - mean_squared_error: 1.2059 - val_loss: 2.7827 - val_mean_squared_error: 2.7827\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.2375 - mean_squared_error: 1.2375 - val_loss: 3.0248 - val_mean_squared_error: 3.0248\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.2977 - mean_squared_error: 1.2977 - val_loss: 2.9588 - val_mean_squared_error: 2.9588\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.1724 - mean_squared_error: 1.1724 - val_loss: 2.8743 - val_mean_squared_error: 2.8743\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.1216 - mean_squared_error: 1.1216 - val_loss: 2.4502 - val_mean_squared_error: 2.4502\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.2713 - mean_squared_error: 1.2713 - val_loss: 2.5107 - val_mean_squared_error: 2.5107\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.2234 - mean_squared_error: 1.2234 - val_loss: 2.6196 - val_mean_squared_error: 2.6196\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.2923 - mean_squared_error: 1.2923 - val_loss: 2.6332 - val_mean_squared_error: 2.6332\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.3206 - mean_squared_error: 1.3206 - val_loss: 2.5577 - val_mean_squared_error: 2.5577\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.2221 - mean_squared_error: 1.2221 - val_loss: 2.7095 - val_mean_squared_error: 2.7095\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.1476 - mean_squared_error: 1.1476 - val_loss: 2.7311 - val_mean_squared_error: 2.7311\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.3532 - mean_squared_error: 1.3532 - val_loss: 4.2770 - val_mean_squared_error: 4.2770\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.3209 - mean_squared_error: 1.3209 - val_loss: 2.7934 - val_mean_squared_error: 2.7934\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.1118 - mean_squared_error: 1.1118 - val_loss: 2.5689 - val_mean_squared_error: 2.5689\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.2408 - mean_squared_error: 1.2408 - val_loss: 2.5306 - val_mean_squared_error: 2.5306\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.1859 - mean_squared_error: 1.1859 - val_loss: 2.3951 - val_mean_squared_error: 2.3951\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.3438 - mean_squared_error: 1.3438 - val_loss: 2.8269 - val_mean_squared_error: 2.8269\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.3306 - mean_squared_error: 1.3306 - val_loss: 2.8763 - val_mean_squared_error: 2.8763\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.3357 - mean_squared_error: 1.3357 - val_loss: 2.9770 - val_mean_squared_error: 2.9770\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.3269 - mean_squared_error: 1.3269 - val_loss: 2.6921 - val_mean_squared_error: 2.6921\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.2716 - mean_squared_error: 1.2716 - val_loss: 2.5899 - val_mean_squared_error: 2.5899\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.1159 - mean_squared_error: 1.1159 - val_loss: 2.5471 - val_mean_squared_error: 2.5471\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.2099 - mean_squared_error: 1.2099 - val_loss: 2.6891 - val_mean_squared_error: 2.6891\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.1239 - mean_squared_error: 1.1239 - val_loss: 2.6009 - val_mean_squared_error: 2.6009\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.1448 - mean_squared_error: 1.1448 - val_loss: 2.5045 - val_mean_squared_error: 2.5045\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.0398 - mean_squared_error: 1.0398 - val_loss: 2.4693 - val_mean_squared_error: 2.4693\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.1381 - mean_squared_error: 1.1381 - val_loss: 2.4873 - val_mean_squared_error: 2.4873\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.1221 - mean_squared_error: 1.1221 - val_loss: 2.5647 - val_mean_squared_error: 2.5647\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 1.0641 - mean_squared_error: 1.0641 - val_loss: 2.5330 - val_mean_squared_error: 2.5330\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.0047 - mean_squared_error: 1.0047 - val_loss: 2.4096 - val_mean_squared_error: 2.4096\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.0001 - mean_squared_error: 1.0001 - val_loss: 2.7065 - val_mean_squared_error: 2.7065\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.0538 - mean_squared_error: 1.0538 - val_loss: 2.3824 - val_mean_squared_error: 2.3824\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 0.9784 - mean_squared_error: 0.9784 - val_loss: 2.5915 - val_mean_squared_error: 2.5915\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.0592 - mean_squared_error: 1.0592 - val_loss: 2.7778 - val_mean_squared_error: 2.7778\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 0.9745 - mean_squared_error: 0.9745 - val_loss: 2.4855 - val_mean_squared_error: 2.4855\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.0874 - mean_squared_error: 1.0874 - val_loss: 2.4316 - val_mean_squared_error: 2.4316\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.0615 - mean_squared_error: 1.0615 - val_loss: 2.7687 - val_mean_squared_error: 2.7687\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.0316 - mean_squared_error: 1.0316 - val_loss: 2.5719 - val_mean_squared_error: 2.5719\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.0761 - mean_squared_error: 1.0761 - val_loss: 2.5263 - val_mean_squared_error: 2.5263\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.0740 - mean_squared_error: 1.0740 - val_loss: 2.5780 - val_mean_squared_error: 2.5780\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.1300 - mean_squared_error: 1.1300 - val_loss: 2.4398 - val_mean_squared_error: 2.4398\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.0556 - mean_squared_error: 1.0556 - val_loss: 2.5574 - val_mean_squared_error: 2.5574\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.0847 - mean_squared_error: 1.0847 - val_loss: 2.3961 - val_mean_squared_error: 2.3961\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.0314 - mean_squared_error: 1.0314 - val_loss: 2.4741 - val_mean_squared_error: 2.4741\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.0862 - mean_squared_error: 1.0862 - val_loss: 3.3869 - val_mean_squared_error: 3.3869\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.0548 - mean_squared_error: 1.0548 - val_loss: 2.5455 - val_mean_squared_error: 2.5455\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 0.9916 - mean_squared_error: 0.9916 - val_loss: 2.3334 - val_mean_squared_error: 2.3334\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 0.9849 - mean_squared_error: 0.9849 - val_loss: 2.2450 - val_mean_squared_error: 2.2450\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.0282 - mean_squared_error: 1.0281 - val_loss: 2.5090 - val_mean_squared_error: 2.5089\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 0.9845 - mean_squared_error: 0.9845 - val_loss: 2.7302 - val_mean_squared_error: 2.7302\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 0.9720 - mean_squared_error: 0.9720 - val_loss: 2.4838 - val_mean_squared_error: 2.4838\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.0309 - mean_squared_error: 1.0309 - val_loss: 2.3570 - val_mean_squared_error: 2.3570\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.0474 - mean_squared_error: 1.0474 - val_loss: 2.3638 - val_mean_squared_error: 2.3638\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 0.9576 - mean_squared_error: 0.9576 - val_loss: 2.8438 - val_mean_squared_error: 2.8438\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.0356 - mean_squared_error: 1.0356 - val_loss: 2.6602 - val_mean_squared_error: 2.6602\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.0526 - mean_squared_error: 1.0526 - val_loss: 2.9604 - val_mean_squared_error: 2.9604\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.9639 - mean_squared_error: 0.9639 - val_loss: 2.4999 - val_mean_squared_error: 2.4999\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 0.9980 - mean_squared_error: 0.9980 - val_loss: 2.3608 - val_mean_squared_error: 2.3608\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 0.9644 - mean_squared_error: 0.9644 - val_loss: 2.6158 - val_mean_squared_error: 2.6158\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 0.9973 - mean_squared_error: 0.9973 - val_loss: 2.4804 - val_mean_squared_error: 2.4804\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 0.9271 - mean_squared_error: 0.9271 - val_loss: 2.3509 - val_mean_squared_error: 2.3509\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 0.9275 - mean_squared_error: 0.9275 - val_loss: 2.2991 - val_mean_squared_error: 2.2991\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 0.8182 - mean_squared_error: 0.8182 - val_loss: 2.6219 - val_mean_squared_error: 2.6219\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 2.5358 - val_mean_squared_error: 2.5358\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 0.9446 - mean_squared_error: 0.9446 - val_loss: 2.9698 - val_mean_squared_error: 2.9698\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 2.3721 - val_mean_squared_error: 2.3721\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 0.9638 - mean_squared_error: 0.9638 - val_loss: 3.0823 - val_mean_squared_error: 3.0823\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 0.9611 - mean_squared_error: 0.9611 - val_loss: 2.5520 - val_mean_squared_error: 2.5520\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 0.9400 - mean_squared_error: 0.9400 - val_loss: 2.4093 - val_mean_squared_error: 2.4093\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 0.8643 - mean_squared_error: 0.8643 - val_loss: 2.5583 - val_mean_squared_error: 2.5583\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 0.8739 - mean_squared_error: 0.8739 - val_loss: 2.6267 - val_mean_squared_error: 2.6267\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 0.9894 - mean_squared_error: 0.9894 - val_loss: 2.7365 - val_mean_squared_error: 2.7365\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 0.9633 - mean_squared_error: 0.9633 - val_loss: 2.4468 - val_mean_squared_error: 2.4468\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 0.8501 - mean_squared_error: 0.8501 - val_loss: 2.5219 - val_mean_squared_error: 2.5219\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 0.9506 - mean_squared_error: 0.9506 - val_loss: 2.4134 - val_mean_squared_error: 2.4134\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 0.8153 - mean_squared_error: 0.8153 - val_loss: 2.3163 - val_mean_squared_error: 2.3163\n",
            "==================================================\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 969us/sample - loss: 1533.7654 - mean_squared_error: 1533.7649 - val_loss: 1231.5241 - val_mean_squared_error: 1231.5240\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 240.8387 - mean_squared_error: 240.8387 - val_loss: 331.6874 - val_mean_squared_error: 331.6873\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 35.3428 - mean_squared_error: 35.3428 - val_loss: 170.1576 - val_mean_squared_error: 170.1576\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 21.8876 - mean_squared_error: 21.8876 - val_loss: 78.8138 - val_mean_squared_error: 78.8138\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 18.1811 - mean_squared_error: 18.1811 - val_loss: 32.7347 - val_mean_squared_error: 32.7347\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 15.9610 - mean_squared_error: 15.9610 - val_loss: 18.7201 - val_mean_squared_error: 18.7201\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 15.7953 - mean_squared_error: 15.7952 - val_loss: 11.9901 - val_mean_squared_error: 11.9901\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 440us/sample - loss: 17.2002 - mean_squared_error: 17.2002 - val_loss: 21.8495 - val_mean_squared_error: 21.8494\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 13.8156 - mean_squared_error: 13.8156 - val_loss: 11.2696 - val_mean_squared_error: 11.2696\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 13.9395 - mean_squared_error: 13.9395 - val_loss: 13.9962 - val_mean_squared_error: 13.9962\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 12.1562 - mean_squared_error: 12.1562 - val_loss: 13.0655 - val_mean_squared_error: 13.0655\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 13.1128 - mean_squared_error: 13.1129 - val_loss: 12.0792 - val_mean_squared_error: 12.0792\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 12.1415 - mean_squared_error: 12.1415 - val_loss: 11.4310 - val_mean_squared_error: 11.4310\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 11.5253 - mean_squared_error: 11.5253 - val_loss: 11.7839 - val_mean_squared_error: 11.7839\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 10.7868 - mean_squared_error: 10.7868 - val_loss: 9.4027 - val_mean_squared_error: 9.4027\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 10.4351 - mean_squared_error: 10.4351 - val_loss: 11.3655 - val_mean_squared_error: 11.3655\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 9.7331 - mean_squared_error: 9.7331 - val_loss: 8.9788 - val_mean_squared_error: 8.9788\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 9.1780 - mean_squared_error: 9.1780 - val_loss: 10.9107 - val_mean_squared_error: 10.9107\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 8.5767 - mean_squared_error: 8.5767 - val_loss: 9.2150 - val_mean_squared_error: 9.2150\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 8.7579 - mean_squared_error: 8.7579 - val_loss: 8.4100 - val_mean_squared_error: 8.4100\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 8.1224 - mean_squared_error: 8.1224 - val_loss: 12.0821 - val_mean_squared_error: 12.0821\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 8.1493 - mean_squared_error: 8.1493 - val_loss: 8.8818 - val_mean_squared_error: 8.8818\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 7.4912 - mean_squared_error: 7.4912 - val_loss: 8.7295 - val_mean_squared_error: 8.7295\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 7.3997 - mean_squared_error: 7.3997 - val_loss: 11.1630 - val_mean_squared_error: 11.1630\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 7.0723 - mean_squared_error: 7.0723 - val_loss: 7.3098 - val_mean_squared_error: 7.3098\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 6.1455 - mean_squared_error: 6.1455 - val_loss: 6.4079 - val_mean_squared_error: 6.4079\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 5.9803 - mean_squared_error: 5.9803 - val_loss: 8.9216 - val_mean_squared_error: 8.9216\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 6.0519 - mean_squared_error: 6.0519 - val_loss: 5.4808 - val_mean_squared_error: 5.4808\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 5.2827 - mean_squared_error: 5.2827 - val_loss: 5.4374 - val_mean_squared_error: 5.4374\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 5.0661 - mean_squared_error: 5.0661 - val_loss: 5.7183 - val_mean_squared_error: 5.7183\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 4.8551 - mean_squared_error: 4.8551 - val_loss: 5.6741 - val_mean_squared_error: 5.6741\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 5.1782 - mean_squared_error: 5.1782 - val_loss: 4.7463 - val_mean_squared_error: 4.7463\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 4.1942 - mean_squared_error: 4.1942 - val_loss: 4.7770 - val_mean_squared_error: 4.7770\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 4.3776 - mean_squared_error: 4.3776 - val_loss: 4.9754 - val_mean_squared_error: 4.9754\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 4.4387 - mean_squared_error: 4.4387 - val_loss: 6.2216 - val_mean_squared_error: 6.2216\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 4.5247 - mean_squared_error: 4.5247 - val_loss: 4.2996 - val_mean_squared_error: 4.2996\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 4.0241 - mean_squared_error: 4.0241 - val_loss: 4.7179 - val_mean_squared_error: 4.7179\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.8842 - mean_squared_error: 3.8842 - val_loss: 5.1363 - val_mean_squared_error: 5.1363\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 4.2723 - mean_squared_error: 4.2723 - val_loss: 4.1159 - val_mean_squared_error: 4.1159\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 3.8864 - mean_squared_error: 3.8864 - val_loss: 4.6010 - val_mean_squared_error: 4.6010\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.4972 - mean_squared_error: 3.4972 - val_loss: 4.1067 - val_mean_squared_error: 4.1067\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 3.7327 - mean_squared_error: 3.7327 - val_loss: 4.3555 - val_mean_squared_error: 4.3555\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 3.4078 - mean_squared_error: 3.4078 - val_loss: 3.5400 - val_mean_squared_error: 3.5400\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.2321 - mean_squared_error: 3.2321 - val_loss: 5.0627 - val_mean_squared_error: 5.0627\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 3.1324 - mean_squared_error: 3.1324 - val_loss: 3.6145 - val_mean_squared_error: 3.6145\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.1989 - mean_squared_error: 3.1989 - val_loss: 4.4867 - val_mean_squared_error: 4.4867\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 3.2235 - mean_squared_error: 3.2235 - val_loss: 4.4956 - val_mean_squared_error: 4.4956\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 3.1498 - mean_squared_error: 3.1498 - val_loss: 4.6845 - val_mean_squared_error: 4.6845\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 2.7866 - mean_squared_error: 2.7866 - val_loss: 4.1794 - val_mean_squared_error: 4.1794\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 3.1360 - mean_squared_error: 3.1360 - val_loss: 3.6931 - val_mean_squared_error: 3.6931\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 3.2957 - mean_squared_error: 3.2957 - val_loss: 4.8210 - val_mean_squared_error: 4.8210\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 3.0513 - mean_squared_error: 3.0513 - val_loss: 3.9327 - val_mean_squared_error: 3.9327\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 2.8796 - mean_squared_error: 2.8796 - val_loss: 3.5598 - val_mean_squared_error: 3.5598\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.7493 - mean_squared_error: 2.7493 - val_loss: 3.7861 - val_mean_squared_error: 3.7861\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.6939 - mean_squared_error: 2.6939 - val_loss: 3.9906 - val_mean_squared_error: 3.9906\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.6314 - mean_squared_error: 2.6314 - val_loss: 3.2446 - val_mean_squared_error: 3.2446\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.7011 - mean_squared_error: 2.7011 - val_loss: 3.4583 - val_mean_squared_error: 3.4583\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.6443 - mean_squared_error: 2.6443 - val_loss: 3.5879 - val_mean_squared_error: 3.5879\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.7177 - mean_squared_error: 2.7177 - val_loss: 3.6561 - val_mean_squared_error: 3.6561\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.3944 - mean_squared_error: 2.3944 - val_loss: 3.0356 - val_mean_squared_error: 3.0356\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.5490 - mean_squared_error: 2.5490 - val_loss: 3.5794 - val_mean_squared_error: 3.5794\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 2.5626 - mean_squared_error: 2.5626 - val_loss: 3.7472 - val_mean_squared_error: 3.7472\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.5156 - mean_squared_error: 2.5156 - val_loss: 3.3829 - val_mean_squared_error: 3.3829\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.4220 - mean_squared_error: 2.4220 - val_loss: 3.1498 - val_mean_squared_error: 3.1498\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.3902 - mean_squared_error: 2.3902 - val_loss: 3.1564 - val_mean_squared_error: 3.1564\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.4416 - mean_squared_error: 2.4416 - val_loss: 3.0357 - val_mean_squared_error: 3.0357\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.4318 - mean_squared_error: 2.4318 - val_loss: 2.7664 - val_mean_squared_error: 2.7664\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.3708 - mean_squared_error: 2.3708 - val_loss: 5.3623 - val_mean_squared_error: 5.3623\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.3299 - mean_squared_error: 2.3299 - val_loss: 2.9723 - val_mean_squared_error: 2.9723\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.3042 - mean_squared_error: 2.3042 - val_loss: 4.3745 - val_mean_squared_error: 4.3745\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.3780 - mean_squared_error: 2.3780 - val_loss: 3.5979 - val_mean_squared_error: 3.5979\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.1786 - mean_squared_error: 2.1786 - val_loss: 3.4690 - val_mean_squared_error: 3.4690\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.2214 - mean_squared_error: 2.2214 - val_loss: 3.4880 - val_mean_squared_error: 3.4880\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.1723 - mean_squared_error: 2.1723 - val_loss: 3.8109 - val_mean_squared_error: 3.8109\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.0809 - mean_squared_error: 2.0809 - val_loss: 3.5638 - val_mean_squared_error: 3.5638\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.1140 - mean_squared_error: 2.1140 - val_loss: 3.1817 - val_mean_squared_error: 3.1817\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.0929 - mean_squared_error: 2.0929 - val_loss: 2.9129 - val_mean_squared_error: 2.9129\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.9677 - mean_squared_error: 1.9677 - val_loss: 3.3923 - val_mean_squared_error: 3.3923\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.9022 - mean_squared_error: 1.9022 - val_loss: 2.5762 - val_mean_squared_error: 2.5762\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.0145 - mean_squared_error: 2.0145 - val_loss: 3.4232 - val_mean_squared_error: 3.4232\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.2249 - mean_squared_error: 2.2249 - val_loss: 3.4767 - val_mean_squared_error: 3.4767\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.7874 - mean_squared_error: 1.7874 - val_loss: 3.0888 - val_mean_squared_error: 3.0888\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.8491 - mean_squared_error: 1.8491 - val_loss: 2.5860 - val_mean_squared_error: 2.5860\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.8449 - mean_squared_error: 1.8449 - val_loss: 2.6427 - val_mean_squared_error: 2.6427\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.8124 - mean_squared_error: 1.8124 - val_loss: 3.0429 - val_mean_squared_error: 3.0429\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.9585 - mean_squared_error: 1.9585 - val_loss: 4.2193 - val_mean_squared_error: 4.2193\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.1252 - mean_squared_error: 2.1252 - val_loss: 3.4596 - val_mean_squared_error: 3.4596\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.8305 - mean_squared_error: 1.8305 - val_loss: 2.7827 - val_mean_squared_error: 2.7827\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.8488 - mean_squared_error: 1.8488 - val_loss: 2.7136 - val_mean_squared_error: 2.7136\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.8616 - mean_squared_error: 1.8616 - val_loss: 2.7381 - val_mean_squared_error: 2.7381\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.8210 - mean_squared_error: 1.8210 - val_loss: 3.5060 - val_mean_squared_error: 3.5060\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7829 - mean_squared_error: 1.7829 - val_loss: 3.2441 - val_mean_squared_error: 3.2441\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.9259 - mean_squared_error: 1.9259 - val_loss: 2.8134 - val_mean_squared_error: 2.8134\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.5949 - mean_squared_error: 1.5949 - val_loss: 2.8697 - val_mean_squared_error: 2.8697\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.6464 - mean_squared_error: 1.6464 - val_loss: 2.4076 - val_mean_squared_error: 2.4076\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7840 - mean_squared_error: 1.7840 - val_loss: 2.5619 - val_mean_squared_error: 2.5619\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.8462 - mean_squared_error: 1.8462 - val_loss: 3.9456 - val_mean_squared_error: 3.9456\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.6915 - mean_squared_error: 1.6915 - val_loss: 2.8477 - val_mean_squared_error: 2.8477\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.7678 - mean_squared_error: 1.7678 - val_loss: 2.9085 - val_mean_squared_error: 2.9085\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.7329 - mean_squared_error: 1.7329 - val_loss: 2.8969 - val_mean_squared_error: 2.8969\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.7846 - mean_squared_error: 1.7846 - val_loss: 2.9269 - val_mean_squared_error: 2.9269\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.7763 - mean_squared_error: 1.7763 - val_loss: 3.9303 - val_mean_squared_error: 3.9303\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.5971 - mean_squared_error: 1.5971 - val_loss: 2.5303 - val_mean_squared_error: 2.5303\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.6215 - mean_squared_error: 1.6215 - val_loss: 2.6634 - val_mean_squared_error: 2.6634\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.7688 - mean_squared_error: 1.7688 - val_loss: 2.7367 - val_mean_squared_error: 2.7367\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.5188 - mean_squared_error: 1.5188 - val_loss: 2.8909 - val_mean_squared_error: 2.8909\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.5635 - mean_squared_error: 1.5635 - val_loss: 2.5821 - val_mean_squared_error: 2.5821\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.7405 - mean_squared_error: 1.7405 - val_loss: 2.7237 - val_mean_squared_error: 2.7237\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.6286 - mean_squared_error: 1.6286 - val_loss: 2.5939 - val_mean_squared_error: 2.5939\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.6850 - mean_squared_error: 1.6850 - val_loss: 2.4530 - val_mean_squared_error: 2.4530\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.6471 - mean_squared_error: 1.6471 - val_loss: 2.5292 - val_mean_squared_error: 2.5292\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.4751 - mean_squared_error: 1.4751 - val_loss: 2.4770 - val_mean_squared_error: 2.4770\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.5640 - mean_squared_error: 1.5640 - val_loss: 4.1067 - val_mean_squared_error: 4.1067\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.7030 - mean_squared_error: 1.7030 - val_loss: 2.9384 - val_mean_squared_error: 2.9384\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.5696 - mean_squared_error: 1.5696 - val_loss: 2.8578 - val_mean_squared_error: 2.8578\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.5754 - mean_squared_error: 1.5754 - val_loss: 2.6086 - val_mean_squared_error: 2.6086\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.5776 - mean_squared_error: 1.5776 - val_loss: 3.4482 - val_mean_squared_error: 3.4482\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.5640 - mean_squared_error: 1.5640 - val_loss: 2.4505 - val_mean_squared_error: 2.4505\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.6139 - mean_squared_error: 1.6139 - val_loss: 2.6857 - val_mean_squared_error: 2.6857\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.4881 - mean_squared_error: 1.4881 - val_loss: 2.6505 - val_mean_squared_error: 2.6505\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.4014 - mean_squared_error: 1.4014 - val_loss: 2.4373 - val_mean_squared_error: 2.4373\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.6131 - mean_squared_error: 1.6131 - val_loss: 4.7234 - val_mean_squared_error: 4.7234\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.5295 - mean_squared_error: 1.5295 - val_loss: 3.2449 - val_mean_squared_error: 3.2449\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.4369 - mean_squared_error: 1.4369 - val_loss: 2.5088 - val_mean_squared_error: 2.5088\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.3880 - mean_squared_error: 1.3880 - val_loss: 2.6324 - val_mean_squared_error: 2.6324\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.3369 - mean_squared_error: 1.3369 - val_loss: 2.8268 - val_mean_squared_error: 2.8268\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.3029 - mean_squared_error: 1.3029 - val_loss: 3.0410 - val_mean_squared_error: 3.0410\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.4064 - mean_squared_error: 1.4064 - val_loss: 3.4142 - val_mean_squared_error: 3.4142\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.4131 - mean_squared_error: 1.4131 - val_loss: 2.8806 - val_mean_squared_error: 2.8806\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.3375 - mean_squared_error: 1.3375 - val_loss: 2.2944 - val_mean_squared_error: 2.2944\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.4880 - mean_squared_error: 1.4880 - val_loss: 2.5366 - val_mean_squared_error: 2.5366\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.4238 - mean_squared_error: 1.4238 - val_loss: 2.9764 - val_mean_squared_error: 2.9764\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.4593 - mean_squared_error: 1.4593 - val_loss: 2.7358 - val_mean_squared_error: 2.7358\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.4248 - mean_squared_error: 1.4248 - val_loss: 2.8495 - val_mean_squared_error: 2.8495\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.5098 - mean_squared_error: 1.5098 - val_loss: 2.5933 - val_mean_squared_error: 2.5933\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.3906 - mean_squared_error: 1.3906 - val_loss: 2.6814 - val_mean_squared_error: 2.6814\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.3466 - mean_squared_error: 1.3466 - val_loss: 2.5303 - val_mean_squared_error: 2.5303\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.3247 - mean_squared_error: 1.3247 - val_loss: 2.6571 - val_mean_squared_error: 2.6571\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.3172 - mean_squared_error: 1.3172 - val_loss: 3.4337 - val_mean_squared_error: 3.4337\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.2806 - mean_squared_error: 1.2806 - val_loss: 2.3405 - val_mean_squared_error: 2.3405\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1844 - mean_squared_error: 1.1844 - val_loss: 2.2299 - val_mean_squared_error: 2.2299\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.3238 - mean_squared_error: 1.3238 - val_loss: 2.6015 - val_mean_squared_error: 2.6015\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3066 - mean_squared_error: 1.3066 - val_loss: 2.7301 - val_mean_squared_error: 2.7301\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.3801 - mean_squared_error: 1.3801 - val_loss: 2.4186 - val_mean_squared_error: 2.4186\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.4024 - mean_squared_error: 1.4024 - val_loss: 2.5962 - val_mean_squared_error: 2.5962\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.2759 - mean_squared_error: 1.2759 - val_loss: 2.1690 - val_mean_squared_error: 2.1690\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2621 - mean_squared_error: 1.2621 - val_loss: 2.3013 - val_mean_squared_error: 2.3013\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.1907 - mean_squared_error: 1.1907 - val_loss: 2.4917 - val_mean_squared_error: 2.4917\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2174 - mean_squared_error: 1.2174 - val_loss: 3.1425 - val_mean_squared_error: 3.1425\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1969 - mean_squared_error: 1.1969 - val_loss: 3.1743 - val_mean_squared_error: 3.1743\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.1906 - mean_squared_error: 1.1906 - val_loss: 3.1785 - val_mean_squared_error: 3.1785\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.2563 - mean_squared_error: 1.2563 - val_loss: 3.0067 - val_mean_squared_error: 3.0067\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2297 - mean_squared_error: 1.2297 - val_loss: 2.3925 - val_mean_squared_error: 2.3925\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.2445 - mean_squared_error: 1.2445 - val_loss: 2.6027 - val_mean_squared_error: 2.6027\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.2309 - mean_squared_error: 1.2309 - val_loss: 2.4587 - val_mean_squared_error: 2.4587\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1966 - mean_squared_error: 1.1966 - val_loss: 2.6839 - val_mean_squared_error: 2.6839\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.2425 - mean_squared_error: 1.2425 - val_loss: 2.4260 - val_mean_squared_error: 2.4260\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.1516 - mean_squared_error: 1.1516 - val_loss: 2.6387 - val_mean_squared_error: 2.6387\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1498 - mean_squared_error: 1.1498 - val_loss: 2.2267 - val_mean_squared_error: 2.2267\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.1610 - mean_squared_error: 1.1610 - val_loss: 2.2431 - val_mean_squared_error: 2.2431\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.2493 - mean_squared_error: 1.2493 - val_loss: 3.5597 - val_mean_squared_error: 3.5597\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.1557 - mean_squared_error: 1.1557 - val_loss: 2.7888 - val_mean_squared_error: 2.7888\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.2064 - mean_squared_error: 1.2064 - val_loss: 2.7349 - val_mean_squared_error: 2.7349\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.2021 - mean_squared_error: 1.2021 - val_loss: 2.2151 - val_mean_squared_error: 2.2151\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1894 - mean_squared_error: 1.1894 - val_loss: 2.4200 - val_mean_squared_error: 2.4200\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.2794 - mean_squared_error: 1.2794 - val_loss: 2.3092 - val_mean_squared_error: 2.3092\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.1655 - mean_squared_error: 1.1655 - val_loss: 3.0507 - val_mean_squared_error: 3.0507\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.0761 - mean_squared_error: 1.0761 - val_loss: 2.3058 - val_mean_squared_error: 2.3058\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1431 - mean_squared_error: 1.1431 - val_loss: 2.6688 - val_mean_squared_error: 2.6688\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1641 - mean_squared_error: 1.1641 - val_loss: 2.3833 - val_mean_squared_error: 2.3833\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.1259 - mean_squared_error: 1.1259 - val_loss: 2.3620 - val_mean_squared_error: 2.3620\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.0626 - mean_squared_error: 1.0626 - val_loss: 2.3616 - val_mean_squared_error: 2.3616\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.0913 - mean_squared_error: 1.0913 - val_loss: 2.3888 - val_mean_squared_error: 2.3888\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.1321 - mean_squared_error: 1.1321 - val_loss: 2.4171 - val_mean_squared_error: 2.4171\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2195 - mean_squared_error: 1.2195 - val_loss: 3.0221 - val_mean_squared_error: 3.0221\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.1784 - mean_squared_error: 1.1784 - val_loss: 2.9581 - val_mean_squared_error: 2.9581\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.0825 - mean_squared_error: 1.0825 - val_loss: 2.8772 - val_mean_squared_error: 2.8772\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.1505 - mean_squared_error: 1.1505 - val_loss: 2.2347 - val_mean_squared_error: 2.2347\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.0693 - mean_squared_error: 1.0693 - val_loss: 2.5212 - val_mean_squared_error: 2.5212\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1146 - mean_squared_error: 1.1146 - val_loss: 2.3367 - val_mean_squared_error: 2.3367\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.1069 - mean_squared_error: 1.1069 - val_loss: 2.4106 - val_mean_squared_error: 2.4106\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.0944 - mean_squared_error: 1.0944 - val_loss: 2.2041 - val_mean_squared_error: 2.2041\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.0890 - mean_squared_error: 1.0890 - val_loss: 2.5584 - val_mean_squared_error: 2.5584\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.0154 - mean_squared_error: 1.0154 - val_loss: 2.5308 - val_mean_squared_error: 2.5308\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 2.2097 - val_mean_squared_error: 2.2097\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.0444 - mean_squared_error: 1.0444 - val_loss: 2.0927 - val_mean_squared_error: 2.0927\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.1864 - mean_squared_error: 1.1864 - val_loss: 2.4159 - val_mean_squared_error: 2.4159\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.0596 - mean_squared_error: 1.0596 - val_loss: 2.2562 - val_mean_squared_error: 2.2562\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.0426 - mean_squared_error: 1.0426 - val_loss: 2.3278 - val_mean_squared_error: 2.3278\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.0285 - mean_squared_error: 1.0285 - val_loss: 2.5954 - val_mean_squared_error: 2.5954\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.0684 - mean_squared_error: 1.0684 - val_loss: 2.1823 - val_mean_squared_error: 2.1823\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.0414 - mean_squared_error: 1.0414 - val_loss: 2.3048 - val_mean_squared_error: 2.3048\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.0697 - mean_squared_error: 1.0697 - val_loss: 2.5380 - val_mean_squared_error: 2.5380\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 2.3491 - val_mean_squared_error: 2.3491\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.1483 - mean_squared_error: 1.1483 - val_loss: 2.6519 - val_mean_squared_error: 2.6519\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.0374 - mean_squared_error: 1.0374 - val_loss: 2.3966 - val_mean_squared_error: 2.3966\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.0665 - mean_squared_error: 1.0665 - val_loss: 2.4726 - val_mean_squared_error: 2.4726\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.0758 - mean_squared_error: 1.0757 - val_loss: 2.2376 - val_mean_squared_error: 2.2376\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.0447 - mean_squared_error: 1.0447 - val_loss: 3.5173 - val_mean_squared_error: 3.5173\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.1456 - mean_squared_error: 1.1456 - val_loss: 2.4318 - val_mean_squared_error: 2.4318\n",
            "==================================================\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 1551.0239 - mean_squared_error: 1551.0239 - val_loss: 1531.3066 - val_mean_squared_error: 1531.3065\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 240.1167 - mean_squared_error: 240.1167 - val_loss: 385.2489 - val_mean_squared_error: 385.2489\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 31.7824 - mean_squared_error: 31.7824 - val_loss: 128.7735 - val_mean_squared_error: 128.7735\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 18.0104 - mean_squared_error: 18.0104 - val_loss: 44.3799 - val_mean_squared_error: 44.3799\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 22.0732 - mean_squared_error: 22.0732 - val_loss: 59.5999 - val_mean_squared_error: 59.5999\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 18.0292 - mean_squared_error: 18.0292 - val_loss: 31.0874 - val_mean_squared_error: 31.0874\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 435us/sample - loss: 14.6726 - mean_squared_error: 14.6726 - val_loss: 15.1309 - val_mean_squared_error: 15.1309\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 13.9488 - mean_squared_error: 13.9488 - val_loss: 12.7408 - val_mean_squared_error: 12.7408\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 14.5687 - mean_squared_error: 14.5687 - val_loss: 12.9480 - val_mean_squared_error: 12.9480\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 12.4740 - mean_squared_error: 12.4740 - val_loss: 12.0507 - val_mean_squared_error: 12.0507\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 12.3392 - mean_squared_error: 12.3392 - val_loss: 16.4123 - val_mean_squared_error: 16.4123\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 10.7002 - mean_squared_error: 10.7002 - val_loss: 10.3839 - val_mean_squared_error: 10.3839\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 10.5155 - mean_squared_error: 10.5155 - val_loss: 13.2388 - val_mean_squared_error: 13.2388\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 10.3474 - mean_squared_error: 10.3474 - val_loss: 9.6654 - val_mean_squared_error: 9.6654\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 9.1838 - mean_squared_error: 9.1838 - val_loss: 8.4679 - val_mean_squared_error: 8.4679\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 9.3229 - mean_squared_error: 9.3229 - val_loss: 10.3539 - val_mean_squared_error: 10.3539\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 8.3342 - mean_squared_error: 8.3342 - val_loss: 9.1173 - val_mean_squared_error: 9.1173\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 8.6697 - mean_squared_error: 8.6697 - val_loss: 8.3832 - val_mean_squared_error: 8.3832\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 7.4253 - mean_squared_error: 7.4253 - val_loss: 7.4177 - val_mean_squared_error: 7.4177\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 7.2177 - mean_squared_error: 7.2177 - val_loss: 8.1811 - val_mean_squared_error: 8.1811\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 6.9559 - mean_squared_error: 6.9559 - val_loss: 6.7636 - val_mean_squared_error: 6.7636\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 6.4907 - mean_squared_error: 6.4907 - val_loss: 8.1086 - val_mean_squared_error: 8.1086\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 6.4097 - mean_squared_error: 6.4097 - val_loss: 7.3438 - val_mean_squared_error: 7.3438\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 6.0938 - mean_squared_error: 6.0938 - val_loss: 6.6644 - val_mean_squared_error: 6.6644\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 5.9827 - mean_squared_error: 5.9827 - val_loss: 6.1986 - val_mean_squared_error: 6.1986\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 6.7148 - mean_squared_error: 6.7148 - val_loss: 5.9885 - val_mean_squared_error: 5.9885\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 5.6066 - mean_squared_error: 5.6066 - val_loss: 5.0307 - val_mean_squared_error: 5.0307\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 5.3892 - mean_squared_error: 5.3892 - val_loss: 7.3561 - val_mean_squared_error: 7.3561\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 5.3362 - mean_squared_error: 5.3362 - val_loss: 6.3450 - val_mean_squared_error: 6.3450\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 4.8343 - mean_squared_error: 4.8343 - val_loss: 4.5833 - val_mean_squared_error: 4.5833\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 4.5879 - mean_squared_error: 4.5879 - val_loss: 5.0756 - val_mean_squared_error: 5.0756\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 4.3517 - mean_squared_error: 4.3517 - val_loss: 4.9827 - val_mean_squared_error: 4.9827\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 4.5850 - mean_squared_error: 4.5850 - val_loss: 5.3758 - val_mean_squared_error: 5.3758\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 4.1823 - mean_squared_error: 4.1823 - val_loss: 4.9132 - val_mean_squared_error: 4.9132\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 4.0754 - mean_squared_error: 4.0754 - val_loss: 4.2378 - val_mean_squared_error: 4.2378\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 4.0340 - mean_squared_error: 4.0340 - val_loss: 4.5383 - val_mean_squared_error: 4.5383\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 3.8837 - mean_squared_error: 3.8837 - val_loss: 3.9982 - val_mean_squared_error: 3.9982\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 3.8737 - mean_squared_error: 3.8737 - val_loss: 4.3599 - val_mean_squared_error: 4.3599\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 3.9024 - mean_squared_error: 3.9024 - val_loss: 4.0480 - val_mean_squared_error: 4.0480\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 3.5331 - mean_squared_error: 3.5331 - val_loss: 4.1880 - val_mean_squared_error: 4.1880\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 3.4624 - mean_squared_error: 3.4624 - val_loss: 3.4788 - val_mean_squared_error: 3.4788\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 3.6694 - mean_squared_error: 3.6694 - val_loss: 3.9712 - val_mean_squared_error: 3.9712\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 3.2554 - mean_squared_error: 3.2554 - val_loss: 3.7342 - val_mean_squared_error: 3.7342\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 3.1744 - mean_squared_error: 3.1744 - val_loss: 3.5011 - val_mean_squared_error: 3.5011\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.2580 - mean_squared_error: 3.2580 - val_loss: 3.8553 - val_mean_squared_error: 3.8553\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 3.0386 - mean_squared_error: 3.0386 - val_loss: 3.7093 - val_mean_squared_error: 3.7093\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.8936 - mean_squared_error: 2.8936 - val_loss: 3.4981 - val_mean_squared_error: 3.4981\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 3.0916 - mean_squared_error: 3.0916 - val_loss: 3.4992 - val_mean_squared_error: 3.4992\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 3.2909 - mean_squared_error: 3.2909 - val_loss: 5.4007 - val_mean_squared_error: 5.4007\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.8627 - mean_squared_error: 2.8627 - val_loss: 4.0005 - val_mean_squared_error: 4.0005\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.0729 - mean_squared_error: 3.0729 - val_loss: 3.3911 - val_mean_squared_error: 3.3911\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.9701 - mean_squared_error: 2.9701 - val_loss: 3.4589 - val_mean_squared_error: 3.4589\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 3.0098 - mean_squared_error: 3.0098 - val_loss: 3.3628 - val_mean_squared_error: 3.3628\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.6797 - mean_squared_error: 2.6797 - val_loss: 3.7888 - val_mean_squared_error: 3.7888\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.8772 - mean_squared_error: 2.8772 - val_loss: 3.7177 - val_mean_squared_error: 3.7177\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 2.8025 - mean_squared_error: 2.8025 - val_loss: 3.3245 - val_mean_squared_error: 3.3245\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.7248 - mean_squared_error: 2.7248 - val_loss: 3.8280 - val_mean_squared_error: 3.8280\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.9976 - mean_squared_error: 2.9976 - val_loss: 3.5323 - val_mean_squared_error: 3.5323\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 3.0344 - mean_squared_error: 3.0344 - val_loss: 2.7860 - val_mean_squared_error: 2.7860\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 2.5227 - mean_squared_error: 2.5227 - val_loss: 3.0783 - val_mean_squared_error: 3.0783\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.5841 - mean_squared_error: 2.5841 - val_loss: 4.0020 - val_mean_squared_error: 4.0020\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.6605 - mean_squared_error: 2.6605 - val_loss: 5.2484 - val_mean_squared_error: 5.2484\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 2.4419 - mean_squared_error: 2.4419 - val_loss: 3.0998 - val_mean_squared_error: 3.0998\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.4911 - mean_squared_error: 2.4911 - val_loss: 3.5846 - val_mean_squared_error: 3.5846\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.4740 - mean_squared_error: 2.4740 - val_loss: 3.4792 - val_mean_squared_error: 3.4792\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.3997 - mean_squared_error: 2.3997 - val_loss: 2.9580 - val_mean_squared_error: 2.9580\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.3691 - mean_squared_error: 2.3691 - val_loss: 3.5909 - val_mean_squared_error: 3.5909\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.5197 - mean_squared_error: 2.5197 - val_loss: 2.8392 - val_mean_squared_error: 2.8392\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.6717 - mean_squared_error: 2.6717 - val_loss: 3.2901 - val_mean_squared_error: 3.2901\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.4483 - mean_squared_error: 2.4483 - val_loss: 4.2175 - val_mean_squared_error: 4.2175\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.3486 - mean_squared_error: 2.3486 - val_loss: 2.8462 - val_mean_squared_error: 2.8462\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.4862 - mean_squared_error: 2.4862 - val_loss: 3.8960 - val_mean_squared_error: 3.8960\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.3995 - mean_squared_error: 2.3995 - val_loss: 3.1223 - val_mean_squared_error: 3.1223\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.2900 - mean_squared_error: 2.2900 - val_loss: 2.8725 - val_mean_squared_error: 2.8725\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.5676 - mean_squared_error: 2.5676 - val_loss: 2.9722 - val_mean_squared_error: 2.9722\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.2354 - mean_squared_error: 2.2354 - val_loss: 3.1996 - val_mean_squared_error: 3.1996\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.2425 - mean_squared_error: 2.2425 - val_loss: 3.0013 - val_mean_squared_error: 3.0013\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 2.2377 - mean_squared_error: 2.2377 - val_loss: 3.5286 - val_mean_squared_error: 3.5286\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.1280 - mean_squared_error: 2.1280 - val_loss: 2.7657 - val_mean_squared_error: 2.7657\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.0992 - mean_squared_error: 2.0992 - val_loss: 3.3229 - val_mean_squared_error: 3.3229\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.9415 - mean_squared_error: 1.9415 - val_loss: 2.8349 - val_mean_squared_error: 2.8349\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.0532 - mean_squared_error: 2.0532 - val_loss: 3.2056 - val_mean_squared_error: 3.2056\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.0180 - mean_squared_error: 2.0180 - val_loss: 4.2600 - val_mean_squared_error: 4.2600\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.0865 - mean_squared_error: 2.0865 - val_loss: 2.9344 - val_mean_squared_error: 2.9344\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.0300 - mean_squared_error: 2.0300 - val_loss: 2.7495 - val_mean_squared_error: 2.7495\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.0809 - mean_squared_error: 2.0809 - val_loss: 2.9683 - val_mean_squared_error: 2.9683\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 2.3341 - mean_squared_error: 2.3341 - val_loss: 3.8427 - val_mean_squared_error: 3.8427\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 2.1265 - mean_squared_error: 2.1265 - val_loss: 2.5118 - val_mean_squared_error: 2.5118\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.9310 - mean_squared_error: 1.9310 - val_loss: 3.1657 - val_mean_squared_error: 3.1657\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 2.0925 - mean_squared_error: 2.0925 - val_loss: 2.5797 - val_mean_squared_error: 2.5797\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 2.0322 - mean_squared_error: 2.0322 - val_loss: 2.7110 - val_mean_squared_error: 2.7110\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.9913 - mean_squared_error: 1.9913 - val_loss: 3.2676 - val_mean_squared_error: 3.2676\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.0129 - mean_squared_error: 2.0129 - val_loss: 3.0740 - val_mean_squared_error: 3.0740\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.9097 - mean_squared_error: 1.9097 - val_loss: 2.5175 - val_mean_squared_error: 2.5175\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.9833 - mean_squared_error: 1.9833 - val_loss: 2.5083 - val_mean_squared_error: 2.5083\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.9875 - mean_squared_error: 1.9875 - val_loss: 2.7847 - val_mean_squared_error: 2.7847\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.8752 - mean_squared_error: 1.8752 - val_loss: 2.6329 - val_mean_squared_error: 2.6329\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.7549 - mean_squared_error: 1.7549 - val_loss: 3.0387 - val_mean_squared_error: 3.0387\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.8789 - mean_squared_error: 1.8789 - val_loss: 2.9679 - val_mean_squared_error: 2.9679\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.9738 - mean_squared_error: 1.9738 - val_loss: 3.4140 - val_mean_squared_error: 3.4140\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 2.0356 - mean_squared_error: 2.0356 - val_loss: 3.3760 - val_mean_squared_error: 3.3760\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.7705 - mean_squared_error: 1.7705 - val_loss: 2.8077 - val_mean_squared_error: 2.8077\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.8955 - mean_squared_error: 1.8955 - val_loss: 2.9254 - val_mean_squared_error: 2.9254\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.7404 - mean_squared_error: 1.7404 - val_loss: 3.0125 - val_mean_squared_error: 3.0125\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.8949 - mean_squared_error: 1.8949 - val_loss: 2.6334 - val_mean_squared_error: 2.6334\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.7474 - mean_squared_error: 1.7474 - val_loss: 2.8965 - val_mean_squared_error: 2.8965\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.7579 - mean_squared_error: 1.7579 - val_loss: 2.6440 - val_mean_squared_error: 2.6440\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.8640 - mean_squared_error: 1.8640 - val_loss: 2.8121 - val_mean_squared_error: 2.8121\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.7535 - mean_squared_error: 1.7535 - val_loss: 2.4671 - val_mean_squared_error: 2.4671\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.7692 - mean_squared_error: 1.7692 - val_loss: 2.8057 - val_mean_squared_error: 2.8057\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.6355 - mean_squared_error: 1.6355 - val_loss: 3.0309 - val_mean_squared_error: 3.0309\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.6445 - mean_squared_error: 1.6445 - val_loss: 2.6483 - val_mean_squared_error: 2.6483\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.5323 - mean_squared_error: 1.5323 - val_loss: 2.7213 - val_mean_squared_error: 2.7213\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.6025 - mean_squared_error: 1.6025 - val_loss: 3.8314 - val_mean_squared_error: 3.8314\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.6721 - mean_squared_error: 1.6721 - val_loss: 2.8927 - val_mean_squared_error: 2.8927\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.5745 - mean_squared_error: 1.5745 - val_loss: 2.6747 - val_mean_squared_error: 2.6747\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.7700 - mean_squared_error: 1.7700 - val_loss: 2.7501 - val_mean_squared_error: 2.7501\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.6259 - mean_squared_error: 1.6259 - val_loss: 2.8910 - val_mean_squared_error: 2.8910\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.6281 - mean_squared_error: 1.6281 - val_loss: 2.4478 - val_mean_squared_error: 2.4478\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.5460 - mean_squared_error: 1.5460 - val_loss: 3.8795 - val_mean_squared_error: 3.8795\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.5766 - mean_squared_error: 1.5766 - val_loss: 2.5229 - val_mean_squared_error: 2.5229\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.5617 - mean_squared_error: 1.5617 - val_loss: 3.3485 - val_mean_squared_error: 3.3485\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.6465 - mean_squared_error: 1.6465 - val_loss: 2.5478 - val_mean_squared_error: 2.5478\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.6020 - mean_squared_error: 1.6020 - val_loss: 2.9892 - val_mean_squared_error: 2.9892\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.5807 - mean_squared_error: 1.5807 - val_loss: 3.1029 - val_mean_squared_error: 3.1029\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.5727 - mean_squared_error: 1.5727 - val_loss: 3.3841 - val_mean_squared_error: 3.3841\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.6297 - mean_squared_error: 1.6297 - val_loss: 2.7844 - val_mean_squared_error: 2.7844\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.4565 - mean_squared_error: 1.4565 - val_loss: 2.7092 - val_mean_squared_error: 2.7092\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.6278 - mean_squared_error: 1.6278 - val_loss: 3.3265 - val_mean_squared_error: 3.3265\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.5175 - mean_squared_error: 1.5175 - val_loss: 2.3322 - val_mean_squared_error: 2.3322\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.5164 - mean_squared_error: 1.5164 - val_loss: 2.3866 - val_mean_squared_error: 2.3866\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 1.5894 - mean_squared_error: 1.5894 - val_loss: 2.7133 - val_mean_squared_error: 2.7133\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.5371 - mean_squared_error: 1.5371 - val_loss: 2.4886 - val_mean_squared_error: 2.4886\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.4861 - mean_squared_error: 1.4861 - val_loss: 2.6635 - val_mean_squared_error: 2.6635\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.4710 - mean_squared_error: 1.4710 - val_loss: 2.5586 - val_mean_squared_error: 2.5586\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.5337 - mean_squared_error: 1.5337 - val_loss: 2.4900 - val_mean_squared_error: 2.4900\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.5644 - mean_squared_error: 1.5644 - val_loss: 2.5283 - val_mean_squared_error: 2.5283\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.4013 - mean_squared_error: 1.4013 - val_loss: 2.6243 - val_mean_squared_error: 2.6243\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.4435 - mean_squared_error: 1.4435 - val_loss: 2.6447 - val_mean_squared_error: 2.6447\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3274 - mean_squared_error: 1.3274 - val_loss: 2.2819 - val_mean_squared_error: 2.2819\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.4245 - mean_squared_error: 1.4245 - val_loss: 2.4777 - val_mean_squared_error: 2.4777\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.3652 - mean_squared_error: 1.3652 - val_loss: 2.3819 - val_mean_squared_error: 2.3819\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.3907 - mean_squared_error: 1.3907 - val_loss: 2.3582 - val_mean_squared_error: 2.3582\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 1.3794 - mean_squared_error: 1.3794 - val_loss: 2.6980 - val_mean_squared_error: 2.6980\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.3927 - mean_squared_error: 1.3927 - val_loss: 2.4635 - val_mean_squared_error: 2.4635\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3564 - mean_squared_error: 1.3564 - val_loss: 2.3165 - val_mean_squared_error: 2.3165\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.4451 - mean_squared_error: 1.4451 - val_loss: 2.6445 - val_mean_squared_error: 2.6445\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.4260 - mean_squared_error: 1.4260 - val_loss: 2.7378 - val_mean_squared_error: 2.7378\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.4541 - mean_squared_error: 1.4541 - val_loss: 2.6263 - val_mean_squared_error: 2.6263\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.5385 - mean_squared_error: 1.5385 - val_loss: 2.4057 - val_mean_squared_error: 2.4057\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3564 - mean_squared_error: 1.3564 - val_loss: 3.0521 - val_mean_squared_error: 3.0521\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.4513 - mean_squared_error: 1.4513 - val_loss: 2.6703 - val_mean_squared_error: 2.6703\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.4942 - mean_squared_error: 1.4942 - val_loss: 2.4928 - val_mean_squared_error: 2.4928\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2999 - mean_squared_error: 1.2999 - val_loss: 2.4197 - val_mean_squared_error: 2.4197\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3628 - mean_squared_error: 1.3628 - val_loss: 3.4887 - val_mean_squared_error: 3.4887\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.2977 - mean_squared_error: 1.2977 - val_loss: 2.1586 - val_mean_squared_error: 2.1586\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2567 - mean_squared_error: 1.2567 - val_loss: 2.6185 - val_mean_squared_error: 2.6185\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.2828 - mean_squared_error: 1.2828 - val_loss: 2.3601 - val_mean_squared_error: 2.3601\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3519 - mean_squared_error: 1.3519 - val_loss: 2.4533 - val_mean_squared_error: 2.4533\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3584 - mean_squared_error: 1.3584 - val_loss: 2.4671 - val_mean_squared_error: 2.4671\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3677 - mean_squared_error: 1.3677 - val_loss: 2.3614 - val_mean_squared_error: 2.3614\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.3956 - mean_squared_error: 1.3956 - val_loss: 2.7726 - val_mean_squared_error: 2.7726\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3658 - mean_squared_error: 1.3658 - val_loss: 2.4168 - val_mean_squared_error: 2.4168\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2238 - mean_squared_error: 1.2238 - val_loss: 2.4884 - val_mean_squared_error: 2.4884\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.3247 - mean_squared_error: 1.3247 - val_loss: 2.4641 - val_mean_squared_error: 2.4641\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3567 - mean_squared_error: 1.3567 - val_loss: 2.8494 - val_mean_squared_error: 2.8494\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.3692 - mean_squared_error: 1.3692 - val_loss: 2.7410 - val_mean_squared_error: 2.7410\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.3087 - mean_squared_error: 1.3087 - val_loss: 2.2627 - val_mean_squared_error: 2.2627\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.1830 - mean_squared_error: 1.1830 - val_loss: 2.3021 - val_mean_squared_error: 2.3021\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2520 - mean_squared_error: 1.2520 - val_loss: 2.3531 - val_mean_squared_error: 2.3531\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.2621 - mean_squared_error: 1.2621 - val_loss: 4.2621 - val_mean_squared_error: 4.2621\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.2021 - mean_squared_error: 1.2021 - val_loss: 2.7003 - val_mean_squared_error: 2.7003\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1968 - mean_squared_error: 1.1968 - val_loss: 2.1691 - val_mean_squared_error: 2.1691\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.1847 - mean_squared_error: 1.1847 - val_loss: 2.3046 - val_mean_squared_error: 2.3046\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1776 - mean_squared_error: 1.1776 - val_loss: 2.5877 - val_mean_squared_error: 2.5877\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.2637 - mean_squared_error: 1.2637 - val_loss: 2.6132 - val_mean_squared_error: 2.6132\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.3483 - mean_squared_error: 1.3483 - val_loss: 2.9845 - val_mean_squared_error: 2.9845\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.2702 - mean_squared_error: 1.2702 - val_loss: 2.9992 - val_mean_squared_error: 2.9992\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.3695 - mean_squared_error: 1.3695 - val_loss: 2.3113 - val_mean_squared_error: 2.3113\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.1566 - mean_squared_error: 1.1566 - val_loss: 2.6710 - val_mean_squared_error: 2.6710\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.2319 - mean_squared_error: 1.2319 - val_loss: 2.5394 - val_mean_squared_error: 2.5394\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2275 - mean_squared_error: 1.2275 - val_loss: 2.4366 - val_mean_squared_error: 2.4366\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2239 - mean_squared_error: 1.2239 - val_loss: 2.5364 - val_mean_squared_error: 2.5364\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.3276 - mean_squared_error: 1.3276 - val_loss: 3.2909 - val_mean_squared_error: 3.2909\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.5259 - mean_squared_error: 1.5259 - val_loss: 2.6021 - val_mean_squared_error: 2.6021\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.2167 - mean_squared_error: 1.2167 - val_loss: 3.0366 - val_mean_squared_error: 3.0366\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.2651 - mean_squared_error: 1.2651 - val_loss: 2.2163 - val_mean_squared_error: 2.2163\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.0843 - mean_squared_error: 1.0843 - val_loss: 2.3416 - val_mean_squared_error: 2.3416\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1444 - mean_squared_error: 1.1444 - val_loss: 2.3150 - val_mean_squared_error: 2.3150\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1340 - mean_squared_error: 1.1340 - val_loss: 2.2545 - val_mean_squared_error: 2.2545\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.1370 - mean_squared_error: 1.1370 - val_loss: 2.3633 - val_mean_squared_error: 2.3633\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1544 - mean_squared_error: 1.1544 - val_loss: 2.2476 - val_mean_squared_error: 2.2476\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1134 - mean_squared_error: 1.1134 - val_loss: 2.3600 - val_mean_squared_error: 2.3600\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1362 - mean_squared_error: 1.1362 - val_loss: 2.2166 - val_mean_squared_error: 2.2166\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.1397 - mean_squared_error: 1.1397 - val_loss: 2.2627 - val_mean_squared_error: 2.2627\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2362 - mean_squared_error: 1.2362 - val_loss: 2.5442 - val_mean_squared_error: 2.5442\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.2119 - mean_squared_error: 1.2119 - val_loss: 2.2014 - val_mean_squared_error: 2.2014\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.1640 - mean_squared_error: 1.1640 - val_loss: 2.2560 - val_mean_squared_error: 2.2560\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1618 - mean_squared_error: 1.1618 - val_loss: 2.4718 - val_mean_squared_error: 2.4718\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 2.5675 - val_mean_squared_error: 2.5675\n",
            "==================================================\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_24 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 1548.3708 - mean_squared_error: 1548.3711 - val_loss: 1156.1791 - val_mean_squared_error: 1156.1791\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 245.7980 - mean_squared_error: 245.7980 - val_loss: 281.6049 - val_mean_squared_error: 281.6049\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 36.3173 - mean_squared_error: 36.3173 - val_loss: 111.4351 - val_mean_squared_error: 111.4351\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 20.0018 - mean_squared_error: 20.0018 - val_loss: 60.3180 - val_mean_squared_error: 60.3180\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 18.1614 - mean_squared_error: 18.1614 - val_loss: 32.5994 - val_mean_squared_error: 32.5994\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 438us/sample - loss: 17.8158 - mean_squared_error: 17.8158 - val_loss: 31.7266 - val_mean_squared_error: 31.7266\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 14.8116 - mean_squared_error: 14.8116 - val_loss: 14.3013 - val_mean_squared_error: 14.3013\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 14.7469 - mean_squared_error: 14.7469 - val_loss: 11.4701 - val_mean_squared_error: 11.4701\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 14.4837 - mean_squared_error: 14.4837 - val_loss: 10.7606 - val_mean_squared_error: 10.7606\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 12.6705 - mean_squared_error: 12.6706 - val_loss: 12.2870 - val_mean_squared_error: 12.2870\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 12.4599 - mean_squared_error: 12.4599 - val_loss: 15.1529 - val_mean_squared_error: 15.1529\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 11.9424 - mean_squared_error: 11.9424 - val_loss: 10.5189 - val_mean_squared_error: 10.5189\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 11.2819 - mean_squared_error: 11.2819 - val_loss: 11.5669 - val_mean_squared_error: 11.5669\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 11.7593 - mean_squared_error: 11.7593 - val_loss: 16.9834 - val_mean_squared_error: 16.9834\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 10.4187 - mean_squared_error: 10.4187 - val_loss: 14.0712 - val_mean_squared_error: 14.0712\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 10.2840 - mean_squared_error: 10.2840 - val_loss: 10.3311 - val_mean_squared_error: 10.3311\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 9.2396 - mean_squared_error: 9.2396 - val_loss: 11.2663 - val_mean_squared_error: 11.2663\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 8.5797 - mean_squared_error: 8.5797 - val_loss: 10.9380 - val_mean_squared_error: 10.9380\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 8.4702 - mean_squared_error: 8.4702 - val_loss: 9.1662 - val_mean_squared_error: 9.1662\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 7.7805 - mean_squared_error: 7.7805 - val_loss: 9.2118 - val_mean_squared_error: 9.2118\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 8.1172 - mean_squared_error: 8.1172 - val_loss: 6.9148 - val_mean_squared_error: 6.9148\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 7.9806 - mean_squared_error: 7.9806 - val_loss: 8.9237 - val_mean_squared_error: 8.9237\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 6.7613 - mean_squared_error: 6.7613 - val_loss: 8.1065 - val_mean_squared_error: 8.1065\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 6.8160 - mean_squared_error: 6.8160 - val_loss: 6.1154 - val_mean_squared_error: 6.1154\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 6.3600 - mean_squared_error: 6.3600 - val_loss: 6.6461 - val_mean_squared_error: 6.6461\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 6.5613 - mean_squared_error: 6.5613 - val_loss: 7.0762 - val_mean_squared_error: 7.0762\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 5.7338 - mean_squared_error: 5.7338 - val_loss: 9.4074 - val_mean_squared_error: 9.4074\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 388us/sample - loss: 5.2685 - mean_squared_error: 5.2685 - val_loss: 5.1798 - val_mean_squared_error: 5.1798\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 5.2313 - mean_squared_error: 5.2313 - val_loss: 7.1963 - val_mean_squared_error: 7.1963\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 5.1978 - mean_squared_error: 5.1978 - val_loss: 6.6431 - val_mean_squared_error: 6.6431\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 4.6756 - mean_squared_error: 4.6756 - val_loss: 6.3529 - val_mean_squared_error: 6.3529\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 4.7753 - mean_squared_error: 4.7753 - val_loss: 4.6637 - val_mean_squared_error: 4.6637\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 4.5253 - mean_squared_error: 4.5253 - val_loss: 4.4375 - val_mean_squared_error: 4.4375\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 4.4532 - mean_squared_error: 4.4532 - val_loss: 4.0647 - val_mean_squared_error: 4.0647\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 4.3140 - mean_squared_error: 4.3140 - val_loss: 4.8424 - val_mean_squared_error: 4.8424\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 4.0796 - mean_squared_error: 4.0796 - val_loss: 3.9599 - val_mean_squared_error: 3.9599\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 4.6280 - mean_squared_error: 4.6280 - val_loss: 5.0406 - val_mean_squared_error: 5.0406\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 4.1055 - mean_squared_error: 4.1055 - val_loss: 3.8462 - val_mean_squared_error: 3.8462\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 4.0658 - mean_squared_error: 4.0658 - val_loss: 6.2382 - val_mean_squared_error: 6.2382\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 4.0065 - mean_squared_error: 4.0065 - val_loss: 3.9531 - val_mean_squared_error: 3.9531\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 3.8691 - mean_squared_error: 3.8691 - val_loss: 3.9732 - val_mean_squared_error: 3.9732\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 3.8645 - mean_squared_error: 3.8645 - val_loss: 4.1480 - val_mean_squared_error: 4.1480\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 3.8625 - mean_squared_error: 3.8625 - val_loss: 3.6529 - val_mean_squared_error: 3.6529\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 3.7374 - mean_squared_error: 3.7374 - val_loss: 3.8315 - val_mean_squared_error: 3.8315\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 3.5695 - mean_squared_error: 3.5695 - val_loss: 3.6914 - val_mean_squared_error: 3.6914\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 3.3417 - mean_squared_error: 3.3417 - val_loss: 3.7414 - val_mean_squared_error: 3.7414\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 3.1031 - mean_squared_error: 3.1031 - val_loss: 3.2179 - val_mean_squared_error: 3.2179\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 3.2782 - mean_squared_error: 3.2782 - val_loss: 3.5290 - val_mean_squared_error: 3.5290\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 3.4136 - mean_squared_error: 3.4136 - val_loss: 3.7424 - val_mean_squared_error: 3.7424\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 3.7005 - mean_squared_error: 3.7005 - val_loss: 3.4395 - val_mean_squared_error: 3.4395\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 3.1194 - mean_squared_error: 3.1194 - val_loss: 3.3351 - val_mean_squared_error: 3.3351\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 3.1954 - mean_squared_error: 3.1954 - val_loss: 3.3580 - val_mean_squared_error: 3.3580\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 3.2109 - mean_squared_error: 3.2109 - val_loss: 3.8091 - val_mean_squared_error: 3.8091\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.0753 - mean_squared_error: 3.0753 - val_loss: 3.5325 - val_mean_squared_error: 3.5325\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.9187 - mean_squared_error: 2.9187 - val_loss: 3.0864 - val_mean_squared_error: 3.0864\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.9510 - mean_squared_error: 2.9510 - val_loss: 3.5854 - val_mean_squared_error: 3.5854\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.8677 - mean_squared_error: 2.8677 - val_loss: 3.2050 - val_mean_squared_error: 3.2050\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.9193 - mean_squared_error: 2.9193 - val_loss: 3.2532 - val_mean_squared_error: 3.2532\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 2.6550 - mean_squared_error: 2.6550 - val_loss: 3.0065 - val_mean_squared_error: 3.0065\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.8085 - mean_squared_error: 2.8085 - val_loss: 3.0291 - val_mean_squared_error: 3.0291\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.8654 - mean_squared_error: 2.8654 - val_loss: 3.2678 - val_mean_squared_error: 3.2678\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.7776 - mean_squared_error: 2.7776 - val_loss: 3.3535 - val_mean_squared_error: 3.3535\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.6180 - mean_squared_error: 2.6180 - val_loss: 3.5887 - val_mean_squared_error: 3.5887\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.8883 - mean_squared_error: 2.8883 - val_loss: 3.5392 - val_mean_squared_error: 3.5392\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.5739 - mean_squared_error: 2.5739 - val_loss: 3.4358 - val_mean_squared_error: 3.4358\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.9932 - mean_squared_error: 2.9932 - val_loss: 4.0325 - val_mean_squared_error: 4.0325\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.7491 - mean_squared_error: 2.7491 - val_loss: 3.0668 - val_mean_squared_error: 3.0668\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.6148 - mean_squared_error: 2.6148 - val_loss: 3.3276 - val_mean_squared_error: 3.3276\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.4949 - mean_squared_error: 2.4949 - val_loss: 3.4407 - val_mean_squared_error: 3.4407\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.5987 - mean_squared_error: 2.5987 - val_loss: 3.2845 - val_mean_squared_error: 3.2845\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.5672 - mean_squared_error: 2.5672 - val_loss: 2.9964 - val_mean_squared_error: 2.9964\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.5957 - mean_squared_error: 2.5957 - val_loss: 4.2649 - val_mean_squared_error: 4.2649\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.5104 - mean_squared_error: 2.5104 - val_loss: 3.0114 - val_mean_squared_error: 3.0114\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.3195 - mean_squared_error: 2.3195 - val_loss: 2.8834 - val_mean_squared_error: 2.8834\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.3055 - mean_squared_error: 2.3055 - val_loss: 3.0120 - val_mean_squared_error: 3.0120\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.2975 - mean_squared_error: 2.2975 - val_loss: 3.1073 - val_mean_squared_error: 3.1073\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.4107 - mean_squared_error: 2.4107 - val_loss: 3.0965 - val_mean_squared_error: 3.0965\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.3465 - mean_squared_error: 2.3465 - val_loss: 3.0390 - val_mean_squared_error: 3.0390\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.2018 - mean_squared_error: 2.2018 - val_loss: 2.7086 - val_mean_squared_error: 2.7086\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 2.2317 - mean_squared_error: 2.2317 - val_loss: 2.9926 - val_mean_squared_error: 2.9926\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.4662 - mean_squared_error: 2.4662 - val_loss: 2.9517 - val_mean_squared_error: 2.9517\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.1138 - mean_squared_error: 2.1138 - val_loss: 2.7093 - val_mean_squared_error: 2.7093\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.2426 - mean_squared_error: 2.2426 - val_loss: 3.0153 - val_mean_squared_error: 3.0153\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.2328 - mean_squared_error: 2.2328 - val_loss: 2.7399 - val_mean_squared_error: 2.7399\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.2082 - mean_squared_error: 2.2082 - val_loss: 2.7605 - val_mean_squared_error: 2.7605\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.3247 - mean_squared_error: 2.3247 - val_loss: 3.1930 - val_mean_squared_error: 3.1930\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.3283 - mean_squared_error: 2.3283 - val_loss: 2.6067 - val_mean_squared_error: 2.6067\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.1794 - mean_squared_error: 2.1794 - val_loss: 2.7591 - val_mean_squared_error: 2.7591\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.0095 - mean_squared_error: 2.0095 - val_loss: 2.4813 - val_mean_squared_error: 2.4813\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.8752 - mean_squared_error: 1.8752 - val_loss: 2.4327 - val_mean_squared_error: 2.4327\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.1642 - mean_squared_error: 2.1642 - val_loss: 3.1341 - val_mean_squared_error: 3.1341\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 2.1398 - mean_squared_error: 2.1398 - val_loss: 3.2910 - val_mean_squared_error: 3.2910\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.2958 - mean_squared_error: 2.2958 - val_loss: 3.0944 - val_mean_squared_error: 3.0944\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.9255 - mean_squared_error: 1.9255 - val_loss: 2.7855 - val_mean_squared_error: 2.7855\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.1187 - mean_squared_error: 2.1187 - val_loss: 2.8270 - val_mean_squared_error: 2.8270\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.0378 - mean_squared_error: 2.0378 - val_loss: 2.8427 - val_mean_squared_error: 2.8427\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.1118 - mean_squared_error: 2.1118 - val_loss: 2.8408 - val_mean_squared_error: 2.8408\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.9860 - mean_squared_error: 1.9860 - val_loss: 2.5852 - val_mean_squared_error: 2.5852\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.9533 - mean_squared_error: 1.9533 - val_loss: 2.8446 - val_mean_squared_error: 2.8446\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.9524 - mean_squared_error: 1.9524 - val_loss: 2.6007 - val_mean_squared_error: 2.6007\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.9482 - mean_squared_error: 1.9482 - val_loss: 2.7203 - val_mean_squared_error: 2.7203\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.9284 - mean_squared_error: 1.9284 - val_loss: 2.7030 - val_mean_squared_error: 2.7030\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 1.8988 - mean_squared_error: 1.8988 - val_loss: 2.4567 - val_mean_squared_error: 2.4567\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.0584 - mean_squared_error: 2.0584 - val_loss: 2.7595 - val_mean_squared_error: 2.7595\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.9984 - mean_squared_error: 1.9984 - val_loss: 2.6909 - val_mean_squared_error: 2.6909\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.9414 - mean_squared_error: 1.9414 - val_loss: 2.4844 - val_mean_squared_error: 2.4844\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.8803 - mean_squared_error: 1.8803 - val_loss: 2.6010 - val_mean_squared_error: 2.6010\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.9343 - mean_squared_error: 1.9344 - val_loss: 3.6242 - val_mean_squared_error: 3.6242\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.8398 - mean_squared_error: 1.8398 - val_loss: 2.5411 - val_mean_squared_error: 2.5411\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.8258 - mean_squared_error: 1.8258 - val_loss: 2.2884 - val_mean_squared_error: 2.2884\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.8818 - mean_squared_error: 1.8818 - val_loss: 2.8428 - val_mean_squared_error: 2.8428\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.9099 - mean_squared_error: 1.9099 - val_loss: 2.6582 - val_mean_squared_error: 2.6582\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.8772 - mean_squared_error: 1.8772 - val_loss: 2.6245 - val_mean_squared_error: 2.6245\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.9229 - mean_squared_error: 1.9229 - val_loss: 2.8281 - val_mean_squared_error: 2.8281\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.7317 - mean_squared_error: 1.7317 - val_loss: 2.3787 - val_mean_squared_error: 2.3787\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.8594 - mean_squared_error: 1.8594 - val_loss: 3.0363 - val_mean_squared_error: 3.0363\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.6891 - mean_squared_error: 1.6891 - val_loss: 2.4965 - val_mean_squared_error: 2.4965\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.6784 - mean_squared_error: 1.6784 - val_loss: 2.3401 - val_mean_squared_error: 2.3401\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.7296 - mean_squared_error: 1.7296 - val_loss: 2.4673 - val_mean_squared_error: 2.4673\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.6573 - mean_squared_error: 1.6573 - val_loss: 2.3501 - val_mean_squared_error: 2.3501\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.6343 - mean_squared_error: 1.6343 - val_loss: 2.5335 - val_mean_squared_error: 2.5335\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7270 - mean_squared_error: 1.7270 - val_loss: 2.2410 - val_mean_squared_error: 2.2410\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.6271 - mean_squared_error: 1.6271 - val_loss: 2.4101 - val_mean_squared_error: 2.4101\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.5422 - mean_squared_error: 1.5422 - val_loss: 2.2986 - val_mean_squared_error: 2.2986\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.6909 - mean_squared_error: 1.6909 - val_loss: 3.3017 - val_mean_squared_error: 3.3017\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.6292 - mean_squared_error: 1.6292 - val_loss: 2.8851 - val_mean_squared_error: 2.8851\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.8039 - mean_squared_error: 1.8039 - val_loss: 2.4806 - val_mean_squared_error: 2.4806\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.8329 - mean_squared_error: 1.8329 - val_loss: 2.7541 - val_mean_squared_error: 2.7541\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.6883 - mean_squared_error: 1.6883 - val_loss: 2.8282 - val_mean_squared_error: 2.8282\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.6408 - mean_squared_error: 1.6408 - val_loss: 2.3171 - val_mean_squared_error: 2.3171\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.6230 - mean_squared_error: 1.6230 - val_loss: 2.4205 - val_mean_squared_error: 2.4205\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5841 - mean_squared_error: 1.5841 - val_loss: 2.7761 - val_mean_squared_error: 2.7761\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.6956 - mean_squared_error: 1.6956 - val_loss: 2.7674 - val_mean_squared_error: 2.7674\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.5810 - mean_squared_error: 1.5810 - val_loss: 2.4463 - val_mean_squared_error: 2.4463\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.7988 - mean_squared_error: 1.7988 - val_loss: 2.5492 - val_mean_squared_error: 2.5492\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.6164 - mean_squared_error: 1.6164 - val_loss: 2.6348 - val_mean_squared_error: 2.6348\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.7030 - mean_squared_error: 1.7030 - val_loss: 2.4139 - val_mean_squared_error: 2.4139\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7286 - mean_squared_error: 1.7286 - val_loss: 2.3529 - val_mean_squared_error: 2.3529\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.5925 - mean_squared_error: 1.5925 - val_loss: 2.4574 - val_mean_squared_error: 2.4574\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.4758 - mean_squared_error: 1.4758 - val_loss: 2.9730 - val_mean_squared_error: 2.9730\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.4480 - mean_squared_error: 1.4480 - val_loss: 2.7587 - val_mean_squared_error: 2.7587\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.5394 - mean_squared_error: 1.5394 - val_loss: 2.4784 - val_mean_squared_error: 2.4784\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.4725 - mean_squared_error: 1.4725 - val_loss: 2.5789 - val_mean_squared_error: 2.5789\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4612 - mean_squared_error: 1.4612 - val_loss: 2.2357 - val_mean_squared_error: 2.2357\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.5175 - mean_squared_error: 1.5175 - val_loss: 2.8718 - val_mean_squared_error: 2.8718\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.5376 - mean_squared_error: 1.5376 - val_loss: 2.3946 - val_mean_squared_error: 2.3946\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.4616 - mean_squared_error: 1.4616 - val_loss: 3.1089 - val_mean_squared_error: 3.1089\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.4248 - mean_squared_error: 1.4248 - val_loss: 2.3458 - val_mean_squared_error: 2.3458\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.6789 - mean_squared_error: 1.6789 - val_loss: 3.0164 - val_mean_squared_error: 3.0164\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.5584 - mean_squared_error: 1.5584 - val_loss: 2.7854 - val_mean_squared_error: 2.7854\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.4458 - mean_squared_error: 1.4458 - val_loss: 2.8798 - val_mean_squared_error: 2.8798\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.4487 - mean_squared_error: 1.4487 - val_loss: 2.4868 - val_mean_squared_error: 2.4868\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.4976 - mean_squared_error: 1.4976 - val_loss: 2.4153 - val_mean_squared_error: 2.4153\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.4942 - mean_squared_error: 1.4942 - val_loss: 2.5245 - val_mean_squared_error: 2.5245\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3714 - mean_squared_error: 1.3714 - val_loss: 2.3159 - val_mean_squared_error: 2.3159\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4623 - mean_squared_error: 1.4623 - val_loss: 2.4093 - val_mean_squared_error: 2.4093\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.4493 - mean_squared_error: 1.4493 - val_loss: 2.7375 - val_mean_squared_error: 2.7375\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.4627 - mean_squared_error: 1.4627 - val_loss: 2.4074 - val_mean_squared_error: 2.4074\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.3487 - mean_squared_error: 1.3487 - val_loss: 2.2697 - val_mean_squared_error: 2.2697\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.5726 - mean_squared_error: 1.5726 - val_loss: 2.5347 - val_mean_squared_error: 2.5347\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.3764 - mean_squared_error: 1.3764 - val_loss: 2.1504 - val_mean_squared_error: 2.1504\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.3441 - mean_squared_error: 1.3441 - val_loss: 2.3102 - val_mean_squared_error: 2.3102\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.4289 - mean_squared_error: 1.4289 - val_loss: 2.3331 - val_mean_squared_error: 2.3331\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.5650 - mean_squared_error: 1.5650 - val_loss: 2.5204 - val_mean_squared_error: 2.5204\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.4865 - mean_squared_error: 1.4865 - val_loss: 2.6545 - val_mean_squared_error: 2.6545\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3779 - mean_squared_error: 1.3779 - val_loss: 2.2888 - val_mean_squared_error: 2.2888\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.4379 - mean_squared_error: 1.4379 - val_loss: 2.3527 - val_mean_squared_error: 2.3527\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.3141 - mean_squared_error: 1.3141 - val_loss: 2.2629 - val_mean_squared_error: 2.2629\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3332 - mean_squared_error: 1.3332 - val_loss: 2.2490 - val_mean_squared_error: 2.2490\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.3288 - mean_squared_error: 1.3288 - val_loss: 2.3845 - val_mean_squared_error: 2.3845\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.3817 - mean_squared_error: 1.3817 - val_loss: 2.2372 - val_mean_squared_error: 2.2372\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3642 - mean_squared_error: 1.3642 - val_loss: 2.6834 - val_mean_squared_error: 2.6834\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3434 - mean_squared_error: 1.3434 - val_loss: 2.4287 - val_mean_squared_error: 2.4287\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.4083 - mean_squared_error: 1.4083 - val_loss: 2.9880 - val_mean_squared_error: 2.9880\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.5052 - mean_squared_error: 1.5052 - val_loss: 3.1096 - val_mean_squared_error: 3.1096\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.3368 - mean_squared_error: 1.3368 - val_loss: 2.5738 - val_mean_squared_error: 2.5738\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.4075 - mean_squared_error: 1.4075 - val_loss: 2.3817 - val_mean_squared_error: 2.3817\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.3837 - mean_squared_error: 1.3837 - val_loss: 2.3551 - val_mean_squared_error: 2.3551\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.2865 - mean_squared_error: 1.2865 - val_loss: 2.1129 - val_mean_squared_error: 2.1129\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.3112 - mean_squared_error: 1.3112 - val_loss: 2.4732 - val_mean_squared_error: 2.4732\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.2904 - mean_squared_error: 1.2904 - val_loss: 2.2208 - val_mean_squared_error: 2.2208\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.3616 - mean_squared_error: 1.3616 - val_loss: 2.5728 - val_mean_squared_error: 2.5728\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.3124 - mean_squared_error: 1.3124 - val_loss: 2.7111 - val_mean_squared_error: 2.7111\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.2633 - mean_squared_error: 1.2633 - val_loss: 2.4040 - val_mean_squared_error: 2.4040\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.3398 - mean_squared_error: 1.3398 - val_loss: 2.2973 - val_mean_squared_error: 2.2973\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3226 - mean_squared_error: 1.3226 - val_loss: 2.6107 - val_mean_squared_error: 2.6107\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3733 - mean_squared_error: 1.3733 - val_loss: 2.6767 - val_mean_squared_error: 2.6766\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2286 - mean_squared_error: 1.2286 - val_loss: 2.1520 - val_mean_squared_error: 2.1520\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.2038 - mean_squared_error: 1.2038 - val_loss: 2.5645 - val_mean_squared_error: 2.5645\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2493 - mean_squared_error: 1.2493 - val_loss: 2.3292 - val_mean_squared_error: 2.3292\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.3472 - mean_squared_error: 1.3472 - val_loss: 2.2117 - val_mean_squared_error: 2.2117\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.3068 - mean_squared_error: 1.3068 - val_loss: 2.3769 - val_mean_squared_error: 2.3769\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2061 - mean_squared_error: 1.2061 - val_loss: 2.4470 - val_mean_squared_error: 2.4470\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.2070 - mean_squared_error: 1.2070 - val_loss: 2.2987 - val_mean_squared_error: 2.2987\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.1670 - mean_squared_error: 1.1670 - val_loss: 2.3538 - val_mean_squared_error: 2.3538\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.2623 - mean_squared_error: 1.2623 - val_loss: 2.2430 - val_mean_squared_error: 2.2430\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2450 - mean_squared_error: 1.2450 - val_loss: 2.2644 - val_mean_squared_error: 2.2644\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1802 - mean_squared_error: 1.1802 - val_loss: 2.2200 - val_mean_squared_error: 2.2200\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1755 - mean_squared_error: 1.1755 - val_loss: 2.2876 - val_mean_squared_error: 2.2876\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.1845 - mean_squared_error: 1.1845 - val_loss: 2.5249 - val_mean_squared_error: 2.5249\n",
            "==================================================\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_27 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 750.0222 - mean_squared_error: 750.0222 - val_loss: 508.0744 - val_mean_squared_error: 508.0744\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 26.2142 - mean_squared_error: 26.2142 - val_loss: 207.2201 - val_mean_squared_error: 207.2201\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 21.1227 - mean_squared_error: 21.1227 - val_loss: 74.3040 - val_mean_squared_error: 74.3040\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 17.3442 - mean_squared_error: 17.3442 - val_loss: 54.4440 - val_mean_squared_error: 54.4440\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 17.3550 - mean_squared_error: 17.3550 - val_loss: 36.7612 - val_mean_squared_error: 36.7612\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 17.7891 - mean_squared_error: 17.7891 - val_loss: 22.5571 - val_mean_squared_error: 22.5571\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 15.5083 - mean_squared_error: 15.5083 - val_loss: 18.5126 - val_mean_squared_error: 18.5126\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 15.7265 - mean_squared_error: 15.7265 - val_loss: 15.7932 - val_mean_squared_error: 15.7932\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 15.8353 - mean_squared_error: 15.8353 - val_loss: 12.9643 - val_mean_squared_error: 12.9643\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 13.9586 - mean_squared_error: 13.9586 - val_loss: 14.7404 - val_mean_squared_error: 14.7404\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 13.3681 - mean_squared_error: 13.3681 - val_loss: 10.6379 - val_mean_squared_error: 10.6379\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 11.6257 - mean_squared_error: 11.6257 - val_loss: 10.7130 - val_mean_squared_error: 10.7130\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 11.3969 - mean_squared_error: 11.3969 - val_loss: 14.0837 - val_mean_squared_error: 14.0837\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 10.1166 - mean_squared_error: 10.1166 - val_loss: 9.9110 - val_mean_squared_error: 9.9110\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 10.3966 - mean_squared_error: 10.3966 - val_loss: 9.2410 - val_mean_squared_error: 9.2410\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 8.9836 - mean_squared_error: 8.9836 - val_loss: 8.6457 - val_mean_squared_error: 8.6457\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 10.0854 - mean_squared_error: 10.0854 - val_loss: 14.1994 - val_mean_squared_error: 14.1994\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 8.3774 - mean_squared_error: 8.3774 - val_loss: 8.0796 - val_mean_squared_error: 8.0796\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 8.6314 - mean_squared_error: 8.6314 - val_loss: 9.2630 - val_mean_squared_error: 9.2630\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 7.6774 - mean_squared_error: 7.6774 - val_loss: 8.6626 - val_mean_squared_error: 8.6626\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 7.4345 - mean_squared_error: 7.4345 - val_loss: 8.1035 - val_mean_squared_error: 8.1035\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 7.7788 - mean_squared_error: 7.7788 - val_loss: 8.3757 - val_mean_squared_error: 8.3757\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 6.8986 - mean_squared_error: 6.8986 - val_loss: 8.0825 - val_mean_squared_error: 8.0825\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 6.0799 - mean_squared_error: 6.0799 - val_loss: 7.3363 - val_mean_squared_error: 7.3363\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 5.7753 - mean_squared_error: 5.7753 - val_loss: 6.8431 - val_mean_squared_error: 6.8431\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 5.1285 - mean_squared_error: 5.1285 - val_loss: 6.2364 - val_mean_squared_error: 6.2364\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 5.4549 - mean_squared_error: 5.4549 - val_loss: 8.0194 - val_mean_squared_error: 8.0194\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 5.0713 - mean_squared_error: 5.0713 - val_loss: 4.7934 - val_mean_squared_error: 4.7934\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 4.7466 - mean_squared_error: 4.7466 - val_loss: 5.4594 - val_mean_squared_error: 5.4594\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 4.3836 - mean_squared_error: 4.3836 - val_loss: 6.0247 - val_mean_squared_error: 6.0247\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 3.9898 - mean_squared_error: 3.9898 - val_loss: 4.4498 - val_mean_squared_error: 4.4498\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 3.7611 - mean_squared_error: 3.7611 - val_loss: 4.1593 - val_mean_squared_error: 4.1593\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 3.5175 - mean_squared_error: 3.5175 - val_loss: 4.5727 - val_mean_squared_error: 4.5727\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 3.8903 - mean_squared_error: 3.8903 - val_loss: 4.7602 - val_mean_squared_error: 4.7602\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 3.3784 - mean_squared_error: 3.3784 - val_loss: 4.5352 - val_mean_squared_error: 4.5352\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 3.6691 - mean_squared_error: 3.6691 - val_loss: 4.2372 - val_mean_squared_error: 4.2372\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 3.2498 - mean_squared_error: 3.2498 - val_loss: 4.3841 - val_mean_squared_error: 4.3841\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 3.3823 - mean_squared_error: 3.3823 - val_loss: 4.0753 - val_mean_squared_error: 4.0753\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 3.3463 - mean_squared_error: 3.3463 - val_loss: 4.6787 - val_mean_squared_error: 4.6787\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 3.0667 - mean_squared_error: 3.0667 - val_loss: 3.3850 - val_mean_squared_error: 3.3850\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 3.0027 - mean_squared_error: 3.0027 - val_loss: 3.8952 - val_mean_squared_error: 3.8952\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 2.9101 - mean_squared_error: 2.9101 - val_loss: 3.7770 - val_mean_squared_error: 3.7770\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.7168 - mean_squared_error: 2.7168 - val_loss: 4.1582 - val_mean_squared_error: 4.1582\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.8381 - mean_squared_error: 2.8381 - val_loss: 3.6091 - val_mean_squared_error: 3.6091\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 2.4444 - mean_squared_error: 2.4444 - val_loss: 3.3356 - val_mean_squared_error: 3.3356\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.4803 - mean_squared_error: 2.4803 - val_loss: 3.5131 - val_mean_squared_error: 3.5131\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.4628 - mean_squared_error: 2.4628 - val_loss: 4.0354 - val_mean_squared_error: 4.0354\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.5419 - mean_squared_error: 2.5419 - val_loss: 3.7453 - val_mean_squared_error: 3.7453\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.6399 - mean_squared_error: 2.6399 - val_loss: 3.9999 - val_mean_squared_error: 3.9999\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.5030 - mean_squared_error: 2.5030 - val_loss: 3.6317 - val_mean_squared_error: 3.6317\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.3366 - mean_squared_error: 2.3366 - val_loss: 3.0817 - val_mean_squared_error: 3.0817\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.2354 - mean_squared_error: 2.2354 - val_loss: 4.5087 - val_mean_squared_error: 4.5087\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.7702 - mean_squared_error: 2.7702 - val_loss: 3.1052 - val_mean_squared_error: 3.1052\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 2.1777 - mean_squared_error: 2.1777 - val_loss: 3.3076 - val_mean_squared_error: 3.3076\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.1416 - mean_squared_error: 2.1416 - val_loss: 3.3624 - val_mean_squared_error: 3.3624\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 2.0528 - mean_squared_error: 2.0528 - val_loss: 3.4599 - val_mean_squared_error: 3.4599\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.5058 - mean_squared_error: 2.5058 - val_loss: 3.3586 - val_mean_squared_error: 3.3586\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.4079 - mean_squared_error: 2.4079 - val_loss: 3.6939 - val_mean_squared_error: 3.6939\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.9509 - mean_squared_error: 1.9509 - val_loss: 3.0994 - val_mean_squared_error: 3.0994\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.1671 - mean_squared_error: 2.1671 - val_loss: 3.0671 - val_mean_squared_error: 3.0671\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 2.1500 - mean_squared_error: 2.1500 - val_loss: 3.1184 - val_mean_squared_error: 3.1184\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.8897 - mean_squared_error: 1.8897 - val_loss: 3.1787 - val_mean_squared_error: 3.1787\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.9471 - mean_squared_error: 1.9471 - val_loss: 2.8330 - val_mean_squared_error: 2.8330\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.8183 - mean_squared_error: 1.8183 - val_loss: 2.9393 - val_mean_squared_error: 2.9393\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.8660 - mean_squared_error: 1.8660 - val_loss: 3.6948 - val_mean_squared_error: 3.6948\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.7897 - mean_squared_error: 1.7897 - val_loss: 3.4718 - val_mean_squared_error: 3.4718\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.8349 - mean_squared_error: 1.8349 - val_loss: 2.9405 - val_mean_squared_error: 2.9405\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.8318 - mean_squared_error: 1.8318 - val_loss: 2.8511 - val_mean_squared_error: 2.8511\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.7320 - mean_squared_error: 1.7320 - val_loss: 3.8205 - val_mean_squared_error: 3.8205\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 348us/sample - loss: 1.7853 - mean_squared_error: 1.7853 - val_loss: 3.0715 - val_mean_squared_error: 3.0715\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.7010 - mean_squared_error: 1.7010 - val_loss: 2.9513 - val_mean_squared_error: 2.9513\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.7290 - mean_squared_error: 1.7290 - val_loss: 3.2218 - val_mean_squared_error: 3.2218\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.7705 - mean_squared_error: 1.7705 - val_loss: 2.7534 - val_mean_squared_error: 2.7534\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.7899 - mean_squared_error: 1.7899 - val_loss: 3.4147 - val_mean_squared_error: 3.4147\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.8896 - mean_squared_error: 1.8896 - val_loss: 2.9387 - val_mean_squared_error: 2.9387\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.9830 - mean_squared_error: 1.9830 - val_loss: 3.3024 - val_mean_squared_error: 3.3024\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.8132 - mean_squared_error: 1.8132 - val_loss: 3.0557 - val_mean_squared_error: 3.0557\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.6499 - mean_squared_error: 1.6499 - val_loss: 2.9142 - val_mean_squared_error: 2.9142\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 1.6712 - mean_squared_error: 1.6712 - val_loss: 3.0751 - val_mean_squared_error: 3.0751\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.8679 - mean_squared_error: 1.8679 - val_loss: 2.8989 - val_mean_squared_error: 2.8989\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.6049 - mean_squared_error: 1.6049 - val_loss: 3.4986 - val_mean_squared_error: 3.4986\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.7137 - mean_squared_error: 1.7137 - val_loss: 2.6595 - val_mean_squared_error: 2.6595\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.5187 - mean_squared_error: 1.5187 - val_loss: 3.0428 - val_mean_squared_error: 3.0428\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.6015 - mean_squared_error: 1.6015 - val_loss: 2.8653 - val_mean_squared_error: 2.8653\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.6524 - mean_squared_error: 1.6524 - val_loss: 2.9059 - val_mean_squared_error: 2.9059\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7128 - mean_squared_error: 1.7128 - val_loss: 3.6661 - val_mean_squared_error: 3.6661\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5205 - mean_squared_error: 1.5205 - val_loss: 3.1394 - val_mean_squared_error: 3.1394\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.5453 - mean_squared_error: 1.5453 - val_loss: 2.9688 - val_mean_squared_error: 2.9688\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.4075 - mean_squared_error: 1.4075 - val_loss: 2.8151 - val_mean_squared_error: 2.8151\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.5210 - mean_squared_error: 1.5210 - val_loss: 2.8604 - val_mean_squared_error: 2.8604\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4971 - mean_squared_error: 1.4971 - val_loss: 2.7652 - val_mean_squared_error: 2.7652\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.4345 - mean_squared_error: 1.4345 - val_loss: 2.6474 - val_mean_squared_error: 2.6474\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.3443 - mean_squared_error: 1.3443 - val_loss: 2.9632 - val_mean_squared_error: 2.9632\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3613 - mean_squared_error: 1.3613 - val_loss: 2.9004 - val_mean_squared_error: 2.9004\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.4547 - mean_squared_error: 1.4547 - val_loss: 2.7684 - val_mean_squared_error: 2.7684\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.4979 - mean_squared_error: 1.4979 - val_loss: 2.6402 - val_mean_squared_error: 2.6402\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.4251 - mean_squared_error: 1.4251 - val_loss: 3.0483 - val_mean_squared_error: 3.0483\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.5185 - mean_squared_error: 1.5185 - val_loss: 2.8340 - val_mean_squared_error: 2.8340\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.3185 - mean_squared_error: 1.3185 - val_loss: 2.8643 - val_mean_squared_error: 2.8643\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.3889 - mean_squared_error: 1.3889 - val_loss: 2.9580 - val_mean_squared_error: 2.9580\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.3254 - mean_squared_error: 1.3254 - val_loss: 2.9765 - val_mean_squared_error: 2.9765\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.5492 - mean_squared_error: 1.5492 - val_loss: 3.0100 - val_mean_squared_error: 3.0100\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.3179 - mean_squared_error: 1.3179 - val_loss: 2.8567 - val_mean_squared_error: 2.8567\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.5238 - mean_squared_error: 1.5238 - val_loss: 3.4973 - val_mean_squared_error: 3.4973\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.5292 - mean_squared_error: 1.5292 - val_loss: 3.3228 - val_mean_squared_error: 3.3228\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.4371 - mean_squared_error: 1.4371 - val_loss: 3.3852 - val_mean_squared_error: 3.3852\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.3211 - mean_squared_error: 1.3211 - val_loss: 2.6537 - val_mean_squared_error: 2.6537\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.2340 - mean_squared_error: 1.2340 - val_loss: 2.7629 - val_mean_squared_error: 2.7629\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.2750 - mean_squared_error: 1.2750 - val_loss: 2.8138 - val_mean_squared_error: 2.8138\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.2638 - mean_squared_error: 1.2638 - val_loss: 2.6043 - val_mean_squared_error: 2.6043\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.2668 - mean_squared_error: 1.2668 - val_loss: 3.0289 - val_mean_squared_error: 3.0289\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 344us/sample - loss: 1.3138 - mean_squared_error: 1.3138 - val_loss: 3.2353 - val_mean_squared_error: 3.2353\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.2992 - mean_squared_error: 1.2992 - val_loss: 2.9731 - val_mean_squared_error: 2.9731\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.1864 - mean_squared_error: 1.1864 - val_loss: 2.6431 - val_mean_squared_error: 2.6431\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.1581 - mean_squared_error: 1.1581 - val_loss: 2.6960 - val_mean_squared_error: 2.6960\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.2805 - mean_squared_error: 1.2805 - val_loss: 2.8545 - val_mean_squared_error: 2.8545\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.1871 - mean_squared_error: 1.1871 - val_loss: 2.7827 - val_mean_squared_error: 2.7827\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.1155 - mean_squared_error: 1.1155 - val_loss: 3.0073 - val_mean_squared_error: 3.0073\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.2522 - mean_squared_error: 1.2522 - val_loss: 2.6087 - val_mean_squared_error: 2.6087\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.3338 - mean_squared_error: 1.3338 - val_loss: 2.7257 - val_mean_squared_error: 2.7257\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.1174 - mean_squared_error: 1.1174 - val_loss: 2.7024 - val_mean_squared_error: 2.7024\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 1.1264 - mean_squared_error: 1.1264 - val_loss: 2.6662 - val_mean_squared_error: 2.6662\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.1048 - mean_squared_error: 1.1048 - val_loss: 2.6468 - val_mean_squared_error: 2.6468\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0453 - mean_squared_error: 1.0453 - val_loss: 2.5768 - val_mean_squared_error: 2.5768\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.1667 - mean_squared_error: 1.1667 - val_loss: 2.8576 - val_mean_squared_error: 2.8576\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.1253 - mean_squared_error: 1.1253 - val_loss: 2.6221 - val_mean_squared_error: 2.6221\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.1752 - mean_squared_error: 1.1752 - val_loss: 2.6077 - val_mean_squared_error: 2.6077\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.3141 - mean_squared_error: 1.3141 - val_loss: 2.8937 - val_mean_squared_error: 2.8937\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 1.2319 - mean_squared_error: 1.2319 - val_loss: 2.9501 - val_mean_squared_error: 2.9501\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.1383 - mean_squared_error: 1.1383 - val_loss: 2.8600 - val_mean_squared_error: 2.8600\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.2239 - mean_squared_error: 1.2239 - val_loss: 2.5676 - val_mean_squared_error: 2.5676\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.1185 - mean_squared_error: 1.1185 - val_loss: 2.6020 - val_mean_squared_error: 2.6020\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.0823 - mean_squared_error: 1.0823 - val_loss: 2.6541 - val_mean_squared_error: 2.6541\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.1237 - mean_squared_error: 1.1237 - val_loss: 2.4939 - val_mean_squared_error: 2.4939\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.9937 - mean_squared_error: 0.9937 - val_loss: 2.4448 - val_mean_squared_error: 2.4448\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.1005 - mean_squared_error: 1.1005 - val_loss: 2.7427 - val_mean_squared_error: 2.7427\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.0410 - mean_squared_error: 1.0410 - val_loss: 2.5636 - val_mean_squared_error: 2.5636\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.0891 - mean_squared_error: 1.0891 - val_loss: 2.5823 - val_mean_squared_error: 2.5823\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.0348 - mean_squared_error: 1.0348 - val_loss: 2.4897 - val_mean_squared_error: 2.4897\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.9419 - mean_squared_error: 0.9419 - val_loss: 2.8554 - val_mean_squared_error: 2.8554\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.0627 - mean_squared_error: 1.0627 - val_loss: 2.5111 - val_mean_squared_error: 2.5111\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.0216 - mean_squared_error: 1.0216 - val_loss: 2.4277 - val_mean_squared_error: 2.4277\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.0791 - mean_squared_error: 1.0791 - val_loss: 3.0716 - val_mean_squared_error: 3.0716\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.0480 - mean_squared_error: 1.0480 - val_loss: 2.6563 - val_mean_squared_error: 2.6563\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.9237 - mean_squared_error: 0.9237 - val_loss: 2.5239 - val_mean_squared_error: 2.5239\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.9733 - mean_squared_error: 0.9733 - val_loss: 2.5709 - val_mean_squared_error: 2.5709\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 1.0632 - mean_squared_error: 1.0632 - val_loss: 2.5520 - val_mean_squared_error: 2.5520\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.9922 - mean_squared_error: 0.9922 - val_loss: 2.5686 - val_mean_squared_error: 2.5686\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0278 - mean_squared_error: 1.0278 - val_loss: 2.6220 - val_mean_squared_error: 2.6220\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.2478 - mean_squared_error: 1.2478 - val_loss: 2.6088 - val_mean_squared_error: 2.6088\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0074 - mean_squared_error: 1.0074 - val_loss: 2.7655 - val_mean_squared_error: 2.7655\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.0255 - mean_squared_error: 1.0255 - val_loss: 2.4049 - val_mean_squared_error: 2.4049\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.9785 - mean_squared_error: 0.9785 - val_loss: 2.4843 - val_mean_squared_error: 2.4843\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9080 - mean_squared_error: 0.9080 - val_loss: 2.9515 - val_mean_squared_error: 2.9515\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.0244 - mean_squared_error: 1.0244 - val_loss: 2.5588 - val_mean_squared_error: 2.5588\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.9115 - mean_squared_error: 0.9115 - val_loss: 2.4771 - val_mean_squared_error: 2.4771\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.9632 - mean_squared_error: 0.9632 - val_loss: 2.5862 - val_mean_squared_error: 2.5862\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 1.1541 - mean_squared_error: 1.1541 - val_loss: 2.5334 - val_mean_squared_error: 2.5334\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 0.9549 - mean_squared_error: 0.9549 - val_loss: 2.4470 - val_mean_squared_error: 2.4470\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 347us/sample - loss: 0.8508 - mean_squared_error: 0.8508 - val_loss: 2.4415 - val_mean_squared_error: 2.4415\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 0.8647 - mean_squared_error: 0.8647 - val_loss: 2.5678 - val_mean_squared_error: 2.5678\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.8366 - mean_squared_error: 0.8366 - val_loss: 2.4085 - val_mean_squared_error: 2.4085\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.8947 - mean_squared_error: 0.8947 - val_loss: 2.5755 - val_mean_squared_error: 2.5755\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 2.4764 - val_mean_squared_error: 2.4764\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.9648 - mean_squared_error: 0.9648 - val_loss: 2.4910 - val_mean_squared_error: 2.4910\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 0.9453 - mean_squared_error: 0.9453 - val_loss: 2.4403 - val_mean_squared_error: 2.4403\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.8743 - mean_squared_error: 0.8743 - val_loss: 2.4570 - val_mean_squared_error: 2.4570\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.9713 - mean_squared_error: 0.9713 - val_loss: 2.3651 - val_mean_squared_error: 2.3651\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 0.8544 - mean_squared_error: 0.8544 - val_loss: 2.7420 - val_mean_squared_error: 2.7420\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.8888 - mean_squared_error: 0.8888 - val_loss: 2.4482 - val_mean_squared_error: 2.4482\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.8328 - mean_squared_error: 0.8328 - val_loss: 2.5783 - val_mean_squared_error: 2.5783\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 0.9088 - mean_squared_error: 0.9088 - val_loss: 2.4379 - val_mean_squared_error: 2.4379\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.8821 - mean_squared_error: 0.8821 - val_loss: 2.4559 - val_mean_squared_error: 2.4559\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 0.9110 - mean_squared_error: 0.9110 - val_loss: 2.6503 - val_mean_squared_error: 2.6503\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.7204 - mean_squared_error: 0.7204 - val_loss: 2.3517 - val_mean_squared_error: 2.3517\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.7614 - mean_squared_error: 0.7614 - val_loss: 2.3597 - val_mean_squared_error: 2.3597\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 0.8260 - mean_squared_error: 0.8260 - val_loss: 2.5155 - val_mean_squared_error: 2.5155\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.9024 - mean_squared_error: 0.9024 - val_loss: 2.4724 - val_mean_squared_error: 2.4724\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0622 - mean_squared_error: 1.0622 - val_loss: 2.7030 - val_mean_squared_error: 2.7030\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.8620 - mean_squared_error: 0.8620 - val_loss: 2.3594 - val_mean_squared_error: 2.3594\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.8540 - mean_squared_error: 0.8540 - val_loss: 2.4489 - val_mean_squared_error: 2.4489\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 349us/sample - loss: 0.8951 - mean_squared_error: 0.8951 - val_loss: 2.3678 - val_mean_squared_error: 2.3678\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.7774 - mean_squared_error: 0.7774 - val_loss: 2.5844 - val_mean_squared_error: 2.5844\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 0.8670 - mean_squared_error: 0.8670 - val_loss: 2.5780 - val_mean_squared_error: 2.5780\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.8875 - mean_squared_error: 0.8875 - val_loss: 2.9077 - val_mean_squared_error: 2.9077\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8862 - mean_squared_error: 0.8862 - val_loss: 2.6038 - val_mean_squared_error: 2.6038\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8175 - mean_squared_error: 0.8175 - val_loss: 2.4290 - val_mean_squared_error: 2.4290\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.8542 - mean_squared_error: 0.8542 - val_loss: 2.5618 - val_mean_squared_error: 2.5618\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.8578 - mean_squared_error: 0.8578 - val_loss: 2.3747 - val_mean_squared_error: 2.3747\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.9125 - mean_squared_error: 0.9125 - val_loss: 2.4424 - val_mean_squared_error: 2.4424\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.7717 - mean_squared_error: 0.7717 - val_loss: 2.5507 - val_mean_squared_error: 2.5507\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.7731 - mean_squared_error: 0.7731 - val_loss: 2.4202 - val_mean_squared_error: 2.4202\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 350us/sample - loss: 0.8248 - mean_squared_error: 0.8248 - val_loss: 2.7570 - val_mean_squared_error: 2.7570\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.7798 - mean_squared_error: 0.7798 - val_loss: 2.2949 - val_mean_squared_error: 2.2949\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.8396 - mean_squared_error: 0.8396 - val_loss: 2.2164 - val_mean_squared_error: 2.2164\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 0.7504 - mean_squared_error: 0.7504 - val_loss: 2.4139 - val_mean_squared_error: 2.4139\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.8763 - mean_squared_error: 0.8763 - val_loss: 2.4174 - val_mean_squared_error: 2.4174\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.8030 - mean_squared_error: 0.8030 - val_loss: 2.4663 - val_mean_squared_error: 2.4663\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 0.6851 - mean_squared_error: 0.6851 - val_loss: 2.4933 - val_mean_squared_error: 2.4933\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.7430 - mean_squared_error: 0.7430 - val_loss: 2.2772 - val_mean_squared_error: 2.2772\n",
            "==================================================\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_30 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_32 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 750.6943 - mean_squared_error: 750.6944 - val_loss: 176.1146 - val_mean_squared_error: 176.1146\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 31.9763 - mean_squared_error: 31.9763 - val_loss: 233.9745 - val_mean_squared_error: 233.9745\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 18.1991 - mean_squared_error: 18.1991 - val_loss: 98.2630 - val_mean_squared_error: 98.2630\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 20.4544 - mean_squared_error: 20.4545 - val_loss: 50.2282 - val_mean_squared_error: 50.2282\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 20.3456 - mean_squared_error: 20.3456 - val_loss: 56.1190 - val_mean_squared_error: 56.1190\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 17.0750 - mean_squared_error: 17.0750 - val_loss: 19.0127 - val_mean_squared_error: 19.0127\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 14.2384 - mean_squared_error: 14.2384 - val_loss: 13.6259 - val_mean_squared_error: 13.6259\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 14.7280 - mean_squared_error: 14.7280 - val_loss: 14.8727 - val_mean_squared_error: 14.8727\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 15.3908 - mean_squared_error: 15.3908 - val_loss: 15.4870 - val_mean_squared_error: 15.4870\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 14.5363 - mean_squared_error: 14.5363 - val_loss: 12.7658 - val_mean_squared_error: 12.7658\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 13.6531 - mean_squared_error: 13.6531 - val_loss: 11.7162 - val_mean_squared_error: 11.7162\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 12.1494 - mean_squared_error: 12.1494 - val_loss: 11.2893 - val_mean_squared_error: 11.2893\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 12.1299 - mean_squared_error: 12.1299 - val_loss: 12.0154 - val_mean_squared_error: 12.0154\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 11.1469 - mean_squared_error: 11.1469 - val_loss: 12.8172 - val_mean_squared_error: 12.8172\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 11.6853 - mean_squared_error: 11.6853 - val_loss: 12.7246 - val_mean_squared_error: 12.7246\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 11.3794 - mean_squared_error: 11.3794 - val_loss: 9.4222 - val_mean_squared_error: 9.4222\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 10.9480 - mean_squared_error: 10.9480 - val_loss: 16.1034 - val_mean_squared_error: 16.1034\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 9.7887 - mean_squared_error: 9.7887 - val_loss: 8.5678 - val_mean_squared_error: 8.5678\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 8.7524 - mean_squared_error: 8.7524 - val_loss: 8.0653 - val_mean_squared_error: 8.0653\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 8.3005 - mean_squared_error: 8.3005 - val_loss: 10.0111 - val_mean_squared_error: 10.0111\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 8.3456 - mean_squared_error: 8.3456 - val_loss: 13.0152 - val_mean_squared_error: 13.0152\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 7.4309 - mean_squared_error: 7.4309 - val_loss: 10.3828 - val_mean_squared_error: 10.3828\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 7.3927 - mean_squared_error: 7.3927 - val_loss: 7.6635 - val_mean_squared_error: 7.6635\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 6.8474 - mean_squared_error: 6.8474 - val_loss: 6.7520 - val_mean_squared_error: 6.7520\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 6.7235 - mean_squared_error: 6.7235 - val_loss: 7.3969 - val_mean_squared_error: 7.3969\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 5.9593 - mean_squared_error: 5.9593 - val_loss: 7.9972 - val_mean_squared_error: 7.9972\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 5.8299 - mean_squared_error: 5.8299 - val_loss: 6.0941 - val_mean_squared_error: 6.0941\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 5.3701 - mean_squared_error: 5.3701 - val_loss: 8.9710 - val_mean_squared_error: 8.9710\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 5.1579 - mean_squared_error: 5.1579 - val_loss: 5.8493 - val_mean_squared_error: 5.8493\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 5.1808 - mean_squared_error: 5.1808 - val_loss: 5.8991 - val_mean_squared_error: 5.8991\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 4.7682 - mean_squared_error: 4.7682 - val_loss: 5.7045 - val_mean_squared_error: 5.7045\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 4.4318 - mean_squared_error: 4.4318 - val_loss: 4.6154 - val_mean_squared_error: 4.6154\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 3.9792 - mean_squared_error: 3.9792 - val_loss: 4.1914 - val_mean_squared_error: 4.1914\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 4.1936 - mean_squared_error: 4.1936 - val_loss: 4.5211 - val_mean_squared_error: 4.5211\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 3.9655 - mean_squared_error: 3.9655 - val_loss: 4.6047 - val_mean_squared_error: 4.6047\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 3.8286 - mean_squared_error: 3.8286 - val_loss: 4.4305 - val_mean_squared_error: 4.4305\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 3.5757 - mean_squared_error: 3.5757 - val_loss: 6.7848 - val_mean_squared_error: 6.7848\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 3.5799 - mean_squared_error: 3.5799 - val_loss: 4.0555 - val_mean_squared_error: 4.0555\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 3.6203 - mean_squared_error: 3.6203 - val_loss: 4.2486 - val_mean_squared_error: 4.2486\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 3.1824 - mean_squared_error: 3.1824 - val_loss: 3.7674 - val_mean_squared_error: 3.7674\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 3.3912 - mean_squared_error: 3.3912 - val_loss: 3.8766 - val_mean_squared_error: 3.8766\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.9612 - mean_squared_error: 2.9612 - val_loss: 3.6261 - val_mean_squared_error: 3.6261\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.9158 - mean_squared_error: 2.9158 - val_loss: 3.4726 - val_mean_squared_error: 3.4726\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 2.8907 - mean_squared_error: 2.8907 - val_loss: 3.3452 - val_mean_squared_error: 3.3452\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.8657 - mean_squared_error: 2.8657 - val_loss: 3.8804 - val_mean_squared_error: 3.8804\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.9369 - mean_squared_error: 2.9369 - val_loss: 3.5152 - val_mean_squared_error: 3.5152\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.8168 - mean_squared_error: 2.8168 - val_loss: 3.5653 - val_mean_squared_error: 3.5653\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.7646 - mean_squared_error: 2.7646 - val_loss: 3.7598 - val_mean_squared_error: 3.7598\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 2.8305 - mean_squared_error: 2.8305 - val_loss: 3.6627 - val_mean_squared_error: 3.6627\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 3.0942 - mean_squared_error: 3.0942 - val_loss: 3.7171 - val_mean_squared_error: 3.7171\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.6746 - mean_squared_error: 2.6746 - val_loss: 3.4793 - val_mean_squared_error: 3.4793\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.5010 - mean_squared_error: 2.5010 - val_loss: 3.8379 - val_mean_squared_error: 3.8379\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.4536 - mean_squared_error: 2.4536 - val_loss: 3.3091 - val_mean_squared_error: 3.3091\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.6537 - mean_squared_error: 2.6537 - val_loss: 3.8125 - val_mean_squared_error: 3.8125\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.8818 - mean_squared_error: 2.8818 - val_loss: 3.8228 - val_mean_squared_error: 3.8228\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.5145 - mean_squared_error: 2.5145 - val_loss: 3.4522 - val_mean_squared_error: 3.4522\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.3661 - mean_squared_error: 2.3661 - val_loss: 3.1094 - val_mean_squared_error: 3.1094\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.3994 - mean_squared_error: 2.3994 - val_loss: 3.4409 - val_mean_squared_error: 3.4409\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.2156 - mean_squared_error: 2.2156 - val_loss: 3.3178 - val_mean_squared_error: 3.3178\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.3239 - mean_squared_error: 2.3239 - val_loss: 4.2366 - val_mean_squared_error: 4.2366\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.4764 - mean_squared_error: 2.4764 - val_loss: 3.6845 - val_mean_squared_error: 3.6845\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.4433 - mean_squared_error: 2.4433 - val_loss: 3.0879 - val_mean_squared_error: 3.0879\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.1560 - mean_squared_error: 2.1560 - val_loss: 2.9347 - val_mean_squared_error: 2.9347\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.2742 - mean_squared_error: 2.2742 - val_loss: 3.4926 - val_mean_squared_error: 3.4926\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.2382 - mean_squared_error: 2.2382 - val_loss: 3.1051 - val_mean_squared_error: 3.1051\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.9941 - mean_squared_error: 1.9941 - val_loss: 2.9807 - val_mean_squared_error: 2.9807\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.9949 - mean_squared_error: 1.9949 - val_loss: 3.8292 - val_mean_squared_error: 3.8292\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.9981 - mean_squared_error: 1.9981 - val_loss: 2.8437 - val_mean_squared_error: 2.8437\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.9591 - mean_squared_error: 1.9591 - val_loss: 2.7794 - val_mean_squared_error: 2.7794\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.0550 - mean_squared_error: 2.0550 - val_loss: 3.0465 - val_mean_squared_error: 3.0465\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 2.2000 - mean_squared_error: 2.2000 - val_loss: 3.5199 - val_mean_squared_error: 3.5199\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.1582 - mean_squared_error: 2.1582 - val_loss: 3.1654 - val_mean_squared_error: 3.1654\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.8890 - mean_squared_error: 1.8890 - val_loss: 3.0202 - val_mean_squared_error: 3.0202\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.9446 - mean_squared_error: 1.9446 - val_loss: 2.8342 - val_mean_squared_error: 2.8342\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.7367 - mean_squared_error: 1.7367 - val_loss: 2.6593 - val_mean_squared_error: 2.6593\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.8027 - mean_squared_error: 1.8027 - val_loss: 2.7215 - val_mean_squared_error: 2.7215\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.2481 - mean_squared_error: 2.2481 - val_loss: 3.3375 - val_mean_squared_error: 3.3375\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.9490 - mean_squared_error: 1.9490 - val_loss: 2.7256 - val_mean_squared_error: 2.7256\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.9035 - mean_squared_error: 1.9035 - val_loss: 3.0613 - val_mean_squared_error: 3.0613\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.9438 - mean_squared_error: 1.9438 - val_loss: 2.9958 - val_mean_squared_error: 2.9958\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.8891 - mean_squared_error: 1.8891 - val_loss: 3.1943 - val_mean_squared_error: 3.1943\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.9418 - mean_squared_error: 1.9418 - val_loss: 2.7360 - val_mean_squared_error: 2.7360\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.6541 - mean_squared_error: 1.6541 - val_loss: 2.6830 - val_mean_squared_error: 2.6830\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.5975 - mean_squared_error: 1.5975 - val_loss: 2.5917 - val_mean_squared_error: 2.5917\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.8404 - mean_squared_error: 1.8404 - val_loss: 3.1119 - val_mean_squared_error: 3.1119\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.9240 - mean_squared_error: 1.9240 - val_loss: 3.0878 - val_mean_squared_error: 3.0878\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.8054 - mean_squared_error: 1.8054 - val_loss: 2.9033 - val_mean_squared_error: 2.9033\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.6529 - mean_squared_error: 1.6529 - val_loss: 2.7315 - val_mean_squared_error: 2.7315\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.7335 - mean_squared_error: 1.7335 - val_loss: 2.9087 - val_mean_squared_error: 2.9087\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.7098 - mean_squared_error: 1.7098 - val_loss: 2.5153 - val_mean_squared_error: 2.5153\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.8115 - mean_squared_error: 1.8115 - val_loss: 3.1397 - val_mean_squared_error: 3.1397\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6536 - mean_squared_error: 1.6536 - val_loss: 2.8593 - val_mean_squared_error: 2.8593\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.6130 - mean_squared_error: 1.6130 - val_loss: 2.9574 - val_mean_squared_error: 2.9574\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.6144 - mean_squared_error: 1.6144 - val_loss: 3.0395 - val_mean_squared_error: 3.0395\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4584 - mean_squared_error: 1.4584 - val_loss: 2.8315 - val_mean_squared_error: 2.8315\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.4975 - mean_squared_error: 1.4975 - val_loss: 2.9186 - val_mean_squared_error: 2.9186\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.4890 - mean_squared_error: 1.4890 - val_loss: 2.7207 - val_mean_squared_error: 2.7207\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.5147 - mean_squared_error: 1.5147 - val_loss: 2.7231 - val_mean_squared_error: 2.7231\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.5591 - mean_squared_error: 1.5591 - val_loss: 2.7392 - val_mean_squared_error: 2.7392\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.5636 - mean_squared_error: 1.5636 - val_loss: 2.7766 - val_mean_squared_error: 2.7766\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.5026 - mean_squared_error: 1.5026 - val_loss: 2.5005 - val_mean_squared_error: 2.5005\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.6089 - mean_squared_error: 1.6089 - val_loss: 2.7773 - val_mean_squared_error: 2.7773\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.5849 - mean_squared_error: 1.5849 - val_loss: 2.6952 - val_mean_squared_error: 2.6952\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.5442 - mean_squared_error: 1.5442 - val_loss: 2.5356 - val_mean_squared_error: 2.5356\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.4331 - mean_squared_error: 1.4331 - val_loss: 2.6302 - val_mean_squared_error: 2.6302\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.4555 - mean_squared_error: 1.4555 - val_loss: 2.4996 - val_mean_squared_error: 2.4996\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.5254 - mean_squared_error: 1.5254 - val_loss: 2.8262 - val_mean_squared_error: 2.8262\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.4580 - mean_squared_error: 1.4580 - val_loss: 2.7809 - val_mean_squared_error: 2.7809\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4513 - mean_squared_error: 1.4513 - val_loss: 2.5539 - val_mean_squared_error: 2.5539\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.4405 - mean_squared_error: 1.4405 - val_loss: 2.7891 - val_mean_squared_error: 2.7891\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.4532 - mean_squared_error: 1.4532 - val_loss: 3.3313 - val_mean_squared_error: 3.3313\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.4729 - mean_squared_error: 1.4729 - val_loss: 2.4221 - val_mean_squared_error: 2.4221\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.3787 - mean_squared_error: 1.3787 - val_loss: 2.6683 - val_mean_squared_error: 2.6683\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.2583 - mean_squared_error: 1.2583 - val_loss: 2.4175 - val_mean_squared_error: 2.4175\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.4094 - mean_squared_error: 1.4094 - val_loss: 2.5933 - val_mean_squared_error: 2.5933\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4592 - mean_squared_error: 1.4592 - val_loss: 2.8263 - val_mean_squared_error: 2.8263\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.2504 - mean_squared_error: 1.2504 - val_loss: 2.5188 - val_mean_squared_error: 2.5188\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.4698 - mean_squared_error: 1.4698 - val_loss: 2.4461 - val_mean_squared_error: 2.4461\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.2797 - mean_squared_error: 1.2797 - val_loss: 2.4640 - val_mean_squared_error: 2.4640\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.3551 - mean_squared_error: 1.3551 - val_loss: 2.6199 - val_mean_squared_error: 2.6199\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4022 - mean_squared_error: 1.4022 - val_loss: 2.6469 - val_mean_squared_error: 2.6469\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.3938 - mean_squared_error: 1.3938 - val_loss: 2.7313 - val_mean_squared_error: 2.7313\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3724 - mean_squared_error: 1.3724 - val_loss: 2.3712 - val_mean_squared_error: 2.3712\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2034 - mean_squared_error: 1.2034 - val_loss: 2.4954 - val_mean_squared_error: 2.4954\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.1817 - mean_squared_error: 1.1817 - val_loss: 2.5777 - val_mean_squared_error: 2.5777\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3118 - mean_squared_error: 1.3118 - val_loss: 2.5267 - val_mean_squared_error: 2.5267\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.2214 - mean_squared_error: 1.2214 - val_loss: 2.7852 - val_mean_squared_error: 2.7852\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.3229 - mean_squared_error: 1.3229 - val_loss: 2.6409 - val_mean_squared_error: 2.6409\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.3282 - mean_squared_error: 1.3282 - val_loss: 2.9359 - val_mean_squared_error: 2.9359\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.3115 - mean_squared_error: 1.3115 - val_loss: 2.5067 - val_mean_squared_error: 2.5067\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.4514 - mean_squared_error: 1.4514 - val_loss: 2.5670 - val_mean_squared_error: 2.5670\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.2080 - mean_squared_error: 1.2080 - val_loss: 2.5741 - val_mean_squared_error: 2.5741\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.3065 - mean_squared_error: 1.3065 - val_loss: 2.4030 - val_mean_squared_error: 2.4030\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.2625 - mean_squared_error: 1.2625 - val_loss: 2.3700 - val_mean_squared_error: 2.3700\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.2114 - mean_squared_error: 1.2114 - val_loss: 2.5192 - val_mean_squared_error: 2.5192\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.2012 - mean_squared_error: 1.2012 - val_loss: 2.3506 - val_mean_squared_error: 2.3506\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1467 - mean_squared_error: 1.1467 - val_loss: 2.5031 - val_mean_squared_error: 2.5031\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.2745 - mean_squared_error: 1.2745 - val_loss: 2.4887 - val_mean_squared_error: 2.4887\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.2040 - mean_squared_error: 1.2040 - val_loss: 2.3739 - val_mean_squared_error: 2.3739\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0819 - mean_squared_error: 1.0819 - val_loss: 2.3062 - val_mean_squared_error: 2.3062\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.1411 - mean_squared_error: 1.1411 - val_loss: 2.3958 - val_mean_squared_error: 2.3958\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.1211 - mean_squared_error: 1.1211 - val_loss: 2.5983 - val_mean_squared_error: 2.5983\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.1631 - mean_squared_error: 1.1631 - val_loss: 2.4917 - val_mean_squared_error: 2.4917\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.1725 - mean_squared_error: 1.1725 - val_loss: 2.1739 - val_mean_squared_error: 2.1739\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.0805 - mean_squared_error: 1.0805 - val_loss: 2.3133 - val_mean_squared_error: 2.3133\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 2.3483 - val_mean_squared_error: 2.3483\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.0795 - mean_squared_error: 1.0795 - val_loss: 2.7468 - val_mean_squared_error: 2.7469\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1853 - mean_squared_error: 1.1853 - val_loss: 2.4038 - val_mean_squared_error: 2.4038\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1414 - mean_squared_error: 1.1414 - val_loss: 2.4798 - val_mean_squared_error: 2.4798\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 353us/sample - loss: 1.0354 - mean_squared_error: 1.0354 - val_loss: 2.6224 - val_mean_squared_error: 2.6224\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.1260 - mean_squared_error: 1.1260 - val_loss: 2.7559 - val_mean_squared_error: 2.7559\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0979 - mean_squared_error: 1.0979 - val_loss: 2.5160 - val_mean_squared_error: 2.5160\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0765 - mean_squared_error: 1.0765 - val_loss: 2.2306 - val_mean_squared_error: 2.2306\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.1135 - mean_squared_error: 1.1135 - val_loss: 2.4810 - val_mean_squared_error: 2.4810\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0546 - mean_squared_error: 1.0546 - val_loss: 2.3302 - val_mean_squared_error: 2.3302\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.9824 - mean_squared_error: 0.9824 - val_loss: 2.1250 - val_mean_squared_error: 2.1250\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.9627 - mean_squared_error: 0.9627 - val_loss: 2.2449 - val_mean_squared_error: 2.2449\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.0564 - mean_squared_error: 1.0564 - val_loss: 2.6057 - val_mean_squared_error: 2.6057\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 2.5537 - val_mean_squared_error: 2.5537\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0426 - mean_squared_error: 1.0426 - val_loss: 2.4166 - val_mean_squared_error: 2.4166\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.0749 - mean_squared_error: 1.0749 - val_loss: 2.4403 - val_mean_squared_error: 2.4403\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1465 - mean_squared_error: 1.1465 - val_loss: 2.3266 - val_mean_squared_error: 2.3266\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1381 - mean_squared_error: 1.1381 - val_loss: 2.4487 - val_mean_squared_error: 2.4487\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.1148 - mean_squared_error: 1.1148 - val_loss: 2.2685 - val_mean_squared_error: 2.2685\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9501 - mean_squared_error: 0.9501 - val_loss: 2.4012 - val_mean_squared_error: 2.4012\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 2.2030 - val_mean_squared_error: 2.2030\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.9416 - mean_squared_error: 0.9416 - val_loss: 2.2382 - val_mean_squared_error: 2.2382\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.9354 - mean_squared_error: 0.9354 - val_loss: 2.3631 - val_mean_squared_error: 2.3631\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9508 - mean_squared_error: 0.9508 - val_loss: 2.2576 - val_mean_squared_error: 2.2576\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.0628 - mean_squared_error: 1.0628 - val_loss: 2.5812 - val_mean_squared_error: 2.5812\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.9478 - mean_squared_error: 0.9478 - val_loss: 2.2796 - val_mean_squared_error: 2.2796\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9964 - mean_squared_error: 0.9964 - val_loss: 2.5090 - val_mean_squared_error: 2.5090\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.0527 - mean_squared_error: 1.0527 - val_loss: 2.4610 - val_mean_squared_error: 2.4610\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9604 - mean_squared_error: 0.9604 - val_loss: 2.5280 - val_mean_squared_error: 2.5280\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.9446 - mean_squared_error: 0.9446 - val_loss: 2.2863 - val_mean_squared_error: 2.2863\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 351us/sample - loss: 0.9984 - mean_squared_error: 0.9984 - val_loss: 2.2232 - val_mean_squared_error: 2.2232\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 0.8621 - mean_squared_error: 0.8621 - val_loss: 2.1915 - val_mean_squared_error: 2.1915\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 0.8990 - mean_squared_error: 0.8990 - val_loss: 2.2157 - val_mean_squared_error: 2.2157\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.8700 - mean_squared_error: 0.8700 - val_loss: 2.1891 - val_mean_squared_error: 2.1891\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.9445 - mean_squared_error: 0.9445 - val_loss: 2.3553 - val_mean_squared_error: 2.3553\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 352us/sample - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 2.7447 - val_mean_squared_error: 2.7447\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.9512 - mean_squared_error: 0.9512 - val_loss: 2.1259 - val_mean_squared_error: 2.1259\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.8616 - mean_squared_error: 0.8616 - val_loss: 2.1555 - val_mean_squared_error: 2.1555\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.9247 - mean_squared_error: 0.9247 - val_loss: 2.2156 - val_mean_squared_error: 2.2156\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9498 - mean_squared_error: 0.9498 - val_loss: 2.3529 - val_mean_squared_error: 2.3529\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9424 - mean_squared_error: 0.9424 - val_loss: 2.2930 - val_mean_squared_error: 2.2930\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.8101 - mean_squared_error: 0.8101 - val_loss: 2.4248 - val_mean_squared_error: 2.4248\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.8882 - mean_squared_error: 0.8882 - val_loss: 2.3728 - val_mean_squared_error: 2.3728\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 0.9317 - mean_squared_error: 0.9317 - val_loss: 2.5592 - val_mean_squared_error: 2.5592\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.8729 - mean_squared_error: 0.8729 - val_loss: 2.4841 - val_mean_squared_error: 2.4841\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.9211 - mean_squared_error: 0.9211 - val_loss: 2.3825 - val_mean_squared_error: 2.3825\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.9133 - mean_squared_error: 0.9133 - val_loss: 2.2487 - val_mean_squared_error: 2.2487\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.9049 - mean_squared_error: 0.9049 - val_loss: 2.1880 - val_mean_squared_error: 2.1880\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.9042 - mean_squared_error: 0.9042 - val_loss: 2.4900 - val_mean_squared_error: 2.4900\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9154 - mean_squared_error: 0.9154 - val_loss: 2.2577 - val_mean_squared_error: 2.2577\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.8228 - mean_squared_error: 0.8228 - val_loss: 2.3956 - val_mean_squared_error: 2.3956\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.9032 - mean_squared_error: 0.9032 - val_loss: 2.1554 - val_mean_squared_error: 2.1554\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 0.7802 - mean_squared_error: 0.7802 - val_loss: 2.3113 - val_mean_squared_error: 2.3113\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8511 - mean_squared_error: 0.8511 - val_loss: 2.2460 - val_mean_squared_error: 2.2460\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.8208 - mean_squared_error: 0.8208 - val_loss: 2.2457 - val_mean_squared_error: 2.2457\n",
            "==================================================\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_33 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_34 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_57 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_35 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_58 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_59 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 752.1986 - mean_squared_error: 752.1985 - val_loss: 534.1517 - val_mean_squared_error: 534.1518\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 24.3845 - mean_squared_error: 24.3845 - val_loss: 248.4816 - val_mean_squared_error: 248.4816\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 19.8738 - mean_squared_error: 19.8738 - val_loss: 98.9694 - val_mean_squared_error: 98.9694\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 16.7640 - mean_squared_error: 16.7640 - val_loss: 54.1145 - val_mean_squared_error: 54.1145\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 16.8459 - mean_squared_error: 16.8459 - val_loss: 34.9952 - val_mean_squared_error: 34.9952\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 14.7466 - mean_squared_error: 14.7466 - val_loss: 21.0725 - val_mean_squared_error: 21.0725\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 15.6489 - mean_squared_error: 15.6489 - val_loss: 16.1047 - val_mean_squared_error: 16.1047\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 14.3627 - mean_squared_error: 14.3627 - val_loss: 14.6660 - val_mean_squared_error: 14.6660\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 12.4579 - mean_squared_error: 12.4579 - val_loss: 12.5658 - val_mean_squared_error: 12.5658\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 12.4075 - mean_squared_error: 12.4075 - val_loss: 11.0527 - val_mean_squared_error: 11.0527\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 12.6290 - mean_squared_error: 12.6290 - val_loss: 13.2095 - val_mean_squared_error: 13.2095\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 11.1727 - mean_squared_error: 11.1727 - val_loss: 10.0876 - val_mean_squared_error: 10.0876\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 10.3542 - mean_squared_error: 10.3542 - val_loss: 9.7482 - val_mean_squared_error: 9.7482\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 10.2630 - mean_squared_error: 10.2630 - val_loss: 10.6784 - val_mean_squared_error: 10.6784\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 10.5202 - mean_squared_error: 10.5202 - val_loss: 10.2482 - val_mean_squared_error: 10.2482\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 9.9334 - mean_squared_error: 9.9334 - val_loss: 9.4758 - val_mean_squared_error: 9.4758\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 9.0175 - mean_squared_error: 9.0175 - val_loss: 7.9077 - val_mean_squared_error: 7.9077\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 8.0286 - mean_squared_error: 8.0286 - val_loss: 8.2970 - val_mean_squared_error: 8.2970\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 8.1455 - mean_squared_error: 8.1455 - val_loss: 9.4227 - val_mean_squared_error: 9.4227\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 7.6024 - mean_squared_error: 7.6024 - val_loss: 9.2771 - val_mean_squared_error: 9.2771\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 7.7661 - mean_squared_error: 7.7661 - val_loss: 6.4719 - val_mean_squared_error: 6.4719\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 6.8875 - mean_squared_error: 6.8875 - val_loss: 6.4349 - val_mean_squared_error: 6.4349\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 6.2862 - mean_squared_error: 6.2862 - val_loss: 5.6345 - val_mean_squared_error: 5.6345\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 5.4928 - mean_squared_error: 5.4928 - val_loss: 6.3206 - val_mean_squared_error: 6.3206\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 5.5448 - mean_squared_error: 5.5448 - val_loss: 6.9210 - val_mean_squared_error: 6.9210\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 5.6824 - mean_squared_error: 5.6824 - val_loss: 7.3765 - val_mean_squared_error: 7.3765\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 5.0526 - mean_squared_error: 5.0526 - val_loss: 6.2780 - val_mean_squared_error: 6.2780\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 5.3514 - mean_squared_error: 5.3514 - val_loss: 4.7147 - val_mean_squared_error: 4.7147\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 4.5073 - mean_squared_error: 4.5073 - val_loss: 4.5604 - val_mean_squared_error: 4.5604\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 4.3030 - mean_squared_error: 4.3030 - val_loss: 4.2244 - val_mean_squared_error: 4.2244\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 4.0508 - mean_squared_error: 4.0508 - val_loss: 4.9321 - val_mean_squared_error: 4.9321\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.8754 - mean_squared_error: 3.8754 - val_loss: 3.9640 - val_mean_squared_error: 3.9640\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 4.2685 - mean_squared_error: 4.2685 - val_loss: 3.9723 - val_mean_squared_error: 3.9723\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 3.7145 - mean_squared_error: 3.7145 - val_loss: 4.5816 - val_mean_squared_error: 4.5816\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.6295 - mean_squared_error: 3.6295 - val_loss: 3.8054 - val_mean_squared_error: 3.8054\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 3.6687 - mean_squared_error: 3.6687 - val_loss: 4.9714 - val_mean_squared_error: 4.9714\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 3.3654 - mean_squared_error: 3.3654 - val_loss: 4.4141 - val_mean_squared_error: 4.4141\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 3.5009 - mean_squared_error: 3.5009 - val_loss: 3.5451 - val_mean_squared_error: 3.5451\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 3.2190 - mean_squared_error: 3.2190 - val_loss: 3.4112 - val_mean_squared_error: 3.4112\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.2970 - mean_squared_error: 3.2970 - val_loss: 3.1970 - val_mean_squared_error: 3.1970\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 3.2285 - mean_squared_error: 3.2285 - val_loss: 4.0807 - val_mean_squared_error: 4.0807\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 3.0722 - mean_squared_error: 3.0722 - val_loss: 4.7109 - val_mean_squared_error: 4.7109\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 3.0998 - mean_squared_error: 3.0998 - val_loss: 3.2946 - val_mean_squared_error: 3.2946\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 3.2267 - mean_squared_error: 3.2267 - val_loss: 3.6321 - val_mean_squared_error: 3.6321\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 3.1035 - mean_squared_error: 3.1035 - val_loss: 3.3005 - val_mean_squared_error: 3.3005\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.1403 - mean_squared_error: 3.1403 - val_loss: 3.1860 - val_mean_squared_error: 3.1860\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.8081 - mean_squared_error: 2.8081 - val_loss: 3.1400 - val_mean_squared_error: 3.1400\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 2.6955 - mean_squared_error: 2.6955 - val_loss: 3.2461 - val_mean_squared_error: 3.2461\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.6811 - mean_squared_error: 2.6811 - val_loss: 3.6074 - val_mean_squared_error: 3.6074\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.7350 - mean_squared_error: 2.7350 - val_loss: 2.9631 - val_mean_squared_error: 2.9631\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 2.8626 - mean_squared_error: 2.8626 - val_loss: 3.2306 - val_mean_squared_error: 3.2306\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.6092 - mean_squared_error: 2.6092 - val_loss: 3.5948 - val_mean_squared_error: 3.5948\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.6041 - mean_squared_error: 2.6041 - val_loss: 3.1378 - val_mean_squared_error: 3.1378\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.4501 - mean_squared_error: 2.4501 - val_loss: 3.0456 - val_mean_squared_error: 3.0456\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.6547 - mean_squared_error: 2.6547 - val_loss: 3.4038 - val_mean_squared_error: 3.4038\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.8570 - mean_squared_error: 2.8570 - val_loss: 3.5684 - val_mean_squared_error: 3.5684\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 2.6893 - mean_squared_error: 2.6893 - val_loss: 4.9947 - val_mean_squared_error: 4.9947\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.6713 - mean_squared_error: 2.6713 - val_loss: 3.9542 - val_mean_squared_error: 3.9542\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.6668 - mean_squared_error: 2.6668 - val_loss: 3.2717 - val_mean_squared_error: 3.2717\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.6563 - mean_squared_error: 2.6563 - val_loss: 3.2141 - val_mean_squared_error: 3.2141\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.3759 - mean_squared_error: 2.3759 - val_loss: 3.0971 - val_mean_squared_error: 3.0971\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.2362 - mean_squared_error: 2.2362 - val_loss: 3.0688 - val_mean_squared_error: 3.0688\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.1922 - mean_squared_error: 2.1922 - val_loss: 2.9677 - val_mean_squared_error: 2.9677\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.3284 - mean_squared_error: 2.3284 - val_loss: 3.2215 - val_mean_squared_error: 3.2215\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.1304 - mean_squared_error: 2.1304 - val_loss: 3.0090 - val_mean_squared_error: 3.0090\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.3140 - mean_squared_error: 2.3140 - val_loss: 4.2960 - val_mean_squared_error: 4.2960\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.2229 - mean_squared_error: 2.2229 - val_loss: 3.0986 - val_mean_squared_error: 3.0986\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.0881 - mean_squared_error: 2.0881 - val_loss: 2.7570 - val_mean_squared_error: 2.7570\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.1357 - mean_squared_error: 2.1357 - val_loss: 2.7784 - val_mean_squared_error: 2.7784\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.1188 - mean_squared_error: 2.1188 - val_loss: 3.0322 - val_mean_squared_error: 3.0322\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.1576 - mean_squared_error: 2.1576 - val_loss: 3.0662 - val_mean_squared_error: 3.0661\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.1658 - mean_squared_error: 2.1658 - val_loss: 3.1087 - val_mean_squared_error: 3.1087\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 2.2649 - mean_squared_error: 2.2649 - val_loss: 2.9215 - val_mean_squared_error: 2.9215\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.2024 - mean_squared_error: 2.2024 - val_loss: 2.7287 - val_mean_squared_error: 2.7287\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.0518 - mean_squared_error: 2.0518 - val_loss: 2.5431 - val_mean_squared_error: 2.5431\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.8179 - mean_squared_error: 1.8179 - val_loss: 2.4742 - val_mean_squared_error: 2.4742\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.8153 - mean_squared_error: 1.8153 - val_loss: 2.8756 - val_mean_squared_error: 2.8756\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.9486 - mean_squared_error: 1.9486 - val_loss: 2.5526 - val_mean_squared_error: 2.5526\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.0740 - mean_squared_error: 2.0740 - val_loss: 3.1856 - val_mean_squared_error: 3.1856\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.9375 - mean_squared_error: 1.9375 - val_loss: 2.7138 - val_mean_squared_error: 2.7138\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.0821 - mean_squared_error: 2.0821 - val_loss: 2.8163 - val_mean_squared_error: 2.8163\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.8090 - mean_squared_error: 1.8090 - val_loss: 2.9758 - val_mean_squared_error: 2.9758\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.3308 - mean_squared_error: 2.3308 - val_loss: 2.9677 - val_mean_squared_error: 2.9677\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.9497 - mean_squared_error: 1.9497 - val_loss: 2.7577 - val_mean_squared_error: 2.7577\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.8596 - mean_squared_error: 1.8596 - val_loss: 2.4212 - val_mean_squared_error: 2.4212\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.7618 - mean_squared_error: 1.7618 - val_loss: 2.6277 - val_mean_squared_error: 2.6277\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.9479 - mean_squared_error: 1.9479 - val_loss: 2.6791 - val_mean_squared_error: 2.6791\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.9175 - mean_squared_error: 1.9175 - val_loss: 2.7412 - val_mean_squared_error: 2.7412\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.8554 - mean_squared_error: 1.8554 - val_loss: 2.7587 - val_mean_squared_error: 2.7587\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.9116 - mean_squared_error: 1.9116 - val_loss: 2.4971 - val_mean_squared_error: 2.4971\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.6563 - mean_squared_error: 1.6563 - val_loss: 2.8362 - val_mean_squared_error: 2.8362\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.7758 - mean_squared_error: 1.7758 - val_loss: 2.8262 - val_mean_squared_error: 2.8262\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.7134 - mean_squared_error: 1.7134 - val_loss: 3.5328 - val_mean_squared_error: 3.5328\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.7982 - mean_squared_error: 1.7982 - val_loss: 2.4426 - val_mean_squared_error: 2.4426\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.7507 - mean_squared_error: 1.7507 - val_loss: 3.0024 - val_mean_squared_error: 3.0024\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.8164 - mean_squared_error: 1.8164 - val_loss: 2.7491 - val_mean_squared_error: 2.7491\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.6892 - mean_squared_error: 1.6892 - val_loss: 3.1048 - val_mean_squared_error: 3.1048\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.8877 - mean_squared_error: 1.8877 - val_loss: 2.4411 - val_mean_squared_error: 2.4411\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.6656 - mean_squared_error: 1.6656 - val_loss: 2.8577 - val_mean_squared_error: 2.8577\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.6569 - mean_squared_error: 1.6569 - val_loss: 2.4723 - val_mean_squared_error: 2.4723\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.6613 - mean_squared_error: 1.6613 - val_loss: 2.4679 - val_mean_squared_error: 2.4679\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.5090 - mean_squared_error: 1.5090 - val_loss: 2.8371 - val_mean_squared_error: 2.8371\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.8839 - mean_squared_error: 1.8839 - val_loss: 2.7366 - val_mean_squared_error: 2.7366\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.6416 - mean_squared_error: 1.6416 - val_loss: 2.3460 - val_mean_squared_error: 2.3460\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.9036 - mean_squared_error: 1.9036 - val_loss: 2.6199 - val_mean_squared_error: 2.6199\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6189 - mean_squared_error: 1.6189 - val_loss: 4.2692 - val_mean_squared_error: 4.2692\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.5758 - mean_squared_error: 1.5758 - val_loss: 2.2797 - val_mean_squared_error: 2.2797\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.5486 - mean_squared_error: 1.5486 - val_loss: 2.4350 - val_mean_squared_error: 2.4350\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.5136 - mean_squared_error: 1.5136 - val_loss: 2.3951 - val_mean_squared_error: 2.3951\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.6169 - mean_squared_error: 1.6169 - val_loss: 2.5829 - val_mean_squared_error: 2.5829\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.5414 - mean_squared_error: 1.5414 - val_loss: 2.6355 - val_mean_squared_error: 2.6355\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.5867 - mean_squared_error: 1.5867 - val_loss: 2.2788 - val_mean_squared_error: 2.2788\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.4014 - mean_squared_error: 1.4014 - val_loss: 2.3455 - val_mean_squared_error: 2.3455\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.4817 - mean_squared_error: 1.4817 - val_loss: 2.3173 - val_mean_squared_error: 2.3173\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.3983 - mean_squared_error: 1.3983 - val_loss: 3.4181 - val_mean_squared_error: 3.4181\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.5110 - mean_squared_error: 1.5110 - val_loss: 2.3870 - val_mean_squared_error: 2.3870\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4351 - mean_squared_error: 1.4351 - val_loss: 2.4293 - val_mean_squared_error: 2.4293\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.6235 - mean_squared_error: 1.6235 - val_loss: 2.4864 - val_mean_squared_error: 2.4864\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.4229 - mean_squared_error: 1.4229 - val_loss: 2.6238 - val_mean_squared_error: 2.6238\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.4823 - mean_squared_error: 1.4823 - val_loss: 2.3457 - val_mean_squared_error: 2.3457\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.3169 - mean_squared_error: 1.3169 - val_loss: 2.2587 - val_mean_squared_error: 2.2587\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.4494 - mean_squared_error: 1.4494 - val_loss: 2.2011 - val_mean_squared_error: 2.2011\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.3293 - mean_squared_error: 1.3293 - val_loss: 2.3077 - val_mean_squared_error: 2.3077\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.3495 - mean_squared_error: 1.3495 - val_loss: 2.5291 - val_mean_squared_error: 2.5291\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.3434 - mean_squared_error: 1.3434 - val_loss: 2.1875 - val_mean_squared_error: 2.1875\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.4021 - mean_squared_error: 1.4021 - val_loss: 2.3828 - val_mean_squared_error: 2.3828\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 1.2606 - mean_squared_error: 1.2606 - val_loss: 2.1962 - val_mean_squared_error: 2.1962\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.3369 - mean_squared_error: 1.3369 - val_loss: 2.4195 - val_mean_squared_error: 2.4195\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.3881 - mean_squared_error: 1.3881 - val_loss: 2.3533 - val_mean_squared_error: 2.3533\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.4829 - mean_squared_error: 1.4829 - val_loss: 3.0559 - val_mean_squared_error: 3.0559\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.2455 - mean_squared_error: 1.2455 - val_loss: 2.1961 - val_mean_squared_error: 2.1961\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.2759 - mean_squared_error: 1.2759 - val_loss: 2.4746 - val_mean_squared_error: 2.4746\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.3192 - mean_squared_error: 1.3192 - val_loss: 2.5398 - val_mean_squared_error: 2.5398\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.3562 - mean_squared_error: 1.3562 - val_loss: 2.2581 - val_mean_squared_error: 2.2581\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.1850 - mean_squared_error: 1.1850 - val_loss: 2.5764 - val_mean_squared_error: 2.5764\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.2066 - mean_squared_error: 1.2066 - val_loss: 2.6194 - val_mean_squared_error: 2.6194\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.2501 - mean_squared_error: 1.2501 - val_loss: 2.2233 - val_mean_squared_error: 2.2233\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.2946 - mean_squared_error: 1.2946 - val_loss: 2.7840 - val_mean_squared_error: 2.7840\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3935 - mean_squared_error: 1.3935 - val_loss: 2.2108 - val_mean_squared_error: 2.2108\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3284 - mean_squared_error: 1.3284 - val_loss: 2.3022 - val_mean_squared_error: 2.3022\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.2520 - mean_squared_error: 1.2520 - val_loss: 2.3099 - val_mean_squared_error: 2.3099\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.3061 - mean_squared_error: 1.3061 - val_loss: 2.1250 - val_mean_squared_error: 2.1250\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.3170 - mean_squared_error: 1.3170 - val_loss: 2.1705 - val_mean_squared_error: 2.1705\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2321 - mean_squared_error: 1.2321 - val_loss: 2.3150 - val_mean_squared_error: 2.3150\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1603 - mean_squared_error: 1.1603 - val_loss: 2.1484 - val_mean_squared_error: 2.1484\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.1962 - mean_squared_error: 1.1962 - val_loss: 2.3739 - val_mean_squared_error: 2.3739\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.3321 - mean_squared_error: 1.3321 - val_loss: 2.3498 - val_mean_squared_error: 2.3498\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2715 - mean_squared_error: 1.2715 - val_loss: 2.3346 - val_mean_squared_error: 2.3346\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.2357 - mean_squared_error: 1.2357 - val_loss: 2.1247 - val_mean_squared_error: 2.1247\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2748 - mean_squared_error: 1.2748 - val_loss: 2.3861 - val_mean_squared_error: 2.3861\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2475 - mean_squared_error: 1.2475 - val_loss: 2.5283 - val_mean_squared_error: 2.5283\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.1414 - mean_squared_error: 1.1414 - val_loss: 2.2124 - val_mean_squared_error: 2.2124\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2894 - mean_squared_error: 1.2894 - val_loss: 2.4712 - val_mean_squared_error: 2.4712\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.1821 - mean_squared_error: 1.1821 - val_loss: 2.2930 - val_mean_squared_error: 2.2930\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.1155 - mean_squared_error: 1.1155 - val_loss: 2.1339 - val_mean_squared_error: 2.1339\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.1553 - mean_squared_error: 1.1553 - val_loss: 2.2563 - val_mean_squared_error: 2.2563\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2439 - mean_squared_error: 1.2439 - val_loss: 2.1525 - val_mean_squared_error: 2.1525\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1279 - mean_squared_error: 1.1279 - val_loss: 2.2014 - val_mean_squared_error: 2.2014\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.2116 - mean_squared_error: 1.2116 - val_loss: 2.2219 - val_mean_squared_error: 2.2219\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.1376 - mean_squared_error: 1.1376 - val_loss: 2.3575 - val_mean_squared_error: 2.3575\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2018 - mean_squared_error: 1.2018 - val_loss: 2.1707 - val_mean_squared_error: 2.1707\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.1327 - mean_squared_error: 1.1327 - val_loss: 2.1658 - val_mean_squared_error: 2.1658\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.0184 - mean_squared_error: 1.0184 - val_loss: 2.0903 - val_mean_squared_error: 2.0903\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0570 - mean_squared_error: 1.0570 - val_loss: 2.4442 - val_mean_squared_error: 2.4442\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0841 - mean_squared_error: 1.0841 - val_loss: 2.1440 - val_mean_squared_error: 2.1440\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9954 - mean_squared_error: 0.9954 - val_loss: 2.1729 - val_mean_squared_error: 2.1729\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9920 - mean_squared_error: 0.9920 - val_loss: 2.1099 - val_mean_squared_error: 2.1099\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.9963 - mean_squared_error: 0.9963 - val_loss: 2.2615 - val_mean_squared_error: 2.2615\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9983 - mean_squared_error: 0.9983 - val_loss: 2.1167 - val_mean_squared_error: 2.1167\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.1002 - mean_squared_error: 1.1002 - val_loss: 2.4127 - val_mean_squared_error: 2.4127\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0980 - mean_squared_error: 1.0980 - val_loss: 2.3205 - val_mean_squared_error: 2.3205\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1612 - mean_squared_error: 1.1612 - val_loss: 2.4647 - val_mean_squared_error: 2.4647\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1085 - mean_squared_error: 1.1085 - val_loss: 1.9791 - val_mean_squared_error: 1.9791\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9925 - mean_squared_error: 0.9925 - val_loss: 2.1290 - val_mean_squared_error: 2.1290\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.0365 - mean_squared_error: 1.0365 - val_loss: 2.2745 - val_mean_squared_error: 2.2745\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0356 - mean_squared_error: 1.0356 - val_loss: 2.2691 - val_mean_squared_error: 2.2691\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0191 - mean_squared_error: 1.0191 - val_loss: 1.9765 - val_mean_squared_error: 1.9765\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1015 - mean_squared_error: 1.1015 - val_loss: 2.2427 - val_mean_squared_error: 2.2427\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 2.3609 - val_mean_squared_error: 2.3609\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0315 - mean_squared_error: 1.0315 - val_loss: 2.1489 - val_mean_squared_error: 2.1489\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.1122 - mean_squared_error: 1.1122 - val_loss: 2.2758 - val_mean_squared_error: 2.2758\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0691 - mean_squared_error: 1.0691 - val_loss: 2.1859 - val_mean_squared_error: 2.1859\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.1282 - mean_squared_error: 1.1282 - val_loss: 2.3373 - val_mean_squared_error: 2.3373\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.0598 - mean_squared_error: 1.0598 - val_loss: 2.2976 - val_mean_squared_error: 2.2976\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0981 - mean_squared_error: 1.0981 - val_loss: 2.2423 - val_mean_squared_error: 2.2423\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9486 - mean_squared_error: 0.9486 - val_loss: 2.1178 - val_mean_squared_error: 2.1178\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.9691 - mean_squared_error: 0.9691 - val_loss: 2.2658 - val_mean_squared_error: 2.2658\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9321 - mean_squared_error: 0.9321 - val_loss: 2.1423 - val_mean_squared_error: 2.1423\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.0795 - mean_squared_error: 1.0795 - val_loss: 2.2842 - val_mean_squared_error: 2.2842\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0715 - mean_squared_error: 1.0715 - val_loss: 2.6895 - val_mean_squared_error: 2.6895\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9993 - mean_squared_error: 0.9993 - val_loss: 1.9700 - val_mean_squared_error: 1.9700\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.9279 - mean_squared_error: 0.9279 - val_loss: 2.0840 - val_mean_squared_error: 2.0840\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9980 - mean_squared_error: 0.9980 - val_loss: 2.0046 - val_mean_squared_error: 2.0046\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0268 - mean_squared_error: 1.0268 - val_loss: 2.0250 - val_mean_squared_error: 2.0250\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.0152 - mean_squared_error: 1.0152 - val_loss: 2.2118 - val_mean_squared_error: 2.2118\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9665 - mean_squared_error: 0.9665 - val_loss: 2.3356 - val_mean_squared_error: 2.3356\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9104 - mean_squared_error: 0.9104 - val_loss: 2.1622 - val_mean_squared_error: 2.1622\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.9322 - mean_squared_error: 0.9322 - val_loss: 2.1543 - val_mean_squared_error: 2.1543\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.9701 - mean_squared_error: 0.9701 - val_loss: 2.0931 - val_mean_squared_error: 2.0931\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9959 - mean_squared_error: 0.9959 - val_loss: 2.0809 - val_mean_squared_error: 2.0809\n",
            "==================================================\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_36 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_60 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_37 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_38 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_50 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_51 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 744.7664 - mean_squared_error: 744.7665 - val_loss: 498.6510 - val_mean_squared_error: 498.6511\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 31.3930 - mean_squared_error: 31.3930 - val_loss: 188.4672 - val_mean_squared_error: 188.4672\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 20.1494 - mean_squared_error: 20.1494 - val_loss: 72.5403 - val_mean_squared_error: 72.5403\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 17.2957 - mean_squared_error: 17.2957 - val_loss: 34.4835 - val_mean_squared_error: 34.4835\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 18.2430 - mean_squared_error: 18.2430 - val_loss: 29.0839 - val_mean_squared_error: 29.0839\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 15.6686 - mean_squared_error: 15.6686 - val_loss: 30.3222 - val_mean_squared_error: 30.3222\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 17.3686 - mean_squared_error: 17.3686 - val_loss: 18.2780 - val_mean_squared_error: 18.2780\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 13.9815 - mean_squared_error: 13.9815 - val_loss: 11.0656 - val_mean_squared_error: 11.0656\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 13.2488 - mean_squared_error: 13.2488 - val_loss: 10.7433 - val_mean_squared_error: 10.7433\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 12.8396 - mean_squared_error: 12.8396 - val_loss: 11.0404 - val_mean_squared_error: 11.0404\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 11.5661 - mean_squared_error: 11.5661 - val_loss: 13.7729 - val_mean_squared_error: 13.7729\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 11.4579 - mean_squared_error: 11.4579 - val_loss: 13.5753 - val_mean_squared_error: 13.5753\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 10.7504 - mean_squared_error: 10.7504 - val_loss: 10.3545 - val_mean_squared_error: 10.3545\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 10.0759 - mean_squared_error: 10.0759 - val_loss: 13.8030 - val_mean_squared_error: 13.8030\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 10.1458 - mean_squared_error: 10.1458 - val_loss: 10.2259 - val_mean_squared_error: 10.2259\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 8.5863 - mean_squared_error: 8.5863 - val_loss: 10.0732 - val_mean_squared_error: 10.0732\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 8.7602 - mean_squared_error: 8.7602 - val_loss: 9.2241 - val_mean_squared_error: 9.2241\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 8.6665 - mean_squared_error: 8.6665 - val_loss: 8.3891 - val_mean_squared_error: 8.3891\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 7.8400 - mean_squared_error: 7.8400 - val_loss: 7.9285 - val_mean_squared_error: 7.9285\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 7.2093 - mean_squared_error: 7.2093 - val_loss: 7.6086 - val_mean_squared_error: 7.6086\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 7.2693 - mean_squared_error: 7.2693 - val_loss: 7.1022 - val_mean_squared_error: 7.1022\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 7.4667 - mean_squared_error: 7.4667 - val_loss: 6.2520 - val_mean_squared_error: 6.2520\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 6.2485 - mean_squared_error: 6.2485 - val_loss: 6.1972 - val_mean_squared_error: 6.1972\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 5.6106 - mean_squared_error: 5.6106 - val_loss: 5.8409 - val_mean_squared_error: 5.8409\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 5.6618 - mean_squared_error: 5.6618 - val_loss: 5.4778 - val_mean_squared_error: 5.4778\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 5.4279 - mean_squared_error: 5.4279 - val_loss: 5.6356 - val_mean_squared_error: 5.6356\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 5.0637 - mean_squared_error: 5.0637 - val_loss: 4.9591 - val_mean_squared_error: 4.9591\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 4.6501 - mean_squared_error: 4.6501 - val_loss: 5.3719 - val_mean_squared_error: 5.3719\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 4.5210 - mean_squared_error: 4.5210 - val_loss: 4.1210 - val_mean_squared_error: 4.1210\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 4.5525 - mean_squared_error: 4.5525 - val_loss: 4.5281 - val_mean_squared_error: 4.5281\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 4.5493 - mean_squared_error: 4.5493 - val_loss: 4.4851 - val_mean_squared_error: 4.4851\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 4.2779 - mean_squared_error: 4.2779 - val_loss: 3.7843 - val_mean_squared_error: 3.7843\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 4.2088 - mean_squared_error: 4.2088 - val_loss: 4.4585 - val_mean_squared_error: 4.4585\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.7948 - mean_squared_error: 3.7948 - val_loss: 4.3356 - val_mean_squared_error: 4.3356\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.8005 - mean_squared_error: 3.8005 - val_loss: 4.4593 - val_mean_squared_error: 4.4593\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 3.5982 - mean_squared_error: 3.5982 - val_loss: 3.5955 - val_mean_squared_error: 3.5955\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 3.9677 - mean_squared_error: 3.9677 - val_loss: 3.6547 - val_mean_squared_error: 3.6547\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.8666 - mean_squared_error: 3.8666 - val_loss: 3.6495 - val_mean_squared_error: 3.6495\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.6389 - mean_squared_error: 3.6389 - val_loss: 3.3577 - val_mean_squared_error: 3.3577\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 3.8272 - mean_squared_error: 3.8272 - val_loss: 4.9095 - val_mean_squared_error: 4.9095\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.4793 - mean_squared_error: 3.4793 - val_loss: 4.0986 - val_mean_squared_error: 4.0986\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 3.5569 - mean_squared_error: 3.5569 - val_loss: 3.5576 - val_mean_squared_error: 3.5576\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 3.3134 - mean_squared_error: 3.3134 - val_loss: 4.3686 - val_mean_squared_error: 4.3686\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 3.0659 - mean_squared_error: 3.0659 - val_loss: 3.3438 - val_mean_squared_error: 3.3438\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 3.2455 - mean_squared_error: 3.2455 - val_loss: 4.2986 - val_mean_squared_error: 4.2986\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.9685 - mean_squared_error: 2.9685 - val_loss: 3.4461 - val_mean_squared_error: 3.4461\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.0872 - mean_squared_error: 3.0872 - val_loss: 3.2406 - val_mean_squared_error: 3.2406\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.9136 - mean_squared_error: 2.9136 - val_loss: 3.2224 - val_mean_squared_error: 3.2224\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.9970 - mean_squared_error: 2.9970 - val_loss: 3.0736 - val_mean_squared_error: 3.0736\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.0005 - mean_squared_error: 3.0005 - val_loss: 3.1038 - val_mean_squared_error: 3.1038\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.8061 - mean_squared_error: 2.8061 - val_loss: 3.0358 - val_mean_squared_error: 3.0358\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.8333 - mean_squared_error: 2.8333 - val_loss: 3.4064 - val_mean_squared_error: 3.4064\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.8243 - mean_squared_error: 2.8243 - val_loss: 3.8932 - val_mean_squared_error: 3.8932\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.9184 - mean_squared_error: 2.9184 - val_loss: 3.6671 - val_mean_squared_error: 3.6671\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.7147 - mean_squared_error: 2.7147 - val_loss: 2.8932 - val_mean_squared_error: 2.8932\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.7333 - mean_squared_error: 2.7333 - val_loss: 3.9129 - val_mean_squared_error: 3.9129\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.7616 - mean_squared_error: 2.7616 - val_loss: 3.1524 - val_mean_squared_error: 3.1524\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.5217 - mean_squared_error: 2.5217 - val_loss: 2.7445 - val_mean_squared_error: 2.7445\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.4664 - mean_squared_error: 2.4664 - val_loss: 3.6373 - val_mean_squared_error: 3.6373\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.5176 - mean_squared_error: 2.5176 - val_loss: 3.1708 - val_mean_squared_error: 3.1708\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.4913 - mean_squared_error: 2.4913 - val_loss: 3.3340 - val_mean_squared_error: 3.3340\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.4545 - mean_squared_error: 2.4545 - val_loss: 3.1687 - val_mean_squared_error: 3.1687\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 2.5585 - mean_squared_error: 2.5585 - val_loss: 3.7236 - val_mean_squared_error: 3.7236\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 2.6561 - mean_squared_error: 2.6561 - val_loss: 3.1687 - val_mean_squared_error: 3.1687\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.7840 - mean_squared_error: 2.7840 - val_loss: 3.1641 - val_mean_squared_error: 3.1641\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 2.5260 - mean_squared_error: 2.5260 - val_loss: 2.8832 - val_mean_squared_error: 2.8832\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.4678 - mean_squared_error: 2.4678 - val_loss: 2.8352 - val_mean_squared_error: 2.8352\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.2909 - mean_squared_error: 2.2909 - val_loss: 3.3281 - val_mean_squared_error: 3.3281\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.2477 - mean_squared_error: 2.2477 - val_loss: 2.8469 - val_mean_squared_error: 2.8469\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.2574 - mean_squared_error: 2.2574 - val_loss: 3.6339 - val_mean_squared_error: 3.6339\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.2573 - mean_squared_error: 2.2573 - val_loss: 3.2940 - val_mean_squared_error: 3.2940\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.3617 - mean_squared_error: 2.3617 - val_loss: 2.4790 - val_mean_squared_error: 2.4790\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.4411 - mean_squared_error: 2.4411 - val_loss: 3.1831 - val_mean_squared_error: 3.1831\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.1997 - mean_squared_error: 2.1997 - val_loss: 2.7107 - val_mean_squared_error: 2.7107\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.1457 - mean_squared_error: 2.1457 - val_loss: 2.8729 - val_mean_squared_error: 2.8729\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.0063 - mean_squared_error: 2.0063 - val_loss: 2.8514 - val_mean_squared_error: 2.8514\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.0822 - mean_squared_error: 2.0822 - val_loss: 2.8635 - val_mean_squared_error: 2.8635\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.1237 - mean_squared_error: 2.1237 - val_loss: 2.4750 - val_mean_squared_error: 2.4750\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 2.3085 - mean_squared_error: 2.3085 - val_loss: 2.8874 - val_mean_squared_error: 2.8874\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.0254 - mean_squared_error: 2.0254 - val_loss: 2.8180 - val_mean_squared_error: 2.8180\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.9348 - mean_squared_error: 1.9348 - val_loss: 2.9486 - val_mean_squared_error: 2.9486\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.8511 - mean_squared_error: 1.8511 - val_loss: 2.5530 - val_mean_squared_error: 2.5530\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.8896 - mean_squared_error: 1.8896 - val_loss: 2.5076 - val_mean_squared_error: 2.5076\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.9918 - mean_squared_error: 1.9918 - val_loss: 3.2569 - val_mean_squared_error: 3.2569\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.9767 - mean_squared_error: 1.9767 - val_loss: 2.7913 - val_mean_squared_error: 2.7913\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.8878 - mean_squared_error: 1.8878 - val_loss: 2.3898 - val_mean_squared_error: 2.3898\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.0900 - mean_squared_error: 2.0900 - val_loss: 3.6339 - val_mean_squared_error: 3.6339\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.9939 - mean_squared_error: 1.9939 - val_loss: 2.3909 - val_mean_squared_error: 2.3909\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.8288 - mean_squared_error: 1.8288 - val_loss: 2.7325 - val_mean_squared_error: 2.7325\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.9897 - mean_squared_error: 1.9897 - val_loss: 2.9673 - val_mean_squared_error: 2.9673\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.9780 - mean_squared_error: 1.9780 - val_loss: 2.4469 - val_mean_squared_error: 2.4469\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.9697 - mean_squared_error: 1.9697 - val_loss: 2.9802 - val_mean_squared_error: 2.9802\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.7208 - mean_squared_error: 1.7208 - val_loss: 2.5833 - val_mean_squared_error: 2.5833\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.0056 - mean_squared_error: 2.0056 - val_loss: 2.5204 - val_mean_squared_error: 2.5204\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.7907 - mean_squared_error: 1.7907 - val_loss: 2.4330 - val_mean_squared_error: 2.4330\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.8186 - mean_squared_error: 1.8186 - val_loss: 2.5936 - val_mean_squared_error: 2.5936\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.6979 - mean_squared_error: 1.6979 - val_loss: 2.3131 - val_mean_squared_error: 2.3131\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.7359 - mean_squared_error: 1.7359 - val_loss: 2.5654 - val_mean_squared_error: 2.5654\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.7557 - mean_squared_error: 1.7557 - val_loss: 2.5708 - val_mean_squared_error: 2.5708\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7049 - mean_squared_error: 1.7049 - val_loss: 2.5552 - val_mean_squared_error: 2.5552\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6274 - mean_squared_error: 1.6274 - val_loss: 3.1053 - val_mean_squared_error: 3.1053\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.6094 - mean_squared_error: 1.6094 - val_loss: 2.6774 - val_mean_squared_error: 2.6774\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.6772 - mean_squared_error: 1.6772 - val_loss: 2.5400 - val_mean_squared_error: 2.5400\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.6797 - mean_squared_error: 1.6797 - val_loss: 2.5937 - val_mean_squared_error: 2.5937\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7029 - mean_squared_error: 1.7029 - val_loss: 2.5015 - val_mean_squared_error: 2.5015\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.6431 - mean_squared_error: 1.6431 - val_loss: 2.7047 - val_mean_squared_error: 2.7047\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7337 - mean_squared_error: 1.7337 - val_loss: 2.2543 - val_mean_squared_error: 2.2543\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.7086 - mean_squared_error: 1.7086 - val_loss: 2.2739 - val_mean_squared_error: 2.2739\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5550 - mean_squared_error: 1.5550 - val_loss: 3.0210 - val_mean_squared_error: 3.0210\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.5375 - mean_squared_error: 1.5375 - val_loss: 2.2888 - val_mean_squared_error: 2.2888\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5560 - mean_squared_error: 1.5560 - val_loss: 2.4733 - val_mean_squared_error: 2.4733\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5845 - mean_squared_error: 1.5845 - val_loss: 2.4023 - val_mean_squared_error: 2.4023\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.5845 - mean_squared_error: 1.5845 - val_loss: 3.6543 - val_mean_squared_error: 3.6543\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.7443 - mean_squared_error: 1.7443 - val_loss: 2.2730 - val_mean_squared_error: 2.2730\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 356us/sample - loss: 1.6137 - mean_squared_error: 1.6137 - val_loss: 2.5785 - val_mean_squared_error: 2.5785\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.5617 - mean_squared_error: 1.5617 - val_loss: 2.6495 - val_mean_squared_error: 2.6495\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.6300 - mean_squared_error: 1.6300 - val_loss: 2.5898 - val_mean_squared_error: 2.5898\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 355us/sample - loss: 1.6138 - mean_squared_error: 1.6138 - val_loss: 2.4724 - val_mean_squared_error: 2.4724\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4704 - mean_squared_error: 1.4704 - val_loss: 2.1782 - val_mean_squared_error: 2.1782\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 354us/sample - loss: 1.4452 - mean_squared_error: 1.4452 - val_loss: 2.2634 - val_mean_squared_error: 2.2634\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.4382 - mean_squared_error: 1.4382 - val_loss: 2.4765 - val_mean_squared_error: 2.4765\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.5336 - mean_squared_error: 1.5336 - val_loss: 2.7176 - val_mean_squared_error: 2.7176\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.4525 - mean_squared_error: 1.4525 - val_loss: 2.1976 - val_mean_squared_error: 2.1976\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.4805 - mean_squared_error: 1.4804 - val_loss: 2.3275 - val_mean_squared_error: 2.3275\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4755 - mean_squared_error: 1.4755 - val_loss: 2.2820 - val_mean_squared_error: 2.2820\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5107 - mean_squared_error: 1.5107 - val_loss: 2.5054 - val_mean_squared_error: 2.5054\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.6007 - mean_squared_error: 1.6007 - val_loss: 2.4419 - val_mean_squared_error: 2.4419\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3281 - mean_squared_error: 1.3281 - val_loss: 2.2774 - val_mean_squared_error: 2.2774\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.5225 - mean_squared_error: 1.5225 - val_loss: 2.3579 - val_mean_squared_error: 2.3579\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.3712 - mean_squared_error: 1.3712 - val_loss: 2.4636 - val_mean_squared_error: 2.4636\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.4159 - mean_squared_error: 1.4159 - val_loss: 2.4332 - val_mean_squared_error: 2.4332\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3592 - mean_squared_error: 1.3592 - val_loss: 2.2298 - val_mean_squared_error: 2.2298\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3637 - mean_squared_error: 1.3637 - val_loss: 2.1555 - val_mean_squared_error: 2.1555\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.4614 - mean_squared_error: 1.4614 - val_loss: 2.2743 - val_mean_squared_error: 2.2743\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.3735 - mean_squared_error: 1.3735 - val_loss: 2.3385 - val_mean_squared_error: 2.3385\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.3413 - mean_squared_error: 1.3413 - val_loss: 2.3043 - val_mean_squared_error: 2.3043\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.4111 - mean_squared_error: 1.4111 - val_loss: 2.2122 - val_mean_squared_error: 2.2122\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.3732 - mean_squared_error: 1.3732 - val_loss: 2.2829 - val_mean_squared_error: 2.2829\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.4266 - mean_squared_error: 1.4266 - val_loss: 2.1884 - val_mean_squared_error: 2.1884\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.1911 - mean_squared_error: 1.1911 - val_loss: 2.3118 - val_mean_squared_error: 2.3118\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3604 - mean_squared_error: 1.3604 - val_loss: 2.2564 - val_mean_squared_error: 2.2564\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3844 - mean_squared_error: 1.3844 - val_loss: 2.1640 - val_mean_squared_error: 2.1640\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1843 - mean_squared_error: 1.1843 - val_loss: 2.1473 - val_mean_squared_error: 2.1473\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.3183 - mean_squared_error: 1.3183 - val_loss: 2.3268 - val_mean_squared_error: 2.3268\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.4229 - mean_squared_error: 1.4229 - val_loss: 2.6671 - val_mean_squared_error: 2.6671\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.3663 - mean_squared_error: 1.3663 - val_loss: 2.1952 - val_mean_squared_error: 2.1952\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.3716 - mean_squared_error: 1.3716 - val_loss: 2.1986 - val_mean_squared_error: 2.1986\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.3507 - mean_squared_error: 1.3507 - val_loss: 2.0669 - val_mean_squared_error: 2.0669\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.3093 - mean_squared_error: 1.3093 - val_loss: 2.3107 - val_mean_squared_error: 2.3107\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2419 - mean_squared_error: 1.2419 - val_loss: 2.1713 - val_mean_squared_error: 2.1713\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3208 - mean_squared_error: 1.3208 - val_loss: 2.4019 - val_mean_squared_error: 2.4019\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2385 - mean_squared_error: 1.2385 - val_loss: 2.2552 - val_mean_squared_error: 2.2552\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2226 - mean_squared_error: 1.2226 - val_loss: 2.0960 - val_mean_squared_error: 2.0960\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1517 - mean_squared_error: 1.1517 - val_loss: 2.1489 - val_mean_squared_error: 2.1489\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.2870 - mean_squared_error: 1.2870 - val_loss: 2.0862 - val_mean_squared_error: 2.0862\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.2595 - mean_squared_error: 1.2595 - val_loss: 2.2626 - val_mean_squared_error: 2.2626\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3970 - mean_squared_error: 1.3970 - val_loss: 2.2959 - val_mean_squared_error: 2.2959\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2829 - mean_squared_error: 1.2829 - val_loss: 2.3139 - val_mean_squared_error: 2.3139\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1949 - mean_squared_error: 1.1949 - val_loss: 2.2368 - val_mean_squared_error: 2.2368\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.2412 - mean_squared_error: 1.2412 - val_loss: 2.1772 - val_mean_squared_error: 2.1772\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1304 - mean_squared_error: 1.1304 - val_loss: 2.0791 - val_mean_squared_error: 2.0791\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1819 - mean_squared_error: 1.1819 - val_loss: 1.9861 - val_mean_squared_error: 1.9861\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2681 - mean_squared_error: 1.2681 - val_loss: 2.2493 - val_mean_squared_error: 2.2493\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1077 - mean_squared_error: 1.1077 - val_loss: 2.1157 - val_mean_squared_error: 2.1157\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2278 - mean_squared_error: 1.2278 - val_loss: 2.0587 - val_mean_squared_error: 2.0587\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2580 - mean_squared_error: 1.2580 - val_loss: 2.1662 - val_mean_squared_error: 2.1662\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0257 - mean_squared_error: 1.0257 - val_loss: 2.0818 - val_mean_squared_error: 2.0818\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.0923 - mean_squared_error: 1.0923 - val_loss: 2.0511 - val_mean_squared_error: 2.0511\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.1949 - mean_squared_error: 1.1949 - val_loss: 2.2058 - val_mean_squared_error: 2.2058\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1442 - mean_squared_error: 1.1442 - val_loss: 2.1597 - val_mean_squared_error: 2.1597\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1076 - mean_squared_error: 1.1076 - val_loss: 1.9340 - val_mean_squared_error: 1.9340\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1871 - mean_squared_error: 1.1871 - val_loss: 2.0702 - val_mean_squared_error: 2.0702\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1748 - mean_squared_error: 1.1748 - val_loss: 2.0336 - val_mean_squared_error: 2.0336\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2314 - mean_squared_error: 1.2314 - val_loss: 2.4283 - val_mean_squared_error: 2.4283\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0883 - mean_squared_error: 1.0883 - val_loss: 2.2362 - val_mean_squared_error: 2.2362\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0318 - mean_squared_error: 1.0318 - val_loss: 2.0358 - val_mean_squared_error: 2.0358\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.0320 - mean_squared_error: 1.0320 - val_loss: 2.4448 - val_mean_squared_error: 2.4448\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 2.2951 - val_mean_squared_error: 2.2951\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0135 - mean_squared_error: 1.0135 - val_loss: 2.0049 - val_mean_squared_error: 2.0049\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0465 - mean_squared_error: 1.0465 - val_loss: 1.9664 - val_mean_squared_error: 1.9664\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0363 - mean_squared_error: 1.0363 - val_loss: 2.0912 - val_mean_squared_error: 2.0912\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9784 - mean_squared_error: 0.9784 - val_loss: 2.1574 - val_mean_squared_error: 2.1574\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.0120 - mean_squared_error: 1.0120 - val_loss: 2.0406 - val_mean_squared_error: 2.0406\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0380 - mean_squared_error: 1.0380 - val_loss: 2.0789 - val_mean_squared_error: 2.0789\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.0501 - mean_squared_error: 1.0501 - val_loss: 2.1814 - val_mean_squared_error: 2.1814\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.0705 - mean_squared_error: 1.0705 - val_loss: 2.0276 - val_mean_squared_error: 2.0276\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.9434 - mean_squared_error: 0.9434 - val_loss: 2.0416 - val_mean_squared_error: 2.0416\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1239 - mean_squared_error: 1.1239 - val_loss: 1.9965 - val_mean_squared_error: 1.9965\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0584 - mean_squared_error: 1.0584 - val_loss: 1.9297 - val_mean_squared_error: 1.9297\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0387 - mean_squared_error: 1.0387 - val_loss: 2.1367 - val_mean_squared_error: 2.1367\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.0928 - mean_squared_error: 1.0928 - val_loss: 2.2209 - val_mean_squared_error: 2.2209\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.0288 - mean_squared_error: 1.0288 - val_loss: 2.3100 - val_mean_squared_error: 2.3100\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1001 - mean_squared_error: 1.1001 - val_loss: 2.0450 - val_mean_squared_error: 2.0450\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0502 - mean_squared_error: 1.0502 - val_loss: 2.1577 - val_mean_squared_error: 2.1577\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.9957 - mean_squared_error: 0.9957 - val_loss: 2.1361 - val_mean_squared_error: 2.1361\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 2.2196 - val_mean_squared_error: 2.2196\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.0501 - mean_squared_error: 1.0501 - val_loss: 2.0946 - val_mean_squared_error: 2.0946\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.9383 - mean_squared_error: 0.9383 - val_loss: 2.1165 - val_mean_squared_error: 2.1165\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9182 - mean_squared_error: 0.9182 - val_loss: 2.6021 - val_mean_squared_error: 2.6021\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0233 - mean_squared_error: 1.0233 - val_loss: 2.1529 - val_mean_squared_error: 2.1529\n",
            "==================================================\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_39 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_39 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_52 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_40 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_53 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_41 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_54 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_68 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_55 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_69 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 745.6133 - mean_squared_error: 745.6135 - val_loss: 303.8260 - val_mean_squared_error: 303.8260\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 429us/sample - loss: 27.8673 - mean_squared_error: 27.8673 - val_loss: 153.7515 - val_mean_squared_error: 153.7515\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 20.8401 - mean_squared_error: 20.8401 - val_loss: 65.6920 - val_mean_squared_error: 65.6920\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 16.6916 - mean_squared_error: 16.6916 - val_loss: 31.7187 - val_mean_squared_error: 31.7187\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 17.5166 - mean_squared_error: 17.5166 - val_loss: 20.4461 - val_mean_squared_error: 20.4461\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 16.2899 - mean_squared_error: 16.2899 - val_loss: 28.1037 - val_mean_squared_error: 28.1037\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 15.5508 - mean_squared_error: 15.5508 - val_loss: 34.0748 - val_mean_squared_error: 34.0748\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 13.7372 - mean_squared_error: 13.7372 - val_loss: 21.8118 - val_mean_squared_error: 21.8118\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 13.9834 - mean_squared_error: 13.9834 - val_loss: 16.2481 - val_mean_squared_error: 16.2481\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 12.3348 - mean_squared_error: 12.3348 - val_loss: 15.4518 - val_mean_squared_error: 15.4518\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 12.6907 - mean_squared_error: 12.6907 - val_loss: 11.5376 - val_mean_squared_error: 11.5376\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 11.1371 - mean_squared_error: 11.1371 - val_loss: 14.9451 - val_mean_squared_error: 14.9451\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 10.3166 - mean_squared_error: 10.3166 - val_loss: 9.7148 - val_mean_squared_error: 9.7148\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 10.7966 - mean_squared_error: 10.7966 - val_loss: 9.1983 - val_mean_squared_error: 9.1983\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 9.1412 - mean_squared_error: 9.1412 - val_loss: 10.0934 - val_mean_squared_error: 10.0934\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 9.4348 - mean_squared_error: 9.4348 - val_loss: 9.3583 - val_mean_squared_error: 9.3583\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 8.4294 - mean_squared_error: 8.4294 - val_loss: 10.0675 - val_mean_squared_error: 10.0675\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 8.1602 - mean_squared_error: 8.1602 - val_loss: 9.9098 - val_mean_squared_error: 9.9098\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 7.7183 - mean_squared_error: 7.7183 - val_loss: 7.8585 - val_mean_squared_error: 7.8585\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 8.0797 - mean_squared_error: 8.0797 - val_loss: 7.4049 - val_mean_squared_error: 7.4049\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 7.2127 - mean_squared_error: 7.2127 - val_loss: 9.5262 - val_mean_squared_error: 9.5262\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 5.8596 - mean_squared_error: 5.8596 - val_loss: 7.0908 - val_mean_squared_error: 7.0908\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 5.5701 - mean_squared_error: 5.5701 - val_loss: 8.2753 - val_mean_squared_error: 8.2753\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 5.8409 - mean_squared_error: 5.8409 - val_loss: 6.7143 - val_mean_squared_error: 6.7143\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 5.1985 - mean_squared_error: 5.1985 - val_loss: 8.5983 - val_mean_squared_error: 8.5983\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 4.8552 - mean_squared_error: 4.8552 - val_loss: 5.5380 - val_mean_squared_error: 5.5380\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 5.0369 - mean_squared_error: 5.0369 - val_loss: 5.2841 - val_mean_squared_error: 5.2841\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 4.5474 - mean_squared_error: 4.5474 - val_loss: 6.0543 - val_mean_squared_error: 6.0543\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 4.2366 - mean_squared_error: 4.2366 - val_loss: 5.8112 - val_mean_squared_error: 5.8112\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 4.1318 - mean_squared_error: 4.1318 - val_loss: 4.1216 - val_mean_squared_error: 4.1216\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 4.1546 - mean_squared_error: 4.1546 - val_loss: 5.5824 - val_mean_squared_error: 5.5824\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 3.8492 - mean_squared_error: 3.8492 - val_loss: 5.7024 - val_mean_squared_error: 5.7024\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 3.4541 - mean_squared_error: 3.4541 - val_loss: 4.1712 - val_mean_squared_error: 4.1712\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 3.4203 - mean_squared_error: 3.4203 - val_loss: 5.7653 - val_mean_squared_error: 5.7653\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 3.5375 - mean_squared_error: 3.5375 - val_loss: 4.0001 - val_mean_squared_error: 4.0001\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 3.3105 - mean_squared_error: 3.3105 - val_loss: 4.3544 - val_mean_squared_error: 4.3544\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 3.3663 - mean_squared_error: 3.3663 - val_loss: 3.5655 - val_mean_squared_error: 3.5655\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.1105 - mean_squared_error: 3.1105 - val_loss: 3.9568 - val_mean_squared_error: 3.9568\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 3.1144 - mean_squared_error: 3.1144 - val_loss: 3.6330 - val_mean_squared_error: 3.6330\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.9434 - mean_squared_error: 2.9434 - val_loss: 5.2661 - val_mean_squared_error: 5.2661\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.9767 - mean_squared_error: 2.9767 - val_loss: 3.2099 - val_mean_squared_error: 3.2099\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 2.6067 - mean_squared_error: 2.6067 - val_loss: 3.5518 - val_mean_squared_error: 3.5518\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.8585 - mean_squared_error: 2.8585 - val_loss: 4.1120 - val_mean_squared_error: 4.1120\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.7646 - mean_squared_error: 2.7646 - val_loss: 3.8210 - val_mean_squared_error: 3.8210\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.7175 - mean_squared_error: 2.7175 - val_loss: 3.4013 - val_mean_squared_error: 3.4013\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 2.6004 - mean_squared_error: 2.6004 - val_loss: 3.7921 - val_mean_squared_error: 3.7921\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 2.8821 - mean_squared_error: 2.8821 - val_loss: 3.9603 - val_mean_squared_error: 3.9603\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.6933 - mean_squared_error: 2.6933 - val_loss: 3.6574 - val_mean_squared_error: 3.6574\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.6365 - mean_squared_error: 2.6365 - val_loss: 3.5027 - val_mean_squared_error: 3.5027\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.5787 - mean_squared_error: 2.5787 - val_loss: 4.0338 - val_mean_squared_error: 4.0338\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.3672 - mean_squared_error: 2.3672 - val_loss: 3.4640 - val_mean_squared_error: 3.4640\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.2316 - mean_squared_error: 2.2316 - val_loss: 3.0240 - val_mean_squared_error: 3.0240\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.2397 - mean_squared_error: 2.2397 - val_loss: 3.8662 - val_mean_squared_error: 3.8662\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.2076 - mean_squared_error: 2.2076 - val_loss: 4.0191 - val_mean_squared_error: 4.0191\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.3432 - mean_squared_error: 2.3432 - val_loss: 2.8821 - val_mean_squared_error: 2.8821\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 2.2975 - mean_squared_error: 2.2975 - val_loss: 3.1151 - val_mean_squared_error: 3.1151\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.4811 - mean_squared_error: 2.4811 - val_loss: 3.3429 - val_mean_squared_error: 3.3429\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.0555 - mean_squared_error: 2.0555 - val_loss: 4.2722 - val_mean_squared_error: 4.2722\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 2.2696 - mean_squared_error: 2.2696 - val_loss: 2.8939 - val_mean_squared_error: 2.8939\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.0093 - mean_squared_error: 2.0093 - val_loss: 3.4436 - val_mean_squared_error: 3.4436\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 2.0629 - mean_squared_error: 2.0629 - val_loss: 3.2624 - val_mean_squared_error: 3.2624\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.0194 - mean_squared_error: 2.0194 - val_loss: 2.5811 - val_mean_squared_error: 2.5811\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.8057 - mean_squared_error: 1.8057 - val_loss: 2.8072 - val_mean_squared_error: 2.8072\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.9500 - mean_squared_error: 1.9500 - val_loss: 3.2399 - val_mean_squared_error: 3.2399\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.8382 - mean_squared_error: 1.8382 - val_loss: 2.8729 - val_mean_squared_error: 2.8729\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.8863 - mean_squared_error: 1.8863 - val_loss: 3.1060 - val_mean_squared_error: 3.1060\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.7791 - mean_squared_error: 1.7791 - val_loss: 2.8204 - val_mean_squared_error: 2.8204\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.7289 - mean_squared_error: 1.7289 - val_loss: 3.2007 - val_mean_squared_error: 3.2007\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.6494 - mean_squared_error: 1.6494 - val_loss: 2.7110 - val_mean_squared_error: 2.7110\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.8325 - mean_squared_error: 1.8325 - val_loss: 2.6197 - val_mean_squared_error: 2.6197\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.8252 - mean_squared_error: 1.8252 - val_loss: 3.1516 - val_mean_squared_error: 3.1516\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.8135 - mean_squared_error: 1.8135 - val_loss: 2.8175 - val_mean_squared_error: 2.8175\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.6712 - mean_squared_error: 1.6712 - val_loss: 3.0783 - val_mean_squared_error: 3.0783\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.1722 - mean_squared_error: 2.1722 - val_loss: 2.9674 - val_mean_squared_error: 2.9674\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.7360 - mean_squared_error: 1.7360 - val_loss: 3.1685 - val_mean_squared_error: 3.1685\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.6402 - mean_squared_error: 1.6402 - val_loss: 2.6865 - val_mean_squared_error: 2.6865\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.7943 - mean_squared_error: 1.7943 - val_loss: 3.6520 - val_mean_squared_error: 3.6520\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.7131 - mean_squared_error: 1.7131 - val_loss: 3.1272 - val_mean_squared_error: 3.1272\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.6605 - mean_squared_error: 1.6605 - val_loss: 3.7808 - val_mean_squared_error: 3.7808\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.5172 - mean_squared_error: 1.5172 - val_loss: 3.1616 - val_mean_squared_error: 3.1616\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.6389 - mean_squared_error: 1.6389 - val_loss: 2.7515 - val_mean_squared_error: 2.7515\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.7106 - mean_squared_error: 1.7106 - val_loss: 2.9848 - val_mean_squared_error: 2.9848\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.5406 - mean_squared_error: 1.5406 - val_loss: 3.4108 - val_mean_squared_error: 3.4108\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.7183 - mean_squared_error: 1.7183 - val_loss: 3.1289 - val_mean_squared_error: 3.1289\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.5187 - mean_squared_error: 1.5187 - val_loss: 2.7475 - val_mean_squared_error: 2.7475\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.5121 - mean_squared_error: 1.5121 - val_loss: 2.7486 - val_mean_squared_error: 2.7486\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 1.5240 - mean_squared_error: 1.5240 - val_loss: 4.7895 - val_mean_squared_error: 4.7895\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7989 - mean_squared_error: 1.7989 - val_loss: 2.9316 - val_mean_squared_error: 2.9316\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7458 - mean_squared_error: 1.7458 - val_loss: 3.6794 - val_mean_squared_error: 3.6794\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.6285 - mean_squared_error: 1.6285 - val_loss: 3.0512 - val_mean_squared_error: 3.0512\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.5131 - mean_squared_error: 1.5131 - val_loss: 2.8959 - val_mean_squared_error: 2.8959\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.5737 - mean_squared_error: 1.5737 - val_loss: 2.9068 - val_mean_squared_error: 2.9068\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.7463 - mean_squared_error: 1.7463 - val_loss: 2.8158 - val_mean_squared_error: 2.8158\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.3833 - mean_squared_error: 1.3833 - val_loss: 2.4751 - val_mean_squared_error: 2.4751\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.4725 - mean_squared_error: 1.4725 - val_loss: 2.7711 - val_mean_squared_error: 2.7711\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.5298 - mean_squared_error: 1.5298 - val_loss: 2.6938 - val_mean_squared_error: 2.6938\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.4706 - mean_squared_error: 1.4706 - val_loss: 2.9522 - val_mean_squared_error: 2.9522\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.4295 - mean_squared_error: 1.4295 - val_loss: 2.5477 - val_mean_squared_error: 2.5477\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4450 - mean_squared_error: 1.4450 - val_loss: 2.7101 - val_mean_squared_error: 2.7101\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4297 - mean_squared_error: 1.4297 - val_loss: 2.8083 - val_mean_squared_error: 2.8083\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.3473 - mean_squared_error: 1.3473 - val_loss: 2.5506 - val_mean_squared_error: 2.5506\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2573 - mean_squared_error: 1.2573 - val_loss: 2.7891 - val_mean_squared_error: 2.7891\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.4580 - mean_squared_error: 1.4580 - val_loss: 2.6443 - val_mean_squared_error: 2.6443\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.4894 - mean_squared_error: 1.4894 - val_loss: 3.2256 - val_mean_squared_error: 3.2256\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.4837 - mean_squared_error: 1.4837 - val_loss: 3.2296 - val_mean_squared_error: 3.2296\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.5350 - mean_squared_error: 1.5350 - val_loss: 3.3703 - val_mean_squared_error: 3.3703\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3471 - mean_squared_error: 1.3471 - val_loss: 3.0538 - val_mean_squared_error: 3.0538\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.3356 - mean_squared_error: 1.3356 - val_loss: 2.7943 - val_mean_squared_error: 2.7943\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.2876 - mean_squared_error: 1.2876 - val_loss: 2.4374 - val_mean_squared_error: 2.4374\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.3024 - mean_squared_error: 1.3024 - val_loss: 2.6729 - val_mean_squared_error: 2.6729\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.4270 - mean_squared_error: 1.4270 - val_loss: 2.8585 - val_mean_squared_error: 2.8585\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2861 - mean_squared_error: 1.2861 - val_loss: 2.4633 - val_mean_squared_error: 2.4633\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.2813 - mean_squared_error: 1.2813 - val_loss: 2.9591 - val_mean_squared_error: 2.9591\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4085 - mean_squared_error: 1.4085 - val_loss: 2.4398 - val_mean_squared_error: 2.4398\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1953 - mean_squared_error: 1.1953 - val_loss: 2.5410 - val_mean_squared_error: 2.5410\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.2925 - mean_squared_error: 1.2925 - val_loss: 2.6545 - val_mean_squared_error: 2.6545\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.1109 - mean_squared_error: 1.1109 - val_loss: 2.4855 - val_mean_squared_error: 2.4855\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 1.2884 - mean_squared_error: 1.2884 - val_loss: 2.5215 - val_mean_squared_error: 2.5215\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.1541 - mean_squared_error: 1.1541 - val_loss: 2.5572 - val_mean_squared_error: 2.5572\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1528 - mean_squared_error: 1.1528 - val_loss: 2.4909 - val_mean_squared_error: 2.4909\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2254 - mean_squared_error: 1.2254 - val_loss: 2.6764 - val_mean_squared_error: 2.6764\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.3492 - mean_squared_error: 1.3492 - val_loss: 2.4223 - val_mean_squared_error: 2.4223\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1926 - mean_squared_error: 1.1926 - val_loss: 2.6280 - val_mean_squared_error: 2.6280\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1666 - mean_squared_error: 1.1666 - val_loss: 2.4311 - val_mean_squared_error: 2.4311\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1634 - mean_squared_error: 1.1634 - val_loss: 2.6984 - val_mean_squared_error: 2.6984\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.1051 - mean_squared_error: 1.1051 - val_loss: 2.4609 - val_mean_squared_error: 2.4609\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.0881 - mean_squared_error: 1.0881 - val_loss: 2.4505 - val_mean_squared_error: 2.4505\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.2237 - mean_squared_error: 1.2237 - val_loss: 3.2393 - val_mean_squared_error: 3.2393\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 1.2490 - mean_squared_error: 1.2490 - val_loss: 2.5808 - val_mean_squared_error: 2.5808\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2039 - mean_squared_error: 1.2039 - val_loss: 2.6345 - val_mean_squared_error: 2.6345\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.2898 - mean_squared_error: 1.2898 - val_loss: 2.5468 - val_mean_squared_error: 2.5468\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.1185 - mean_squared_error: 1.1185 - val_loss: 2.9004 - val_mean_squared_error: 2.9004\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.0369 - mean_squared_error: 1.0369 - val_loss: 2.5447 - val_mean_squared_error: 2.5447\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.0738 - mean_squared_error: 1.0738 - val_loss: 2.5671 - val_mean_squared_error: 2.5671\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.0709 - mean_squared_error: 1.0709 - val_loss: 2.5096 - val_mean_squared_error: 2.5096\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1460 - mean_squared_error: 1.1460 - val_loss: 2.4075 - val_mean_squared_error: 2.4075\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.1030 - mean_squared_error: 1.1030 - val_loss: 2.5298 - val_mean_squared_error: 2.5298\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.2097 - mean_squared_error: 1.2097 - val_loss: 3.0899 - val_mean_squared_error: 3.0899\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.0863 - mean_squared_error: 1.0863 - val_loss: 2.4729 - val_mean_squared_error: 2.4729\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0416 - mean_squared_error: 1.0416 - val_loss: 2.5005 - val_mean_squared_error: 2.5005\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.9876 - mean_squared_error: 0.9876 - val_loss: 3.0387 - val_mean_squared_error: 3.0387\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.1113 - mean_squared_error: 1.1113 - val_loss: 2.6465 - val_mean_squared_error: 2.6465\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.0995 - mean_squared_error: 1.0995 - val_loss: 2.7676 - val_mean_squared_error: 2.7676\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0263 - mean_squared_error: 1.0263 - val_loss: 2.3736 - val_mean_squared_error: 2.3736\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0784 - mean_squared_error: 1.0784 - val_loss: 2.3036 - val_mean_squared_error: 2.3036\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1041 - mean_squared_error: 1.1041 - val_loss: 2.9215 - val_mean_squared_error: 2.9215\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0518 - mean_squared_error: 1.0518 - val_loss: 2.3348 - val_mean_squared_error: 2.3348\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0114 - mean_squared_error: 1.0114 - val_loss: 2.9823 - val_mean_squared_error: 2.9823\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.9514 - mean_squared_error: 0.9514 - val_loss: 2.6070 - val_mean_squared_error: 2.6070\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 0.9568 - mean_squared_error: 0.9568 - val_loss: 2.9025 - val_mean_squared_error: 2.9025\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.0293 - mean_squared_error: 1.0293 - val_loss: 2.4389 - val_mean_squared_error: 2.4389\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 2.3475 - val_mean_squared_error: 2.3475\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0532 - mean_squared_error: 1.0532 - val_loss: 2.3192 - val_mean_squared_error: 2.3192\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9850 - mean_squared_error: 0.9850 - val_loss: 2.5433 - val_mean_squared_error: 2.5433\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.0138 - mean_squared_error: 1.0138 - val_loss: 2.4193 - val_mean_squared_error: 2.4193\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9586 - mean_squared_error: 0.9586 - val_loss: 2.3195 - val_mean_squared_error: 2.3195\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.9296 - mean_squared_error: 0.9296 - val_loss: 2.4845 - val_mean_squared_error: 2.4845\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 0.9040 - mean_squared_error: 0.9040 - val_loss: 2.4106 - val_mean_squared_error: 2.4106\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 0.9483 - mean_squared_error: 0.9483 - val_loss: 2.3817 - val_mean_squared_error: 2.3817\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9827 - mean_squared_error: 0.9827 - val_loss: 2.4940 - val_mean_squared_error: 2.4940\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.1006 - mean_squared_error: 1.1006 - val_loss: 2.6512 - val_mean_squared_error: 2.6512\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.0196 - mean_squared_error: 1.0196 - val_loss: 2.3680 - val_mean_squared_error: 2.3680\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.9214 - mean_squared_error: 0.9214 - val_loss: 2.5511 - val_mean_squared_error: 2.5511\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.9867 - mean_squared_error: 0.9867 - val_loss: 2.3813 - val_mean_squared_error: 2.3813\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 0.9061 - mean_squared_error: 0.9061 - val_loss: 2.5390 - val_mean_squared_error: 2.5390\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.9754 - mean_squared_error: 0.9754 - val_loss: 2.3755 - val_mean_squared_error: 2.3755\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 2.3366 - val_mean_squared_error: 2.3366\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.8575 - mean_squared_error: 0.8575 - val_loss: 2.4064 - val_mean_squared_error: 2.4064\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 0.8485 - mean_squared_error: 0.8485 - val_loss: 2.4859 - val_mean_squared_error: 2.4859\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.8981 - mean_squared_error: 0.8981 - val_loss: 2.1629 - val_mean_squared_error: 2.1629\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 0.8777 - mean_squared_error: 0.8777 - val_loss: 2.8474 - val_mean_squared_error: 2.8474\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 0.8885 - mean_squared_error: 0.8885 - val_loss: 2.3664 - val_mean_squared_error: 2.3664\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.8689 - mean_squared_error: 0.8689 - val_loss: 2.2518 - val_mean_squared_error: 2.2518\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9085 - mean_squared_error: 0.9085 - val_loss: 2.4091 - val_mean_squared_error: 2.4091\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.0262 - mean_squared_error: 1.0262 - val_loss: 2.3562 - val_mean_squared_error: 2.3562\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.9319 - mean_squared_error: 0.9319 - val_loss: 2.2361 - val_mean_squared_error: 2.2361\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.8412 - mean_squared_error: 0.8412 - val_loss: 2.3402 - val_mean_squared_error: 2.3402\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 0.8830 - mean_squared_error: 0.8830 - val_loss: 2.3363 - val_mean_squared_error: 2.3363\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9089 - mean_squared_error: 0.9089 - val_loss: 2.2249 - val_mean_squared_error: 2.2249\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.8658 - mean_squared_error: 0.8658 - val_loss: 2.2872 - val_mean_squared_error: 2.2872\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.9162 - mean_squared_error: 0.9162 - val_loss: 2.2765 - val_mean_squared_error: 2.2765\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.9146 - mean_squared_error: 0.9146 - val_loss: 2.3920 - val_mean_squared_error: 2.3920\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.8279 - mean_squared_error: 0.8279 - val_loss: 2.3482 - val_mean_squared_error: 2.3482\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 0.8879 - mean_squared_error: 0.8879 - val_loss: 2.6417 - val_mean_squared_error: 2.6417\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 0.8857 - mean_squared_error: 0.8857 - val_loss: 2.1424 - val_mean_squared_error: 2.1424\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9053 - mean_squared_error: 0.9053 - val_loss: 2.2735 - val_mean_squared_error: 2.2735\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9171 - mean_squared_error: 0.9171 - val_loss: 2.3231 - val_mean_squared_error: 2.3231\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9193 - mean_squared_error: 0.9193 - val_loss: 2.5461 - val_mean_squared_error: 2.5461\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 0.8264 - mean_squared_error: 0.8264 - val_loss: 2.4633 - val_mean_squared_error: 2.4633\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.8161 - mean_squared_error: 0.8161 - val_loss: 2.3354 - val_mean_squared_error: 2.3354\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.7867 - mean_squared_error: 0.7867 - val_loss: 2.4526 - val_mean_squared_error: 2.4526\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 0.8838 - mean_squared_error: 0.8838 - val_loss: 2.3398 - val_mean_squared_error: 2.3398\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.8209 - mean_squared_error: 0.8209 - val_loss: 2.3276 - val_mean_squared_error: 2.3276\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 0.7827 - mean_squared_error: 0.7827 - val_loss: 2.3655 - val_mean_squared_error: 2.3655\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.7976 - mean_squared_error: 0.7976 - val_loss: 2.4401 - val_mean_squared_error: 2.4401\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.8704 - mean_squared_error: 0.8704 - val_loss: 2.5527 - val_mean_squared_error: 2.5527\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.7674 - mean_squared_error: 0.7674 - val_loss: 2.2928 - val_mean_squared_error: 2.2928\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.7478 - mean_squared_error: 0.7478 - val_loss: 2.3833 - val_mean_squared_error: 2.3833\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 0.8316 - mean_squared_error: 0.8316 - val_loss: 2.4501 - val_mean_squared_error: 2.4501\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.8223 - mean_squared_error: 0.8223 - val_loss: 2.3273 - val_mean_squared_error: 2.3273\n",
            "==================================================\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_42 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_42 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_56 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_43 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_57 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_58 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_73 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_59 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_74 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 1ms/sample - loss: 743.1793 - mean_squared_error: 743.1794 - val_loss: 411.6637 - val_mean_squared_error: 411.6637\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 30.7726 - mean_squared_error: 30.7726 - val_loss: 182.6833 - val_mean_squared_error: 182.6833\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 21.5008 - mean_squared_error: 21.5008 - val_loss: 126.1154 - val_mean_squared_error: 126.1154\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 17.6650 - mean_squared_error: 17.6650 - val_loss: 46.1510 - val_mean_squared_error: 46.1510\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 16.3376 - mean_squared_error: 16.3376 - val_loss: 27.6476 - val_mean_squared_error: 27.6476\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 16.5621 - mean_squared_error: 16.5621 - val_loss: 22.9356 - val_mean_squared_error: 22.9356\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 14.7701 - mean_squared_error: 14.7701 - val_loss: 14.1347 - val_mean_squared_error: 14.1347\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 13.4100 - mean_squared_error: 13.4100 - val_loss: 11.1211 - val_mean_squared_error: 11.1211\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 13.4876 - mean_squared_error: 13.4876 - val_loss: 12.9272 - val_mean_squared_error: 12.9272\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 13.7582 - mean_squared_error: 13.7582 - val_loss: 11.4282 - val_mean_squared_error: 11.4282\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 12.6098 - mean_squared_error: 12.6098 - val_loss: 10.5650 - val_mean_squared_error: 10.5650\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 11.9811 - mean_squared_error: 11.9811 - val_loss: 12.7312 - val_mean_squared_error: 12.7312\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 10.5635 - mean_squared_error: 10.5635 - val_loss: 9.8527 - val_mean_squared_error: 9.8527\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 10.8585 - mean_squared_error: 10.8585 - val_loss: 10.6739 - val_mean_squared_error: 10.6739\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 10.5921 - mean_squared_error: 10.5921 - val_loss: 16.0467 - val_mean_squared_error: 16.0467\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 10.0769 - mean_squared_error: 10.0769 - val_loss: 9.8508 - val_mean_squared_error: 9.8508\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 9.1165 - mean_squared_error: 9.1165 - val_loss: 9.4303 - val_mean_squared_error: 9.4303\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 7.7997 - mean_squared_error: 7.7997 - val_loss: 7.9824 - val_mean_squared_error: 7.9824\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 8.5688 - mean_squared_error: 8.5688 - val_loss: 7.9258 - val_mean_squared_error: 7.9258\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 7.7676 - mean_squared_error: 7.7676 - val_loss: 8.6440 - val_mean_squared_error: 8.6440\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 7.1361 - mean_squared_error: 7.1361 - val_loss: 7.0364 - val_mean_squared_error: 7.0364\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 6.5011 - mean_squared_error: 6.5011 - val_loss: 7.4390 - val_mean_squared_error: 7.4390\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 6.1379 - mean_squared_error: 6.1379 - val_loss: 6.2372 - val_mean_squared_error: 6.2372\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 5.5918 - mean_squared_error: 5.5918 - val_loss: 5.8169 - val_mean_squared_error: 5.8169\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 5.3805 - mean_squared_error: 5.3805 - val_loss: 5.5332 - val_mean_squared_error: 5.5332\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 5.3690 - mean_squared_error: 5.3690 - val_loss: 8.2816 - val_mean_squared_error: 8.2816\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 4.9244 - mean_squared_error: 4.9244 - val_loss: 5.3611 - val_mean_squared_error: 5.3611\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 4.8016 - mean_squared_error: 4.8016 - val_loss: 4.8772 - val_mean_squared_error: 4.8772\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 4.3167 - mean_squared_error: 4.3167 - val_loss: 4.7712 - val_mean_squared_error: 4.7712\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 4.3730 - mean_squared_error: 4.3730 - val_loss: 4.8211 - val_mean_squared_error: 4.8211\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 4.5438 - mean_squared_error: 4.5438 - val_loss: 4.2221 - val_mean_squared_error: 4.2221\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 3.9664 - mean_squared_error: 3.9664 - val_loss: 4.6743 - val_mean_squared_error: 4.6743\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.6629 - mean_squared_error: 3.6629 - val_loss: 4.1294 - val_mean_squared_error: 4.1294\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 3.6327 - mean_squared_error: 3.6327 - val_loss: 4.0934 - val_mean_squared_error: 4.0934\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 3.6399 - mean_squared_error: 3.6399 - val_loss: 4.5390 - val_mean_squared_error: 4.5390\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 3.6622 - mean_squared_error: 3.6622 - val_loss: 3.8313 - val_mean_squared_error: 3.8313\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 3.3605 - mean_squared_error: 3.3605 - val_loss: 4.0505 - val_mean_squared_error: 4.0505\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 3.1278 - mean_squared_error: 3.1278 - val_loss: 3.4002 - val_mean_squared_error: 3.4002\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 3.2628 - mean_squared_error: 3.2628 - val_loss: 4.6368 - val_mean_squared_error: 4.6368\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 3.2076 - mean_squared_error: 3.2076 - val_loss: 3.9250 - val_mean_squared_error: 3.9250\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 3.4058 - mean_squared_error: 3.4058 - val_loss: 3.6158 - val_mean_squared_error: 3.6158\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 3.0812 - mean_squared_error: 3.0812 - val_loss: 3.5619 - val_mean_squared_error: 3.5619\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.9096 - mean_squared_error: 2.9096 - val_loss: 3.5798 - val_mean_squared_error: 3.5798\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.6488 - mean_squared_error: 2.6488 - val_loss: 3.0100 - val_mean_squared_error: 3.0100\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.9331 - mean_squared_error: 2.9331 - val_loss: 3.7271 - val_mean_squared_error: 3.7271\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.9408 - mean_squared_error: 2.9408 - val_loss: 3.2108 - val_mean_squared_error: 3.2108\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.7952 - mean_squared_error: 2.7952 - val_loss: 3.3503 - val_mean_squared_error: 3.3503\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.4621 - mean_squared_error: 2.4621 - val_loss: 3.1263 - val_mean_squared_error: 3.1263\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.6798 - mean_squared_error: 2.6798 - val_loss: 3.5906 - val_mean_squared_error: 3.5906\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.5708 - mean_squared_error: 2.5708 - val_loss: 3.0743 - val_mean_squared_error: 3.0743\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 2.4658 - mean_squared_error: 2.4658 - val_loss: 3.1131 - val_mean_squared_error: 3.1131\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.5495 - mean_squared_error: 2.5495 - val_loss: 3.1338 - val_mean_squared_error: 3.1338\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.3966 - mean_squared_error: 2.3966 - val_loss: 2.9962 - val_mean_squared_error: 2.9962\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.4069 - mean_squared_error: 2.4069 - val_loss: 2.8434 - val_mean_squared_error: 2.8434\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.4481 - mean_squared_error: 2.4481 - val_loss: 4.0425 - val_mean_squared_error: 4.0425\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.2073 - mean_squared_error: 2.2073 - val_loss: 3.3820 - val_mean_squared_error: 3.3820\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.3938 - mean_squared_error: 2.3938 - val_loss: 2.9715 - val_mean_squared_error: 2.9715\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.4014 - mean_squared_error: 2.4014 - val_loss: 3.4581 - val_mean_squared_error: 3.4581\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.4768 - mean_squared_error: 2.4768 - val_loss: 3.3404 - val_mean_squared_error: 3.3404\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.3032 - mean_squared_error: 2.3032 - val_loss: 2.9064 - val_mean_squared_error: 2.9064\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.1181 - mean_squared_error: 2.1181 - val_loss: 3.2509 - val_mean_squared_error: 3.2509\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.0955 - mean_squared_error: 2.0955 - val_loss: 3.2530 - val_mean_squared_error: 3.2530\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.0910 - mean_squared_error: 2.0910 - val_loss: 2.9411 - val_mean_squared_error: 2.9411\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 2.0335 - mean_squared_error: 2.0335 - val_loss: 2.7311 - val_mean_squared_error: 2.7311\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.8403 - mean_squared_error: 1.8403 - val_loss: 2.8389 - val_mean_squared_error: 2.8389\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.9382 - mean_squared_error: 1.9382 - val_loss: 2.6558 - val_mean_squared_error: 2.6558\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.9388 - mean_squared_error: 1.9388 - val_loss: 2.9566 - val_mean_squared_error: 2.9566\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.9198 - mean_squared_error: 1.9198 - val_loss: 2.9705 - val_mean_squared_error: 2.9705\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 2.0065 - mean_squared_error: 2.0065 - val_loss: 2.8616 - val_mean_squared_error: 2.8616\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.8824 - mean_squared_error: 1.8824 - val_loss: 2.6958 - val_mean_squared_error: 2.6958\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.9751 - mean_squared_error: 1.9751 - val_loss: 2.8253 - val_mean_squared_error: 2.8253\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.9699 - mean_squared_error: 1.9699 - val_loss: 3.7658 - val_mean_squared_error: 3.7658\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.9771 - mean_squared_error: 1.9771 - val_loss: 3.3412 - val_mean_squared_error: 3.3412\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 2.1992 - mean_squared_error: 2.1992 - val_loss: 2.7774 - val_mean_squared_error: 2.7774\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.7563 - mean_squared_error: 1.7563 - val_loss: 2.8693 - val_mean_squared_error: 2.8693\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.7965 - mean_squared_error: 1.7965 - val_loss: 2.6083 - val_mean_squared_error: 2.6083\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 2.0056 - mean_squared_error: 2.0056 - val_loss: 4.0336 - val_mean_squared_error: 4.0336\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.9595 - mean_squared_error: 1.9595 - val_loss: 3.0917 - val_mean_squared_error: 3.0917\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.9030 - mean_squared_error: 1.9030 - val_loss: 3.0691 - val_mean_squared_error: 3.0691\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.7763 - mean_squared_error: 1.7763 - val_loss: 3.3287 - val_mean_squared_error: 3.3287\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.7053 - mean_squared_error: 1.7053 - val_loss: 2.7387 - val_mean_squared_error: 2.7387\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.7425 - mean_squared_error: 1.7425 - val_loss: 3.0481 - val_mean_squared_error: 3.0481\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.7605 - mean_squared_error: 1.7605 - val_loss: 3.0878 - val_mean_squared_error: 3.0878\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.8717 - mean_squared_error: 1.8717 - val_loss: 2.7271 - val_mean_squared_error: 2.7271\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.8512 - mean_squared_error: 1.8512 - val_loss: 3.2179 - val_mean_squared_error: 3.2179\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.8839 - mean_squared_error: 1.8839 - val_loss: 3.6014 - val_mean_squared_error: 3.6014\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.6234 - mean_squared_error: 1.6234 - val_loss: 3.2575 - val_mean_squared_error: 3.2575\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.7757 - mean_squared_error: 1.7757 - val_loss: 2.6265 - val_mean_squared_error: 2.6265\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.7103 - mean_squared_error: 1.7103 - val_loss: 2.7903 - val_mean_squared_error: 2.7903\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.6392 - mean_squared_error: 1.6392 - val_loss: 3.2799 - val_mean_squared_error: 3.2799\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.7466 - mean_squared_error: 1.7466 - val_loss: 2.7690 - val_mean_squared_error: 2.7690\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.5465 - mean_squared_error: 1.5465 - val_loss: 2.7383 - val_mean_squared_error: 2.7383\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.6128 - mean_squared_error: 1.6128 - val_loss: 2.8318 - val_mean_squared_error: 2.8318\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.7001 - mean_squared_error: 1.7001 - val_loss: 2.6507 - val_mean_squared_error: 2.6507\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4380 - mean_squared_error: 1.4380 - val_loss: 2.7746 - val_mean_squared_error: 2.7746\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.7599 - mean_squared_error: 1.7599 - val_loss: 2.7466 - val_mean_squared_error: 2.7466\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.6809 - mean_squared_error: 1.6809 - val_loss: 2.7988 - val_mean_squared_error: 2.7988\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.5231 - mean_squared_error: 1.5231 - val_loss: 2.6225 - val_mean_squared_error: 2.6225\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5390 - mean_squared_error: 1.5390 - val_loss: 2.7320 - val_mean_squared_error: 2.7320\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.6243 - mean_squared_error: 1.6243 - val_loss: 2.8335 - val_mean_squared_error: 2.8335\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.6067 - mean_squared_error: 1.6067 - val_loss: 2.7014 - val_mean_squared_error: 2.7014\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.5499 - mean_squared_error: 1.5499 - val_loss: 2.6371 - val_mean_squared_error: 2.6371\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.4416 - mean_squared_error: 1.4416 - val_loss: 2.6394 - val_mean_squared_error: 2.6394\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.3842 - mean_squared_error: 1.3842 - val_loss: 2.6173 - val_mean_squared_error: 2.6173\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.5424 - mean_squared_error: 1.5424 - val_loss: 2.8257 - val_mean_squared_error: 2.8257\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.5696 - mean_squared_error: 1.5696 - val_loss: 2.9212 - val_mean_squared_error: 2.9212\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.4325 - mean_squared_error: 1.4325 - val_loss: 2.7598 - val_mean_squared_error: 2.7598\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.3408 - mean_squared_error: 1.3408 - val_loss: 2.6592 - val_mean_squared_error: 2.6592\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.4234 - mean_squared_error: 1.4234 - val_loss: 2.5908 - val_mean_squared_error: 2.5908\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.3726 - mean_squared_error: 1.3726 - val_loss: 2.3100 - val_mean_squared_error: 2.3100\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.5268 - mean_squared_error: 1.5268 - val_loss: 2.7391 - val_mean_squared_error: 2.7391\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5120 - mean_squared_error: 1.5120 - val_loss: 2.4333 - val_mean_squared_error: 2.4333\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 1.3229 - mean_squared_error: 1.3229 - val_loss: 2.5627 - val_mean_squared_error: 2.5627\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.3861 - mean_squared_error: 1.3861 - val_loss: 2.7265 - val_mean_squared_error: 2.7265\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3503 - mean_squared_error: 1.3503 - val_loss: 2.3467 - val_mean_squared_error: 2.3467\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5341 - mean_squared_error: 1.5341 - val_loss: 2.7507 - val_mean_squared_error: 2.7507\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.2174 - mean_squared_error: 1.2174 - val_loss: 2.3585 - val_mean_squared_error: 2.3585\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.2284 - mean_squared_error: 1.2284 - val_loss: 2.5643 - val_mean_squared_error: 2.5643\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.3173 - mean_squared_error: 1.3173 - val_loss: 2.4127 - val_mean_squared_error: 2.4127\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.5239 - mean_squared_error: 1.5239 - val_loss: 2.8258 - val_mean_squared_error: 2.8258\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3991 - mean_squared_error: 1.3991 - val_loss: 2.5370 - val_mean_squared_error: 2.5370\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.2710 - mean_squared_error: 1.2710 - val_loss: 2.6370 - val_mean_squared_error: 2.6370\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.3357 - mean_squared_error: 1.3357 - val_loss: 2.4036 - val_mean_squared_error: 2.4036\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.1850 - mean_squared_error: 1.1850 - val_loss: 2.3080 - val_mean_squared_error: 2.3080\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.2844 - mean_squared_error: 1.2844 - val_loss: 2.5497 - val_mean_squared_error: 2.5497\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.3077 - mean_squared_error: 1.3077 - val_loss: 2.3727 - val_mean_squared_error: 2.3727\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.2766 - mean_squared_error: 1.2766 - val_loss: 2.3938 - val_mean_squared_error: 2.3938\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.3428 - mean_squared_error: 1.3428 - val_loss: 2.3915 - val_mean_squared_error: 2.3915\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.3314 - mean_squared_error: 1.3314 - val_loss: 2.3449 - val_mean_squared_error: 2.3449\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.2730 - mean_squared_error: 1.2730 - val_loss: 2.3921 - val_mean_squared_error: 2.3921\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.2292 - mean_squared_error: 1.2292 - val_loss: 2.4702 - val_mean_squared_error: 2.4702\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.2500 - mean_squared_error: 1.2500 - val_loss: 2.4635 - val_mean_squared_error: 2.4635\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.2592 - mean_squared_error: 1.2592 - val_loss: 2.2901 - val_mean_squared_error: 2.2901\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.2677 - mean_squared_error: 1.2677 - val_loss: 2.5292 - val_mean_squared_error: 2.5292\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1825 - mean_squared_error: 1.1825 - val_loss: 2.6145 - val_mean_squared_error: 2.6145\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.2642 - mean_squared_error: 1.2642 - val_loss: 2.7472 - val_mean_squared_error: 2.7472\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.3371 - mean_squared_error: 1.3371 - val_loss: 2.3433 - val_mean_squared_error: 2.3433\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.1301 - mean_squared_error: 1.1301 - val_loss: 2.2595 - val_mean_squared_error: 2.2595\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.1060 - mean_squared_error: 1.1060 - val_loss: 2.5805 - val_mean_squared_error: 2.5805\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3300 - mean_squared_error: 1.3300 - val_loss: 2.4703 - val_mean_squared_error: 2.4703\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0991 - mean_squared_error: 1.0991 - val_loss: 2.3172 - val_mean_squared_error: 2.3172\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.2393 - mean_squared_error: 1.2393 - val_loss: 2.3789 - val_mean_squared_error: 2.3789\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.0926 - mean_squared_error: 1.0926 - val_loss: 2.3790 - val_mean_squared_error: 2.3790\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.1138 - mean_squared_error: 1.1138 - val_loss: 2.2285 - val_mean_squared_error: 2.2285\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1540 - mean_squared_error: 1.1540 - val_loss: 2.4760 - val_mean_squared_error: 2.4760\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 2.2075 - val_mean_squared_error: 2.2075\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.9867 - mean_squared_error: 0.9867 - val_loss: 2.1563 - val_mean_squared_error: 2.1563\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.0464 - mean_squared_error: 1.0464 - val_loss: 2.2675 - val_mean_squared_error: 2.2675\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.1406 - mean_squared_error: 1.1406 - val_loss: 2.8604 - val_mean_squared_error: 2.8604\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.0849 - mean_squared_error: 1.0849 - val_loss: 2.4311 - val_mean_squared_error: 2.4311\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.3010 - mean_squared_error: 1.3010 - val_loss: 2.6315 - val_mean_squared_error: 2.6315\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.1262 - mean_squared_error: 1.1262 - val_loss: 2.2936 - val_mean_squared_error: 2.2936\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1548 - mean_squared_error: 1.1548 - val_loss: 2.8016 - val_mean_squared_error: 2.8016\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.2050 - mean_squared_error: 1.2050 - val_loss: 2.1916 - val_mean_squared_error: 2.1916\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0468 - mean_squared_error: 1.0468 - val_loss: 2.4310 - val_mean_squared_error: 2.4310\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.1209 - mean_squared_error: 1.1209 - val_loss: 2.5331 - val_mean_squared_error: 2.5331\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.2376 - mean_squared_error: 1.2376 - val_loss: 2.3343 - val_mean_squared_error: 2.3343\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1616 - mean_squared_error: 1.1616 - val_loss: 2.2910 - val_mean_squared_error: 2.2910\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0762 - mean_squared_error: 1.0762 - val_loss: 2.3514 - val_mean_squared_error: 2.3514\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0311 - mean_squared_error: 1.0311 - val_loss: 2.5703 - val_mean_squared_error: 2.5703\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 0.9724 - mean_squared_error: 0.9724 - val_loss: 2.1681 - val_mean_squared_error: 2.1681\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.0692 - mean_squared_error: 1.0692 - val_loss: 2.3325 - val_mean_squared_error: 2.3325\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1638 - mean_squared_error: 1.1638 - val_loss: 2.2081 - val_mean_squared_error: 2.2081\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.0344 - mean_squared_error: 1.0344 - val_loss: 2.3698 - val_mean_squared_error: 2.3698\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0653 - mean_squared_error: 1.0653 - val_loss: 2.1086 - val_mean_squared_error: 2.1086\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0369 - mean_squared_error: 1.0369 - val_loss: 2.2235 - val_mean_squared_error: 2.2235\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0087 - mean_squared_error: 1.0087 - val_loss: 2.3070 - val_mean_squared_error: 2.3070\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0775 - mean_squared_error: 1.0775 - val_loss: 2.3624 - val_mean_squared_error: 2.3624\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.0813 - mean_squared_error: 1.0813 - val_loss: 2.5639 - val_mean_squared_error: 2.5639\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 2.1615 - val_mean_squared_error: 2.1615\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0290 - mean_squared_error: 1.0290 - val_loss: 2.3684 - val_mean_squared_error: 2.3684\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.9511 - mean_squared_error: 0.9511 - val_loss: 2.3360 - val_mean_squared_error: 2.3360\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9427 - mean_squared_error: 0.9427 - val_loss: 2.2741 - val_mean_squared_error: 2.2741\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.0270 - mean_squared_error: 1.0270 - val_loss: 2.1451 - val_mean_squared_error: 2.1451\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0109 - mean_squared_error: 1.0109 - val_loss: 2.3579 - val_mean_squared_error: 2.3579\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 0.9364 - mean_squared_error: 0.9364 - val_loss: 2.2932 - val_mean_squared_error: 2.2932\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.0271 - mean_squared_error: 1.0271 - val_loss: 2.3476 - val_mean_squared_error: 2.3476\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 0.9349 - mean_squared_error: 0.9349 - val_loss: 2.3855 - val_mean_squared_error: 2.3855\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 0.9712 - mean_squared_error: 0.9712 - val_loss: 2.2780 - val_mean_squared_error: 2.2780\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.8912 - mean_squared_error: 0.8912 - val_loss: 2.1343 - val_mean_squared_error: 2.1343\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.8843 - mean_squared_error: 0.8843 - val_loss: 2.2464 - val_mean_squared_error: 2.2464\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 0.9682 - mean_squared_error: 0.9682 - val_loss: 2.3887 - val_mean_squared_error: 2.3887\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.9269 - mean_squared_error: 0.9269 - val_loss: 2.1364 - val_mean_squared_error: 2.1364\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9473 - mean_squared_error: 0.9473 - val_loss: 2.1062 - val_mean_squared_error: 2.1062\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.9564 - mean_squared_error: 0.9564 - val_loss: 2.3744 - val_mean_squared_error: 2.3744\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 392us/sample - loss: 0.9813 - mean_squared_error: 0.9813 - val_loss: 2.2241 - val_mean_squared_error: 2.2241\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.9978 - mean_squared_error: 0.9978 - val_loss: 2.2983 - val_mean_squared_error: 2.2983\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.0086 - mean_squared_error: 1.0086 - val_loss: 2.5100 - val_mean_squared_error: 2.5100\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.0090 - mean_squared_error: 1.0090 - val_loss: 2.2739 - val_mean_squared_error: 2.2739\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 2.2801 - val_mean_squared_error: 2.2801\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.8900 - mean_squared_error: 0.8900 - val_loss: 2.0954 - val_mean_squared_error: 2.0954\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.8586 - mean_squared_error: 0.8586 - val_loss: 2.3252 - val_mean_squared_error: 2.3252\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.8172 - mean_squared_error: 0.8172 - val_loss: 2.2352 - val_mean_squared_error: 2.2352\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 0.8880 - mean_squared_error: 0.8880 - val_loss: 2.3229 - val_mean_squared_error: 2.3229\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 0.9048 - mean_squared_error: 0.9048 - val_loss: 2.5670 - val_mean_squared_error: 2.5670\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 0.8865 - mean_squared_error: 0.8865 - val_loss: 2.2735 - val_mean_squared_error: 2.2735\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 0.9180 - mean_squared_error: 0.9180 - val_loss: 2.2548 - val_mean_squared_error: 2.2548\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.8912 - mean_squared_error: 0.8912 - val_loss: 2.3753 - val_mean_squared_error: 2.3753\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 0.8958 - mean_squared_error: 0.8958 - val_loss: 2.2707 - val_mean_squared_error: 2.2707\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 0.8132 - mean_squared_error: 0.8132 - val_loss: 2.1822 - val_mean_squared_error: 2.1822\n",
            "==================================================\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_45 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_75 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_60 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_76 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_61 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_77 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_62 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_78 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_63 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_79 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 753.0562 - mean_squared_error: 753.0562 - val_loss: 250.2811 - val_mean_squared_error: 250.2812\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 26.0089 - mean_squared_error: 26.0089 - val_loss: 159.0666 - val_mean_squared_error: 159.0666\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 23.2541 - mean_squared_error: 23.2541 - val_loss: 87.8484 - val_mean_squared_error: 87.8484\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 20.4509 - mean_squared_error: 20.4509 - val_loss: 57.9091 - val_mean_squared_error: 57.9091\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 18.5957 - mean_squared_error: 18.5957 - val_loss: 43.3002 - val_mean_squared_error: 43.3002\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 15.0023 - mean_squared_error: 15.0023 - val_loss: 23.3918 - val_mean_squared_error: 23.3918\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 14.2971 - mean_squared_error: 14.2971 - val_loss: 11.2723 - val_mean_squared_error: 11.2723\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 13.8887 - mean_squared_error: 13.8887 - val_loss: 15.3285 - val_mean_squared_error: 15.3285\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 13.4735 - mean_squared_error: 13.4735 - val_loss: 10.2951 - val_mean_squared_error: 10.2951\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 11.4351 - mean_squared_error: 11.4351 - val_loss: 10.5614 - val_mean_squared_error: 10.5614\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 11.4157 - mean_squared_error: 11.4157 - val_loss: 10.7054 - val_mean_squared_error: 10.7054\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 11.4259 - mean_squared_error: 11.4259 - val_loss: 10.5276 - val_mean_squared_error: 10.5276\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 10.2537 - mean_squared_error: 10.2537 - val_loss: 9.5482 - val_mean_squared_error: 9.5482\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 11.0130 - mean_squared_error: 11.0130 - val_loss: 10.3718 - val_mean_squared_error: 10.3718\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 9.7294 - mean_squared_error: 9.7294 - val_loss: 10.2092 - val_mean_squared_error: 10.2092\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 9.5422 - mean_squared_error: 9.5422 - val_loss: 8.5970 - val_mean_squared_error: 8.5970\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 8.6297 - mean_squared_error: 8.6297 - val_loss: 8.8894 - val_mean_squared_error: 8.8894\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 8.3929 - mean_squared_error: 8.3929 - val_loss: 7.9015 - val_mean_squared_error: 7.9015\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 8.3007 - mean_squared_error: 8.3007 - val_loss: 8.0452 - val_mean_squared_error: 8.0452\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 7.6472 - mean_squared_error: 7.6472 - val_loss: 6.3099 - val_mean_squared_error: 6.3099\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 6.6874 - mean_squared_error: 6.6874 - val_loss: 8.5467 - val_mean_squared_error: 8.5467\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 5.9739 - mean_squared_error: 5.9739 - val_loss: 8.2092 - val_mean_squared_error: 8.2092\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 5.5713 - mean_squared_error: 5.5713 - val_loss: 5.7632 - val_mean_squared_error: 5.7632\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 5.3579 - mean_squared_error: 5.3579 - val_loss: 5.2074 - val_mean_squared_error: 5.2074\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 5.2532 - mean_squared_error: 5.2532 - val_loss: 8.2307 - val_mean_squared_error: 8.2307\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 5.1055 - mean_squared_error: 5.1055 - val_loss: 5.0655 - val_mean_squared_error: 5.0655\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 4.6763 - mean_squared_error: 4.6763 - val_loss: 4.8017 - val_mean_squared_error: 4.8017\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 5.3340 - mean_squared_error: 5.3340 - val_loss: 5.0530 - val_mean_squared_error: 5.0530\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 4.6261 - mean_squared_error: 4.6261 - val_loss: 4.7111 - val_mean_squared_error: 4.7111\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 4.4422 - mean_squared_error: 4.4422 - val_loss: 4.7063 - val_mean_squared_error: 4.7063\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 4.1595 - mean_squared_error: 4.1595 - val_loss: 4.1052 - val_mean_squared_error: 4.1052\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 4.2677 - mean_squared_error: 4.2677 - val_loss: 4.6181 - val_mean_squared_error: 4.6181\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 3.9908 - mean_squared_error: 3.9908 - val_loss: 4.8473 - val_mean_squared_error: 4.8473\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 3.9810 - mean_squared_error: 3.9810 - val_loss: 3.6960 - val_mean_squared_error: 3.6960\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 4.2928 - mean_squared_error: 4.2928 - val_loss: 3.5762 - val_mean_squared_error: 3.5762\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 3.7905 - mean_squared_error: 3.7905 - val_loss: 3.7584 - val_mean_squared_error: 3.7584\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 3.7207 - mean_squared_error: 3.7207 - val_loss: 4.7404 - val_mean_squared_error: 4.7404\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 3.7188 - mean_squared_error: 3.7188 - val_loss: 3.7877 - val_mean_squared_error: 3.7877\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 3.4081 - mean_squared_error: 3.4081 - val_loss: 3.5337 - val_mean_squared_error: 3.5337\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 3.2681 - mean_squared_error: 3.2681 - val_loss: 3.6266 - val_mean_squared_error: 3.6266\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 3.1823 - mean_squared_error: 3.1823 - val_loss: 3.5721 - val_mean_squared_error: 3.5721\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 3.1914 - mean_squared_error: 3.1914 - val_loss: 4.3131 - val_mean_squared_error: 4.3131\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 3.0698 - mean_squared_error: 3.0698 - val_loss: 4.0227 - val_mean_squared_error: 4.0227\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 2.9318 - mean_squared_error: 2.9318 - val_loss: 3.3166 - val_mean_squared_error: 3.3166\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.8608 - mean_squared_error: 2.8608 - val_loss: 2.9856 - val_mean_squared_error: 2.9856\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 3.1245 - mean_squared_error: 3.1245 - val_loss: 3.9469 - val_mean_squared_error: 3.9469\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 3.0963 - mean_squared_error: 3.0963 - val_loss: 3.1999 - val_mean_squared_error: 3.1999\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 3.0552 - mean_squared_error: 3.0552 - val_loss: 3.1298 - val_mean_squared_error: 3.1298\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.6983 - mean_squared_error: 2.6983 - val_loss: 3.7537 - val_mean_squared_error: 3.7537\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.5780 - mean_squared_error: 2.5780 - val_loss: 2.9933 - val_mean_squared_error: 2.9933\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.4753 - mean_squared_error: 2.4753 - val_loss: 2.7652 - val_mean_squared_error: 2.7652\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.6374 - mean_squared_error: 2.6374 - val_loss: 3.2536 - val_mean_squared_error: 3.2536\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.5934 - mean_squared_error: 2.5934 - val_loss: 3.0862 - val_mean_squared_error: 3.0862\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 426us/sample - loss: 2.6970 - mean_squared_error: 2.6970 - val_loss: 4.2265 - val_mean_squared_error: 4.2265\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 2.5365 - mean_squared_error: 2.5365 - val_loss: 2.5958 - val_mean_squared_error: 2.5958\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.4086 - mean_squared_error: 2.4086 - val_loss: 3.0449 - val_mean_squared_error: 3.0449\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.4635 - mean_squared_error: 2.4635 - val_loss: 3.0666 - val_mean_squared_error: 3.0666\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.2313 - mean_squared_error: 2.2313 - val_loss: 3.2674 - val_mean_squared_error: 3.2674\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.3394 - mean_squared_error: 2.3394 - val_loss: 2.9134 - val_mean_squared_error: 2.9134\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.4582 - mean_squared_error: 2.4582 - val_loss: 3.0992 - val_mean_squared_error: 3.0992\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.5455 - mean_squared_error: 2.5455 - val_loss: 2.4473 - val_mean_squared_error: 2.4473\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.3015 - mean_squared_error: 2.3015 - val_loss: 3.1394 - val_mean_squared_error: 3.1394\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.2267 - mean_squared_error: 2.2267 - val_loss: 2.9810 - val_mean_squared_error: 2.9810\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.1398 - mean_squared_error: 2.1398 - val_loss: 2.6451 - val_mean_squared_error: 2.6451\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 2.4800 - mean_squared_error: 2.4800 - val_loss: 2.9124 - val_mean_squared_error: 2.9124\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.1483 - mean_squared_error: 2.1483 - val_loss: 3.2527 - val_mean_squared_error: 3.2527\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.2066 - mean_squared_error: 2.2066 - val_loss: 2.9694 - val_mean_squared_error: 2.9694\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.1634 - mean_squared_error: 2.1634 - val_loss: 2.6760 - val_mean_squared_error: 2.6760\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 2.1036 - mean_squared_error: 2.1036 - val_loss: 3.2113 - val_mean_squared_error: 3.2113\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.0705 - mean_squared_error: 2.0705 - val_loss: 2.5099 - val_mean_squared_error: 2.5099\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.2092 - mean_squared_error: 2.2092 - val_loss: 2.9634 - val_mean_squared_error: 2.9634\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.1668 - mean_squared_error: 2.1668 - val_loss: 3.0380 - val_mean_squared_error: 3.0380\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.9623 - mean_squared_error: 1.9623 - val_loss: 2.6299 - val_mean_squared_error: 2.6299\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.9850 - mean_squared_error: 1.9850 - val_loss: 2.5319 - val_mean_squared_error: 2.5319\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.0601 - mean_squared_error: 2.0601 - val_loss: 3.2501 - val_mean_squared_error: 3.2501\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.1172 - mean_squared_error: 2.1172 - val_loss: 2.6181 - val_mean_squared_error: 2.6181\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.9964 - mean_squared_error: 1.9964 - val_loss: 2.6968 - val_mean_squared_error: 2.6968\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.8942 - mean_squared_error: 1.8942 - val_loss: 2.9468 - val_mean_squared_error: 2.9468\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.9665 - mean_squared_error: 1.9665 - val_loss: 2.9503 - val_mean_squared_error: 2.9503\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.8623 - mean_squared_error: 1.8623 - val_loss: 2.8835 - val_mean_squared_error: 2.8835\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.0771 - mean_squared_error: 2.0771 - val_loss: 2.3437 - val_mean_squared_error: 2.3437\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.9082 - mean_squared_error: 1.9082 - val_loss: 3.0660 - val_mean_squared_error: 3.0660\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.7986 - mean_squared_error: 1.7986 - val_loss: 2.9077 - val_mean_squared_error: 2.9077\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.7632 - mean_squared_error: 1.7632 - val_loss: 2.6371 - val_mean_squared_error: 2.6371\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.8367 - mean_squared_error: 1.8367 - val_loss: 2.3572 - val_mean_squared_error: 2.3572\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.8043 - mean_squared_error: 1.8043 - val_loss: 2.5936 - val_mean_squared_error: 2.5936\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.9114 - mean_squared_error: 1.9114 - val_loss: 2.4415 - val_mean_squared_error: 2.4415\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.7531 - mean_squared_error: 1.7531 - val_loss: 2.5498 - val_mean_squared_error: 2.5498\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.7876 - mean_squared_error: 1.7876 - val_loss: 2.3366 - val_mean_squared_error: 2.3366\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.7784 - mean_squared_error: 1.7784 - val_loss: 2.6403 - val_mean_squared_error: 2.6403\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.8427 - mean_squared_error: 1.8427 - val_loss: 2.5063 - val_mean_squared_error: 2.5063\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.9331 - mean_squared_error: 1.9331 - val_loss: 2.7074 - val_mean_squared_error: 2.7074\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.0808 - mean_squared_error: 2.0808 - val_loss: 2.8196 - val_mean_squared_error: 2.8196\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.7893 - mean_squared_error: 1.7893 - val_loss: 2.4076 - val_mean_squared_error: 2.4076\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.6584 - mean_squared_error: 1.6584 - val_loss: 2.5904 - val_mean_squared_error: 2.5904\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.7397 - mean_squared_error: 1.7397 - val_loss: 2.5156 - val_mean_squared_error: 2.5156\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.8046 - mean_squared_error: 1.8046 - val_loss: 2.5374 - val_mean_squared_error: 2.5374\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.5662 - mean_squared_error: 1.5662 - val_loss: 2.2277 - val_mean_squared_error: 2.2277\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.7006 - mean_squared_error: 1.7006 - val_loss: 2.5084 - val_mean_squared_error: 2.5084\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.7823 - mean_squared_error: 1.7823 - val_loss: 2.4548 - val_mean_squared_error: 2.4548\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.7700 - mean_squared_error: 1.7700 - val_loss: 2.7092 - val_mean_squared_error: 2.7092\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.6116 - mean_squared_error: 1.6116 - val_loss: 2.4991 - val_mean_squared_error: 2.4991\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.5326 - mean_squared_error: 1.5326 - val_loss: 2.7656 - val_mean_squared_error: 2.7656\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.5278 - mean_squared_error: 1.5278 - val_loss: 2.2559 - val_mean_squared_error: 2.2559\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.5933 - mean_squared_error: 1.5933 - val_loss: 3.4635 - val_mean_squared_error: 3.4635\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.6319 - mean_squared_error: 1.6319 - val_loss: 2.6492 - val_mean_squared_error: 2.6492\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.5915 - mean_squared_error: 1.5915 - val_loss: 2.1926 - val_mean_squared_error: 2.1926\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.6261 - mean_squared_error: 1.6261 - val_loss: 2.1652 - val_mean_squared_error: 2.1652\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.4626 - mean_squared_error: 1.4626 - val_loss: 2.7071 - val_mean_squared_error: 2.7071\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.5993 - mean_squared_error: 1.5993 - val_loss: 2.7569 - val_mean_squared_error: 2.7569\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.4946 - mean_squared_error: 1.4946 - val_loss: 2.2736 - val_mean_squared_error: 2.2736\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.4976 - mean_squared_error: 1.4976 - val_loss: 2.4347 - val_mean_squared_error: 2.4347\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.5286 - mean_squared_error: 1.5286 - val_loss: 2.4146 - val_mean_squared_error: 2.4146\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.5504 - mean_squared_error: 1.5504 - val_loss: 2.2641 - val_mean_squared_error: 2.2641\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 1.4255 - mean_squared_error: 1.4255 - val_loss: 2.4199 - val_mean_squared_error: 2.4199\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.4024 - mean_squared_error: 1.4024 - val_loss: 2.1516 - val_mean_squared_error: 2.1516\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.3945 - mean_squared_error: 1.3945 - val_loss: 2.4137 - val_mean_squared_error: 2.4137\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.5724 - mean_squared_error: 1.5724 - val_loss: 2.3808 - val_mean_squared_error: 2.3808\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.4060 - mean_squared_error: 1.4060 - val_loss: 2.1345 - val_mean_squared_error: 2.1345\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.3566 - mean_squared_error: 1.3566 - val_loss: 2.3118 - val_mean_squared_error: 2.3118\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.4805 - mean_squared_error: 1.4805 - val_loss: 2.2211 - val_mean_squared_error: 2.2211\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3435 - mean_squared_error: 1.3435 - val_loss: 2.4292 - val_mean_squared_error: 2.4292\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.4961 - mean_squared_error: 1.4961 - val_loss: 2.1314 - val_mean_squared_error: 2.1314\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.3918 - mean_squared_error: 1.3918 - val_loss: 2.0738 - val_mean_squared_error: 2.0738\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.4596 - mean_squared_error: 1.4596 - val_loss: 2.3113 - val_mean_squared_error: 2.3113\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3738 - mean_squared_error: 1.3738 - val_loss: 2.0718 - val_mean_squared_error: 2.0718\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.2775 - mean_squared_error: 1.2775 - val_loss: 2.1011 - val_mean_squared_error: 2.1011\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.3061 - mean_squared_error: 1.3061 - val_loss: 2.3051 - val_mean_squared_error: 2.3051\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3622 - mean_squared_error: 1.3622 - val_loss: 2.3407 - val_mean_squared_error: 2.3407\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 437us/sample - loss: 1.3996 - mean_squared_error: 1.3996 - val_loss: 2.3482 - val_mean_squared_error: 2.3482\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.3554 - mean_squared_error: 1.3554 - val_loss: 2.3414 - val_mean_squared_error: 2.3414\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.2544 - mean_squared_error: 1.2544 - val_loss: 2.2672 - val_mean_squared_error: 2.2672\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.2665 - mean_squared_error: 1.2665 - val_loss: 2.2190 - val_mean_squared_error: 2.2190\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.2998 - mean_squared_error: 1.2998 - val_loss: 2.4626 - val_mean_squared_error: 2.4626\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.2901 - mean_squared_error: 1.2901 - val_loss: 2.5971 - val_mean_squared_error: 2.5971\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.2352 - mean_squared_error: 1.2352 - val_loss: 2.3038 - val_mean_squared_error: 2.3038\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.1953 - mean_squared_error: 1.1953 - val_loss: 2.1566 - val_mean_squared_error: 2.1566\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.2685 - mean_squared_error: 1.2685 - val_loss: 2.4261 - val_mean_squared_error: 2.4261\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.4433 - mean_squared_error: 1.4433 - val_loss: 2.5511 - val_mean_squared_error: 2.5511\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.2827 - mean_squared_error: 1.2827 - val_loss: 2.2710 - val_mean_squared_error: 2.2710\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.1893 - mean_squared_error: 1.1893 - val_loss: 2.2059 - val_mean_squared_error: 2.2059\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.3752 - mean_squared_error: 1.3752 - val_loss: 2.4817 - val_mean_squared_error: 2.4817\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.1852 - mean_squared_error: 1.1852 - val_loss: 2.1824 - val_mean_squared_error: 2.1824\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.2435 - mean_squared_error: 1.2435 - val_loss: 2.2571 - val_mean_squared_error: 2.2571\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.2307 - mean_squared_error: 1.2307 - val_loss: 2.2904 - val_mean_squared_error: 2.2904\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.2651 - mean_squared_error: 1.2651 - val_loss: 2.1194 - val_mean_squared_error: 2.1194\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.1839 - mean_squared_error: 1.1839 - val_loss: 2.4064 - val_mean_squared_error: 2.4064\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.1142 - mean_squared_error: 1.1142 - val_loss: 2.1343 - val_mean_squared_error: 2.1343\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.2381 - mean_squared_error: 1.2381 - val_loss: 2.3711 - val_mean_squared_error: 2.3711\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.2033 - mean_squared_error: 1.2033 - val_loss: 2.1879 - val_mean_squared_error: 2.1879\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.2752 - mean_squared_error: 1.2752 - val_loss: 2.1999 - val_mean_squared_error: 2.1999\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.1783 - mean_squared_error: 1.1783 - val_loss: 2.3195 - val_mean_squared_error: 2.3195\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.1661 - mean_squared_error: 1.1661 - val_loss: 2.1442 - val_mean_squared_error: 2.1442\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.2431 - mean_squared_error: 1.2431 - val_loss: 2.3498 - val_mean_squared_error: 2.3498\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0818 - mean_squared_error: 1.0818 - val_loss: 2.1458 - val_mean_squared_error: 2.1458\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.2012 - mean_squared_error: 1.2012 - val_loss: 2.0415 - val_mean_squared_error: 2.0415\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.2551 - mean_squared_error: 1.2551 - val_loss: 2.0954 - val_mean_squared_error: 2.0954\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1134 - mean_squared_error: 1.1134 - val_loss: 2.0387 - val_mean_squared_error: 2.0387\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.1197 - mean_squared_error: 1.1197 - val_loss: 2.2099 - val_mean_squared_error: 2.2099\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.0515 - mean_squared_error: 1.0515 - val_loss: 2.0813 - val_mean_squared_error: 2.0813\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.1420 - mean_squared_error: 1.1420 - val_loss: 2.4952 - val_mean_squared_error: 2.4952\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1015 - mean_squared_error: 1.1015 - val_loss: 2.2521 - val_mean_squared_error: 2.2521\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.0384 - mean_squared_error: 1.0384 - val_loss: 2.3415 - val_mean_squared_error: 2.3415\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.1777 - mean_squared_error: 1.1777 - val_loss: 2.3495 - val_mean_squared_error: 2.3495\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0203 - mean_squared_error: 1.0203 - val_loss: 2.0559 - val_mean_squared_error: 2.0559\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.0493 - mean_squared_error: 1.0493 - val_loss: 2.2231 - val_mean_squared_error: 2.2231\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.1037 - mean_squared_error: 1.1037 - val_loss: 2.1634 - val_mean_squared_error: 2.1634\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.1058 - mean_squared_error: 1.1058 - val_loss: 2.2906 - val_mean_squared_error: 2.2906\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0844 - mean_squared_error: 1.0844 - val_loss: 2.2644 - val_mean_squared_error: 2.2644\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.1728 - mean_squared_error: 1.1728 - val_loss: 2.3525 - val_mean_squared_error: 2.3525\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.0744 - mean_squared_error: 1.0744 - val_loss: 2.1168 - val_mean_squared_error: 2.1168\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 2.1928 - val_mean_squared_error: 2.1928\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0883 - mean_squared_error: 1.0883 - val_loss: 2.2490 - val_mean_squared_error: 2.2490\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0579 - mean_squared_error: 1.0579 - val_loss: 2.1001 - val_mean_squared_error: 2.1001\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.1175 - mean_squared_error: 1.1175 - val_loss: 2.2695 - val_mean_squared_error: 2.2695\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.0310 - mean_squared_error: 1.0310 - val_loss: 2.0140 - val_mean_squared_error: 2.0140\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0445 - mean_squared_error: 1.0445 - val_loss: 2.2986 - val_mean_squared_error: 2.2986\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.1041 - mean_squared_error: 1.1041 - val_loss: 2.1554 - val_mean_squared_error: 2.1554\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.0976 - mean_squared_error: 1.0977 - val_loss: 2.1259 - val_mean_squared_error: 2.1259\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0975 - mean_squared_error: 1.0975 - val_loss: 2.1448 - val_mean_squared_error: 2.1448\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 2.2203 - val_mean_squared_error: 2.2203\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9202 - mean_squared_error: 0.9202 - val_loss: 2.0054 - val_mean_squared_error: 2.0054\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0016 - mean_squared_error: 1.0016 - val_loss: 2.0989 - val_mean_squared_error: 2.0989\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9475 - mean_squared_error: 0.9475 - val_loss: 1.9835 - val_mean_squared_error: 1.9835\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.9256 - mean_squared_error: 0.9256 - val_loss: 2.1247 - val_mean_squared_error: 2.1247\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.9553 - mean_squared_error: 0.9553 - val_loss: 2.0015 - val_mean_squared_error: 2.0015\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 2.0495 - val_mean_squared_error: 2.0495\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.0277 - mean_squared_error: 1.0277 - val_loss: 2.2133 - val_mean_squared_error: 2.2133\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 2.1871 - val_mean_squared_error: 2.1871\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 0.9955 - mean_squared_error: 0.9955 - val_loss: 2.1908 - val_mean_squared_error: 2.1908\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0347 - mean_squared_error: 1.0347 - val_loss: 2.2908 - val_mean_squared_error: 2.2908\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.9616 - mean_squared_error: 0.9616 - val_loss: 2.0805 - val_mean_squared_error: 2.0805\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 0.9558 - mean_squared_error: 0.9558 - val_loss: 2.0264 - val_mean_squared_error: 2.0264\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9089 - mean_squared_error: 0.9089 - val_loss: 2.1074 - val_mean_squared_error: 2.1074\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 0.9156 - mean_squared_error: 0.9156 - val_loss: 2.1308 - val_mean_squared_error: 2.1308\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.8591 - mean_squared_error: 0.8591 - val_loss: 2.1233 - val_mean_squared_error: 2.1233\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 0.9310 - mean_squared_error: 0.9310 - val_loss: 2.2993 - val_mean_squared_error: 2.2993\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.9521 - mean_squared_error: 0.9521 - val_loss: 2.0933 - val_mean_squared_error: 2.0933\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.9338 - mean_squared_error: 0.9338 - val_loss: 2.0310 - val_mean_squared_error: 2.0310\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.9333 - mean_squared_error: 0.9333 - val_loss: 1.9910 - val_mean_squared_error: 1.9910\n",
            "==================================================\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_48 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_80 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_48 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_64 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_81 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_49 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_65 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_82 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_50 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_66 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_16 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_83 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_67 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_84 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 740.7165 - mean_squared_error: 740.7165 - val_loss: 150.2956 - val_mean_squared_error: 150.2956\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 24.9611 - mean_squared_error: 24.9611 - val_loss: 215.6149 - val_mean_squared_error: 215.6149\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 22.9044 - mean_squared_error: 22.9044 - val_loss: 98.3007 - val_mean_squared_error: 98.3007\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 20.3983 - mean_squared_error: 20.3983 - val_loss: 42.8952 - val_mean_squared_error: 42.8952\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 18.9170 - mean_squared_error: 18.9170 - val_loss: 46.5930 - val_mean_squared_error: 46.5930\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 17.7848 - mean_squared_error: 17.7848 - val_loss: 20.2060 - val_mean_squared_error: 20.2060\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 18.4044 - mean_squared_error: 18.4044 - val_loss: 12.3409 - val_mean_squared_error: 12.3409\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 426us/sample - loss: 14.4934 - mean_squared_error: 14.4934 - val_loss: 17.3885 - val_mean_squared_error: 17.3885\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 15.2179 - mean_squared_error: 15.2179 - val_loss: 11.7173 - val_mean_squared_error: 11.7173\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 14.4597 - mean_squared_error: 14.4597 - val_loss: 13.5235 - val_mean_squared_error: 13.5235\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 13.5792 - mean_squared_error: 13.5792 - val_loss: 10.9821 - val_mean_squared_error: 10.9821\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 13.4026 - mean_squared_error: 13.4026 - val_loss: 13.3499 - val_mean_squared_error: 13.3499\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 11.5951 - mean_squared_error: 11.5951 - val_loss: 9.6882 - val_mean_squared_error: 9.6882\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 11.6850 - mean_squared_error: 11.6850 - val_loss: 12.2045 - val_mean_squared_error: 12.2045\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 10.2340 - mean_squared_error: 10.2340 - val_loss: 10.0193 - val_mean_squared_error: 10.0193\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 11.4431 - mean_squared_error: 11.4431 - val_loss: 11.0818 - val_mean_squared_error: 11.0818\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 11.7257 - mean_squared_error: 11.7257 - val_loss: 12.7232 - val_mean_squared_error: 12.7232\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 9.2755 - mean_squared_error: 9.2755 - val_loss: 8.6970 - val_mean_squared_error: 8.6970\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 8.2508 - mean_squared_error: 8.2508 - val_loss: 9.2268 - val_mean_squared_error: 9.2268\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 8.4637 - mean_squared_error: 8.4637 - val_loss: 7.9339 - val_mean_squared_error: 7.9339\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 7.5296 - mean_squared_error: 7.5296 - val_loss: 7.2865 - val_mean_squared_error: 7.2865\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 6.8334 - mean_squared_error: 6.8334 - val_loss: 14.3107 - val_mean_squared_error: 14.3107\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 6.9663 - mean_squared_error: 6.9663 - val_loss: 13.5646 - val_mean_squared_error: 13.5646\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 6.6484 - mean_squared_error: 6.6484 - val_loss: 11.0407 - val_mean_squared_error: 11.0407\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 6.8678 - mean_squared_error: 6.8678 - val_loss: 5.0086 - val_mean_squared_error: 5.0086\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 5.9208 - mean_squared_error: 5.9208 - val_loss: 6.3669 - val_mean_squared_error: 6.3669\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 5.6869 - mean_squared_error: 5.6869 - val_loss: 5.5930 - val_mean_squared_error: 5.5930\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 5.2893 - mean_squared_error: 5.2893 - val_loss: 5.9945 - val_mean_squared_error: 5.9945\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 5.2279 - mean_squared_error: 5.2279 - val_loss: 5.9377 - val_mean_squared_error: 5.9377\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 4.9471 - mean_squared_error: 4.9471 - val_loss: 4.9048 - val_mean_squared_error: 4.9048\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 4.9557 - mean_squared_error: 4.9557 - val_loss: 4.1596 - val_mean_squared_error: 4.1596\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 4.3408 - mean_squared_error: 4.3408 - val_loss: 5.1557 - val_mean_squared_error: 5.1557\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 4.3376 - mean_squared_error: 4.3376 - val_loss: 5.8924 - val_mean_squared_error: 5.8924\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 4.0815 - mean_squared_error: 4.0815 - val_loss: 4.6473 - val_mean_squared_error: 4.6473\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 3.9964 - mean_squared_error: 3.9964 - val_loss: 3.9483 - val_mean_squared_error: 3.9483\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 4.0974 - mean_squared_error: 4.0974 - val_loss: 4.4632 - val_mean_squared_error: 4.4632\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 3.6902 - mean_squared_error: 3.6902 - val_loss: 4.7745 - val_mean_squared_error: 4.7745\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 3.6078 - mean_squared_error: 3.6078 - val_loss: 4.1745 - val_mean_squared_error: 4.1745\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 3.6816 - mean_squared_error: 3.6816 - val_loss: 4.0517 - val_mean_squared_error: 4.0517\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 3.5029 - mean_squared_error: 3.5029 - val_loss: 4.2578 - val_mean_squared_error: 4.2578\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 3.6161 - mean_squared_error: 3.6161 - val_loss: 3.5813 - val_mean_squared_error: 3.5813\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 3.3790 - mean_squared_error: 3.3790 - val_loss: 3.3067 - val_mean_squared_error: 3.3067\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 3.3160 - mean_squared_error: 3.3160 - val_loss: 3.3401 - val_mean_squared_error: 3.3401\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 3.0615 - mean_squared_error: 3.0615 - val_loss: 3.3698 - val_mean_squared_error: 3.3698\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.9650 - mean_squared_error: 2.9650 - val_loss: 3.2303 - val_mean_squared_error: 3.2303\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 3.1753 - mean_squared_error: 3.1753 - val_loss: 4.3234 - val_mean_squared_error: 4.3234\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 3.1293 - mean_squared_error: 3.1293 - val_loss: 3.4003 - val_mean_squared_error: 3.4003\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 3.1396 - mean_squared_error: 3.1396 - val_loss: 3.0382 - val_mean_squared_error: 3.0382\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 3.0140 - mean_squared_error: 3.0140 - val_loss: 2.8247 - val_mean_squared_error: 2.8247\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.7658 - mean_squared_error: 2.7658 - val_loss: 3.5215 - val_mean_squared_error: 3.5215\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.6616 - mean_squared_error: 2.6616 - val_loss: 3.3789 - val_mean_squared_error: 3.3789\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.8484 - mean_squared_error: 2.8484 - val_loss: 3.3486 - val_mean_squared_error: 3.3486\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.7012 - mean_squared_error: 2.7012 - val_loss: 3.2813 - val_mean_squared_error: 3.2813\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.7864 - mean_squared_error: 2.7864 - val_loss: 3.4003 - val_mean_squared_error: 3.4003\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 2.6714 - mean_squared_error: 2.6714 - val_loss: 4.0587 - val_mean_squared_error: 4.0587\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.7272 - mean_squared_error: 2.7272 - val_loss: 2.8432 - val_mean_squared_error: 2.8432\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 2.4371 - mean_squared_error: 2.4371 - val_loss: 2.9424 - val_mean_squared_error: 2.9424\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.5822 - mean_squared_error: 2.5822 - val_loss: 3.1044 - val_mean_squared_error: 3.1044\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.7367 - mean_squared_error: 2.7367 - val_loss: 3.6863 - val_mean_squared_error: 3.6863\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.4853 - mean_squared_error: 2.4853 - val_loss: 2.8848 - val_mean_squared_error: 2.8848\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 2.4819 - mean_squared_error: 2.4819 - val_loss: 2.8375 - val_mean_squared_error: 2.8375\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.3850 - mean_squared_error: 2.3850 - val_loss: 4.5973 - val_mean_squared_error: 4.5973\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 2.5087 - mean_squared_error: 2.5087 - val_loss: 3.0785 - val_mean_squared_error: 3.0785\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 2.3293 - mean_squared_error: 2.3293 - val_loss: 2.9698 - val_mean_squared_error: 2.9698\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.3167 - mean_squared_error: 2.3167 - val_loss: 2.6513 - val_mean_squared_error: 2.6513\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.3935 - mean_squared_error: 2.3935 - val_loss: 3.4006 - val_mean_squared_error: 3.4006\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.9109 - mean_squared_error: 2.9109 - val_loss: 3.6884 - val_mean_squared_error: 3.6884\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.4542 - mean_squared_error: 2.4542 - val_loss: 2.8207 - val_mean_squared_error: 2.8207\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 2.2559 - mean_squared_error: 2.2559 - val_loss: 2.7323 - val_mean_squared_error: 2.7323\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 2.1925 - mean_squared_error: 2.1925 - val_loss: 3.0122 - val_mean_squared_error: 3.0122\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.3742 - mean_squared_error: 2.3742 - val_loss: 3.4489 - val_mean_squared_error: 3.4489\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 2.2746 - mean_squared_error: 2.2746 - val_loss: 3.0756 - val_mean_squared_error: 3.0756\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 2.1114 - mean_squared_error: 2.1114 - val_loss: 2.7561 - val_mean_squared_error: 2.7561\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.9945 - mean_squared_error: 1.9945 - val_loss: 2.8292 - val_mean_squared_error: 2.8292\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.2274 - mean_squared_error: 2.2274 - val_loss: 3.1468 - val_mean_squared_error: 3.1468\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.1852 - mean_squared_error: 2.1852 - val_loss: 2.9362 - val_mean_squared_error: 2.9362\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.9834 - mean_squared_error: 1.9834 - val_loss: 2.5934 - val_mean_squared_error: 2.5934\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.8565 - mean_squared_error: 1.8565 - val_loss: 2.7226 - val_mean_squared_error: 2.7226\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.9092 - mean_squared_error: 1.9092 - val_loss: 2.7217 - val_mean_squared_error: 2.7217\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.8650 - mean_squared_error: 1.8650 - val_loss: 2.5629 - val_mean_squared_error: 2.5629\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.7981 - mean_squared_error: 1.7981 - val_loss: 2.7652 - val_mean_squared_error: 2.7652\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.9398 - mean_squared_error: 1.9398 - val_loss: 3.0461 - val_mean_squared_error: 3.0461\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 2.0470 - mean_squared_error: 2.0470 - val_loss: 2.7057 - val_mean_squared_error: 2.7057\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.8805 - mean_squared_error: 1.8805 - val_loss: 2.7274 - val_mean_squared_error: 2.7274\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.9768 - mean_squared_error: 1.9768 - val_loss: 2.2918 - val_mean_squared_error: 2.2918\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.9369 - mean_squared_error: 1.9369 - val_loss: 2.6423 - val_mean_squared_error: 2.6423\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.7990 - mean_squared_error: 1.7990 - val_loss: 2.4877 - val_mean_squared_error: 2.4877\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.8707 - mean_squared_error: 1.8707 - val_loss: 2.6699 - val_mean_squared_error: 2.6699\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.7579 - mean_squared_error: 1.7579 - val_loss: 2.5768 - val_mean_squared_error: 2.5768\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.8453 - mean_squared_error: 1.8453 - val_loss: 2.6844 - val_mean_squared_error: 2.6844\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.8245 - mean_squared_error: 1.8245 - val_loss: 2.5079 - val_mean_squared_error: 2.5079\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.7898 - mean_squared_error: 1.7898 - val_loss: 2.3629 - val_mean_squared_error: 2.3629\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.8269 - mean_squared_error: 1.8269 - val_loss: 2.3799 - val_mean_squared_error: 2.3799\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.8926 - mean_squared_error: 1.8926 - val_loss: 2.6071 - val_mean_squared_error: 2.6071\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.8237 - mean_squared_error: 1.8237 - val_loss: 2.6074 - val_mean_squared_error: 2.6074\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.7589 - mean_squared_error: 1.7589 - val_loss: 2.4828 - val_mean_squared_error: 2.4828\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.8923 - mean_squared_error: 1.8923 - val_loss: 2.7220 - val_mean_squared_error: 2.7220\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.7086 - mean_squared_error: 1.7086 - val_loss: 2.4830 - val_mean_squared_error: 2.4830\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.7620 - mean_squared_error: 1.7620 - val_loss: 2.7111 - val_mean_squared_error: 2.7111\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.6925 - mean_squared_error: 1.6925 - val_loss: 2.5993 - val_mean_squared_error: 2.5993\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.5820 - mean_squared_error: 1.5820 - val_loss: 2.2946 - val_mean_squared_error: 2.2946\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.6128 - mean_squared_error: 1.6128 - val_loss: 2.3597 - val_mean_squared_error: 2.3597\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7012 - mean_squared_error: 1.7012 - val_loss: 2.5800 - val_mean_squared_error: 2.5800\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.7115 - mean_squared_error: 1.7115 - val_loss: 3.0187 - val_mean_squared_error: 3.0187\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.7032 - mean_squared_error: 1.7032 - val_loss: 2.6865 - val_mean_squared_error: 2.6865\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.6236 - mean_squared_error: 1.6236 - val_loss: 2.4241 - val_mean_squared_error: 2.4241\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.7082 - mean_squared_error: 1.7082 - val_loss: 2.4039 - val_mean_squared_error: 2.4039\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.7076 - mean_squared_error: 1.7076 - val_loss: 2.2967 - val_mean_squared_error: 2.2967\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5776 - mean_squared_error: 1.5776 - val_loss: 2.6205 - val_mean_squared_error: 2.6205\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.5427 - mean_squared_error: 1.5427 - val_loss: 2.1924 - val_mean_squared_error: 2.1924\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.6288 - mean_squared_error: 1.6288 - val_loss: 3.0233 - val_mean_squared_error: 3.0233\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.6488 - mean_squared_error: 1.6488 - val_loss: 2.4856 - val_mean_squared_error: 2.4856\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.5139 - mean_squared_error: 1.5139 - val_loss: 3.1656 - val_mean_squared_error: 3.1656\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.5753 - mean_squared_error: 1.5753 - val_loss: 2.3671 - val_mean_squared_error: 2.3671\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.5718 - mean_squared_error: 1.5718 - val_loss: 2.4057 - val_mean_squared_error: 2.4057\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5262 - mean_squared_error: 1.5262 - val_loss: 2.3915 - val_mean_squared_error: 2.3915\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.5943 - mean_squared_error: 1.5943 - val_loss: 2.3766 - val_mean_squared_error: 2.3766\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.5002 - mean_squared_error: 1.5002 - val_loss: 2.3285 - val_mean_squared_error: 2.3285\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.4817 - mean_squared_error: 1.4817 - val_loss: 2.3610 - val_mean_squared_error: 2.3610\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.4510 - mean_squared_error: 1.4510 - val_loss: 2.5535 - val_mean_squared_error: 2.5535\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.5448 - mean_squared_error: 1.5448 - val_loss: 2.3715 - val_mean_squared_error: 2.3715\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.5241 - mean_squared_error: 1.5241 - val_loss: 2.2018 - val_mean_squared_error: 2.2018\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.4802 - mean_squared_error: 1.4802 - val_loss: 2.2154 - val_mean_squared_error: 2.2154\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3660 - mean_squared_error: 1.3660 - val_loss: 2.0900 - val_mean_squared_error: 2.0900\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.3466 - mean_squared_error: 1.3466 - val_loss: 2.3019 - val_mean_squared_error: 2.3019\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.3287 - mean_squared_error: 1.3287 - val_loss: 2.2065 - val_mean_squared_error: 2.2065\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.4125 - mean_squared_error: 1.4125 - val_loss: 2.2722 - val_mean_squared_error: 2.2722\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.4477 - mean_squared_error: 1.4477 - val_loss: 2.3388 - val_mean_squared_error: 2.3388\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.3905 - mean_squared_error: 1.3905 - val_loss: 2.3224 - val_mean_squared_error: 2.3224\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.4414 - mean_squared_error: 1.4414 - val_loss: 2.1277 - val_mean_squared_error: 2.1277\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.4235 - mean_squared_error: 1.4235 - val_loss: 2.2116 - val_mean_squared_error: 2.2116\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.3086 - mean_squared_error: 1.3086 - val_loss: 2.2472 - val_mean_squared_error: 2.2472\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.2584 - mean_squared_error: 1.2584 - val_loss: 2.2079 - val_mean_squared_error: 2.2079\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.2241 - mean_squared_error: 1.2241 - val_loss: 2.5467 - val_mean_squared_error: 2.5467\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3383 - mean_squared_error: 1.3383 - val_loss: 2.2485 - val_mean_squared_error: 2.2485\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.3585 - mean_squared_error: 1.3585 - val_loss: 2.3151 - val_mean_squared_error: 2.3151\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.4136 - mean_squared_error: 1.4136 - val_loss: 2.3679 - val_mean_squared_error: 2.3679\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.3233 - mean_squared_error: 1.3233 - val_loss: 2.2102 - val_mean_squared_error: 2.2102\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.2690 - mean_squared_error: 1.2690 - val_loss: 2.2570 - val_mean_squared_error: 2.2570\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.1937 - mean_squared_error: 1.1937 - val_loss: 2.3854 - val_mean_squared_error: 2.3854\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.3905 - mean_squared_error: 1.3905 - val_loss: 2.0692 - val_mean_squared_error: 2.0692\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.3653 - mean_squared_error: 1.3653 - val_loss: 2.5321 - val_mean_squared_error: 2.5321\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.3266 - mean_squared_error: 1.3266 - val_loss: 2.0609 - val_mean_squared_error: 2.0609\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.3252 - mean_squared_error: 1.3252 - val_loss: 2.1149 - val_mean_squared_error: 2.1149\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.4196 - mean_squared_error: 1.4196 - val_loss: 2.0704 - val_mean_squared_error: 2.0704\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.2191 - mean_squared_error: 1.2191 - val_loss: 2.1395 - val_mean_squared_error: 2.1395\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.1174 - mean_squared_error: 1.1174 - val_loss: 2.1653 - val_mean_squared_error: 2.1653\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.4348 - mean_squared_error: 1.4348 - val_loss: 2.4979 - val_mean_squared_error: 2.4979\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.1371 - mean_squared_error: 1.1371 - val_loss: 2.0068 - val_mean_squared_error: 2.0068\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.3256 - mean_squared_error: 1.3256 - val_loss: 2.2638 - val_mean_squared_error: 2.2638\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.1861 - mean_squared_error: 1.1861 - val_loss: 2.1621 - val_mean_squared_error: 2.1621\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1277 - mean_squared_error: 1.1277 - val_loss: 2.1324 - val_mean_squared_error: 2.1324\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.2065 - mean_squared_error: 1.2065 - val_loss: 1.9916 - val_mean_squared_error: 1.9916\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.1885 - mean_squared_error: 1.1885 - val_loss: 2.1943 - val_mean_squared_error: 2.1943\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.3001 - mean_squared_error: 1.3001 - val_loss: 2.1388 - val_mean_squared_error: 2.1388\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.2389 - mean_squared_error: 1.2389 - val_loss: 2.4125 - val_mean_squared_error: 2.4125\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1189 - mean_squared_error: 1.1189 - val_loss: 2.0630 - val_mean_squared_error: 2.0630\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1281 - mean_squared_error: 1.1281 - val_loss: 2.1922 - val_mean_squared_error: 2.1922\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.1950 - mean_squared_error: 1.1950 - val_loss: 2.1875 - val_mean_squared_error: 2.1875\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.1191 - mean_squared_error: 1.1191 - val_loss: 2.1299 - val_mean_squared_error: 2.1299\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.1466 - mean_squared_error: 1.1466 - val_loss: 2.3212 - val_mean_squared_error: 2.3212\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1844 - mean_squared_error: 1.1844 - val_loss: 2.0311 - val_mean_squared_error: 2.0311\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.1630 - mean_squared_error: 1.1630 - val_loss: 2.1826 - val_mean_squared_error: 2.1826\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.0775 - mean_squared_error: 1.0775 - val_loss: 2.0646 - val_mean_squared_error: 2.0646\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.1792 - mean_squared_error: 1.1792 - val_loss: 2.1648 - val_mean_squared_error: 2.1648\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.0661 - mean_squared_error: 1.0661 - val_loss: 2.0615 - val_mean_squared_error: 2.0615\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.0992 - mean_squared_error: 1.0992 - val_loss: 2.0679 - val_mean_squared_error: 2.0679\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 428us/sample - loss: 1.1462 - mean_squared_error: 1.1462 - val_loss: 2.1947 - val_mean_squared_error: 2.1947\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.1175 - mean_squared_error: 1.1175 - val_loss: 2.0002 - val_mean_squared_error: 2.0002\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1431 - mean_squared_error: 1.1431 - val_loss: 1.9368 - val_mean_squared_error: 1.9368\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.1243 - mean_squared_error: 1.1243 - val_loss: 2.2280 - val_mean_squared_error: 2.2280\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0360 - mean_squared_error: 1.0360 - val_loss: 1.8933 - val_mean_squared_error: 1.8933\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0364 - mean_squared_error: 1.0364 - val_loss: 2.1237 - val_mean_squared_error: 2.1237\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.1034 - mean_squared_error: 1.1034 - val_loss: 2.3500 - val_mean_squared_error: 2.3500\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.1687 - mean_squared_error: 1.1687 - val_loss: 2.2222 - val_mean_squared_error: 2.2222\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.0741 - mean_squared_error: 1.0741 - val_loss: 2.1083 - val_mean_squared_error: 2.1083\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.0801 - mean_squared_error: 1.0801 - val_loss: 2.0477 - val_mean_squared_error: 2.0477\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.1544 - mean_squared_error: 1.1544 - val_loss: 1.8880 - val_mean_squared_error: 1.8880\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1007 - mean_squared_error: 1.1007 - val_loss: 2.3233 - val_mean_squared_error: 2.3233\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.0179 - mean_squared_error: 1.0179 - val_loss: 2.0148 - val_mean_squared_error: 2.0148\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0108 - mean_squared_error: 1.0108 - val_loss: 1.9631 - val_mean_squared_error: 1.9631\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.0317 - mean_squared_error: 1.0317 - val_loss: 1.9363 - val_mean_squared_error: 1.9363\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.9102 - mean_squared_error: 0.9102 - val_loss: 2.0905 - val_mean_squared_error: 2.0905\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 0.9126 - mean_squared_error: 0.9126 - val_loss: 1.9862 - val_mean_squared_error: 1.9862\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0282 - mean_squared_error: 1.0282 - val_loss: 1.9758 - val_mean_squared_error: 1.9758\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.0312 - mean_squared_error: 1.0312 - val_loss: 2.1548 - val_mean_squared_error: 2.1548\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.9864 - mean_squared_error: 0.9864 - val_loss: 2.0231 - val_mean_squared_error: 2.0231\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0512 - mean_squared_error: 1.0512 - val_loss: 2.6688 - val_mean_squared_error: 2.6688\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.0929 - mean_squared_error: 1.0929 - val_loss: 2.3375 - val_mean_squared_error: 2.3375\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.0883 - mean_squared_error: 1.0883 - val_loss: 2.1146 - val_mean_squared_error: 2.1146\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.9917 - mean_squared_error: 0.9917 - val_loss: 2.0380 - val_mean_squared_error: 2.0380\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0052 - mean_squared_error: 1.0052 - val_loss: 2.2400 - val_mean_squared_error: 2.2400\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9581 - mean_squared_error: 0.9581 - val_loss: 1.8816 - val_mean_squared_error: 1.8816\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9297 - mean_squared_error: 0.9297 - val_loss: 1.9738 - val_mean_squared_error: 1.9738\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9709 - mean_squared_error: 0.9709 - val_loss: 1.8909 - val_mean_squared_error: 1.8909\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.9072 - mean_squared_error: 0.9072 - val_loss: 2.1748 - val_mean_squared_error: 2.1748\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9587 - mean_squared_error: 0.9587 - val_loss: 2.0196 - val_mean_squared_error: 2.0196\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 0.9247 - mean_squared_error: 0.9247 - val_loss: 1.9865 - val_mean_squared_error: 1.9865\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.9612 - mean_squared_error: 0.9612 - val_loss: 2.1130 - val_mean_squared_error: 2.1130\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.9485 - mean_squared_error: 0.9485 - val_loss: 1.9281 - val_mean_squared_error: 1.9281\n",
            "==================================================\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_51 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_85 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_51 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_68 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_86 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_52 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_69 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_87 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_53 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_70 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_17 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_88 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_71 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_89 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 430.9327 - mean_squared_error: 430.9329 - val_loss: 1058.7855 - val_mean_squared_error: 1058.7854\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 25.5230 - mean_squared_error: 25.5230 - val_loss: 105.4128 - val_mean_squared_error: 105.4128\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 19.9697 - mean_squared_error: 19.9697 - val_loss: 80.4349 - val_mean_squared_error: 80.4349\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 18.7678 - mean_squared_error: 18.7678 - val_loss: 49.5850 - val_mean_squared_error: 49.5850\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 16.6285 - mean_squared_error: 16.6285 - val_loss: 26.0924 - val_mean_squared_error: 26.0924\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 15.0649 - mean_squared_error: 15.0649 - val_loss: 16.9502 - val_mean_squared_error: 16.9501\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 16.1270 - mean_squared_error: 16.1270 - val_loss: 16.0407 - val_mean_squared_error: 16.0407\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 15.4224 - mean_squared_error: 15.4224 - val_loss: 14.1706 - val_mean_squared_error: 14.1706\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 13.8504 - mean_squared_error: 13.8504 - val_loss: 19.8908 - val_mean_squared_error: 19.8908\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 13.0052 - mean_squared_error: 13.0052 - val_loss: 9.9893 - val_mean_squared_error: 9.9893\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 12.9992 - mean_squared_error: 12.9992 - val_loss: 13.8475 - val_mean_squared_error: 13.8475\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 12.2460 - mean_squared_error: 12.2460 - val_loss: 12.3287 - val_mean_squared_error: 12.3287\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 12.1221 - mean_squared_error: 12.1221 - val_loss: 10.9210 - val_mean_squared_error: 10.9210\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 9.9575 - mean_squared_error: 9.9575 - val_loss: 8.6426 - val_mean_squared_error: 8.6426\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 9.7992 - mean_squared_error: 9.7992 - val_loss: 8.5889 - val_mean_squared_error: 8.5889\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 8.9510 - mean_squared_error: 8.9510 - val_loss: 8.5484 - val_mean_squared_error: 8.5484\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 8.3906 - mean_squared_error: 8.3906 - val_loss: 8.1375 - val_mean_squared_error: 8.1375\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 7.6946 - mean_squared_error: 7.6946 - val_loss: 8.2614 - val_mean_squared_error: 8.2614\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 6.8369 - mean_squared_error: 6.8369 - val_loss: 7.7165 - val_mean_squared_error: 7.7165\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 6.5977 - mean_squared_error: 6.5977 - val_loss: 7.3291 - val_mean_squared_error: 7.3291\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 6.0316 - mean_squared_error: 6.0316 - val_loss: 7.5579 - val_mean_squared_error: 7.5579\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 5.6708 - mean_squared_error: 5.6708 - val_loss: 5.9580 - val_mean_squared_error: 5.9580\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 5.5774 - mean_squared_error: 5.5774 - val_loss: 5.5308 - val_mean_squared_error: 5.5308\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 5.0368 - mean_squared_error: 5.0368 - val_loss: 4.7958 - val_mean_squared_error: 4.7958\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 5.0783 - mean_squared_error: 5.0783 - val_loss: 5.1052 - val_mean_squared_error: 5.1052\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 4.3893 - mean_squared_error: 4.3893 - val_loss: 4.8496 - val_mean_squared_error: 4.8496\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 4.1720 - mean_squared_error: 4.1720 - val_loss: 5.2794 - val_mean_squared_error: 5.2794\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.9461 - mean_squared_error: 3.9461 - val_loss: 4.3288 - val_mean_squared_error: 4.3288\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.8434 - mean_squared_error: 3.8434 - val_loss: 5.0517 - val_mean_squared_error: 5.0517\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.6688 - mean_squared_error: 3.6688 - val_loss: 4.3756 - val_mean_squared_error: 4.3756\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.6963 - mean_squared_error: 3.6963 - val_loss: 4.3694 - val_mean_squared_error: 4.3694\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.1690 - mean_squared_error: 3.1690 - val_loss: 3.8068 - val_mean_squared_error: 3.8068\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 3.3061 - mean_squared_error: 3.3061 - val_loss: 3.8989 - val_mean_squared_error: 3.8989\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 3.0379 - mean_squared_error: 3.0379 - val_loss: 4.0061 - val_mean_squared_error: 4.0061\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.9653 - mean_squared_error: 2.9653 - val_loss: 3.5251 - val_mean_squared_error: 3.5251\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 3.0329 - mean_squared_error: 3.0329 - val_loss: 3.7840 - val_mean_squared_error: 3.7840\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.7493 - mean_squared_error: 2.7493 - val_loss: 4.7237 - val_mean_squared_error: 4.7237\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.7386 - mean_squared_error: 2.7386 - val_loss: 4.8819 - val_mean_squared_error: 4.8819\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.6631 - mean_squared_error: 2.6631 - val_loss: 4.0154 - val_mean_squared_error: 4.0154\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.6855 - mean_squared_error: 2.6855 - val_loss: 4.0935 - val_mean_squared_error: 4.0935\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.9296 - mean_squared_error: 2.9296 - val_loss: 3.7986 - val_mean_squared_error: 3.7986\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.4960 - mean_squared_error: 2.4960 - val_loss: 3.3139 - val_mean_squared_error: 3.3139\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.3519 - mean_squared_error: 2.3519 - val_loss: 3.2999 - val_mean_squared_error: 3.2999\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.5713 - mean_squared_error: 2.5713 - val_loss: 3.8741 - val_mean_squared_error: 3.8741\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.4620 - mean_squared_error: 2.4620 - val_loss: 3.1572 - val_mean_squared_error: 3.1572\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.2132 - mean_squared_error: 2.2132 - val_loss: 3.2936 - val_mean_squared_error: 3.2936\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.0484 - mean_squared_error: 2.0484 - val_loss: 3.8837 - val_mean_squared_error: 3.8837\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.2078 - mean_squared_error: 2.2078 - val_loss: 3.3985 - val_mean_squared_error: 3.3985\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.3053 - mean_squared_error: 2.3053 - val_loss: 3.1700 - val_mean_squared_error: 3.1700\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 2.0829 - mean_squared_error: 2.0829 - val_loss: 3.1448 - val_mean_squared_error: 3.1448\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.0146 - mean_squared_error: 2.0146 - val_loss: 3.4179 - val_mean_squared_error: 3.4179\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.9241 - mean_squared_error: 1.9241 - val_loss: 3.0612 - val_mean_squared_error: 3.0612\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.9214 - mean_squared_error: 1.9214 - val_loss: 3.4187 - val_mean_squared_error: 3.4187\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.1229 - mean_squared_error: 2.1229 - val_loss: 3.4827 - val_mean_squared_error: 3.4827\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.1329 - mean_squared_error: 2.1329 - val_loss: 3.1352 - val_mean_squared_error: 3.1352\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.8467 - mean_squared_error: 1.8467 - val_loss: 3.4026 - val_mean_squared_error: 3.4026\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.8214 - mean_squared_error: 1.8214 - val_loss: 3.1037 - val_mean_squared_error: 3.1037\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.9067 - mean_squared_error: 1.9067 - val_loss: 3.0615 - val_mean_squared_error: 3.0615\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.7898 - mean_squared_error: 1.7898 - val_loss: 2.9600 - val_mean_squared_error: 2.9600\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.7326 - mean_squared_error: 1.7326 - val_loss: 3.0294 - val_mean_squared_error: 3.0294\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.6937 - mean_squared_error: 1.6937 - val_loss: 3.2704 - val_mean_squared_error: 3.2704\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.9196 - mean_squared_error: 1.9196 - val_loss: 4.0891 - val_mean_squared_error: 4.0891\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.8364 - mean_squared_error: 1.8364 - val_loss: 3.3865 - val_mean_squared_error: 3.3865\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.8120 - mean_squared_error: 1.8120 - val_loss: 3.1010 - val_mean_squared_error: 3.1010\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.6315 - mean_squared_error: 1.6315 - val_loss: 3.5935 - val_mean_squared_error: 3.5935\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.7684 - mean_squared_error: 1.7684 - val_loss: 2.9733 - val_mean_squared_error: 2.9733\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.4643 - mean_squared_error: 1.4643 - val_loss: 3.1366 - val_mean_squared_error: 3.1366\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.7212 - mean_squared_error: 1.7212 - val_loss: 3.0150 - val_mean_squared_error: 3.0150\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.7504 - mean_squared_error: 1.7504 - val_loss: 3.2548 - val_mean_squared_error: 3.2548\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.7773 - mean_squared_error: 1.7773 - val_loss: 3.0299 - val_mean_squared_error: 3.0299\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.7301 - mean_squared_error: 1.7301 - val_loss: 3.0273 - val_mean_squared_error: 3.0273\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.6204 - mean_squared_error: 1.6204 - val_loss: 3.0540 - val_mean_squared_error: 3.0540\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.4885 - mean_squared_error: 1.4885 - val_loss: 3.1243 - val_mean_squared_error: 3.1243\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.7571 - mean_squared_error: 1.7571 - val_loss: 2.8481 - val_mean_squared_error: 2.8481\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.6439 - mean_squared_error: 1.6439 - val_loss: 3.5215 - val_mean_squared_error: 3.5215\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.4518 - mean_squared_error: 1.4518 - val_loss: 2.6663 - val_mean_squared_error: 2.6663\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.4180 - mean_squared_error: 1.4180 - val_loss: 2.6267 - val_mean_squared_error: 2.6267\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.5382 - mean_squared_error: 1.5382 - val_loss: 3.0352 - val_mean_squared_error: 3.0352\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.6743 - mean_squared_error: 1.6743 - val_loss: 2.8364 - val_mean_squared_error: 2.8364\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.4876 - mean_squared_error: 1.4876 - val_loss: 2.4852 - val_mean_squared_error: 2.4852\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.2879 - mean_squared_error: 1.2879 - val_loss: 2.6516 - val_mean_squared_error: 2.6516\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4759 - mean_squared_error: 1.4758 - val_loss: 2.7963 - val_mean_squared_error: 2.7963\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.3752 - mean_squared_error: 1.3752 - val_loss: 3.0305 - val_mean_squared_error: 3.0305\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5129 - mean_squared_error: 1.5129 - val_loss: 2.6531 - val_mean_squared_error: 2.6531\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3783 - mean_squared_error: 1.3783 - val_loss: 2.5613 - val_mean_squared_error: 2.5613\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3767 - mean_squared_error: 1.3767 - val_loss: 2.6297 - val_mean_squared_error: 2.6297\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.2971 - mean_squared_error: 1.2971 - val_loss: 3.1369 - val_mean_squared_error: 3.1369\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.4976 - mean_squared_error: 1.4976 - val_loss: 3.0843 - val_mean_squared_error: 3.0843\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.3416 - mean_squared_error: 1.3416 - val_loss: 2.6873 - val_mean_squared_error: 2.6873\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2076 - mean_squared_error: 1.2076 - val_loss: 2.5766 - val_mean_squared_error: 2.5766\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.4169 - mean_squared_error: 1.4169 - val_loss: 3.0155 - val_mean_squared_error: 3.0155\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3111 - mean_squared_error: 1.3111 - val_loss: 2.4734 - val_mean_squared_error: 2.4734\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.0557 - mean_squared_error: 1.0557 - val_loss: 2.7785 - val_mean_squared_error: 2.7785\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.1825 - mean_squared_error: 1.1825 - val_loss: 2.6491 - val_mean_squared_error: 2.6491\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.2398 - mean_squared_error: 1.2398 - val_loss: 3.0080 - val_mean_squared_error: 3.0080\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.3310 - mean_squared_error: 1.3310 - val_loss: 2.9531 - val_mean_squared_error: 2.9531\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2508 - mean_squared_error: 1.2508 - val_loss: 2.4429 - val_mean_squared_error: 2.4429\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.1768 - mean_squared_error: 1.1768 - val_loss: 2.8996 - val_mean_squared_error: 2.8996\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.1630 - mean_squared_error: 1.1630 - val_loss: 3.0198 - val_mean_squared_error: 3.0198\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.6011 - mean_squared_error: 1.6011 - val_loss: 2.9528 - val_mean_squared_error: 2.9528\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.0584 - mean_squared_error: 1.0584 - val_loss: 2.5421 - val_mean_squared_error: 2.5421\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1494 - mean_squared_error: 1.1494 - val_loss: 3.0451 - val_mean_squared_error: 3.0451\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.2504 - mean_squared_error: 1.2504 - val_loss: 2.7452 - val_mean_squared_error: 2.7452\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.1600 - mean_squared_error: 1.1600 - val_loss: 2.4928 - val_mean_squared_error: 2.4928\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.0151 - mean_squared_error: 1.0151 - val_loss: 2.5069 - val_mean_squared_error: 2.5069\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.1597 - mean_squared_error: 1.1597 - val_loss: 2.7526 - val_mean_squared_error: 2.7526\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2175 - mean_squared_error: 1.2175 - val_loss: 2.4394 - val_mean_squared_error: 2.4394\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.1406 - mean_squared_error: 1.1406 - val_loss: 2.7160 - val_mean_squared_error: 2.7160\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.1403 - mean_squared_error: 1.1403 - val_loss: 2.9003 - val_mean_squared_error: 2.9003\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1091 - mean_squared_error: 1.1091 - val_loss: 2.7654 - val_mean_squared_error: 2.7654\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.1319 - mean_squared_error: 1.1319 - val_loss: 2.7540 - val_mean_squared_error: 2.7540\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.2249 - mean_squared_error: 1.2249 - val_loss: 2.4269 - val_mean_squared_error: 2.4269\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.1565 - mean_squared_error: 1.1565 - val_loss: 2.7291 - val_mean_squared_error: 2.7291\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.3250 - mean_squared_error: 1.3250 - val_loss: 2.7059 - val_mean_squared_error: 2.7059\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.0259 - mean_squared_error: 1.0259 - val_loss: 2.8190 - val_mean_squared_error: 2.8190\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.1352 - mean_squared_error: 1.1352 - val_loss: 2.5601 - val_mean_squared_error: 2.5601\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.0281 - mean_squared_error: 1.0281 - val_loss: 2.5447 - val_mean_squared_error: 2.5447\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1579 - mean_squared_error: 1.1579 - val_loss: 2.4240 - val_mean_squared_error: 2.4240\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0946 - mean_squared_error: 1.0946 - val_loss: 3.6818 - val_mean_squared_error: 3.6818\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0955 - mean_squared_error: 1.0955 - val_loss: 3.0463 - val_mean_squared_error: 3.0463\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.2350 - mean_squared_error: 1.2350 - val_loss: 2.6644 - val_mean_squared_error: 2.6644\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 1.1172 - mean_squared_error: 1.1172 - val_loss: 2.4889 - val_mean_squared_error: 2.4889\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.0350 - mean_squared_error: 1.0350 - val_loss: 3.1611 - val_mean_squared_error: 3.1611\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.0562 - mean_squared_error: 1.0562 - val_loss: 2.6938 - val_mean_squared_error: 2.6938\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.9801 - mean_squared_error: 0.9801 - val_loss: 2.5245 - val_mean_squared_error: 2.5245\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1384 - mean_squared_error: 1.1384 - val_loss: 2.6256 - val_mean_squared_error: 2.6256\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.1384 - mean_squared_error: 1.1384 - val_loss: 2.9761 - val_mean_squared_error: 2.9761\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.1546 - mean_squared_error: 1.1546 - val_loss: 2.7967 - val_mean_squared_error: 2.7967\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0812 - mean_squared_error: 1.0812 - val_loss: 2.5133 - val_mean_squared_error: 2.5133\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.0356 - mean_squared_error: 1.0356 - val_loss: 2.3721 - val_mean_squared_error: 2.3721\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.8914 - mean_squared_error: 0.8914 - val_loss: 2.5451 - val_mean_squared_error: 2.5451\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 0.9463 - mean_squared_error: 0.9463 - val_loss: 2.4335 - val_mean_squared_error: 2.4335\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9679 - mean_squared_error: 0.9679 - val_loss: 2.4543 - val_mean_squared_error: 2.4543\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.0876 - mean_squared_error: 1.0876 - val_loss: 2.5147 - val_mean_squared_error: 2.5147\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.9411 - mean_squared_error: 0.9411 - val_loss: 2.3907 - val_mean_squared_error: 2.3907\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 0.8256 - mean_squared_error: 0.8256 - val_loss: 2.4775 - val_mean_squared_error: 2.4775\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.0344 - mean_squared_error: 1.0344 - val_loss: 2.4763 - val_mean_squared_error: 2.4763\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.9723 - mean_squared_error: 0.9723 - val_loss: 2.4177 - val_mean_squared_error: 2.4177\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0963 - mean_squared_error: 1.0963 - val_loss: 2.8690 - val_mean_squared_error: 2.8690\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9336 - mean_squared_error: 0.9336 - val_loss: 2.5603 - val_mean_squared_error: 2.5603\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9853 - mean_squared_error: 0.9853 - val_loss: 2.5237 - val_mean_squared_error: 2.5237\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9562 - mean_squared_error: 0.9562 - val_loss: 2.7671 - val_mean_squared_error: 2.7671\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9595 - mean_squared_error: 0.9595 - val_loss: 2.3691 - val_mean_squared_error: 2.3691\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.8906 - mean_squared_error: 0.8906 - val_loss: 2.3879 - val_mean_squared_error: 2.3879\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9314 - mean_squared_error: 0.9314 - val_loss: 2.6580 - val_mean_squared_error: 2.6580\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0020 - mean_squared_error: 1.0020 - val_loss: 2.3490 - val_mean_squared_error: 2.3490\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.8170 - mean_squared_error: 0.8170 - val_loss: 2.3071 - val_mean_squared_error: 2.3071\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.9801 - mean_squared_error: 0.9801 - val_loss: 2.4158 - val_mean_squared_error: 2.4158\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 0.8906 - mean_squared_error: 0.8906 - val_loss: 2.3794 - val_mean_squared_error: 2.3794\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9209 - mean_squared_error: 0.9209 - val_loss: 2.5152 - val_mean_squared_error: 2.5152\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.9044 - mean_squared_error: 0.9044 - val_loss: 2.4093 - val_mean_squared_error: 2.4093\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.8066 - mean_squared_error: 0.8066 - val_loss: 2.4161 - val_mean_squared_error: 2.4161\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.8238 - mean_squared_error: 0.8238 - val_loss: 2.6522 - val_mean_squared_error: 2.6522\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.9501 - mean_squared_error: 0.9501 - val_loss: 2.3801 - val_mean_squared_error: 2.3801\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.8700 - mean_squared_error: 0.8700 - val_loss: 2.2605 - val_mean_squared_error: 2.2605\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 0.7260 - mean_squared_error: 0.7260 - val_loss: 2.3172 - val_mean_squared_error: 2.3172\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.7686 - mean_squared_error: 0.7686 - val_loss: 2.6266 - val_mean_squared_error: 2.6266\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 0.9787 - mean_squared_error: 0.9787 - val_loss: 2.2892 - val_mean_squared_error: 2.2892\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.8068 - mean_squared_error: 0.8068 - val_loss: 2.2288 - val_mean_squared_error: 2.2288\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.7934 - mean_squared_error: 0.7934 - val_loss: 2.2922 - val_mean_squared_error: 2.2922\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.8947 - mean_squared_error: 0.8947 - val_loss: 2.4290 - val_mean_squared_error: 2.4290\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7982 - mean_squared_error: 0.7982 - val_loss: 2.2783 - val_mean_squared_error: 2.2783\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8816 - mean_squared_error: 0.8816 - val_loss: 2.6282 - val_mean_squared_error: 2.6282\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.8392 - mean_squared_error: 0.8392 - val_loss: 2.4651 - val_mean_squared_error: 2.4651\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.8584 - mean_squared_error: 0.8584 - val_loss: 2.2567 - val_mean_squared_error: 2.2567\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8446 - mean_squared_error: 0.8446 - val_loss: 2.2793 - val_mean_squared_error: 2.2793\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.7515 - mean_squared_error: 0.7515 - val_loss: 2.4248 - val_mean_squared_error: 2.4248\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.8238 - mean_squared_error: 0.8238 - val_loss: 2.3614 - val_mean_squared_error: 2.3614\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.7985 - mean_squared_error: 0.7985 - val_loss: 2.3099 - val_mean_squared_error: 2.3099\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8280 - mean_squared_error: 0.8280 - val_loss: 2.2292 - val_mean_squared_error: 2.2292\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.7472 - mean_squared_error: 0.7472 - val_loss: 2.4743 - val_mean_squared_error: 2.4743\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8071 - mean_squared_error: 0.8071 - val_loss: 2.5587 - val_mean_squared_error: 2.5587\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 2.3405 - val_mean_squared_error: 2.3405\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.7568 - mean_squared_error: 0.7568 - val_loss: 2.5943 - val_mean_squared_error: 2.5943\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8112 - mean_squared_error: 0.8112 - val_loss: 2.3315 - val_mean_squared_error: 2.3315\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8643 - mean_squared_error: 0.8643 - val_loss: 2.3898 - val_mean_squared_error: 2.3898\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9597 - mean_squared_error: 0.9597 - val_loss: 2.8807 - val_mean_squared_error: 2.8807\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8931 - mean_squared_error: 0.8931 - val_loss: 2.3473 - val_mean_squared_error: 2.3473\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.7320 - mean_squared_error: 0.7320 - val_loss: 2.4731 - val_mean_squared_error: 2.4731\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.7046 - mean_squared_error: 0.7046 - val_loss: 2.3760 - val_mean_squared_error: 2.3760\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.8400 - mean_squared_error: 0.8400 - val_loss: 2.2512 - val_mean_squared_error: 2.2512\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.7660 - mean_squared_error: 0.7660 - val_loss: 2.2651 - val_mean_squared_error: 2.2651\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7495 - mean_squared_error: 0.7495 - val_loss: 2.5073 - val_mean_squared_error: 2.5073\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8171 - mean_squared_error: 0.8171 - val_loss: 2.2126 - val_mean_squared_error: 2.2126\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.7384 - mean_squared_error: 0.7384 - val_loss: 2.2209 - val_mean_squared_error: 2.2209\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.7888 - mean_squared_error: 0.7888 - val_loss: 2.1842 - val_mean_squared_error: 2.1842\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.7686 - mean_squared_error: 0.7686 - val_loss: 2.1745 - val_mean_squared_error: 2.1745\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.7141 - mean_squared_error: 0.7141 - val_loss: 2.1811 - val_mean_squared_error: 2.1811\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.7419 - mean_squared_error: 0.7419 - val_loss: 2.3404 - val_mean_squared_error: 2.3404\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.7655 - mean_squared_error: 0.7655 - val_loss: 2.4917 - val_mean_squared_error: 2.4917\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7968 - mean_squared_error: 0.7968 - val_loss: 2.1101 - val_mean_squared_error: 2.1101\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.7525 - mean_squared_error: 0.7525 - val_loss: 2.3355 - val_mean_squared_error: 2.3355\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.6856 - mean_squared_error: 0.6856 - val_loss: 2.2347 - val_mean_squared_error: 2.2347\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8177 - mean_squared_error: 0.8177 - val_loss: 2.3541 - val_mean_squared_error: 2.3541\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.6416 - mean_squared_error: 0.6416 - val_loss: 2.5078 - val_mean_squared_error: 2.5078\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.7029 - mean_squared_error: 0.7029 - val_loss: 2.2412 - val_mean_squared_error: 2.2412\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.6523 - mean_squared_error: 0.6523 - val_loss: 2.4389 - val_mean_squared_error: 2.4389\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.6782 - mean_squared_error: 0.6782 - val_loss: 2.2628 - val_mean_squared_error: 2.2628\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.7141 - mean_squared_error: 0.7141 - val_loss: 2.3942 - val_mean_squared_error: 2.3942\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.6717 - mean_squared_error: 0.6717 - val_loss: 2.1435 - val_mean_squared_error: 2.1435\n",
            "==================================================\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_54 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_90 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_54 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_72 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_91 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_55 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_73 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_92 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_56 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_74 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_18 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_93 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_75 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_54 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_94 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_55 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 428.6786 - mean_squared_error: 428.6786 - val_loss: 615.6566 - val_mean_squared_error: 615.6566\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 23.9186 - mean_squared_error: 23.9187 - val_loss: 163.0092 - val_mean_squared_error: 163.0092\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 19.3289 - mean_squared_error: 19.3289 - val_loss: 73.1411 - val_mean_squared_error: 73.1411\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 19.7389 - mean_squared_error: 19.7389 - val_loss: 52.8824 - val_mean_squared_error: 52.8824\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 17.7648 - mean_squared_error: 17.7648 - val_loss: 26.2673 - val_mean_squared_error: 26.2673\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 15.1859 - mean_squared_error: 15.1859 - val_loss: 18.0131 - val_mean_squared_error: 18.0131\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 14.3092 - mean_squared_error: 14.3092 - val_loss: 19.5632 - val_mean_squared_error: 19.5632\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 14.1019 - mean_squared_error: 14.1019 - val_loss: 11.4687 - val_mean_squared_error: 11.4687\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 12.2902 - mean_squared_error: 12.2902 - val_loss: 13.7582 - val_mean_squared_error: 13.7582\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 11.5285 - mean_squared_error: 11.5285 - val_loss: 10.1523 - val_mean_squared_error: 10.1523\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 12.2988 - mean_squared_error: 12.2988 - val_loss: 9.8216 - val_mean_squared_error: 9.8216\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 10.7700 - mean_squared_error: 10.7700 - val_loss: 10.7992 - val_mean_squared_error: 10.7992\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 9.8836 - mean_squared_error: 9.8836 - val_loss: 10.1370 - val_mean_squared_error: 10.1370\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 9.3488 - mean_squared_error: 9.3488 - val_loss: 8.3176 - val_mean_squared_error: 8.3176\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 8.7290 - mean_squared_error: 8.7290 - val_loss: 8.1832 - val_mean_squared_error: 8.1832\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 7.9337 - mean_squared_error: 7.9337 - val_loss: 7.8077 - val_mean_squared_error: 7.8077\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 7.8395 - mean_squared_error: 7.8395 - val_loss: 7.5739 - val_mean_squared_error: 7.5739\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 6.5681 - mean_squared_error: 6.5681 - val_loss: 7.9082 - val_mean_squared_error: 7.9082\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 6.3193 - mean_squared_error: 6.3193 - val_loss: 10.6534 - val_mean_squared_error: 10.6534\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 5.6246 - mean_squared_error: 5.6246 - val_loss: 5.3889 - val_mean_squared_error: 5.3889\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 5.3301 - mean_squared_error: 5.3301 - val_loss: 5.1867 - val_mean_squared_error: 5.1867\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 4.9720 - mean_squared_error: 4.9720 - val_loss: 4.9478 - val_mean_squared_error: 4.9478\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 4.8464 - mean_squared_error: 4.8464 - val_loss: 5.3046 - val_mean_squared_error: 5.3046\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 4.2994 - mean_squared_error: 4.2994 - val_loss: 4.2816 - val_mean_squared_error: 4.2816\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 4.0811 - mean_squared_error: 4.0811 - val_loss: 4.4112 - val_mean_squared_error: 4.4112\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 4.0538 - mean_squared_error: 4.0538 - val_loss: 4.1257 - val_mean_squared_error: 4.1257\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.9973 - mean_squared_error: 3.9973 - val_loss: 4.3025 - val_mean_squared_error: 4.3025\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.9488 - mean_squared_error: 3.9488 - val_loss: 3.8052 - val_mean_squared_error: 3.8052\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.6075 - mean_squared_error: 3.6075 - val_loss: 3.8159 - val_mean_squared_error: 3.8159\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.4281 - mean_squared_error: 3.4281 - val_loss: 4.2163 - val_mean_squared_error: 4.2163\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.3315 - mean_squared_error: 3.3315 - val_loss: 3.7677 - val_mean_squared_error: 3.7677\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.1338 - mean_squared_error: 3.1338 - val_loss: 3.4373 - val_mean_squared_error: 3.4373\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 3.1819 - mean_squared_error: 3.1819 - val_loss: 3.7768 - val_mean_squared_error: 3.7768\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.9134 - mean_squared_error: 2.9134 - val_loss: 2.9456 - val_mean_squared_error: 2.9456\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.8028 - mean_squared_error: 2.8028 - val_loss: 3.2790 - val_mean_squared_error: 3.2790\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.8758 - mean_squared_error: 2.8758 - val_loss: 3.2155 - val_mean_squared_error: 3.2155\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.5845 - mean_squared_error: 2.5845 - val_loss: 3.6549 - val_mean_squared_error: 3.6549\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.1454 - mean_squared_error: 3.1454 - val_loss: 3.5465 - val_mean_squared_error: 3.5465\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.5500 - mean_squared_error: 2.5500 - val_loss: 3.4028 - val_mean_squared_error: 3.4028\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.4621 - mean_squared_error: 2.4621 - val_loss: 3.8440 - val_mean_squared_error: 3.8440\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.5305 - mean_squared_error: 2.5305 - val_loss: 2.8687 - val_mean_squared_error: 2.8687\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.3129 - mean_squared_error: 2.3129 - val_loss: 3.0525 - val_mean_squared_error: 3.0525\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.4819 - mean_squared_error: 2.4819 - val_loss: 3.2793 - val_mean_squared_error: 3.2793\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.4445 - mean_squared_error: 2.4445 - val_loss: 3.4022 - val_mean_squared_error: 3.4022\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.4921 - mean_squared_error: 2.4921 - val_loss: 3.3525 - val_mean_squared_error: 3.3525\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.4824 - mean_squared_error: 2.4824 - val_loss: 2.8059 - val_mean_squared_error: 2.8059\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.1404 - mean_squared_error: 2.1404 - val_loss: 2.6886 - val_mean_squared_error: 2.6886\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 2.2036 - mean_squared_error: 2.2036 - val_loss: 2.6302 - val_mean_squared_error: 2.6302\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.2964 - mean_squared_error: 2.2964 - val_loss: 2.9426 - val_mean_squared_error: 2.9426\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.0216 - mean_squared_error: 2.0216 - val_loss: 3.1963 - val_mean_squared_error: 3.1963\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.9983 - mean_squared_error: 1.9983 - val_loss: 2.6013 - val_mean_squared_error: 2.6013\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.9124 - mean_squared_error: 1.9124 - val_loss: 2.7638 - val_mean_squared_error: 2.7638\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.1879 - mean_squared_error: 2.1879 - val_loss: 2.6550 - val_mean_squared_error: 2.6550\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.9262 - mean_squared_error: 1.9262 - val_loss: 2.8754 - val_mean_squared_error: 2.8754\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.9771 - mean_squared_error: 1.9771 - val_loss: 3.0161 - val_mean_squared_error: 3.0161\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.0849 - mean_squared_error: 2.0849 - val_loss: 3.4743 - val_mean_squared_error: 3.4743\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.9990 - mean_squared_error: 1.9990 - val_loss: 3.3620 - val_mean_squared_error: 3.3620\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.1521 - mean_squared_error: 2.1521 - val_loss: 3.0371 - val_mean_squared_error: 3.0371\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.1012 - mean_squared_error: 2.1012 - val_loss: 2.6538 - val_mean_squared_error: 2.6538\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.7883 - mean_squared_error: 1.7883 - val_loss: 3.1449 - val_mean_squared_error: 3.1449\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.7283 - mean_squared_error: 1.7283 - val_loss: 2.7009 - val_mean_squared_error: 2.7009\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.7121 - mean_squared_error: 1.7121 - val_loss: 2.3392 - val_mean_squared_error: 2.3392\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.8096 - mean_squared_error: 1.8096 - val_loss: 2.5806 - val_mean_squared_error: 2.5806\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.6941 - mean_squared_error: 1.6941 - val_loss: 2.6885 - val_mean_squared_error: 2.6885\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.6848 - mean_squared_error: 1.6848 - val_loss: 2.7032 - val_mean_squared_error: 2.7032\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.7045 - mean_squared_error: 1.7045 - val_loss: 2.5585 - val_mean_squared_error: 2.5585\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.7466 - mean_squared_error: 1.7466 - val_loss: 2.7773 - val_mean_squared_error: 2.7773\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 1.7958 - mean_squared_error: 1.7958 - val_loss: 2.6197 - val_mean_squared_error: 2.6197\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.7448 - mean_squared_error: 1.7448 - val_loss: 2.5110 - val_mean_squared_error: 2.5110\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.9094 - mean_squared_error: 1.9094 - val_loss: 2.8845 - val_mean_squared_error: 2.8845\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.6569 - mean_squared_error: 1.6569 - val_loss: 2.5355 - val_mean_squared_error: 2.5355\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.5588 - mean_squared_error: 1.5588 - val_loss: 2.8197 - val_mean_squared_error: 2.8197\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.6138 - mean_squared_error: 1.6138 - val_loss: 2.5728 - val_mean_squared_error: 2.5728\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7056 - mean_squared_error: 1.7056 - val_loss: 2.8890 - val_mean_squared_error: 2.8890\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.7974 - mean_squared_error: 1.7974 - val_loss: 2.7150 - val_mean_squared_error: 2.7150\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.6240 - mean_squared_error: 1.6240 - val_loss: 2.6954 - val_mean_squared_error: 2.6954\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.5664 - mean_squared_error: 1.5664 - val_loss: 2.4739 - val_mean_squared_error: 2.4739\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.5822 - mean_squared_error: 1.5822 - val_loss: 2.7296 - val_mean_squared_error: 2.7296\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6083 - mean_squared_error: 1.6083 - val_loss: 2.8219 - val_mean_squared_error: 2.8219\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.5422 - mean_squared_error: 1.5422 - val_loss: 2.6241 - val_mean_squared_error: 2.6241\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.3149 - mean_squared_error: 1.3149 - val_loss: 2.3266 - val_mean_squared_error: 2.3266\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 1.4650 - mean_squared_error: 1.4650 - val_loss: 2.3567 - val_mean_squared_error: 2.3567\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.5084 - mean_squared_error: 1.5084 - val_loss: 2.5849 - val_mean_squared_error: 2.5849\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.6194 - mean_squared_error: 1.6194 - val_loss: 2.7590 - val_mean_squared_error: 2.7590\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.3794 - mean_squared_error: 1.3794 - val_loss: 2.9521 - val_mean_squared_error: 2.9521\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.5677 - mean_squared_error: 1.5677 - val_loss: 3.0100 - val_mean_squared_error: 3.0100\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.3820 - mean_squared_error: 1.3820 - val_loss: 2.2904 - val_mean_squared_error: 2.2904\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.5455 - mean_squared_error: 1.5455 - val_loss: 2.8284 - val_mean_squared_error: 2.8284\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.3920 - mean_squared_error: 1.3920 - val_loss: 2.6326 - val_mean_squared_error: 2.6326\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4498 - mean_squared_error: 1.4498 - val_loss: 2.5823 - val_mean_squared_error: 2.5823\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.3936 - mean_squared_error: 1.3936 - val_loss: 2.4863 - val_mean_squared_error: 2.4863\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.3283 - mean_squared_error: 1.3283 - val_loss: 2.4734 - val_mean_squared_error: 2.4734\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2934 - mean_squared_error: 1.2934 - val_loss: 2.2214 - val_mean_squared_error: 2.2214\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.3919 - mean_squared_error: 1.3919 - val_loss: 2.9126 - val_mean_squared_error: 2.9126\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.2437 - mean_squared_error: 1.2437 - val_loss: 2.3224 - val_mean_squared_error: 2.3224\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3420 - mean_squared_error: 1.3420 - val_loss: 2.3729 - val_mean_squared_error: 2.3729\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 1.2321 - mean_squared_error: 1.2321 - val_loss: 2.3310 - val_mean_squared_error: 2.3310\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1721 - mean_squared_error: 1.1721 - val_loss: 2.3392 - val_mean_squared_error: 2.3392\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.2786 - mean_squared_error: 1.2786 - val_loss: 2.3787 - val_mean_squared_error: 2.3787\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3087 - mean_squared_error: 1.3087 - val_loss: 2.4017 - val_mean_squared_error: 2.4017\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.3934 - mean_squared_error: 1.3934 - val_loss: 2.4495 - val_mean_squared_error: 2.4495\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.3462 - mean_squared_error: 1.3462 - val_loss: 2.3560 - val_mean_squared_error: 2.3560\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2430 - mean_squared_error: 1.2430 - val_loss: 2.4449 - val_mean_squared_error: 2.4449\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.3659 - mean_squared_error: 1.3659 - val_loss: 2.3172 - val_mean_squared_error: 2.3172\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.4333 - mean_squared_error: 1.4333 - val_loss: 2.6012 - val_mean_squared_error: 2.6012\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2574 - mean_squared_error: 1.2574 - val_loss: 2.3555 - val_mean_squared_error: 2.3555\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3175 - mean_squared_error: 1.3175 - val_loss: 2.3279 - val_mean_squared_error: 2.3279\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.3031 - mean_squared_error: 1.3031 - val_loss: 2.3120 - val_mean_squared_error: 2.3120\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.2710 - mean_squared_error: 1.2710 - val_loss: 2.4093 - val_mean_squared_error: 2.4093\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.2235 - mean_squared_error: 1.2235 - val_loss: 2.3998 - val_mean_squared_error: 2.3998\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1920 - mean_squared_error: 1.1920 - val_loss: 2.4827 - val_mean_squared_error: 2.4827\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3387 - mean_squared_error: 1.3387 - val_loss: 2.5139 - val_mean_squared_error: 2.5139\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2489 - mean_squared_error: 1.2489 - val_loss: 2.3785 - val_mean_squared_error: 2.3785\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 1.1985 - mean_squared_error: 1.1985 - val_loss: 2.3240 - val_mean_squared_error: 2.3240\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.2551 - mean_squared_error: 1.2551 - val_loss: 2.6304 - val_mean_squared_error: 2.6304\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2189 - mean_squared_error: 1.2189 - val_loss: 2.6674 - val_mean_squared_error: 2.6674\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.1358 - mean_squared_error: 1.1358 - val_loss: 2.3443 - val_mean_squared_error: 2.3443\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.1254 - mean_squared_error: 1.1254 - val_loss: 2.3837 - val_mean_squared_error: 2.3837\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.3945 - mean_squared_error: 1.3945 - val_loss: 2.4857 - val_mean_squared_error: 2.4857\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 1.1870 - mean_squared_error: 1.1870 - val_loss: 2.8425 - val_mean_squared_error: 2.8425\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1504 - mean_squared_error: 1.1504 - val_loss: 2.2698 - val_mean_squared_error: 2.2698\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.0453 - mean_squared_error: 1.0453 - val_loss: 2.3898 - val_mean_squared_error: 2.3898\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.0985 - mean_squared_error: 1.0985 - val_loss: 2.2262 - val_mean_squared_error: 2.2262\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.0476 - mean_squared_error: 1.0476 - val_loss: 2.1989 - val_mean_squared_error: 2.1989\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.1029 - mean_squared_error: 1.1029 - val_loss: 2.1765 - val_mean_squared_error: 2.1765\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.1655 - mean_squared_error: 1.1655 - val_loss: 2.1942 - val_mean_squared_error: 2.1942\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1885 - mean_squared_error: 1.1885 - val_loss: 2.2190 - val_mean_squared_error: 2.2190\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.9861 - mean_squared_error: 0.9861 - val_loss: 2.1038 - val_mean_squared_error: 2.1038\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.9923 - mean_squared_error: 0.9923 - val_loss: 2.1027 - val_mean_squared_error: 2.1027\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.1114 - mean_squared_error: 1.1114 - val_loss: 2.3167 - val_mean_squared_error: 2.3167\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.1083 - mean_squared_error: 1.1083 - val_loss: 2.1891 - val_mean_squared_error: 2.1891\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0358 - mean_squared_error: 1.0358 - val_loss: 2.1378 - val_mean_squared_error: 2.1378\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0570 - mean_squared_error: 1.0570 - val_loss: 2.0212 - val_mean_squared_error: 2.0212\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.0000 - mean_squared_error: 1.0000 - val_loss: 2.2540 - val_mean_squared_error: 2.2540\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0625 - mean_squared_error: 1.0625 - val_loss: 2.2083 - val_mean_squared_error: 2.2083\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9492 - mean_squared_error: 0.9492 - val_loss: 2.2381 - val_mean_squared_error: 2.2381\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8952 - mean_squared_error: 0.8952 - val_loss: 2.0250 - val_mean_squared_error: 2.0250\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0798 - mean_squared_error: 1.0798 - val_loss: 2.0326 - val_mean_squared_error: 2.0326\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.9278 - mean_squared_error: 0.9278 - val_loss: 2.5856 - val_mean_squared_error: 2.5856\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0559 - mean_squared_error: 1.0559 - val_loss: 2.4170 - val_mean_squared_error: 2.4170\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.3637 - mean_squared_error: 1.3637 - val_loss: 2.5935 - val_mean_squared_error: 2.5935\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0374 - mean_squared_error: 1.0374 - val_loss: 2.2053 - val_mean_squared_error: 2.2053\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.9725 - mean_squared_error: 0.9725 - val_loss: 2.6579 - val_mean_squared_error: 2.6579\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.1627 - mean_squared_error: 1.1627 - val_loss: 2.2827 - val_mean_squared_error: 2.2827\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0309 - mean_squared_error: 1.0309 - val_loss: 2.2186 - val_mean_squared_error: 2.2186\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.8957 - mean_squared_error: 0.8957 - val_loss: 2.0731 - val_mean_squared_error: 2.0731\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.0164 - mean_squared_error: 1.0164 - val_loss: 2.4350 - val_mean_squared_error: 2.4350\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.1762 - mean_squared_error: 1.1762 - val_loss: 2.2261 - val_mean_squared_error: 2.2261\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9955 - mean_squared_error: 0.9955 - val_loss: 2.2821 - val_mean_squared_error: 2.2821\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.0268 - mean_squared_error: 1.0268 - val_loss: 3.5035 - val_mean_squared_error: 3.5035\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0651 - mean_squared_error: 1.0651 - val_loss: 2.0200 - val_mean_squared_error: 2.0200\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.8722 - mean_squared_error: 0.8722 - val_loss: 2.2049 - val_mean_squared_error: 2.2049\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0547 - mean_squared_error: 1.0547 - val_loss: 2.0890 - val_mean_squared_error: 2.0890\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1247 - mean_squared_error: 1.1247 - val_loss: 1.9508 - val_mean_squared_error: 1.9508\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.8458 - mean_squared_error: 0.8458 - val_loss: 1.9658 - val_mean_squared_error: 1.9658\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.8194 - mean_squared_error: 0.8194 - val_loss: 2.1903 - val_mean_squared_error: 2.1903\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.0038 - mean_squared_error: 1.0038 - val_loss: 2.1744 - val_mean_squared_error: 2.1744\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.8187 - mean_squared_error: 0.8187 - val_loss: 1.9096 - val_mean_squared_error: 1.9096\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9063 - mean_squared_error: 0.9063 - val_loss: 2.1571 - val_mean_squared_error: 2.1571\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.9368 - mean_squared_error: 0.9368 - val_loss: 2.1649 - val_mean_squared_error: 2.1649\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.7867 - mean_squared_error: 0.7867 - val_loss: 2.0148 - val_mean_squared_error: 2.0148\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9976 - mean_squared_error: 0.9976 - val_loss: 2.1018 - val_mean_squared_error: 2.1018\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.9570 - mean_squared_error: 0.9570 - val_loss: 2.3011 - val_mean_squared_error: 2.3011\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8370 - mean_squared_error: 0.8370 - val_loss: 2.0218 - val_mean_squared_error: 2.0218\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.9221 - mean_squared_error: 0.9221 - val_loss: 2.2492 - val_mean_squared_error: 2.2492\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8152 - mean_squared_error: 0.8152 - val_loss: 1.9188 - val_mean_squared_error: 1.9188\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.8574 - mean_squared_error: 0.8574 - val_loss: 1.9493 - val_mean_squared_error: 1.9493\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7971 - mean_squared_error: 0.7971 - val_loss: 2.2570 - val_mean_squared_error: 2.2570\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 0.8692 - mean_squared_error: 0.8692 - val_loss: 2.1362 - val_mean_squared_error: 2.1362\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9270 - mean_squared_error: 0.9270 - val_loss: 2.3584 - val_mean_squared_error: 2.3584\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.8560 - mean_squared_error: 0.8560 - val_loss: 2.1884 - val_mean_squared_error: 2.1884\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.9070 - mean_squared_error: 0.9070 - val_loss: 2.0495 - val_mean_squared_error: 2.0495\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9336 - mean_squared_error: 0.9336 - val_loss: 2.2069 - val_mean_squared_error: 2.2069\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 0.8127 - mean_squared_error: 0.8127 - val_loss: 2.2172 - val_mean_squared_error: 2.2172\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.8229 - mean_squared_error: 0.8229 - val_loss: 1.9901 - val_mean_squared_error: 1.9901\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.8411 - mean_squared_error: 0.8411 - val_loss: 2.2582 - val_mean_squared_error: 2.2582\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.7668 - mean_squared_error: 0.7668 - val_loss: 2.0985 - val_mean_squared_error: 2.0985\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.7228 - mean_squared_error: 0.7228 - val_loss: 2.0415 - val_mean_squared_error: 2.0415\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.7843 - mean_squared_error: 0.7843 - val_loss: 2.1484 - val_mean_squared_error: 2.1484\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8579 - mean_squared_error: 0.8579 - val_loss: 2.0915 - val_mean_squared_error: 2.0915\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.7658 - mean_squared_error: 0.7658 - val_loss: 1.9949 - val_mean_squared_error: 1.9949\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.7809 - mean_squared_error: 0.7809 - val_loss: 1.9587 - val_mean_squared_error: 1.9587\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.7708 - mean_squared_error: 0.7708 - val_loss: 2.1720 - val_mean_squared_error: 2.1720\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8331 - mean_squared_error: 0.8331 - val_loss: 2.3951 - val_mean_squared_error: 2.3951\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8634 - mean_squared_error: 0.8634 - val_loss: 1.8582 - val_mean_squared_error: 1.8582\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.8005 - mean_squared_error: 0.8005 - val_loss: 2.0356 - val_mean_squared_error: 2.0356\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.7554 - mean_squared_error: 0.7554 - val_loss: 2.3553 - val_mean_squared_error: 2.3553\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.7712 - mean_squared_error: 0.7712 - val_loss: 2.0958 - val_mean_squared_error: 2.0958\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 0.7537 - mean_squared_error: 0.7537 - val_loss: 1.9287 - val_mean_squared_error: 1.9287\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.7013 - mean_squared_error: 0.7013 - val_loss: 2.2033 - val_mean_squared_error: 2.2033\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8372 - mean_squared_error: 0.8372 - val_loss: 2.0894 - val_mean_squared_error: 2.0894\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7680 - mean_squared_error: 0.7680 - val_loss: 1.9875 - val_mean_squared_error: 1.9875\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.7731 - mean_squared_error: 0.7731 - val_loss: 2.1904 - val_mean_squared_error: 2.1904\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.7750 - mean_squared_error: 0.7750 - val_loss: 2.2533 - val_mean_squared_error: 2.2533\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.8032 - mean_squared_error: 0.8032 - val_loss: 1.8684 - val_mean_squared_error: 1.8684\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.7611 - mean_squared_error: 0.7611 - val_loss: 2.1432 - val_mean_squared_error: 2.1432\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8087 - mean_squared_error: 0.8087 - val_loss: 2.2644 - val_mean_squared_error: 2.2644\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8524 - mean_squared_error: 0.8524 - val_loss: 1.9689 - val_mean_squared_error: 1.9689\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7537 - mean_squared_error: 0.7537 - val_loss: 2.0600 - val_mean_squared_error: 2.0600\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.6896 - mean_squared_error: 0.6896 - val_loss: 2.1662 - val_mean_squared_error: 2.1662\n",
            "==================================================\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_57 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_95 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_57 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_76 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_58 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_96 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_58 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_77 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_97 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_59 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_78 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_98 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_79 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_57 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_99 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_58 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_59 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 426.6153 - mean_squared_error: 426.6153 - val_loss: 1143.9678 - val_mean_squared_error: 1143.9679\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 21.8722 - mean_squared_error: 21.8722 - val_loss: 144.6354 - val_mean_squared_error: 144.6354\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 20.2001 - mean_squared_error: 20.2001 - val_loss: 98.3358 - val_mean_squared_error: 98.3358\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 22.3840 - mean_squared_error: 22.3840 - val_loss: 54.5365 - val_mean_squared_error: 54.5365\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 18.6919 - mean_squared_error: 18.6919 - val_loss: 32.9817 - val_mean_squared_error: 32.9817\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 15.4608 - mean_squared_error: 15.4608 - val_loss: 20.0282 - val_mean_squared_error: 20.0282\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 14.6066 - mean_squared_error: 14.6066 - val_loss: 12.6181 - val_mean_squared_error: 12.6181\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 13.8483 - mean_squared_error: 13.8483 - val_loss: 11.7142 - val_mean_squared_error: 11.7142\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 13.7807 - mean_squared_error: 13.7807 - val_loss: 10.3840 - val_mean_squared_error: 10.3840\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 14.6598 - mean_squared_error: 14.6598 - val_loss: 16.4228 - val_mean_squared_error: 16.4228\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 12.2580 - mean_squared_error: 12.2580 - val_loss: 11.2432 - val_mean_squared_error: 11.2432\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 10.7118 - mean_squared_error: 10.7118 - val_loss: 10.2248 - val_mean_squared_error: 10.2248\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 10.7972 - mean_squared_error: 10.7972 - val_loss: 10.9756 - val_mean_squared_error: 10.9756\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 10.4075 - mean_squared_error: 10.4075 - val_loss: 9.8774 - val_mean_squared_error: 9.8774\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 9.7284 - mean_squared_error: 9.7284 - val_loss: 8.7340 - val_mean_squared_error: 8.7340\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 9.4383 - mean_squared_error: 9.4383 - val_loss: 8.4860 - val_mean_squared_error: 8.4860\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 8.5536 - mean_squared_error: 8.5536 - val_loss: 7.5165 - val_mean_squared_error: 7.5165\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 8.1669 - mean_squared_error: 8.1669 - val_loss: 7.8729 - val_mean_squared_error: 7.8729\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 7.2447 - mean_squared_error: 7.2447 - val_loss: 6.8231 - val_mean_squared_error: 6.8231\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 6.9085 - mean_squared_error: 6.9085 - val_loss: 7.4406 - val_mean_squared_error: 7.4406\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 6.7554 - mean_squared_error: 6.7554 - val_loss: 6.0907 - val_mean_squared_error: 6.0907\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 5.7660 - mean_squared_error: 5.7660 - val_loss: 5.1195 - val_mean_squared_error: 5.1195\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 5.4672 - mean_squared_error: 5.4672 - val_loss: 6.3296 - val_mean_squared_error: 6.3296\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 5.2651 - mean_squared_error: 5.2652 - val_loss: 6.1872 - val_mean_squared_error: 6.1872\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 5.0133 - mean_squared_error: 5.0133 - val_loss: 6.0183 - val_mean_squared_error: 6.0183\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 4.7613 - mean_squared_error: 4.7613 - val_loss: 7.8782 - val_mean_squared_error: 7.8782\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 5.0873 - mean_squared_error: 5.0873 - val_loss: 5.3673 - val_mean_squared_error: 5.3673\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 4.3690 - mean_squared_error: 4.3690 - val_loss: 4.4812 - val_mean_squared_error: 4.4812\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 4.2342 - mean_squared_error: 4.2342 - val_loss: 4.4697 - val_mean_squared_error: 4.4697\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 3.8447 - mean_squared_error: 3.8447 - val_loss: 3.9087 - val_mean_squared_error: 3.9087\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.8826 - mean_squared_error: 3.8826 - val_loss: 4.2703 - val_mean_squared_error: 4.2703\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.9179 - mean_squared_error: 3.9179 - val_loss: 4.6891 - val_mean_squared_error: 4.6891\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 3.5019 - mean_squared_error: 3.5019 - val_loss: 4.0235 - val_mean_squared_error: 4.0235\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 3.4258 - mean_squared_error: 3.4258 - val_loss: 3.9588 - val_mean_squared_error: 3.9588\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 3.3158 - mean_squared_error: 3.3158 - val_loss: 3.4133 - val_mean_squared_error: 3.4133\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 3.1781 - mean_squared_error: 3.1781 - val_loss: 3.2850 - val_mean_squared_error: 3.2850\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 3.1277 - mean_squared_error: 3.1277 - val_loss: 3.3950 - val_mean_squared_error: 3.3950\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.9160 - mean_squared_error: 2.9160 - val_loss: 3.4031 - val_mean_squared_error: 3.4031\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 383us/sample - loss: 2.8458 - mean_squared_error: 2.8458 - val_loss: 3.1154 - val_mean_squared_error: 3.1154\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.8309 - mean_squared_error: 2.8309 - val_loss: 3.5739 - val_mean_squared_error: 3.5739\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.9902 - mean_squared_error: 2.9902 - val_loss: 3.4316 - val_mean_squared_error: 3.4316\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 2.8561 - mean_squared_error: 2.8561 - val_loss: 3.7055 - val_mean_squared_error: 3.7055\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.9818 - mean_squared_error: 2.9818 - val_loss: 3.0011 - val_mean_squared_error: 3.0011\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.6614 - mean_squared_error: 2.6614 - val_loss: 3.1576 - val_mean_squared_error: 3.1576\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.5085 - mean_squared_error: 2.5085 - val_loss: 3.0235 - val_mean_squared_error: 3.0235\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.7180 - mean_squared_error: 2.7180 - val_loss: 3.0542 - val_mean_squared_error: 3.0542\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.6552 - mean_squared_error: 2.6552 - val_loss: 3.9185 - val_mean_squared_error: 3.9185\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 2.6002 - mean_squared_error: 2.6002 - val_loss: 3.4970 - val_mean_squared_error: 3.4970\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.4975 - mean_squared_error: 2.4975 - val_loss: 3.1776 - val_mean_squared_error: 3.1776\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.4238 - mean_squared_error: 2.4238 - val_loss: 3.0948 - val_mean_squared_error: 3.0948\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 2.4679 - mean_squared_error: 2.4679 - val_loss: 2.9817 - val_mean_squared_error: 2.9817\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.4183 - mean_squared_error: 2.4183 - val_loss: 3.0782 - val_mean_squared_error: 3.0782\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.2719 - mean_squared_error: 2.2719 - val_loss: 2.7525 - val_mean_squared_error: 2.7525\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.2524 - mean_squared_error: 2.2524 - val_loss: 2.7415 - val_mean_squared_error: 2.7415\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.2813 - mean_squared_error: 2.2813 - val_loss: 2.7609 - val_mean_squared_error: 2.7609\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.1660 - mean_squared_error: 2.1660 - val_loss: 2.7617 - val_mean_squared_error: 2.7617\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 2.2474 - mean_squared_error: 2.2474 - val_loss: 2.9912 - val_mean_squared_error: 2.9912\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 2.1505 - mean_squared_error: 2.1505 - val_loss: 3.1561 - val_mean_squared_error: 3.1561\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.1804 - mean_squared_error: 2.1804 - val_loss: 2.7873 - val_mean_squared_error: 2.7873\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.9887 - mean_squared_error: 1.9887 - val_loss: 2.7868 - val_mean_squared_error: 2.7868\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.0957 - mean_squared_error: 2.0957 - val_loss: 3.1874 - val_mean_squared_error: 3.1874\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.1519 - mean_squared_error: 2.1519 - val_loss: 3.5275 - val_mean_squared_error: 3.5275\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.8900 - mean_squared_error: 1.8900 - val_loss: 2.5971 - val_mean_squared_error: 2.5971\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.9267 - mean_squared_error: 1.9267 - val_loss: 2.9004 - val_mean_squared_error: 2.9004\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.8904 - mean_squared_error: 1.8904 - val_loss: 2.5404 - val_mean_squared_error: 2.5404\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.9214 - mean_squared_error: 1.9214 - val_loss: 3.3238 - val_mean_squared_error: 3.3238\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.1177 - mean_squared_error: 2.1177 - val_loss: 2.4281 - val_mean_squared_error: 2.4281\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.8718 - mean_squared_error: 1.8718 - val_loss: 2.5634 - val_mean_squared_error: 2.5634\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 2.0192 - mean_squared_error: 2.0192 - val_loss: 2.6452 - val_mean_squared_error: 2.6452\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.8566 - mean_squared_error: 1.8566 - val_loss: 2.8116 - val_mean_squared_error: 2.8116\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.8990 - mean_squared_error: 1.8990 - val_loss: 3.1665 - val_mean_squared_error: 3.1665\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.8871 - mean_squared_error: 1.8871 - val_loss: 3.2921 - val_mean_squared_error: 3.2921\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.8846 - mean_squared_error: 1.8846 - val_loss: 2.4567 - val_mean_squared_error: 2.4567\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.7840 - mean_squared_error: 1.7840 - val_loss: 2.4715 - val_mean_squared_error: 2.4715\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.7733 - mean_squared_error: 1.7733 - val_loss: 2.4820 - val_mean_squared_error: 2.4820\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.6763 - mean_squared_error: 1.6763 - val_loss: 2.3985 - val_mean_squared_error: 2.3985\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.6507 - mean_squared_error: 1.6507 - val_loss: 2.7780 - val_mean_squared_error: 2.7780\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.5483 - mean_squared_error: 1.5483 - val_loss: 2.2758 - val_mean_squared_error: 2.2758\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.9409 - mean_squared_error: 1.9409 - val_loss: 2.5229 - val_mean_squared_error: 2.5229\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.6283 - mean_squared_error: 1.6283 - val_loss: 2.5616 - val_mean_squared_error: 2.5616\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4453 - mean_squared_error: 1.4453 - val_loss: 2.4821 - val_mean_squared_error: 2.4821\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6782 - mean_squared_error: 1.6782 - val_loss: 2.4475 - val_mean_squared_error: 2.4475\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.4957 - mean_squared_error: 1.4957 - val_loss: 2.3695 - val_mean_squared_error: 2.3695\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.5474 - mean_squared_error: 1.5474 - val_loss: 2.6531 - val_mean_squared_error: 2.6531\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.5988 - mean_squared_error: 1.5988 - val_loss: 2.6179 - val_mean_squared_error: 2.6179\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.6614 - mean_squared_error: 1.6614 - val_loss: 2.6222 - val_mean_squared_error: 2.6222\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.5070 - mean_squared_error: 1.5070 - val_loss: 2.3446 - val_mean_squared_error: 2.3446\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.4505 - mean_squared_error: 1.4505 - val_loss: 2.3496 - val_mean_squared_error: 2.3496\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.4129 - mean_squared_error: 1.4129 - val_loss: 2.8425 - val_mean_squared_error: 2.8425\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.4697 - mean_squared_error: 1.4697 - val_loss: 2.3166 - val_mean_squared_error: 2.3166\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.4935 - mean_squared_error: 1.4935 - val_loss: 2.7592 - val_mean_squared_error: 2.7592\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.4395 - mean_squared_error: 1.4395 - val_loss: 2.3179 - val_mean_squared_error: 2.3179\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.3955 - mean_squared_error: 1.3955 - val_loss: 2.1801 - val_mean_squared_error: 2.1801\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3529 - mean_squared_error: 1.3529 - val_loss: 2.7483 - val_mean_squared_error: 2.7483\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4067 - mean_squared_error: 1.4067 - val_loss: 2.5365 - val_mean_squared_error: 2.5365\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.4256 - mean_squared_error: 1.4256 - val_loss: 2.1893 - val_mean_squared_error: 2.1893\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.3680 - mean_squared_error: 1.3680 - val_loss: 2.2493 - val_mean_squared_error: 2.2493\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.3601 - mean_squared_error: 1.3601 - val_loss: 2.3587 - val_mean_squared_error: 2.3587\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.2880 - mean_squared_error: 1.2880 - val_loss: 2.3550 - val_mean_squared_error: 2.3550\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.5029 - mean_squared_error: 1.5029 - val_loss: 2.5827 - val_mean_squared_error: 2.5827\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.3426 - mean_squared_error: 1.3426 - val_loss: 2.4242 - val_mean_squared_error: 2.4242\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.2882 - mean_squared_error: 1.2882 - val_loss: 2.3157 - val_mean_squared_error: 2.3157\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.3570 - mean_squared_error: 1.3570 - val_loss: 2.4012 - val_mean_squared_error: 2.4012\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 382us/sample - loss: 1.3714 - mean_squared_error: 1.3714 - val_loss: 2.3300 - val_mean_squared_error: 2.3300\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.4116 - mean_squared_error: 1.4116 - val_loss: 2.5236 - val_mean_squared_error: 2.5236\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.3879 - mean_squared_error: 1.3879 - val_loss: 3.1167 - val_mean_squared_error: 3.1167\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.4988 - mean_squared_error: 1.4988 - val_loss: 2.2303 - val_mean_squared_error: 2.2303\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.2030 - mean_squared_error: 1.2030 - val_loss: 2.4486 - val_mean_squared_error: 2.4486\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.3012 - mean_squared_error: 1.3012 - val_loss: 2.2890 - val_mean_squared_error: 2.2890\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.2922 - mean_squared_error: 1.2922 - val_loss: 2.4395 - val_mean_squared_error: 2.4395\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.2031 - mean_squared_error: 1.2031 - val_loss: 2.4925 - val_mean_squared_error: 2.4925\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.4507 - mean_squared_error: 1.4507 - val_loss: 2.2449 - val_mean_squared_error: 2.2449\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 378us/sample - loss: 1.2812 - mean_squared_error: 1.2812 - val_loss: 2.4850 - val_mean_squared_error: 2.4850\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.2290 - mean_squared_error: 1.2290 - val_loss: 2.2379 - val_mean_squared_error: 2.2379\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.2805 - mean_squared_error: 1.2805 - val_loss: 2.6597 - val_mean_squared_error: 2.6597\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.1993 - mean_squared_error: 1.1993 - val_loss: 2.2184 - val_mean_squared_error: 2.2184\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.2152 - mean_squared_error: 1.2152 - val_loss: 2.2219 - val_mean_squared_error: 2.2219\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.1723 - mean_squared_error: 1.1723 - val_loss: 2.1882 - val_mean_squared_error: 2.1882\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.1156 - mean_squared_error: 1.1156 - val_loss: 2.2872 - val_mean_squared_error: 2.2872\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.1776 - mean_squared_error: 1.1776 - val_loss: 2.1816 - val_mean_squared_error: 2.1816\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.2312 - mean_squared_error: 1.2312 - val_loss: 2.2313 - val_mean_squared_error: 2.2313\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.0888 - mean_squared_error: 1.0888 - val_loss: 2.2382 - val_mean_squared_error: 2.2382\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1368 - mean_squared_error: 1.1368 - val_loss: 2.0726 - val_mean_squared_error: 2.0726\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2422 - mean_squared_error: 1.2422 - val_loss: 2.2001 - val_mean_squared_error: 2.2001\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.2733 - mean_squared_error: 1.2733 - val_loss: 2.0745 - val_mean_squared_error: 2.0745\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.1050 - mean_squared_error: 1.1050 - val_loss: 2.2787 - val_mean_squared_error: 2.2787\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.1230 - mean_squared_error: 1.1230 - val_loss: 2.1821 - val_mean_squared_error: 2.1821\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0704 - mean_squared_error: 1.0704 - val_loss: 2.6542 - val_mean_squared_error: 2.6542\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2849 - mean_squared_error: 1.2849 - val_loss: 2.1816 - val_mean_squared_error: 2.1816\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1985 - mean_squared_error: 1.1985 - val_loss: 2.4453 - val_mean_squared_error: 2.4453\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.1040 - mean_squared_error: 1.1040 - val_loss: 2.2107 - val_mean_squared_error: 2.2107\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0418 - mean_squared_error: 1.0418 - val_loss: 1.9651 - val_mean_squared_error: 1.9651\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.0862 - mean_squared_error: 1.0862 - val_loss: 2.1148 - val_mean_squared_error: 2.1148\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 387us/sample - loss: 1.1372 - mean_squared_error: 1.1372 - val_loss: 2.3680 - val_mean_squared_error: 2.3680\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0511 - mean_squared_error: 1.0511 - val_loss: 2.1998 - val_mean_squared_error: 2.1998\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.9699 - mean_squared_error: 0.9699 - val_loss: 1.9581 - val_mean_squared_error: 1.9581\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.0293 - mean_squared_error: 1.0293 - val_loss: 2.0708 - val_mean_squared_error: 2.0708\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8700 - mean_squared_error: 0.8700 - val_loss: 2.1299 - val_mean_squared_error: 2.1299\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.9779 - mean_squared_error: 0.9779 - val_loss: 2.3039 - val_mean_squared_error: 2.3039\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0488 - mean_squared_error: 1.0488 - val_loss: 2.1797 - val_mean_squared_error: 2.1797\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9997 - mean_squared_error: 0.9997 - val_loss: 2.2536 - val_mean_squared_error: 2.2536\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.1242 - mean_squared_error: 1.1242 - val_loss: 2.2580 - val_mean_squared_error: 2.2580\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 393us/sample - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 2.1507 - val_mean_squared_error: 2.1507\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 0.9204 - mean_squared_error: 0.9204 - val_loss: 1.9990 - val_mean_squared_error: 1.9990\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 381us/sample - loss: 1.1138 - mean_squared_error: 1.1138 - val_loss: 2.4531 - val_mean_squared_error: 2.4531\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 380us/sample - loss: 1.0591 - mean_squared_error: 1.0591 - val_loss: 1.9582 - val_mean_squared_error: 1.9582\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 390us/sample - loss: 1.1667 - mean_squared_error: 1.1667 - val_loss: 2.5621 - val_mean_squared_error: 2.5621\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.1700 - mean_squared_error: 1.1700 - val_loss: 2.2410 - val_mean_squared_error: 2.2410\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.1350 - mean_squared_error: 1.1350 - val_loss: 2.2178 - val_mean_squared_error: 2.2178\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 0.9873 - mean_squared_error: 0.9873 - val_loss: 2.1039 - val_mean_squared_error: 2.1039\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.9166 - mean_squared_error: 0.9166 - val_loss: 2.2730 - val_mean_squared_error: 2.2730\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.0226 - mean_squared_error: 1.0226 - val_loss: 1.9791 - val_mean_squared_error: 1.9791\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 384us/sample - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 1.9697 - val_mean_squared_error: 1.9697\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 0.8879 - mean_squared_error: 0.8879 - val_loss: 2.3632 - val_mean_squared_error: 2.3632\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 1.0314 - mean_squared_error: 1.0314 - val_loss: 2.1536 - val_mean_squared_error: 2.1536\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 385us/sample - loss: 0.8438 - mean_squared_error: 0.8438 - val_loss: 2.1338 - val_mean_squared_error: 2.1338\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.9538 - mean_squared_error: 0.9538 - val_loss: 2.0675 - val_mean_squared_error: 2.0675\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.0188 - mean_squared_error: 1.0188 - val_loss: 2.3784 - val_mean_squared_error: 2.3784\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.9702 - mean_squared_error: 0.9702 - val_loss: 2.0329 - val_mean_squared_error: 2.0329\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.8893 - mean_squared_error: 0.8893 - val_loss: 1.9328 - val_mean_squared_error: 1.9328\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.9034 - mean_squared_error: 0.9034 - val_loss: 1.9747 - val_mean_squared_error: 1.9747\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8704 - mean_squared_error: 0.8704 - val_loss: 1.9527 - val_mean_squared_error: 1.9527\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8064 - mean_squared_error: 0.8064 - val_loss: 2.1770 - val_mean_squared_error: 2.1770\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0075 - mean_squared_error: 1.0075 - val_loss: 2.2321 - val_mean_squared_error: 2.2321\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9028 - mean_squared_error: 0.9028 - val_loss: 2.2342 - val_mean_squared_error: 2.2342\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 2.2147 - val_mean_squared_error: 2.2147\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.0268 - mean_squared_error: 1.0268 - val_loss: 2.2305 - val_mean_squared_error: 2.2305\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.8919 - mean_squared_error: 0.8919 - val_loss: 2.2604 - val_mean_squared_error: 2.2604\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9118 - mean_squared_error: 0.9118 - val_loss: 2.0153 - val_mean_squared_error: 2.0153\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.9123 - mean_squared_error: 0.9123 - val_loss: 2.3711 - val_mean_squared_error: 2.3711\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0287 - mean_squared_error: 1.0287 - val_loss: 2.4483 - val_mean_squared_error: 2.4483\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.8870 - mean_squared_error: 0.8870 - val_loss: 2.1776 - val_mean_squared_error: 2.1776\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.8777 - mean_squared_error: 0.8777 - val_loss: 2.0088 - val_mean_squared_error: 2.0088\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.8244 - mean_squared_error: 0.8244 - val_loss: 2.2370 - val_mean_squared_error: 2.2370\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.9402 - mean_squared_error: 0.9402 - val_loss: 2.0504 - val_mean_squared_error: 2.0504\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 0.8858 - mean_squared_error: 0.8858 - val_loss: 1.9646 - val_mean_squared_error: 1.9646\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.8285 - mean_squared_error: 0.8285 - val_loss: 2.0114 - val_mean_squared_error: 2.0114\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.9137 - mean_squared_error: 0.9137 - val_loss: 2.4070 - val_mean_squared_error: 2.4070\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.8648 - mean_squared_error: 0.8648 - val_loss: 2.2980 - val_mean_squared_error: 2.2980\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.7853 - mean_squared_error: 0.7853 - val_loss: 2.0156 - val_mean_squared_error: 2.0156\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8987 - mean_squared_error: 0.8987 - val_loss: 2.1708 - val_mean_squared_error: 2.1708\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.8328 - mean_squared_error: 0.8328 - val_loss: 2.0002 - val_mean_squared_error: 2.0002\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8220 - mean_squared_error: 0.8220 - val_loss: 1.9817 - val_mean_squared_error: 1.9817\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 0.8702 - mean_squared_error: 0.8702 - val_loss: 2.2412 - val_mean_squared_error: 2.2412\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.8469 - mean_squared_error: 0.8469 - val_loss: 1.9596 - val_mean_squared_error: 1.9596\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.7713 - mean_squared_error: 0.7713 - val_loss: 2.0808 - val_mean_squared_error: 2.0808\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9653 - mean_squared_error: 0.9653 - val_loss: 2.1548 - val_mean_squared_error: 2.1548\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.8994 - mean_squared_error: 0.8994 - val_loss: 1.9598 - val_mean_squared_error: 1.9598\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8961 - mean_squared_error: 0.8961 - val_loss: 1.9136 - val_mean_squared_error: 1.9136\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.9077 - mean_squared_error: 0.9077 - val_loss: 1.9929 - val_mean_squared_error: 1.9929\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8405 - mean_squared_error: 0.8405 - val_loss: 2.2810 - val_mean_squared_error: 2.2810\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.7825 - mean_squared_error: 0.7825 - val_loss: 2.1755 - val_mean_squared_error: 2.1755\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9636 - mean_squared_error: 0.9636 - val_loss: 1.9043 - val_mean_squared_error: 1.9043\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 2.1135 - val_mean_squared_error: 2.1135\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7479 - mean_squared_error: 0.7479 - val_loss: 1.9204 - val_mean_squared_error: 1.9204\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 0.7815 - mean_squared_error: 0.7815 - val_loss: 2.0875 - val_mean_squared_error: 2.0875\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.8370 - mean_squared_error: 0.8370 - val_loss: 2.1161 - val_mean_squared_error: 2.1161\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8889 - mean_squared_error: 0.8889 - val_loss: 2.1548 - val_mean_squared_error: 2.1548\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 0.7852 - mean_squared_error: 0.7852 - val_loss: 1.9479 - val_mean_squared_error: 1.9479\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8600 - mean_squared_error: 0.8600 - val_loss: 1.9711 - val_mean_squared_error: 1.9711\n",
            "==================================================\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_60 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_100 (Bat (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_60 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_80 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_61 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_101 (Bat (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_61 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_81 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_62 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_102 (Bat (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_62 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_82 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_20 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_103 (Bat (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_83 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_104 (Bat (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_62 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 434.5872 - mean_squared_error: 434.5872 - val_loss: 155.4282 - val_mean_squared_error: 155.4282\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 21.9364 - mean_squared_error: 21.9364 - val_loss: 137.7350 - val_mean_squared_error: 137.7350\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 19.9353 - mean_squared_error: 19.9353 - val_loss: 72.5046 - val_mean_squared_error: 72.5046\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 19.3327 - mean_squared_error: 19.3327 - val_loss: 39.3669 - val_mean_squared_error: 39.3669\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 21.3071 - mean_squared_error: 21.3071 - val_loss: 35.5651 - val_mean_squared_error: 35.5651\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 13.8328 - mean_squared_error: 13.8328 - val_loss: 35.0230 - val_mean_squared_error: 35.0230\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 14.4416 - mean_squared_error: 14.4416 - val_loss: 14.1730 - val_mean_squared_error: 14.1730\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 13.0539 - mean_squared_error: 13.0539 - val_loss: 12.8951 - val_mean_squared_error: 12.8951\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 13.2892 - mean_squared_error: 13.2892 - val_loss: 10.5067 - val_mean_squared_error: 10.5068\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 11.8025 - mean_squared_error: 11.8025 - val_loss: 11.7583 - val_mean_squared_error: 11.7583\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 11.7017 - mean_squared_error: 11.7017 - val_loss: 10.4534 - val_mean_squared_error: 10.4534\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 12.1203 - mean_squared_error: 12.1203 - val_loss: 9.6361 - val_mean_squared_error: 9.6361\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 10.0101 - mean_squared_error: 10.0101 - val_loss: 8.9598 - val_mean_squared_error: 8.9598\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 9.4068 - mean_squared_error: 9.4068 - val_loss: 8.8583 - val_mean_squared_error: 8.8583\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 9.1770 - mean_squared_error: 9.1770 - val_loss: 10.5378 - val_mean_squared_error: 10.5378\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 8.8956 - mean_squared_error: 8.8956 - val_loss: 7.8999 - val_mean_squared_error: 7.8999\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 7.7453 - mean_squared_error: 7.7453 - val_loss: 10.1128 - val_mean_squared_error: 10.1128\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 7.6154 - mean_squared_error: 7.6154 - val_loss: 8.0096 - val_mean_squared_error: 8.0096\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 7.8546 - mean_squared_error: 7.8546 - val_loss: 6.5890 - val_mean_squared_error: 6.5890\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 6.7369 - mean_squared_error: 6.7369 - val_loss: 7.1536 - val_mean_squared_error: 7.1536\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 5.9822 - mean_squared_error: 5.9822 - val_loss: 6.3987 - val_mean_squared_error: 6.3987\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 6.1123 - mean_squared_error: 6.1123 - val_loss: 5.2549 - val_mean_squared_error: 5.2549\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 5.7408 - mean_squared_error: 5.7408 - val_loss: 5.6197 - val_mean_squared_error: 5.6197\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 5.2223 - mean_squared_error: 5.2223 - val_loss: 6.2201 - val_mean_squared_error: 6.2201\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 4.8635 - mean_squared_error: 4.8635 - val_loss: 4.4686 - val_mean_squared_error: 4.4686\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 358us/sample - loss: 4.6578 - mean_squared_error: 4.6578 - val_loss: 5.3812 - val_mean_squared_error: 5.3812\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 4.4877 - mean_squared_error: 4.4877 - val_loss: 5.7714 - val_mean_squared_error: 5.7714\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 4.2148 - mean_squared_error: 4.2148 - val_loss: 4.0059 - val_mean_squared_error: 4.0059\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 4.2785 - mean_squared_error: 4.2785 - val_loss: 4.2265 - val_mean_squared_error: 4.2265\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.9831 - mean_squared_error: 3.9831 - val_loss: 5.1458 - val_mean_squared_error: 5.1458\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 4.1138 - mean_squared_error: 4.1138 - val_loss: 4.2280 - val_mean_squared_error: 4.2280\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 3.8976 - mean_squared_error: 3.8976 - val_loss: 3.5897 - val_mean_squared_error: 3.5897\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.6373 - mean_squared_error: 3.6373 - val_loss: 3.4507 - val_mean_squared_error: 3.4507\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 3.4846 - mean_squared_error: 3.4846 - val_loss: 4.4398 - val_mean_squared_error: 4.4398\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 3.5614 - mean_squared_error: 3.5614 - val_loss: 3.4444 - val_mean_squared_error: 3.4444\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 3.5186 - mean_squared_error: 3.5186 - val_loss: 3.1938 - val_mean_squared_error: 3.1938\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 3.5497 - mean_squared_error: 3.5497 - val_loss: 4.4018 - val_mean_squared_error: 4.4018\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 3.3185 - mean_squared_error: 3.3185 - val_loss: 3.5503 - val_mean_squared_error: 3.5503\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 3.1255 - mean_squared_error: 3.1255 - val_loss: 4.0442 - val_mean_squared_error: 4.0442\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.9737 - mean_squared_error: 2.9737 - val_loss: 3.6310 - val_mean_squared_error: 3.6310\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 3.0693 - mean_squared_error: 3.0693 - val_loss: 3.2468 - val_mean_squared_error: 3.2468\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.9542 - mean_squared_error: 2.9542 - val_loss: 3.0880 - val_mean_squared_error: 3.0880\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 2.8049 - mean_squared_error: 2.8049 - val_loss: 3.5473 - val_mean_squared_error: 3.5473\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.6493 - mean_squared_error: 2.6493 - val_loss: 3.1291 - val_mean_squared_error: 3.1291\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.7417 - mean_squared_error: 2.7417 - val_loss: 2.7860 - val_mean_squared_error: 2.7860\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.6465 - mean_squared_error: 2.6465 - val_loss: 3.2320 - val_mean_squared_error: 3.2320\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 2.6953 - mean_squared_error: 2.6953 - val_loss: 3.3308 - val_mean_squared_error: 3.3308\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.6147 - mean_squared_error: 2.6147 - val_loss: 3.0830 - val_mean_squared_error: 3.0830\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.6176 - mean_squared_error: 2.6176 - val_loss: 2.7876 - val_mean_squared_error: 2.7876\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.6865 - mean_squared_error: 2.6865 - val_loss: 2.8747 - val_mean_squared_error: 2.8747\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.4482 - mean_squared_error: 2.4482 - val_loss: 3.0898 - val_mean_squared_error: 3.0898\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 2.5204 - mean_squared_error: 2.5204 - val_loss: 3.0433 - val_mean_squared_error: 3.0433\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.4757 - mean_squared_error: 2.4757 - val_loss: 2.9001 - val_mean_squared_error: 2.9001\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 2.3879 - mean_squared_error: 2.3879 - val_loss: 2.6680 - val_mean_squared_error: 2.6680\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.4322 - mean_squared_error: 2.4322 - val_loss: 3.9038 - val_mean_squared_error: 3.9039\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 2.4150 - mean_squared_error: 2.4150 - val_loss: 3.0664 - val_mean_squared_error: 3.0664\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 357us/sample - loss: 2.3407 - mean_squared_error: 2.3407 - val_loss: 2.9161 - val_mean_squared_error: 2.9161\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 2.3655 - mean_squared_error: 2.3655 - val_loss: 3.4004 - val_mean_squared_error: 3.4004\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.0986 - mean_squared_error: 2.0986 - val_loss: 2.7760 - val_mean_squared_error: 2.7760\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 2.2515 - mean_squared_error: 2.2515 - val_loss: 2.7088 - val_mean_squared_error: 2.7088\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 2.0459 - mean_squared_error: 2.0459 - val_loss: 2.7441 - val_mean_squared_error: 2.7441\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.0735 - mean_squared_error: 2.0735 - val_loss: 2.7865 - val_mean_squared_error: 2.7865\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 2.1954 - mean_squared_error: 2.1954 - val_loss: 2.7692 - val_mean_squared_error: 2.7692\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 2.0901 - mean_squared_error: 2.0901 - val_loss: 2.5707 - val_mean_squared_error: 2.5707\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 2.0014 - mean_squared_error: 2.0014 - val_loss: 2.6938 - val_mean_squared_error: 2.6938\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 2.0705 - mean_squared_error: 2.0705 - val_loss: 2.9593 - val_mean_squared_error: 2.9593\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.9658 - mean_squared_error: 1.9658 - val_loss: 2.5505 - val_mean_squared_error: 2.5505\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.9122 - mean_squared_error: 1.9122 - val_loss: 2.6709 - val_mean_squared_error: 2.6709\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.7726 - mean_squared_error: 1.7726 - val_loss: 2.4264 - val_mean_squared_error: 2.4264\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.9370 - mean_squared_error: 1.9370 - val_loss: 2.5171 - val_mean_squared_error: 2.5171\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.7608 - mean_squared_error: 1.7608 - val_loss: 2.4133 - val_mean_squared_error: 2.4133\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.9518 - mean_squared_error: 1.9518 - val_loss: 2.7262 - val_mean_squared_error: 2.7262\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.9564 - mean_squared_error: 1.9564 - val_loss: 3.5060 - val_mean_squared_error: 3.5060\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 2.0039 - mean_squared_error: 2.0039 - val_loss: 2.7842 - val_mean_squared_error: 2.7842\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.8509 - mean_squared_error: 1.8509 - val_loss: 2.3729 - val_mean_squared_error: 2.3729\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.7537 - mean_squared_error: 1.7537 - val_loss: 2.4811 - val_mean_squared_error: 2.4811\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.7879 - mean_squared_error: 1.7879 - val_loss: 2.4117 - val_mean_squared_error: 2.4117\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.6894 - mean_squared_error: 1.6894 - val_loss: 2.3036 - val_mean_squared_error: 2.3036\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7063 - mean_squared_error: 1.7063 - val_loss: 2.4750 - val_mean_squared_error: 2.4750\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6163 - mean_squared_error: 1.6163 - val_loss: 2.3922 - val_mean_squared_error: 2.3922\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.6341 - mean_squared_error: 1.6341 - val_loss: 2.8513 - val_mean_squared_error: 2.8513\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.7469 - mean_squared_error: 1.7469 - val_loss: 2.7258 - val_mean_squared_error: 2.7258\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.8312 - mean_squared_error: 1.8312 - val_loss: 2.6049 - val_mean_squared_error: 2.6049\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.7276 - mean_squared_error: 1.7276 - val_loss: 2.5769 - val_mean_squared_error: 2.5769\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.5717 - mean_squared_error: 1.5717 - val_loss: 2.6970 - val_mean_squared_error: 2.6970\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.7738 - mean_squared_error: 1.7738 - val_loss: 3.0548 - val_mean_squared_error: 3.0548\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.6817 - mean_squared_error: 1.6817 - val_loss: 2.3990 - val_mean_squared_error: 2.3990\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.6338 - mean_squared_error: 1.6338 - val_loss: 2.7463 - val_mean_squared_error: 2.7463\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.6488 - mean_squared_error: 1.6488 - val_loss: 2.4876 - val_mean_squared_error: 2.4876\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.7226 - mean_squared_error: 1.7226 - val_loss: 2.8844 - val_mean_squared_error: 2.8844\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.6248 - mean_squared_error: 1.6248 - val_loss: 2.6661 - val_mean_squared_error: 2.6661\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.5504 - mean_squared_error: 1.5504 - val_loss: 2.1839 - val_mean_squared_error: 2.1839\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 1.5007 - mean_squared_error: 1.5007 - val_loss: 2.5135 - val_mean_squared_error: 2.5135\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.6562 - mean_squared_error: 1.6562 - val_loss: 2.0781 - val_mean_squared_error: 2.0781\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.5526 - mean_squared_error: 1.5526 - val_loss: 2.2705 - val_mean_squared_error: 2.2705\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.5397 - mean_squared_error: 1.5397 - val_loss: 2.1859 - val_mean_squared_error: 2.1859\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.4487 - mean_squared_error: 1.4487 - val_loss: 2.6510 - val_mean_squared_error: 2.6510\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.4984 - mean_squared_error: 1.4984 - val_loss: 2.2123 - val_mean_squared_error: 2.2123\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.4339 - mean_squared_error: 1.4339 - val_loss: 2.5116 - val_mean_squared_error: 2.5116\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.4135 - mean_squared_error: 1.4135 - val_loss: 2.4382 - val_mean_squared_error: 2.4382\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.3738 - mean_squared_error: 1.3738 - val_loss: 2.1617 - val_mean_squared_error: 2.1617\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 1.3231 - mean_squared_error: 1.3231 - val_loss: 2.2347 - val_mean_squared_error: 2.2347\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.4309 - mean_squared_error: 1.4309 - val_loss: 2.4155 - val_mean_squared_error: 2.4155\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.3073 - mean_squared_error: 1.3073 - val_loss: 2.2290 - val_mean_squared_error: 2.2290\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.5492 - mean_squared_error: 1.5492 - val_loss: 2.2101 - val_mean_squared_error: 2.2101\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.5553 - mean_squared_error: 1.5553 - val_loss: 2.4382 - val_mean_squared_error: 2.4382\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2990 - mean_squared_error: 1.2990 - val_loss: 2.3674 - val_mean_squared_error: 2.3674\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.4890 - mean_squared_error: 1.4890 - val_loss: 2.0886 - val_mean_squared_error: 2.0886\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2432 - mean_squared_error: 1.2432 - val_loss: 2.0731 - val_mean_squared_error: 2.0731\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2121 - mean_squared_error: 1.2121 - val_loss: 2.0970 - val_mean_squared_error: 2.0970\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.3086 - mean_squared_error: 1.3086 - val_loss: 2.1430 - val_mean_squared_error: 2.1430\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2405 - mean_squared_error: 1.2405 - val_loss: 2.3338 - val_mean_squared_error: 2.3338\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.2569 - mean_squared_error: 1.2569 - val_loss: 2.6190 - val_mean_squared_error: 2.6190\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.4052 - mean_squared_error: 1.4052 - val_loss: 2.3346 - val_mean_squared_error: 2.3346\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.3051 - mean_squared_error: 1.3051 - val_loss: 2.2047 - val_mean_squared_error: 2.2047\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.3941 - mean_squared_error: 1.3941 - val_loss: 2.3710 - val_mean_squared_error: 2.3710\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.3398 - mean_squared_error: 1.3398 - val_loss: 2.2236 - val_mean_squared_error: 2.2236\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 1.3316 - mean_squared_error: 1.3316 - val_loss: 2.3353 - val_mean_squared_error: 2.3353\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3196 - mean_squared_error: 1.3196 - val_loss: 2.0881 - val_mean_squared_error: 2.0881\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1876 - mean_squared_error: 1.1876 - val_loss: 1.9427 - val_mean_squared_error: 1.9427\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.1887 - mean_squared_error: 1.1887 - val_loss: 2.3655 - val_mean_squared_error: 2.3655\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1981 - mean_squared_error: 1.1981 - val_loss: 2.2501 - val_mean_squared_error: 2.2501\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 379us/sample - loss: 1.2577 - mean_squared_error: 1.2577 - val_loss: 2.1970 - val_mean_squared_error: 2.1970\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 1.1821 - mean_squared_error: 1.1821 - val_loss: 2.2366 - val_mean_squared_error: 2.2366\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1863 - mean_squared_error: 1.1863 - val_loss: 2.1768 - val_mean_squared_error: 2.1768\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1824 - mean_squared_error: 1.1824 - val_loss: 2.1951 - val_mean_squared_error: 2.1951\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.2608 - mean_squared_error: 1.2608 - val_loss: 2.5986 - val_mean_squared_error: 2.5986\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2521 - mean_squared_error: 1.2521 - val_loss: 2.2589 - val_mean_squared_error: 2.2589\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.1073 - mean_squared_error: 1.1073 - val_loss: 2.0868 - val_mean_squared_error: 2.0868\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.1105 - mean_squared_error: 1.1105 - val_loss: 2.3951 - val_mean_squared_error: 2.3951\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 1.0395 - mean_squared_error: 1.0395 - val_loss: 2.2324 - val_mean_squared_error: 2.2324\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.0701 - mean_squared_error: 1.0701 - val_loss: 1.9771 - val_mean_squared_error: 1.9771\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.0862 - mean_squared_error: 1.0862 - val_loss: 2.3613 - val_mean_squared_error: 2.3613\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.2991 - mean_squared_error: 1.2991 - val_loss: 2.2021 - val_mean_squared_error: 2.2021\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.2605 - mean_squared_error: 1.2605 - val_loss: 2.1777 - val_mean_squared_error: 2.1777\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.1996 - mean_squared_error: 1.1996 - val_loss: 1.9763 - val_mean_squared_error: 1.9763\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 1.1852 - mean_squared_error: 1.1852 - val_loss: 2.0522 - val_mean_squared_error: 2.0522\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 1.2108 - mean_squared_error: 1.2108 - val_loss: 2.0079 - val_mean_squared_error: 2.0079\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.1044 - mean_squared_error: 1.1044 - val_loss: 2.6387 - val_mean_squared_error: 2.6387\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.3096 - mean_squared_error: 1.3096 - val_loss: 2.1446 - val_mean_squared_error: 2.1446\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.1147 - mean_squared_error: 1.1147 - val_loss: 2.0674 - val_mean_squared_error: 2.0674\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0550 - mean_squared_error: 1.0550 - val_loss: 2.0669 - val_mean_squared_error: 2.0669\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 1.0552 - mean_squared_error: 1.0552 - val_loss: 2.1682 - val_mean_squared_error: 2.1682\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9663 - mean_squared_error: 0.9663 - val_loss: 1.8727 - val_mean_squared_error: 1.8727\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 362us/sample - loss: 1.0017 - mean_squared_error: 1.0017 - val_loss: 2.2134 - val_mean_squared_error: 2.2134\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.0003 - mean_squared_error: 1.0003 - val_loss: 2.2407 - val_mean_squared_error: 2.2407\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.1245 - mean_squared_error: 1.1245 - val_loss: 2.3113 - val_mean_squared_error: 2.3113\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 1.0798 - mean_squared_error: 1.0798 - val_loss: 2.0999 - val_mean_squared_error: 2.0999\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9733 - mean_squared_error: 0.9733 - val_loss: 1.9330 - val_mean_squared_error: 1.9330\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 1.0136 - mean_squared_error: 1.0136 - val_loss: 2.2879 - val_mean_squared_error: 2.2879\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.9600 - mean_squared_error: 0.9600 - val_loss: 2.2402 - val_mean_squared_error: 2.2402\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 0.9941 - mean_squared_error: 0.9941 - val_loss: 2.2128 - val_mean_squared_error: 2.2128\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9862 - mean_squared_error: 0.9862 - val_loss: 2.1124 - val_mean_squared_error: 2.1124\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.2063 - mean_squared_error: 1.2063 - val_loss: 2.0755 - val_mean_squared_error: 2.0755\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9315 - mean_squared_error: 0.9315 - val_loss: 2.0733 - val_mean_squared_error: 2.0733\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.9786 - mean_squared_error: 0.9786 - val_loss: 2.2865 - val_mean_squared_error: 2.2865\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.0705 - mean_squared_error: 1.0705 - val_loss: 2.0851 - val_mean_squared_error: 2.0851\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9685 - mean_squared_error: 0.9685 - val_loss: 2.0335 - val_mean_squared_error: 2.0335\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.8827 - mean_squared_error: 0.8827 - val_loss: 1.9128 - val_mean_squared_error: 1.9128\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 361us/sample - loss: 1.0260 - mean_squared_error: 1.0260 - val_loss: 2.1086 - val_mean_squared_error: 2.1086\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.8621 - mean_squared_error: 0.8621 - val_loss: 1.9814 - val_mean_squared_error: 1.9814\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.9427 - mean_squared_error: 0.9427 - val_loss: 2.0451 - val_mean_squared_error: 2.0451\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 367us/sample - loss: 0.9964 - mean_squared_error: 0.9964 - val_loss: 2.2534 - val_mean_squared_error: 2.2534\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0407 - mean_squared_error: 1.0407 - val_loss: 2.2777 - val_mean_squared_error: 2.2777\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 2.1738 - val_mean_squared_error: 2.1738\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0265 - mean_squared_error: 1.0265 - val_loss: 2.0932 - val_mean_squared_error: 2.0932\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.9607 - mean_squared_error: 0.9607 - val_loss: 2.0359 - val_mean_squared_error: 2.0359\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.9149 - mean_squared_error: 0.9149 - val_loss: 2.0026 - val_mean_squared_error: 2.0026\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.9984 - mean_squared_error: 0.9984 - val_loss: 2.1271 - val_mean_squared_error: 2.1271\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.9189 - mean_squared_error: 0.9189 - val_loss: 1.9008 - val_mean_squared_error: 1.9008\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.9301 - mean_squared_error: 0.9301 - val_loss: 1.9584 - val_mean_squared_error: 1.9584\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 386us/sample - loss: 0.8751 - mean_squared_error: 0.8751 - val_loss: 2.3009 - val_mean_squared_error: 2.3009\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 373us/sample - loss: 1.0570 - mean_squared_error: 1.0570 - val_loss: 2.0713 - val_mean_squared_error: 2.0713\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9547 - mean_squared_error: 0.9547 - val_loss: 2.0468 - val_mean_squared_error: 2.0468\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8819 - mean_squared_error: 0.8819 - val_loss: 2.0435 - val_mean_squared_error: 2.0435\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 0.9473 - mean_squared_error: 0.9473 - val_loss: 2.1747 - val_mean_squared_error: 2.1747\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 363us/sample - loss: 0.8880 - mean_squared_error: 0.8880 - val_loss: 2.0068 - val_mean_squared_error: 2.0068\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 1.0223 - mean_squared_error: 1.0223 - val_loss: 2.4768 - val_mean_squared_error: 2.4768\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.9740 - mean_squared_error: 0.9740 - val_loss: 2.1267 - val_mean_squared_error: 2.1267\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 376us/sample - loss: 0.9069 - mean_squared_error: 0.9069 - val_loss: 2.0401 - val_mean_squared_error: 2.0401\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8396 - mean_squared_error: 0.8396 - val_loss: 2.0219 - val_mean_squared_error: 2.0219\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 371us/sample - loss: 0.8969 - mean_squared_error: 0.8969 - val_loss: 2.1786 - val_mean_squared_error: 2.1786\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 365us/sample - loss: 1.0035 - mean_squared_error: 1.0035 - val_loss: 2.1757 - val_mean_squared_error: 2.1757\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 375us/sample - loss: 0.8584 - mean_squared_error: 0.8584 - val_loss: 2.1698 - val_mean_squared_error: 2.1698\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 372us/sample - loss: 0.8908 - mean_squared_error: 0.8908 - val_loss: 2.7449 - val_mean_squared_error: 2.7449\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.9143 - mean_squared_error: 0.9143 - val_loss: 2.1799 - val_mean_squared_error: 2.1799\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8164 - mean_squared_error: 0.8164 - val_loss: 2.2452 - val_mean_squared_error: 2.2452\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8787 - mean_squared_error: 0.8787 - val_loss: 1.9836 - val_mean_squared_error: 1.9836\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.8571 - mean_squared_error: 0.8571 - val_loss: 2.0335 - val_mean_squared_error: 2.0335\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 368us/sample - loss: 1.0387 - mean_squared_error: 1.0387 - val_loss: 2.0702 - val_mean_squared_error: 2.0702\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 374us/sample - loss: 0.8133 - mean_squared_error: 0.8133 - val_loss: 2.1649 - val_mean_squared_error: 2.1649\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 364us/sample - loss: 0.8213 - mean_squared_error: 0.8213 - val_loss: 2.1420 - val_mean_squared_error: 2.1420\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 370us/sample - loss: 0.8568 - mean_squared_error: 0.8568 - val_loss: 2.1754 - val_mean_squared_error: 2.1754\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 377us/sample - loss: 0.8018 - mean_squared_error: 0.8018 - val_loss: 2.0211 - val_mean_squared_error: 2.0211\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 359us/sample - loss: 0.7538 - mean_squared_error: 0.7538 - val_loss: 2.0559 - val_mean_squared_error: 2.0559\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7710 - mean_squared_error: 0.7710 - val_loss: 2.1141 - val_mean_squared_error: 2.1141\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8615 - mean_squared_error: 0.8615 - val_loss: 2.0452 - val_mean_squared_error: 2.0452\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 360us/sample - loss: 0.8626 - mean_squared_error: 0.8626 - val_loss: 2.0608 - val_mean_squared_error: 2.0608\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 369us/sample - loss: 0.8264 - mean_squared_error: 0.8264 - val_loss: 1.8905 - val_mean_squared_error: 1.8905\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 366us/sample - loss: 0.7948 - mean_squared_error: 0.7948 - val_loss: 2.0180 - val_mean_squared_error: 2.0180\n",
            "==================================================\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_63 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_105 (Bat (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_63 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_84 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_64 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_106 (Bat (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_64 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_85 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_65 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_107 (Bat (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_65 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_86 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_21 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_108 (Bat (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_87 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_109 (Bat (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_64 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_65 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 435.7556 - mean_squared_error: 435.7555 - val_loss: 674.6690 - val_mean_squared_error: 674.6690\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 439us/sample - loss: 26.5904 - mean_squared_error: 26.5904 - val_loss: 204.3783 - val_mean_squared_error: 204.3783\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 19.1073 - mean_squared_error: 19.1073 - val_loss: 95.7703 - val_mean_squared_error: 95.7703\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 17.9118 - mean_squared_error: 17.9118 - val_loss: 39.8561 - val_mean_squared_error: 39.8561\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 16.8827 - mean_squared_error: 16.8827 - val_loss: 25.7815 - val_mean_squared_error: 25.7815\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 15.5188 - mean_squared_error: 15.5188 - val_loss: 27.9373 - val_mean_squared_error: 27.9373\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 13.3579 - mean_squared_error: 13.3579 - val_loss: 14.9442 - val_mean_squared_error: 14.9442\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 13.5001 - mean_squared_error: 13.5001 - val_loss: 16.9691 - val_mean_squared_error: 16.9691\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 14.3462 - mean_squared_error: 14.3462 - val_loss: 18.7997 - val_mean_squared_error: 18.7997\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 14.6973 - mean_squared_error: 14.6973 - val_loss: 14.3484 - val_mean_squared_error: 14.3484\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 11.8980 - mean_squared_error: 11.8980 - val_loss: 10.3229 - val_mean_squared_error: 10.3229\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 12.0574 - mean_squared_error: 12.0574 - val_loss: 11.7732 - val_mean_squared_error: 11.7732\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 11.3080 - mean_squared_error: 11.3080 - val_loss: 9.8323 - val_mean_squared_error: 9.8323\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 10.5594 - mean_squared_error: 10.5594 - val_loss: 9.7571 - val_mean_squared_error: 9.7571\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 8.8900 - mean_squared_error: 8.8900 - val_loss: 9.7912 - val_mean_squared_error: 9.7912\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 8.6572 - mean_squared_error: 8.6572 - val_loss: 8.6413 - val_mean_squared_error: 8.6413\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 8.6855 - mean_squared_error: 8.6855 - val_loss: 7.9110 - val_mean_squared_error: 7.9110\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 7.4352 - mean_squared_error: 7.4352 - val_loss: 7.9442 - val_mean_squared_error: 7.9442\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 6.8161 - mean_squared_error: 6.8161 - val_loss: 7.6068 - val_mean_squared_error: 7.6068\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 6.8647 - mean_squared_error: 6.8647 - val_loss: 6.4209 - val_mean_squared_error: 6.4209\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 394us/sample - loss: 6.3429 - mean_squared_error: 6.3429 - val_loss: 5.8201 - val_mean_squared_error: 5.8201\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 5.2819 - mean_squared_error: 5.2819 - val_loss: 6.1824 - val_mean_squared_error: 6.1824\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 4.9959 - mean_squared_error: 4.9959 - val_loss: 5.0843 - val_mean_squared_error: 5.0843\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 4.9905 - mean_squared_error: 4.9905 - val_loss: 5.2240 - val_mean_squared_error: 5.2240\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 4.8624 - mean_squared_error: 4.8624 - val_loss: 5.5956 - val_mean_squared_error: 5.5956\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 4.4330 - mean_squared_error: 4.4330 - val_loss: 4.6978 - val_mean_squared_error: 4.6978\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 3.9655 - mean_squared_error: 3.9655 - val_loss: 5.0325 - val_mean_squared_error: 5.0325\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 389us/sample - loss: 3.6837 - mean_squared_error: 3.6837 - val_loss: 4.4543 - val_mean_squared_error: 4.4543\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 3.3944 - mean_squared_error: 3.3944 - val_loss: 3.8487 - val_mean_squared_error: 3.8487\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 3.3157 - mean_squared_error: 3.3157 - val_loss: 4.3652 - val_mean_squared_error: 4.3652\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 3.4040 - mean_squared_error: 3.4040 - val_loss: 5.1666 - val_mean_squared_error: 5.1666\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 391us/sample - loss: 3.2595 - mean_squared_error: 3.2595 - val_loss: 3.7293 - val_mean_squared_error: 3.7293\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.9664 - mean_squared_error: 2.9664 - val_loss: 3.6528 - val_mean_squared_error: 3.6528\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 3.0709 - mean_squared_error: 3.0709 - val_loss: 3.5480 - val_mean_squared_error: 3.5480\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.7163 - mean_squared_error: 2.7163 - val_loss: 3.7479 - val_mean_squared_error: 3.7479\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 2.8272 - mean_squared_error: 2.8272 - val_loss: 3.6607 - val_mean_squared_error: 3.6607\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.6091 - mean_squared_error: 2.6091 - val_loss: 3.7936 - val_mean_squared_error: 3.7936\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.6853 - mean_squared_error: 2.6853 - val_loss: 3.4707 - val_mean_squared_error: 3.4707\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.7513 - mean_squared_error: 2.7513 - val_loss: 4.4152 - val_mean_squared_error: 4.4152\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.5796 - mean_squared_error: 2.5796 - val_loss: 3.0548 - val_mean_squared_error: 3.0548\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 396us/sample - loss: 2.4735 - mean_squared_error: 2.4735 - val_loss: 3.4478 - val_mean_squared_error: 3.4478\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.5249 - mean_squared_error: 2.5249 - val_loss: 3.4580 - val_mean_squared_error: 3.4580\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.4469 - mean_squared_error: 2.4469 - val_loss: 3.6063 - val_mean_squared_error: 3.6063\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.2310 - mean_squared_error: 2.2310 - val_loss: 4.4772 - val_mean_squared_error: 4.4772\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.1487 - mean_squared_error: 2.1487 - val_loss: 3.4000 - val_mean_squared_error: 3.4000\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 2.1280 - mean_squared_error: 2.1280 - val_loss: 3.1678 - val_mean_squared_error: 3.1678\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 2.1403 - mean_squared_error: 2.1403 - val_loss: 3.3705 - val_mean_squared_error: 3.3705\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 2.0572 - mean_squared_error: 2.0572 - val_loss: 2.9468 - val_mean_squared_error: 2.9468\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 2.0762 - mean_squared_error: 2.0762 - val_loss: 3.7105 - val_mean_squared_error: 3.7105\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 2.0138 - mean_squared_error: 2.0138 - val_loss: 3.1421 - val_mean_squared_error: 3.1421\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 2.0929 - mean_squared_error: 2.0929 - val_loss: 3.7048 - val_mean_squared_error: 3.7048\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 2.0072 - mean_squared_error: 2.0072 - val_loss: 3.2157 - val_mean_squared_error: 3.2157\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 2.0184 - mean_squared_error: 2.0184 - val_loss: 3.3607 - val_mean_squared_error: 3.3607\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.7560 - mean_squared_error: 1.7560 - val_loss: 4.1636 - val_mean_squared_error: 4.1636\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 2.0833 - mean_squared_error: 2.0833 - val_loss: 3.2000 - val_mean_squared_error: 3.2000\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.8807 - mean_squared_error: 1.8807 - val_loss: 2.6727 - val_mean_squared_error: 2.6727\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.8125 - mean_squared_error: 1.8125 - val_loss: 2.7581 - val_mean_squared_error: 2.7581\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.8328 - mean_squared_error: 1.8328 - val_loss: 2.9527 - val_mean_squared_error: 2.9527\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.8211 - mean_squared_error: 1.8211 - val_loss: 2.8116 - val_mean_squared_error: 2.8116\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.7198 - mean_squared_error: 1.7198 - val_loss: 2.9306 - val_mean_squared_error: 2.9306\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.6428 - mean_squared_error: 1.6428 - val_loss: 2.9298 - val_mean_squared_error: 2.9298\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.5921 - mean_squared_error: 1.5921 - val_loss: 2.8755 - val_mean_squared_error: 2.8755\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 1.7720 - mean_squared_error: 1.7720 - val_loss: 2.8054 - val_mean_squared_error: 2.8054\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.8431 - mean_squared_error: 1.8431 - val_loss: 2.6573 - val_mean_squared_error: 2.6573\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.7207 - mean_squared_error: 1.7207 - val_loss: 2.8840 - val_mean_squared_error: 2.8840\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.6049 - mean_squared_error: 1.6049 - val_loss: 2.4149 - val_mean_squared_error: 2.4149\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.6197 - mean_squared_error: 1.6197 - val_loss: 3.1083 - val_mean_squared_error: 3.1083\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.6858 - mean_squared_error: 1.6858 - val_loss: 2.8697 - val_mean_squared_error: 2.8697\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.4687 - mean_squared_error: 1.4687 - val_loss: 2.8688 - val_mean_squared_error: 2.8688\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.5913 - mean_squared_error: 1.5913 - val_loss: 2.9017 - val_mean_squared_error: 2.9017\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.7102 - mean_squared_error: 1.7102 - val_loss: 2.5575 - val_mean_squared_error: 2.5575\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.5142 - mean_squared_error: 1.5142 - val_loss: 2.9989 - val_mean_squared_error: 2.9989\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.5247 - mean_squared_error: 1.5247 - val_loss: 2.4198 - val_mean_squared_error: 2.4198\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.3199 - mean_squared_error: 1.3199 - val_loss: 2.6846 - val_mean_squared_error: 2.6846\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.4017 - mean_squared_error: 1.4017 - val_loss: 2.8106 - val_mean_squared_error: 2.8106\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.4254 - mean_squared_error: 1.4254 - val_loss: 2.6860 - val_mean_squared_error: 2.6860\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.4742 - mean_squared_error: 1.4742 - val_loss: 2.6523 - val_mean_squared_error: 2.6523\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.3484 - mean_squared_error: 1.3484 - val_loss: 2.6842 - val_mean_squared_error: 2.6842\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.4781 - mean_squared_error: 1.4781 - val_loss: 3.5319 - val_mean_squared_error: 3.5319\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.5322 - mean_squared_error: 1.5322 - val_loss: 2.3910 - val_mean_squared_error: 2.3910\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.4503 - mean_squared_error: 1.4503 - val_loss: 3.0645 - val_mean_squared_error: 3.0645\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.4264 - mean_squared_error: 1.4264 - val_loss: 3.0044 - val_mean_squared_error: 3.0044\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 1.3597 - mean_squared_error: 1.3597 - val_loss: 2.4515 - val_mean_squared_error: 2.4515\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.3376 - mean_squared_error: 1.3376 - val_loss: 3.3169 - val_mean_squared_error: 3.3169\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 1.3922 - mean_squared_error: 1.3922 - val_loss: 2.7167 - val_mean_squared_error: 2.7167\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.2781 - mean_squared_error: 1.2781 - val_loss: 2.6139 - val_mean_squared_error: 2.6139\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.3505 - mean_squared_error: 1.3505 - val_loss: 2.5966 - val_mean_squared_error: 2.5966\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.3639 - mean_squared_error: 1.3639 - val_loss: 2.6802 - val_mean_squared_error: 2.6802\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.4347 - mean_squared_error: 1.4347 - val_loss: 2.6315 - val_mean_squared_error: 2.6315\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.3544 - mean_squared_error: 1.3544 - val_loss: 2.4947 - val_mean_squared_error: 2.4947\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.3093 - mean_squared_error: 1.3093 - val_loss: 2.8110 - val_mean_squared_error: 2.8110\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.4915 - mean_squared_error: 1.4915 - val_loss: 2.4658 - val_mean_squared_error: 2.4658\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.4327 - mean_squared_error: 1.4327 - val_loss: 2.3154 - val_mean_squared_error: 2.3154\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.2560 - mean_squared_error: 1.2560 - val_loss: 2.7920 - val_mean_squared_error: 2.7920\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1190 - mean_squared_error: 1.1190 - val_loss: 2.3222 - val_mean_squared_error: 2.3222\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.2032 - mean_squared_error: 1.2032 - val_loss: 2.6625 - val_mean_squared_error: 2.6625\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.2208 - mean_squared_error: 1.2208 - val_loss: 2.3966 - val_mean_squared_error: 2.3966\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.2222 - mean_squared_error: 1.2222 - val_loss: 2.3331 - val_mean_squared_error: 2.3331\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0859 - mean_squared_error: 1.0859 - val_loss: 2.5125 - val_mean_squared_error: 2.5125\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 1.1633 - mean_squared_error: 1.1633 - val_loss: 2.5684 - val_mean_squared_error: 2.5684\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.2334 - mean_squared_error: 1.2334 - val_loss: 2.2422 - val_mean_squared_error: 2.2422\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.1346 - mean_squared_error: 1.1346 - val_loss: 2.4419 - val_mean_squared_error: 2.4419\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 1.1633 - mean_squared_error: 1.1633 - val_loss: 2.1887 - val_mean_squared_error: 2.1887\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 1.1656 - mean_squared_error: 1.1656 - val_loss: 2.5879 - val_mean_squared_error: 2.5879\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.0957 - mean_squared_error: 1.0957 - val_loss: 3.5082 - val_mean_squared_error: 3.5082\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.2690 - mean_squared_error: 1.2690 - val_loss: 2.3086 - val_mean_squared_error: 2.3086\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 1.1406 - mean_squared_error: 1.1406 - val_loss: 2.3962 - val_mean_squared_error: 2.3962\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.1084 - mean_squared_error: 1.1084 - val_loss: 2.3895 - val_mean_squared_error: 2.3895\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.1511 - mean_squared_error: 1.1511 - val_loss: 2.2177 - val_mean_squared_error: 2.2177\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.0717 - mean_squared_error: 1.0717 - val_loss: 2.7471 - val_mean_squared_error: 2.7471\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.1091 - mean_squared_error: 1.1091 - val_loss: 2.3507 - val_mean_squared_error: 2.3507\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 0.9784 - mean_squared_error: 0.9784 - val_loss: 2.2903 - val_mean_squared_error: 2.2903\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.0808 - mean_squared_error: 1.0808 - val_loss: 2.6100 - val_mean_squared_error: 2.6100\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.0637 - mean_squared_error: 1.0637 - val_loss: 2.6113 - val_mean_squared_error: 2.6113\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 398us/sample - loss: 1.0867 - mean_squared_error: 1.0867 - val_loss: 2.4107 - val_mean_squared_error: 2.4107\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 2.0899 - val_mean_squared_error: 2.0899\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 0.9673 - mean_squared_error: 0.9673 - val_loss: 2.5644 - val_mean_squared_error: 2.5644\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.1671 - mean_squared_error: 1.1671 - val_loss: 2.4289 - val_mean_squared_error: 2.4289\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 0.9959 - mean_squared_error: 0.9959 - val_loss: 2.1977 - val_mean_squared_error: 2.1977\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0470 - mean_squared_error: 1.0470 - val_loss: 2.4880 - val_mean_squared_error: 2.4880\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0732 - mean_squared_error: 1.0732 - val_loss: 2.9755 - val_mean_squared_error: 2.9755\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 1.2139 - mean_squared_error: 1.2139 - val_loss: 3.3457 - val_mean_squared_error: 3.3457\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.0199 - mean_squared_error: 1.0199 - val_loss: 2.0993 - val_mean_squared_error: 2.0993\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.9132 - mean_squared_error: 0.9132 - val_loss: 2.1583 - val_mean_squared_error: 2.1583\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0383 - mean_squared_error: 1.0383 - val_loss: 2.1874 - val_mean_squared_error: 2.1874\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 0.9994 - mean_squared_error: 0.9994 - val_loss: 2.1375 - val_mean_squared_error: 2.1375\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 1.1730 - mean_squared_error: 1.1730 - val_loss: 2.4620 - val_mean_squared_error: 2.4620\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.9482 - mean_squared_error: 0.9482 - val_loss: 2.3542 - val_mean_squared_error: 2.3542\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.8507 - mean_squared_error: 0.8507 - val_loss: 2.0933 - val_mean_squared_error: 2.0933\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.9484 - mean_squared_error: 0.9484 - val_loss: 2.3103 - val_mean_squared_error: 2.3103\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 0.9250 - mean_squared_error: 0.9250 - val_loss: 2.3162 - val_mean_squared_error: 2.3162\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 1.0092 - mean_squared_error: 1.0092 - val_loss: 2.2414 - val_mean_squared_error: 2.2414\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 0.9800 - mean_squared_error: 0.9800 - val_loss: 2.5735 - val_mean_squared_error: 2.5735\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.9981 - mean_squared_error: 0.9981 - val_loss: 2.3759 - val_mean_squared_error: 2.3759\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 0.9147 - mean_squared_error: 0.9147 - val_loss: 2.2888 - val_mean_squared_error: 2.2888\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 395us/sample - loss: 0.8323 - mean_squared_error: 0.8323 - val_loss: 2.4423 - val_mean_squared_error: 2.4423\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.8993 - mean_squared_error: 0.8993 - val_loss: 2.4489 - val_mean_squared_error: 2.4489\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9619 - mean_squared_error: 0.9619 - val_loss: 2.6546 - val_mean_squared_error: 2.6546\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 1.0290 - mean_squared_error: 1.0290 - val_loss: 2.2706 - val_mean_squared_error: 2.2706\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.8992 - mean_squared_error: 0.8992 - val_loss: 2.3039 - val_mean_squared_error: 2.3039\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.8661 - mean_squared_error: 0.8661 - val_loss: 2.1321 - val_mean_squared_error: 2.1321\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.9729 - mean_squared_error: 0.9729 - val_loss: 2.2907 - val_mean_squared_error: 2.2907\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 0.9910 - mean_squared_error: 0.9910 - val_loss: 2.4985 - val_mean_squared_error: 2.4985\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.0051 - mean_squared_error: 1.0051 - val_loss: 2.5932 - val_mean_squared_error: 2.5932\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 0.8574 - mean_squared_error: 0.8574 - val_loss: 2.5828 - val_mean_squared_error: 2.5828\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 0.9428 - mean_squared_error: 0.9428 - val_loss: 2.6775 - val_mean_squared_error: 2.6775\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.9432 - mean_squared_error: 0.9432 - val_loss: 2.5991 - val_mean_squared_error: 2.5991\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.9177 - mean_squared_error: 0.9177 - val_loss: 2.7753 - val_mean_squared_error: 2.7753\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.8323 - mean_squared_error: 0.8323 - val_loss: 2.5028 - val_mean_squared_error: 2.5028\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 0.8873 - mean_squared_error: 0.8873 - val_loss: 2.4787 - val_mean_squared_error: 2.4787\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.7827 - mean_squared_error: 0.7827 - val_loss: 2.2422 - val_mean_squared_error: 2.2422\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 0.8333 - mean_squared_error: 0.8333 - val_loss: 2.2000 - val_mean_squared_error: 2.2000\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 0.8409 - mean_squared_error: 0.8409 - val_loss: 2.5370 - val_mean_squared_error: 2.5370\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 0.7965 - mean_squared_error: 0.7965 - val_loss: 2.4531 - val_mean_squared_error: 2.4531\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 0.7409 - mean_squared_error: 0.7409 - val_loss: 2.2329 - val_mean_squared_error: 2.2329\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.7188 - mean_squared_error: 0.7188 - val_loss: 2.0519 - val_mean_squared_error: 2.0519\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 0.7904 - mean_squared_error: 0.7904 - val_loss: 2.1181 - val_mean_squared_error: 2.1181\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.9303 - mean_squared_error: 0.9303 - val_loss: 2.2279 - val_mean_squared_error: 2.2279\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.7770 - mean_squared_error: 0.7770 - val_loss: 2.0755 - val_mean_squared_error: 2.0755\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.7959 - mean_squared_error: 0.7959 - val_loss: 2.3336 - val_mean_squared_error: 2.3336\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 0.8705 - mean_squared_error: 0.8705 - val_loss: 2.2232 - val_mean_squared_error: 2.2232\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.7791 - mean_squared_error: 0.7791 - val_loss: 2.2116 - val_mean_squared_error: 2.2116\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.8022 - mean_squared_error: 0.8022 - val_loss: 2.1405 - val_mean_squared_error: 2.1405\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.7834 - mean_squared_error: 0.7834 - val_loss: 2.3223 - val_mean_squared_error: 2.3223\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9678 - mean_squared_error: 0.9678 - val_loss: 2.4984 - val_mean_squared_error: 2.4984\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.8494 - mean_squared_error: 0.8494 - val_loss: 2.2902 - val_mean_squared_error: 2.2902\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 0.9043 - mean_squared_error: 0.9043 - val_loss: 2.4261 - val_mean_squared_error: 2.4261\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 408us/sample - loss: 0.9483 - mean_squared_error: 0.9483 - val_loss: 2.2800 - val_mean_squared_error: 2.2800\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 0.8368 - mean_squared_error: 0.8368 - val_loss: 2.0677 - val_mean_squared_error: 2.0677\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.8337 - mean_squared_error: 0.8337 - val_loss: 2.4368 - val_mean_squared_error: 2.4368\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.6943 - mean_squared_error: 0.6943 - val_loss: 2.1778 - val_mean_squared_error: 2.1778\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 0.7139 - mean_squared_error: 0.7139 - val_loss: 2.1064 - val_mean_squared_error: 2.1064\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.6949 - mean_squared_error: 0.6949 - val_loss: 2.3017 - val_mean_squared_error: 2.3017\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.7788 - mean_squared_error: 0.7788 - val_loss: 2.1838 - val_mean_squared_error: 2.1838\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.7578 - mean_squared_error: 0.7578 - val_loss: 2.0792 - val_mean_squared_error: 2.0792\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.9362 - mean_squared_error: 0.9362 - val_loss: 2.1004 - val_mean_squared_error: 2.1004\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 0.6999 - mean_squared_error: 0.6999 - val_loss: 2.2307 - val_mean_squared_error: 2.2307\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.8478 - mean_squared_error: 0.8478 - val_loss: 2.3371 - val_mean_squared_error: 2.3371\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 0.8501 - mean_squared_error: 0.8501 - val_loss: 2.6596 - val_mean_squared_error: 2.6596\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.7791 - mean_squared_error: 0.7791 - val_loss: 2.2654 - val_mean_squared_error: 2.2654\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.8083 - mean_squared_error: 0.8083 - val_loss: 2.4180 - val_mean_squared_error: 2.4180\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.6768 - mean_squared_error: 0.6768 - val_loss: 2.1567 - val_mean_squared_error: 2.1567\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 402us/sample - loss: 0.6465 - mean_squared_error: 0.6465 - val_loss: 2.0315 - val_mean_squared_error: 2.0315\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 0.7483 - mean_squared_error: 0.7483 - val_loss: 2.3864 - val_mean_squared_error: 2.3864\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.7393 - mean_squared_error: 0.7393 - val_loss: 1.9850 - val_mean_squared_error: 1.9850\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.7174 - mean_squared_error: 0.7174 - val_loss: 2.4405 - val_mean_squared_error: 2.4405\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 403us/sample - loss: 0.7497 - mean_squared_error: 0.7497 - val_loss: 2.1534 - val_mean_squared_error: 2.1534\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 397us/sample - loss: 0.7603 - mean_squared_error: 0.7603 - val_loss: 2.0284 - val_mean_squared_error: 2.0284\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 404us/sample - loss: 0.6232 - mean_squared_error: 0.6232 - val_loss: 2.3256 - val_mean_squared_error: 2.3256\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 0.6614 - mean_squared_error: 0.6614 - val_loss: 2.0648 - val_mean_squared_error: 2.0648\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 400us/sample - loss: 0.7596 - mean_squared_error: 0.7596 - val_loss: 2.2070 - val_mean_squared_error: 2.2070\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.8482 - mean_squared_error: 0.8482 - val_loss: 2.2242 - val_mean_squared_error: 2.2242\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.7857 - mean_squared_error: 0.7857 - val_loss: 2.4788 - val_mean_squared_error: 2.4788\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 406us/sample - loss: 0.7754 - mean_squared_error: 0.7754 - val_loss: 2.1644 - val_mean_squared_error: 2.1644\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 399us/sample - loss: 0.7190 - mean_squared_error: 0.7190 - val_loss: 2.0403 - val_mean_squared_error: 2.0403\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.7210 - mean_squared_error: 0.7210 - val_loss: 2.1472 - val_mean_squared_error: 2.1472\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.6601 - mean_squared_error: 0.6601 - val_loss: 2.1760 - val_mean_squared_error: 2.1760\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 401us/sample - loss: 0.7613 - mean_squared_error: 0.7613 - val_loss: 2.0458 - val_mean_squared_error: 2.0458\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 407us/sample - loss: 0.6954 - mean_squared_error: 0.6954 - val_loss: 2.2517 - val_mean_squared_error: 2.2517\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 405us/sample - loss: 0.8059 - mean_squared_error: 0.8059 - val_loss: 2.5002 - val_mean_squared_error: 2.5002\n",
            "==================================================\n",
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_66 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_110 (Bat (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_66 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_88 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_67 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_111 (Bat (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_67 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_89 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_68 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_112 (Bat (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_68 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_90 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_22 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_113 (Bat (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_91 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_66 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_114 (Bat (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_67 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_68 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 4s 2ms/sample - loss: 437.5176 - mean_squared_error: 437.5175 - val_loss: 3749.0243 - val_mean_squared_error: 3749.0244\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 491us/sample - loss: 19.9134 - mean_squared_error: 19.9134 - val_loss: 82.0867 - val_mean_squared_error: 82.0867\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 17.1657 - mean_squared_error: 17.1657 - val_loss: 89.1242 - val_mean_squared_error: 89.1242\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 16.3735 - mean_squared_error: 16.3735 - val_loss: 69.6415 - val_mean_squared_error: 69.6415\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 14.9973 - mean_squared_error: 14.9973 - val_loss: 35.8690 - val_mean_squared_error: 35.8690\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 14.5025 - mean_squared_error: 14.5025 - val_loss: 18.9886 - val_mean_squared_error: 18.9886\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 11.8687 - mean_squared_error: 11.8687 - val_loss: 14.2482 - val_mean_squared_error: 14.2482\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 12.5166 - mean_squared_error: 12.5166 - val_loss: 14.4081 - val_mean_squared_error: 14.4081\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 11.8452 - mean_squared_error: 11.8452 - val_loss: 12.5020 - val_mean_squared_error: 12.5020\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 10.7142 - mean_squared_error: 10.7142 - val_loss: 9.7094 - val_mean_squared_error: 9.7094\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 10.5313 - mean_squared_error: 10.5313 - val_loss: 12.5790 - val_mean_squared_error: 12.5790\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 10.6849 - mean_squared_error: 10.6849 - val_loss: 13.1979 - val_mean_squared_error: 13.1979\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 410us/sample - loss: 10.8407 - mean_squared_error: 10.8407 - val_loss: 9.0066 - val_mean_squared_error: 9.0066\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 9.4029 - mean_squared_error: 9.4029 - val_loss: 10.6420 - val_mean_squared_error: 10.6420\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 8.4578 - mean_squared_error: 8.4578 - val_loss: 8.6541 - val_mean_squared_error: 8.6541\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 8.1547 - mean_squared_error: 8.1547 - val_loss: 7.7704 - val_mean_squared_error: 7.7704\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 7.7339 - mean_squared_error: 7.7339 - val_loss: 9.0822 - val_mean_squared_error: 9.0822\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 6.9422 - mean_squared_error: 6.9422 - val_loss: 6.3010 - val_mean_squared_error: 6.3010\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 7.1553 - mean_squared_error: 7.1553 - val_loss: 7.0474 - val_mean_squared_error: 7.0474\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 6.2846 - mean_squared_error: 6.2846 - val_loss: 6.1113 - val_mean_squared_error: 6.1113\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 5.8167 - mean_squared_error: 5.8167 - val_loss: 5.7456 - val_mean_squared_error: 5.7456\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 6.1267 - mean_squared_error: 6.1267 - val_loss: 7.1977 - val_mean_squared_error: 7.1977\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 5.2921 - mean_squared_error: 5.2921 - val_loss: 5.6627 - val_mean_squared_error: 5.6627\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 5.1150 - mean_squared_error: 5.1150 - val_loss: 5.1804 - val_mean_squared_error: 5.1804\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 4.6331 - mean_squared_error: 4.6331 - val_loss: 5.6799 - val_mean_squared_error: 5.6799\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 4.5856 - mean_squared_error: 4.5856 - val_loss: 4.3600 - val_mean_squared_error: 4.3600\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 4.2585 - mean_squared_error: 4.2585 - val_loss: 4.3248 - val_mean_squared_error: 4.3248\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 4.1508 - mean_squared_error: 4.1508 - val_loss: 4.3417 - val_mean_squared_error: 4.3417\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 3.9610 - mean_squared_error: 3.9610 - val_loss: 4.1994 - val_mean_squared_error: 4.1994\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 4.1334 - mean_squared_error: 4.1334 - val_loss: 4.4825 - val_mean_squared_error: 4.4825\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 3.5403 - mean_squared_error: 3.5403 - val_loss: 3.9024 - val_mean_squared_error: 3.9024\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 3.3467 - mean_squared_error: 3.3467 - val_loss: 4.6181 - val_mean_squared_error: 4.6181\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 3.4031 - mean_squared_error: 3.4031 - val_loss: 4.1581 - val_mean_squared_error: 4.1581\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 3.4111 - mean_squared_error: 3.4111 - val_loss: 3.6121 - val_mean_squared_error: 3.6121\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 3.1426 - mean_squared_error: 3.1426 - val_loss: 3.7487 - val_mean_squared_error: 3.7487\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 3.2908 - mean_squared_error: 3.2908 - val_loss: 3.8874 - val_mean_squared_error: 3.8874\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 3.2430 - mean_squared_error: 3.2430 - val_loss: 3.6498 - val_mean_squared_error: 3.6498\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 3.0179 - mean_squared_error: 3.0179 - val_loss: 3.5699 - val_mean_squared_error: 3.5699\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 2.9155 - mean_squared_error: 2.9155 - val_loss: 3.4397 - val_mean_squared_error: 3.4397\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 3.0342 - mean_squared_error: 3.0342 - val_loss: 3.3243 - val_mean_squared_error: 3.3243\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 2.6486 - mean_squared_error: 2.6486 - val_loss: 3.5206 - val_mean_squared_error: 3.5206\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 3.0353 - mean_squared_error: 3.0353 - val_loss: 4.0437 - val_mean_squared_error: 4.0437\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 2.6449 - mean_squared_error: 2.6449 - val_loss: 3.3960 - val_mean_squared_error: 3.3960\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 2.8796 - mean_squared_error: 2.8796 - val_loss: 3.4062 - val_mean_squared_error: 3.4062\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 2.6043 - mean_squared_error: 2.6043 - val_loss: 4.7661 - val_mean_squared_error: 4.7661\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 426us/sample - loss: 2.5709 - mean_squared_error: 2.5709 - val_loss: 4.0813 - val_mean_squared_error: 4.0813\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 2.4381 - mean_squared_error: 2.4381 - val_loss: 3.5271 - val_mean_squared_error: 3.5271\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 2.3359 - mean_squared_error: 2.3359 - val_loss: 2.8184 - val_mean_squared_error: 2.8184\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 2.4735 - mean_squared_error: 2.4735 - val_loss: 2.7784 - val_mean_squared_error: 2.7784\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 2.2612 - mean_squared_error: 2.2612 - val_loss: 3.6450 - val_mean_squared_error: 3.6450\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 2.3348 - mean_squared_error: 2.3348 - val_loss: 2.8833 - val_mean_squared_error: 2.8833\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 2.1075 - mean_squared_error: 2.1075 - val_loss: 3.1200 - val_mean_squared_error: 3.1200\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 2.3357 - mean_squared_error: 2.3357 - val_loss: 3.2217 - val_mean_squared_error: 3.2217\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 432us/sample - loss: 2.1326 - mean_squared_error: 2.1326 - val_loss: 3.0175 - val_mean_squared_error: 3.0175\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 426us/sample - loss: 2.0958 - mean_squared_error: 2.0958 - val_loss: 3.2534 - val_mean_squared_error: 3.2534\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 2.1727 - mean_squared_error: 2.1727 - val_loss: 3.0403 - val_mean_squared_error: 3.0403\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 2.1171 - mean_squared_error: 2.1171 - val_loss: 2.8516 - val_mean_squared_error: 2.8516\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 2.1235 - mean_squared_error: 2.1235 - val_loss: 3.2705 - val_mean_squared_error: 3.2705\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 2.0348 - mean_squared_error: 2.0348 - val_loss: 2.9533 - val_mean_squared_error: 2.9533\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.9282 - mean_squared_error: 1.9282 - val_loss: 2.9460 - val_mean_squared_error: 2.9460\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 2.1196 - mean_squared_error: 2.1196 - val_loss: 3.0348 - val_mean_squared_error: 3.0348\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.8834 - mean_squared_error: 1.8834 - val_loss: 2.8115 - val_mean_squared_error: 2.8115\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.8942 - mean_squared_error: 1.8942 - val_loss: 3.0035 - val_mean_squared_error: 3.0035\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.8040 - mean_squared_error: 1.8040 - val_loss: 2.9067 - val_mean_squared_error: 2.9067\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.8541 - mean_squared_error: 1.8541 - val_loss: 2.5103 - val_mean_squared_error: 2.5103\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.6635 - mean_squared_error: 1.6635 - val_loss: 2.8426 - val_mean_squared_error: 2.8426\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.7573 - mean_squared_error: 1.7573 - val_loss: 2.4006 - val_mean_squared_error: 2.4006\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.7437 - mean_squared_error: 1.7437 - val_loss: 2.7636 - val_mean_squared_error: 2.7636\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.7369 - mean_squared_error: 1.7369 - val_loss: 2.4621 - val_mean_squared_error: 2.4621\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.8362 - mean_squared_error: 1.8362 - val_loss: 2.9583 - val_mean_squared_error: 2.9583\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 427us/sample - loss: 1.7807 - mean_squared_error: 1.7807 - val_loss: 2.8566 - val_mean_squared_error: 2.8566\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.5186 - mean_squared_error: 1.5186 - val_loss: 2.5347 - val_mean_squared_error: 2.5347\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 1.7171 - mean_squared_error: 1.7171 - val_loss: 3.1934 - val_mean_squared_error: 3.1934\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.8133 - mean_squared_error: 1.8133 - val_loss: 2.7679 - val_mean_squared_error: 2.7679\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.6645 - mean_squared_error: 1.6645 - val_loss: 2.6690 - val_mean_squared_error: 2.6690\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.6718 - mean_squared_error: 1.6718 - val_loss: 2.8350 - val_mean_squared_error: 2.8350\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.6227 - mean_squared_error: 1.6227 - val_loss: 2.9620 - val_mean_squared_error: 2.9620\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.6037 - mean_squared_error: 1.6037 - val_loss: 2.8314 - val_mean_squared_error: 2.8314\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.5530 - mean_squared_error: 1.5530 - val_loss: 2.3550 - val_mean_squared_error: 2.3550\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 416us/sample - loss: 1.5271 - mean_squared_error: 1.5271 - val_loss: 2.5968 - val_mean_squared_error: 2.5968\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.5294 - mean_squared_error: 1.5294 - val_loss: 2.9645 - val_mean_squared_error: 2.9645\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.5631 - mean_squared_error: 1.5631 - val_loss: 2.4934 - val_mean_squared_error: 2.4934\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.4769 - mean_squared_error: 1.4769 - val_loss: 2.3847 - val_mean_squared_error: 2.3847\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.5068 - mean_squared_error: 1.5068 - val_loss: 2.4044 - val_mean_squared_error: 2.4044\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.5322 - mean_squared_error: 1.5322 - val_loss: 2.5844 - val_mean_squared_error: 2.5844\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.4226 - mean_squared_error: 1.4226 - val_loss: 2.4944 - val_mean_squared_error: 2.4944\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.5916 - mean_squared_error: 1.5916 - val_loss: 2.4259 - val_mean_squared_error: 2.4259\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 409us/sample - loss: 1.5568 - mean_squared_error: 1.5568 - val_loss: 2.6723 - val_mean_squared_error: 2.6723\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 433us/sample - loss: 1.4771 - mean_squared_error: 1.4771 - val_loss: 2.2924 - val_mean_squared_error: 2.2924\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.3228 - mean_squared_error: 1.3228 - val_loss: 2.7723 - val_mean_squared_error: 2.7723\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.3852 - mean_squared_error: 1.3852 - val_loss: 2.6242 - val_mean_squared_error: 2.6242\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.4892 - mean_squared_error: 1.4892 - val_loss: 2.2853 - val_mean_squared_error: 2.2853\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.5114 - mean_squared_error: 1.5114 - val_loss: 2.7461 - val_mean_squared_error: 2.7461\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.2731 - mean_squared_error: 1.2731 - val_loss: 2.3242 - val_mean_squared_error: 2.3242\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.2847 - mean_squared_error: 1.2847 - val_loss: 3.1307 - val_mean_squared_error: 3.1307\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.4692 - mean_squared_error: 1.4692 - val_loss: 2.4418 - val_mean_squared_error: 2.4418\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.3869 - mean_squared_error: 1.3869 - val_loss: 2.6604 - val_mean_squared_error: 2.6604\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 1.2960 - mean_squared_error: 1.2960 - val_loss: 2.2149 - val_mean_squared_error: 2.2149\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.3992 - mean_squared_error: 1.3992 - val_loss: 2.3365 - val_mean_squared_error: 2.3365\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.4745 - mean_squared_error: 1.4745 - val_loss: 2.3143 - val_mean_squared_error: 2.3143\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 415us/sample - loss: 1.4461 - mean_squared_error: 1.4461 - val_loss: 2.8584 - val_mean_squared_error: 2.8584\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 426us/sample - loss: 1.3747 - mean_squared_error: 1.3747 - val_loss: 2.1127 - val_mean_squared_error: 2.1127\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.1474 - mean_squared_error: 1.1474 - val_loss: 2.3362 - val_mean_squared_error: 2.3362\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.5437 - mean_squared_error: 1.5437 - val_loss: 2.4539 - val_mean_squared_error: 2.4539\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.2308 - mean_squared_error: 1.2308 - val_loss: 2.6070 - val_mean_squared_error: 2.6070\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.3101 - mean_squared_error: 1.3101 - val_loss: 2.2182 - val_mean_squared_error: 2.2182\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 1.2008 - mean_squared_error: 1.2008 - val_loss: 2.0061 - val_mean_squared_error: 2.0061\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.1399 - mean_squared_error: 1.1399 - val_loss: 2.3828 - val_mean_squared_error: 2.3828\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.2794 - mean_squared_error: 1.2794 - val_loss: 2.9447 - val_mean_squared_error: 2.9447\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.1716 - mean_squared_error: 1.1716 - val_loss: 2.7478 - val_mean_squared_error: 2.7478\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 432us/sample - loss: 1.3308 - mean_squared_error: 1.3308 - val_loss: 2.4110 - val_mean_squared_error: 2.4110\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.0872 - mean_squared_error: 1.0872 - val_loss: 2.2959 - val_mean_squared_error: 2.2959\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.3168 - mean_squared_error: 1.3168 - val_loss: 2.3126 - val_mean_squared_error: 2.3126\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 425us/sample - loss: 1.1592 - mean_squared_error: 1.1592 - val_loss: 2.1980 - val_mean_squared_error: 2.1980\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.3326 - mean_squared_error: 1.3326 - val_loss: 2.1960 - val_mean_squared_error: 2.1960\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.1945 - mean_squared_error: 1.1945 - val_loss: 2.3429 - val_mean_squared_error: 2.3429\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 1.0473 - mean_squared_error: 1.0473 - val_loss: 2.2856 - val_mean_squared_error: 2.2856\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.0780 - mean_squared_error: 1.0780 - val_loss: 2.1208 - val_mean_squared_error: 2.1208\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 417us/sample - loss: 1.0727 - mean_squared_error: 1.0727 - val_loss: 2.1256 - val_mean_squared_error: 2.1256\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.0246 - mean_squared_error: 1.0246 - val_loss: 2.0856 - val_mean_squared_error: 2.0856\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.2010 - mean_squared_error: 1.2010 - val_loss: 2.0179 - val_mean_squared_error: 2.0179\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.0854 - mean_squared_error: 1.0854 - val_loss: 2.2039 - val_mean_squared_error: 2.2039\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.0786 - mean_squared_error: 1.0786 - val_loss: 2.2607 - val_mean_squared_error: 2.2607\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 414us/sample - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 2.3839 - val_mean_squared_error: 2.3839\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.0841 - mean_squared_error: 1.0841 - val_loss: 2.3506 - val_mean_squared_error: 2.3506\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.1119 - mean_squared_error: 1.1119 - val_loss: 2.1834 - val_mean_squared_error: 2.1834\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 418us/sample - loss: 0.9728 - mean_squared_error: 0.9728 - val_loss: 2.1246 - val_mean_squared_error: 2.1246\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 412us/sample - loss: 1.2406 - mean_squared_error: 1.2406 - val_loss: 2.2786 - val_mean_squared_error: 2.2786\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.1242 - mean_squared_error: 1.1242 - val_loss: 2.2454 - val_mean_squared_error: 2.2454\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.1333 - mean_squared_error: 1.1333 - val_loss: 2.7552 - val_mean_squared_error: 2.7552\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.0687 - mean_squared_error: 1.0687 - val_loss: 2.2505 - val_mean_squared_error: 2.2505\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 413us/sample - loss: 0.9922 - mean_squared_error: 0.9922 - val_loss: 2.1218 - val_mean_squared_error: 2.1218\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 0.9910 - mean_squared_error: 0.9910 - val_loss: 2.1222 - val_mean_squared_error: 2.1222\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 0.9855 - mean_squared_error: 0.9855 - val_loss: 2.3845 - val_mean_squared_error: 2.3845\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 1.0266 - mean_squared_error: 1.0266 - val_loss: 2.1853 - val_mean_squared_error: 2.1853\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.0925 - mean_squared_error: 1.0925 - val_loss: 2.2356 - val_mean_squared_error: 2.2356\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 430us/sample - loss: 1.1664 - mean_squared_error: 1.1664 - val_loss: 2.1801 - val_mean_squared_error: 2.1801\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 423us/sample - loss: 1.0350 - mean_squared_error: 1.0350 - val_loss: 2.3915 - val_mean_squared_error: 2.3915\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 424us/sample - loss: 1.0791 - mean_squared_error: 1.0791 - val_loss: 2.1319 - val_mean_squared_error: 2.1319\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.1720 - mean_squared_error: 1.1720 - val_loss: 2.3416 - val_mean_squared_error: 2.3416\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 420us/sample - loss: 1.0906 - mean_squared_error: 1.0906 - val_loss: 2.2780 - val_mean_squared_error: 2.2780\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 421us/sample - loss: 1.0373 - mean_squared_error: 1.0373 - val_loss: 2.2494 - val_mean_squared_error: 2.2494\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 428us/sample - loss: 1.0126 - mean_squared_error: 1.0126 - val_loss: 2.0082 - val_mean_squared_error: 2.0082\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 411us/sample - loss: 0.9717 - mean_squared_error: 0.9717 - val_loss: 2.4839 - val_mean_squared_error: 2.4839\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 422us/sample - loss: 0.9399 - mean_squared_error: 0.9399 - val_loss: 2.3310 - val_mean_squared_error: 2.3310\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 419us/sample - loss: 1.0037 - mean_squared_error: 1.0037 - val_loss: 2.3127 - val_mean_squared_error: 2.3127\n",
            "Epoch 147/200\n",
            " 608/1819 [=========>....................] - ETA: 0s - loss: 1.1153 - mean_squared_error: 1.1153Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOwRMEPveUor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_vgg_model(start_filter, d, step, bias):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model \n",
        "    '''\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "    \n",
        "    # Input layer is our grayscale image that is 96 pixels by 96 pixels\n",
        "    cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    \n",
        "    # Add our first convolution layers which is two back-to-back conv with 3x3 kernel and same padding\n",
        "    # Add depth with filters\n",
        "    # Our output from these convolutions will be (96,96,start_filter)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (48,48,32)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add some random dropout\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    \n",
        "    # Add our second convolution layers which is three back-to-back conv with 3x3 kernel and same padding\n",
        "    # Double filter depth - output layer will be (48,48,start_filter*2)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "    \n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (24,24,start_filter*2)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add some random dropout\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    \n",
        "    # Add our third convolution layers which is three back-to-back conv with 3x3 kernel and same padding\n",
        "    # Double filter depth again - output layer will be (24,24,start_filter*4)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "    \n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (12,12,start_filter*4)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add some random dropout\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+2*step))\n",
        "    \n",
        "    # Add our fourth and final convolution layers which is three back-to-back conv with 3x3 kernel and same padding\n",
        "    # Double filter depth again - output layer will be (12,12,start_filter*8)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        \n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (6,6,start_filter*8)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Flatten and transition to fully connected layers\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(500))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(500))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(30))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLx43YZAeU0F",
        "colab_type": "code",
        "outputId": "380eff5c-33ac-4532-d165-1771932c05bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Redefine optimizer list to just focus on adam and sgd\n",
        "opt_list = {'adam':adam}\n",
        "\n",
        "# Use an early stopping callback and our timing callback\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                              patience=100, mode='auto')\n",
        "time_callback = TimeHistory()\n",
        "\n",
        "# Initialize a new data frame to hold our output data\n",
        "cnn_vgg_lr_df = pd.DataFrame()\n",
        "\n",
        "# Start filter list\n",
        "start_filters = [12, 16]\n",
        "\n",
        "# Flag for using or not using bias term\n",
        "biases = [False]\n",
        "\n",
        "# Create a list of initial dropout values and steps to increase\n",
        "dropouts = [(0.0,0.02)]\n",
        "\n",
        "\n",
        "for lr_factor in [5, 10]:\n",
        "  for opt_name, opt in opt_list.items():\n",
        "      for start_filter in start_filters:\n",
        "          for bias in biases:\n",
        "              for d in dropouts:\n",
        "                  adam = optimizers.Adam(lr=0.001*lr_factor, beta_1=0.9, beta_2=0.999)\n",
        "                  model = create_vgg_model(start_filter, d[0], d[1], bias)\n",
        "                  model.compile(\n",
        "                        optimizer=opt,\n",
        "                        loss='mean_squared_error',\n",
        "                        metrics=['mean_squared_error'])\n",
        "                  history = model.fit(\n",
        "                      X.astype(np.float32), y.astype(np.float32),\n",
        "                      epochs=200,\n",
        "                      validation_split=0.15, callbacks=[time_callback, early_stop])\n",
        "                  times = time_callback.times\n",
        "\n",
        "                  # Convert to dataframe\n",
        "                  hist = pd.DataFrame(history.history)\n",
        "                  hist['epoch'] = history.epoch\n",
        "                  hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "                  hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "                  hist['times'] = times\n",
        "                  hist['starting_filter'] = start_filter\n",
        "                  hist['layers'] = 4\n",
        "                  hist['pooling'] = 'yes'\n",
        "                  hist['fc_layer'] = 500\n",
        "                  hist['activation'] = 'relu'\n",
        "                  hist['optimizer'] = opt_name\n",
        "                  hist['lrate'] = opt.get_config()['learning_rate']\n",
        "                  hist['dropout_initial'] = d[0]\n",
        "                  hist['dropout_step'] = d[1]\n",
        "                  hist['batch_norm'] = 1\n",
        "                  hist['bias'] = int(bias)\n",
        "                  hist['arch'] = 'vgg'\n",
        "\n",
        "                  # Keep concatenating to dataframe\n",
        "                  cnn_vgg_lr_df = pd.concat([cnn_vgg_lr_df,hist])\n",
        "\n",
        "                  # Re-pickle after every model to retain progress\n",
        "                  cnn_vgg_lr_df.to_pickle(drive_path + \"OutputData/cnn_vgg_lr_df.pkl\")\n",
        "\n",
        "                  # Save models.\n",
        "                  filename = \"cnn_vgg_lr_model_{}_d{}_s{}_sf{}\".format(opt_name, d[0], d[1], start_filter)\n",
        "                  model.save(drive_path + \"Models/\"+filename+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0722 10:39:28.058912 140495902832512 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 96, 96, 12)        108       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 96, 96, 12)        1296      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 96, 96, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 48, 48, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 48, 48, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 48, 48, 24)        2592      \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 48, 48, 24)        5184      \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 48, 48, 24)        5184      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 24, 24, 48)        10368     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 48)        20736     \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 24, 24, 48)        20736     \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 24, 24, 48)        20736     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 48)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 12, 12, 96)        41472     \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 12, 12, 96)        82944     \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 12, 12, 96)        82944     \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 12, 12, 96)        82944     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 96)        384       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 96)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3456)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               1728500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 2,375,994\n",
            "Trainable params: 2,373,634\n",
            "Non-trainable params: 2,360\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 5s 3ms/sample - loss: 781.7488 - mean_squared_error: 781.7488 - val_loss: 213.4187 - val_mean_squared_error: 213.4187\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 654us/sample - loss: 33.0116 - mean_squared_error: 33.0116 - val_loss: 323.8731 - val_mean_squared_error: 323.8730\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 655us/sample - loss: 21.4419 - mean_squared_error: 21.4419 - val_loss: 46.9439 - val_mean_squared_error: 46.9439\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 657us/sample - loss: 20.4273 - mean_squared_error: 20.4273 - val_loss: 17.3434 - val_mean_squared_error: 17.3434\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 20.3017 - mean_squared_error: 20.3017 - val_loss: 16.5353 - val_mean_squared_error: 16.5353\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 663us/sample - loss: 19.7075 - mean_squared_error: 19.7075 - val_loss: 13.1390 - val_mean_squared_error: 13.1390\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 16.7445 - mean_squared_error: 16.7445 - val_loss: 141.4397 - val_mean_squared_error: 141.4397\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 17.4659 - mean_squared_error: 17.4659 - val_loss: 18.8964 - val_mean_squared_error: 18.8965\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 15.2210 - mean_squared_error: 15.2210 - val_loss: 27.0519 - val_mean_squared_error: 27.0519\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 14.5126 - mean_squared_error: 14.5126 - val_loss: 10.8032 - val_mean_squared_error: 10.8032\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 12.8241 - mean_squared_error: 12.8241 - val_loss: 12.9085 - val_mean_squared_error: 12.9085\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 13.3964 - mean_squared_error: 13.3964 - val_loss: 11.7909 - val_mean_squared_error: 11.7909\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 14.4514 - mean_squared_error: 14.4514 - val_loss: 520.1386 - val_mean_squared_error: 520.1386\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 15.5599 - mean_squared_error: 15.5599 - val_loss: 25.7744 - val_mean_squared_error: 25.7744\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 14.1538 - mean_squared_error: 14.1538 - val_loss: 11.9240 - val_mean_squared_error: 11.9240\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 657us/sample - loss: 12.6293 - mean_squared_error: 12.6293 - val_loss: 20.3850 - val_mean_squared_error: 20.3850\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 657us/sample - loss: 12.0446 - mean_squared_error: 12.0446 - val_loss: 9.8792 - val_mean_squared_error: 9.8792\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 12.0523 - mean_squared_error: 12.0523 - val_loss: 9.6812 - val_mean_squared_error: 9.6812\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 660us/sample - loss: 11.3910 - mean_squared_error: 11.3910 - val_loss: 10.0952 - val_mean_squared_error: 10.0952\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 11.1157 - mean_squared_error: 11.1157 - val_loss: 10.7322 - val_mean_squared_error: 10.7322\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 676us/sample - loss: 11.8150 - mean_squared_error: 11.8150 - val_loss: 13.6929 - val_mean_squared_error: 13.6929\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 683us/sample - loss: 11.3459 - mean_squared_error: 11.3459 - val_loss: 11.1519 - val_mean_squared_error: 11.1519\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 678us/sample - loss: 11.5824 - mean_squared_error: 11.5824 - val_loss: 9.7110 - val_mean_squared_error: 9.7110\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 11.5645 - mean_squared_error: 11.5645 - val_loss: 11.7049 - val_mean_squared_error: 11.7049\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 11.2882 - mean_squared_error: 11.2882 - val_loss: 10.7311 - val_mean_squared_error: 10.7311\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 11.1574 - mean_squared_error: 11.1574 - val_loss: 10.6219 - val_mean_squared_error: 10.6219\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 11.4434 - mean_squared_error: 11.4434 - val_loss: 9.6728 - val_mean_squared_error: 9.6728\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 676us/sample - loss: 10.5530 - mean_squared_error: 10.5530 - val_loss: 10.1547 - val_mean_squared_error: 10.1547\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 674us/sample - loss: 10.3133 - mean_squared_error: 10.3133 - val_loss: 10.2442 - val_mean_squared_error: 10.2442\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 677us/sample - loss: 11.0315 - mean_squared_error: 11.0315 - val_loss: 10.7732 - val_mean_squared_error: 10.7732\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 10.5117 - mean_squared_error: 10.5117 - val_loss: 11.2396 - val_mean_squared_error: 11.2396\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 10.3841 - mean_squared_error: 10.3841 - val_loss: 10.3234 - val_mean_squared_error: 10.3234\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 10.2838 - mean_squared_error: 10.2838 - val_loss: 11.0230 - val_mean_squared_error: 11.0230\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 10.2839 - mean_squared_error: 10.2839 - val_loss: 10.2624 - val_mean_squared_error: 10.2624\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 10.1579 - mean_squared_error: 10.1579 - val_loss: 9.7536 - val_mean_squared_error: 9.7536\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 9.8685 - mean_squared_error: 9.8685 - val_loss: 9.7529 - val_mean_squared_error: 9.7529\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 10.0016 - mean_squared_error: 10.0016 - val_loss: 11.7652 - val_mean_squared_error: 11.7652\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 9.5580 - mean_squared_error: 9.5580 - val_loss: 9.4250 - val_mean_squared_error: 9.4250\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 9.4760 - mean_squared_error: 9.4760 - val_loss: 9.6632 - val_mean_squared_error: 9.6632\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 9.9115 - mean_squared_error: 9.9115 - val_loss: 10.5957 - val_mean_squared_error: 10.5957\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 9.5592 - mean_squared_error: 9.5592 - val_loss: 9.9106 - val_mean_squared_error: 9.9106\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 9.5861 - mean_squared_error: 9.5861 - val_loss: 13.6348 - val_mean_squared_error: 13.6348\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 9.4420 - mean_squared_error: 9.4420 - val_loss: 9.3828 - val_mean_squared_error: 9.3828\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 9.1607 - mean_squared_error: 9.1607 - val_loss: 9.0999 - val_mean_squared_error: 9.0999\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 9.3557 - mean_squared_error: 9.3557 - val_loss: 10.6301 - val_mean_squared_error: 10.6301\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 8.9450 - mean_squared_error: 8.9450 - val_loss: 12.4846 - val_mean_squared_error: 12.4846\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 8.4611 - mean_squared_error: 8.4611 - val_loss: 10.2994 - val_mean_squared_error: 10.2994\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 8.4522 - mean_squared_error: 8.4522 - val_loss: 9.5915 - val_mean_squared_error: 9.5915\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 8.1157 - mean_squared_error: 8.1157 - val_loss: 8.0825 - val_mean_squared_error: 8.0825\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 8.0416 - mean_squared_error: 8.0416 - val_loss: 8.3795 - val_mean_squared_error: 8.3795\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 7.6043 - mean_squared_error: 7.6043 - val_loss: 8.1233 - val_mean_squared_error: 8.1233\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 7.6726 - mean_squared_error: 7.6726 - val_loss: 10.6667 - val_mean_squared_error: 10.6667\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 7.3175 - mean_squared_error: 7.3175 - val_loss: 7.9211 - val_mean_squared_error: 7.9211\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 7.4542 - mean_squared_error: 7.4542 - val_loss: 7.1623 - val_mean_squared_error: 7.1623\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 6.8886 - mean_squared_error: 6.8886 - val_loss: 7.3377 - val_mean_squared_error: 7.3377\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 6.6909 - mean_squared_error: 6.6909 - val_loss: 7.0870 - val_mean_squared_error: 7.0870\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 6.3353 - mean_squared_error: 6.3353 - val_loss: 7.1091 - val_mean_squared_error: 7.1091\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 6.2692 - mean_squared_error: 6.2692 - val_loss: 7.3138 - val_mean_squared_error: 7.3138\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 6.0298 - mean_squared_error: 6.0298 - val_loss: 6.0455 - val_mean_squared_error: 6.0455\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 676us/sample - loss: 5.7625 - mean_squared_error: 5.7625 - val_loss: 7.5024 - val_mean_squared_error: 7.5024\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 5.3185 - mean_squared_error: 5.3185 - val_loss: 5.8191 - val_mean_squared_error: 5.8191\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 5.0421 - mean_squared_error: 5.0421 - val_loss: 6.5544 - val_mean_squared_error: 6.5544\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 4.7464 - mean_squared_error: 4.7464 - val_loss: 5.2862 - val_mean_squared_error: 5.2862\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 4.6785 - mean_squared_error: 4.6785 - val_loss: 6.6575 - val_mean_squared_error: 6.6575\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 4.5185 - mean_squared_error: 4.5185 - val_loss: 5.0154 - val_mean_squared_error: 5.0154\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 4.3239 - mean_squared_error: 4.3239 - val_loss: 4.8475 - val_mean_squared_error: 4.8475\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 674us/sample - loss: 4.1968 - mean_squared_error: 4.1968 - val_loss: 5.6522 - val_mean_squared_error: 5.6522\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 4.2169 - mean_squared_error: 4.2169 - val_loss: 4.5850 - val_mean_squared_error: 4.5850\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 4.0747 - mean_squared_error: 4.0747 - val_loss: 5.2850 - val_mean_squared_error: 5.2850\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 4.2512 - mean_squared_error: 4.2512 - val_loss: 5.5351 - val_mean_squared_error: 5.5351\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 3.8330 - mean_squared_error: 3.8330 - val_loss: 4.4725 - val_mean_squared_error: 4.4725\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 3.5864 - mean_squared_error: 3.5864 - val_loss: 4.4639 - val_mean_squared_error: 4.4639\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 3.5063 - mean_squared_error: 3.5063 - val_loss: 4.0759 - val_mean_squared_error: 4.0759\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 3.6088 - mean_squared_error: 3.6088 - val_loss: 4.9594 - val_mean_squared_error: 4.9594\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 3.5377 - mean_squared_error: 3.5377 - val_loss: 5.1274 - val_mean_squared_error: 5.1274\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 661us/sample - loss: 3.3203 - mean_squared_error: 3.3203 - val_loss: 4.6053 - val_mean_squared_error: 4.6053\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 3.3047 - mean_squared_error: 3.3047 - val_loss: 3.9096 - val_mean_squared_error: 3.9096\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 3.2879 - mean_squared_error: 3.2879 - val_loss: 3.8551 - val_mean_squared_error: 3.8551\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.8942 - mean_squared_error: 2.8942 - val_loss: 3.5435 - val_mean_squared_error: 3.5435\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 3.1107 - mean_squared_error: 3.1107 - val_loss: 4.3281 - val_mean_squared_error: 4.3281\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 2.9916 - mean_squared_error: 2.9916 - val_loss: 3.5635 - val_mean_squared_error: 3.5635\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 664us/sample - loss: 2.7491 - mean_squared_error: 2.7491 - val_loss: 3.9909 - val_mean_squared_error: 3.9909\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 2.8301 - mean_squared_error: 2.8301 - val_loss: 3.9517 - val_mean_squared_error: 3.9517\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.7259 - mean_squared_error: 2.7259 - val_loss: 3.6731 - val_mean_squared_error: 3.6731\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.7306 - mean_squared_error: 2.7306 - val_loss: 4.0923 - val_mean_squared_error: 4.0923\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 664us/sample - loss: 2.7359 - mean_squared_error: 2.7359 - val_loss: 3.7006 - val_mean_squared_error: 3.7006\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 661us/sample - loss: 2.6547 - mean_squared_error: 2.6547 - val_loss: 3.5455 - val_mean_squared_error: 3.5455\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 2.6200 - mean_squared_error: 2.6200 - val_loss: 3.9449 - val_mean_squared_error: 3.9449\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 2.5168 - mean_squared_error: 2.5168 - val_loss: 3.5784 - val_mean_squared_error: 3.5784\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 658us/sample - loss: 2.7307 - mean_squared_error: 2.7307 - val_loss: 3.6002 - val_mean_squared_error: 3.6002\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 660us/sample - loss: 2.6080 - mean_squared_error: 2.6080 - val_loss: 3.6788 - val_mean_squared_error: 3.6788\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.3676 - mean_squared_error: 2.3676 - val_loss: 3.5532 - val_mean_squared_error: 3.5532\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.2929 - mean_squared_error: 2.2929 - val_loss: 3.5577 - val_mean_squared_error: 3.5577\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 2.3902 - mean_squared_error: 2.3902 - val_loss: 3.2699 - val_mean_squared_error: 3.2699\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.3489 - mean_squared_error: 2.3489 - val_loss: 4.4773 - val_mean_squared_error: 4.4773\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 2.1969 - mean_squared_error: 2.1969 - val_loss: 3.5717 - val_mean_squared_error: 3.5717\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 663us/sample - loss: 2.2736 - mean_squared_error: 2.2736 - val_loss: 3.4035 - val_mean_squared_error: 3.4035\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 2.1394 - mean_squared_error: 2.1394 - val_loss: 3.2530 - val_mean_squared_error: 3.2530\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.1236 - mean_squared_error: 2.1236 - val_loss: 3.5594 - val_mean_squared_error: 3.5594\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.1007 - mean_squared_error: 2.1007 - val_loss: 3.3337 - val_mean_squared_error: 3.3337\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.1726 - mean_squared_error: 2.1726 - val_loss: 3.4282 - val_mean_squared_error: 3.4282\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 2.1330 - mean_squared_error: 2.1330 - val_loss: 3.3860 - val_mean_squared_error: 3.3860\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.9995 - mean_squared_error: 1.9995 - val_loss: 3.1529 - val_mean_squared_error: 3.1529\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.9982 - mean_squared_error: 1.9982 - val_loss: 3.1133 - val_mean_squared_error: 3.1133\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.0883 - mean_squared_error: 2.0883 - val_loss: 3.2291 - val_mean_squared_error: 3.2291\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.0609 - mean_squared_error: 2.0609 - val_loss: 3.0584 - val_mean_squared_error: 3.0584\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.8807 - mean_squared_error: 1.8807 - val_loss: 3.1473 - val_mean_squared_error: 3.1473\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.9055 - mean_squared_error: 1.9055 - val_loss: 2.8346 - val_mean_squared_error: 2.8346\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 1.7902 - mean_squared_error: 1.7902 - val_loss: 3.4267 - val_mean_squared_error: 3.4267\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 676us/sample - loss: 1.8433 - mean_squared_error: 1.8433 - val_loss: 3.4882 - val_mean_squared_error: 3.4882\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.0085 - mean_squared_error: 2.0085 - val_loss: 3.3058 - val_mean_squared_error: 3.3058\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.8577 - mean_squared_error: 1.8577 - val_loss: 3.1193 - val_mean_squared_error: 3.1193\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.9528 - mean_squared_error: 1.9528 - val_loss: 3.1749 - val_mean_squared_error: 3.1749\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.8028 - mean_squared_error: 1.8028 - val_loss: 3.2943 - val_mean_squared_error: 3.2943\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.7760 - mean_squared_error: 1.7760 - val_loss: 3.2595 - val_mean_squared_error: 3.2595\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.7441 - mean_squared_error: 1.7441 - val_loss: 3.0925 - val_mean_squared_error: 3.0925\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.8537 - mean_squared_error: 1.8537 - val_loss: 3.0208 - val_mean_squared_error: 3.0208\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.6527 - mean_squared_error: 1.6527 - val_loss: 2.9037 - val_mean_squared_error: 2.9037\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.6069 - mean_squared_error: 1.6069 - val_loss: 2.8961 - val_mean_squared_error: 2.8961\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 1.7075 - mean_squared_error: 1.7075 - val_loss: 2.8007 - val_mean_squared_error: 2.8007\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 1.7059 - mean_squared_error: 1.7059 - val_loss: 2.8529 - val_mean_squared_error: 2.8529\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.6784 - mean_squared_error: 1.6784 - val_loss: 3.0307 - val_mean_squared_error: 3.0307\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.6287 - mean_squared_error: 1.6287 - val_loss: 2.9501 - val_mean_squared_error: 2.9501\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5836 - mean_squared_error: 1.5836 - val_loss: 2.8622 - val_mean_squared_error: 2.8622\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.5142 - mean_squared_error: 1.5142 - val_loss: 3.0432 - val_mean_squared_error: 3.0432\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.4984 - mean_squared_error: 1.4984 - val_loss: 2.9539 - val_mean_squared_error: 2.9539\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.5808 - mean_squared_error: 1.5808 - val_loss: 2.8908 - val_mean_squared_error: 2.8908\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.4875 - mean_squared_error: 1.4875 - val_loss: 3.2125 - val_mean_squared_error: 3.2125\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5390 - mean_squared_error: 1.5390 - val_loss: 2.8865 - val_mean_squared_error: 2.8865\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5137 - mean_squared_error: 1.5137 - val_loss: 3.2376 - val_mean_squared_error: 3.2376\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5833 - mean_squared_error: 1.5833 - val_loss: 3.0944 - val_mean_squared_error: 3.0944\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.4758 - mean_squared_error: 1.4758 - val_loss: 3.3393 - val_mean_squared_error: 3.3393\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.4668 - mean_squared_error: 1.4668 - val_loss: 3.2321 - val_mean_squared_error: 3.2321\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.4128 - mean_squared_error: 1.4128 - val_loss: 2.6181 - val_mean_squared_error: 2.6181\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 1.4526 - mean_squared_error: 1.4526 - val_loss: 2.7075 - val_mean_squared_error: 2.7075\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.4075 - mean_squared_error: 1.4075 - val_loss: 2.6173 - val_mean_squared_error: 2.6173\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.3516 - mean_squared_error: 1.3516 - val_loss: 2.6175 - val_mean_squared_error: 2.6175\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.3323 - mean_squared_error: 1.3323 - val_loss: 2.6932 - val_mean_squared_error: 2.6932\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.3929 - mean_squared_error: 1.3929 - val_loss: 2.6788 - val_mean_squared_error: 2.6788\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 1.3954 - mean_squared_error: 1.3954 - val_loss: 2.7899 - val_mean_squared_error: 2.7899\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2841 - mean_squared_error: 1.2841 - val_loss: 2.8571 - val_mean_squared_error: 2.8571\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.4039 - mean_squared_error: 1.4039 - val_loss: 2.5903 - val_mean_squared_error: 2.5903\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.5930 - mean_squared_error: 1.5930 - val_loss: 3.0941 - val_mean_squared_error: 3.0941\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.3589 - mean_squared_error: 1.3589 - val_loss: 2.7713 - val_mean_squared_error: 2.7713\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.3106 - mean_squared_error: 1.3106 - val_loss: 2.8425 - val_mean_squared_error: 2.8425\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.3236 - mean_squared_error: 1.3236 - val_loss: 2.5247 - val_mean_squared_error: 2.5247\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.3223 - mean_squared_error: 1.3223 - val_loss: 2.7986 - val_mean_squared_error: 2.7986\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.3095 - mean_squared_error: 1.3095 - val_loss: 3.3503 - val_mean_squared_error: 3.3503\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2898 - mean_squared_error: 1.2898 - val_loss: 2.7936 - val_mean_squared_error: 2.7936\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.3018 - mean_squared_error: 1.3018 - val_loss: 2.7062 - val_mean_squared_error: 2.7062\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 661us/sample - loss: 1.2810 - mean_squared_error: 1.2810 - val_loss: 2.9768 - val_mean_squared_error: 2.9768\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.2980 - mean_squared_error: 1.2980 - val_loss: 2.4653 - val_mean_squared_error: 2.4653\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.3378 - mean_squared_error: 1.3378 - val_loss: 2.7352 - val_mean_squared_error: 2.7352\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2834 - mean_squared_error: 1.2834 - val_loss: 2.8166 - val_mean_squared_error: 2.8166\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2247 - mean_squared_error: 1.2247 - val_loss: 3.3077 - val_mean_squared_error: 3.3077\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.2764 - mean_squared_error: 1.2764 - val_loss: 2.6752 - val_mean_squared_error: 2.6752\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 1.1730 - mean_squared_error: 1.1730 - val_loss: 2.5317 - val_mean_squared_error: 2.5317\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 658us/sample - loss: 1.2203 - mean_squared_error: 1.2203 - val_loss: 2.6737 - val_mean_squared_error: 2.6737\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.1753 - mean_squared_error: 1.1753 - val_loss: 2.5449 - val_mean_squared_error: 2.5449\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.4003 - mean_squared_error: 1.4003 - val_loss: 2.5955 - val_mean_squared_error: 2.5955\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.2116 - mean_squared_error: 1.2116 - val_loss: 2.7069 - val_mean_squared_error: 2.7069\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 658us/sample - loss: 1.3028 - mean_squared_error: 1.3028 - val_loss: 2.6700 - val_mean_squared_error: 2.6700\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.0947 - mean_squared_error: 1.0947 - val_loss: 2.6218 - val_mean_squared_error: 2.6218\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1501 - mean_squared_error: 1.1501 - val_loss: 2.7646 - val_mean_squared_error: 2.7646\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.2130 - mean_squared_error: 1.2130 - val_loss: 2.5896 - val_mean_squared_error: 2.5896\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 1.1233 - mean_squared_error: 1.1233 - val_loss: 2.7454 - val_mean_squared_error: 2.7454\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 658us/sample - loss: 1.2586 - mean_squared_error: 1.2586 - val_loss: 2.6570 - val_mean_squared_error: 2.6570\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.1925 - mean_squared_error: 1.1925 - val_loss: 2.8514 - val_mean_squared_error: 2.8514\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.1625 - mean_squared_error: 1.1625 - val_loss: 2.5137 - val_mean_squared_error: 2.5137\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.2470 - mean_squared_error: 1.2470 - val_loss: 2.5911 - val_mean_squared_error: 2.5911\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1259 - mean_squared_error: 1.1259 - val_loss: 2.4710 - val_mean_squared_error: 2.4710\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 1.2117 - mean_squared_error: 1.2117 - val_loss: 2.7863 - val_mean_squared_error: 2.7863\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 663us/sample - loss: 1.1724 - mean_squared_error: 1.1724 - val_loss: 2.7623 - val_mean_squared_error: 2.7623\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0330 - mean_squared_error: 1.0330 - val_loss: 2.6495 - val_mean_squared_error: 2.6495\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.1306 - mean_squared_error: 1.1306 - val_loss: 2.7890 - val_mean_squared_error: 2.7890\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.0980 - mean_squared_error: 1.0980 - val_loss: 2.4605 - val_mean_squared_error: 2.4605\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.0215 - mean_squared_error: 1.0215 - val_loss: 2.5733 - val_mean_squared_error: 2.5733\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.0710 - mean_squared_error: 1.0710 - val_loss: 2.4814 - val_mean_squared_error: 2.4814\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.1621 - mean_squared_error: 1.1621 - val_loss: 2.8667 - val_mean_squared_error: 2.8667\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.1974 - mean_squared_error: 1.1974 - val_loss: 2.5795 - val_mean_squared_error: 2.5795\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.1507 - mean_squared_error: 1.1507 - val_loss: 2.5841 - val_mean_squared_error: 2.5841\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.0835 - mean_squared_error: 1.0835 - val_loss: 2.7142 - val_mean_squared_error: 2.7142\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.0939 - mean_squared_error: 1.0939 - val_loss: 2.6409 - val_mean_squared_error: 2.6409\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0306 - mean_squared_error: 1.0306 - val_loss: 2.6295 - val_mean_squared_error: 2.6295\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0605 - mean_squared_error: 1.0605 - val_loss: 2.4907 - val_mean_squared_error: 2.4907\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 0.9623 - mean_squared_error: 0.9623 - val_loss: 2.4525 - val_mean_squared_error: 2.4525\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 0.9777 - mean_squared_error: 0.9777 - val_loss: 2.5021 - val_mean_squared_error: 2.5021\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 0.9957 - mean_squared_error: 0.9957 - val_loss: 16.5056 - val_mean_squared_error: 16.5056\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 1.0914 - mean_squared_error: 1.0914 - val_loss: 2.5721 - val_mean_squared_error: 2.5721\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 662us/sample - loss: 0.9815 - mean_squared_error: 0.9815 - val_loss: 2.2492 - val_mean_squared_error: 2.2492\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 0.9615 - mean_squared_error: 0.9615 - val_loss: 2.5312 - val_mean_squared_error: 2.5312\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 1.0336 - mean_squared_error: 1.0336 - val_loss: 2.7734 - val_mean_squared_error: 2.7734\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.0494 - mean_squared_error: 1.0494 - val_loss: 2.4974 - val_mean_squared_error: 2.4974\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 0.9679 - mean_squared_error: 0.9679 - val_loss: 2.7406 - val_mean_squared_error: 2.7406\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 0.9539 - mean_squared_error: 0.9539 - val_loss: 2.3828 - val_mean_squared_error: 2.3828\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 0.9334 - mean_squared_error: 0.9334 - val_loss: 2.6093 - val_mean_squared_error: 2.6093\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 0.9557 - mean_squared_error: 0.9557 - val_loss: 2.3734 - val_mean_squared_error: 2.3734\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 1.0251 - mean_squared_error: 1.0251 - val_loss: 2.4811 - val_mean_squared_error: 2.4811\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0644 - mean_squared_error: 1.0644 - val_loss: 2.4911 - val_mean_squared_error: 2.4911\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.0329 - mean_squared_error: 1.0329 - val_loss: 2.8734 - val_mean_squared_error: 2.8734\n",
            "==================================================\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 96, 96, 16)        144       \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 96, 96, 16)        2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 96, 96, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 48, 48, 32)        4608      \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 24, 24, 64)        18432     \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 12, 12, 128)       73728     \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 500)               2304500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,245,598\n",
            "Trainable params: 3,243,118\n",
            "Non-trainable params: 2,480\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 2ms/sample - loss: 283.0856 - mean_squared_error: 283.0856 - val_loss: 6778.1706 - val_mean_squared_error: 6778.1704\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 2s 826us/sample - loss: 25.7418 - mean_squared_error: 25.7418 - val_loss: 190.2086 - val_mean_squared_error: 190.2086\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 21.4531 - mean_squared_error: 21.4531 - val_loss: 75.9990 - val_mean_squared_error: 75.9990\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 22.6889 - mean_squared_error: 22.6889 - val_loss: 53.2356 - val_mean_squared_error: 53.2356\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 17.1887 - mean_squared_error: 17.1887 - val_loss: 19.3702 - val_mean_squared_error: 19.3702\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 15.1843 - mean_squared_error: 15.1843 - val_loss: 12.2080 - val_mean_squared_error: 12.2080\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 16.1694 - mean_squared_error: 16.1694 - val_loss: 10.8145 - val_mean_squared_error: 10.8145\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 803us/sample - loss: 13.9313 - mean_squared_error: 13.9313 - val_loss: 10.2722 - val_mean_squared_error: 10.2722\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 14.0369 - mean_squared_error: 14.0369 - val_loss: 10.7588 - val_mean_squared_error: 10.7588\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 14.2582 - mean_squared_error: 14.2582 - val_loss: 11.1919 - val_mean_squared_error: 11.1919\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 13.3297 - mean_squared_error: 13.3297 - val_loss: 11.2106 - val_mean_squared_error: 11.2106\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 804us/sample - loss: 12.5237 - mean_squared_error: 12.5237 - val_loss: 10.8805 - val_mean_squared_error: 10.8805\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 12.7321 - mean_squared_error: 12.7321 - val_loss: 10.4583 - val_mean_squared_error: 10.4583\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 12.7837 - mean_squared_error: 12.7837 - val_loss: 10.4101 - val_mean_squared_error: 10.4101\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 12.1136 - mean_squared_error: 12.1136 - val_loss: 9.7048 - val_mean_squared_error: 9.7048\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 11.7687 - mean_squared_error: 11.7687 - val_loss: 11.9355 - val_mean_squared_error: 11.9355\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 11.6543 - mean_squared_error: 11.6543 - val_loss: 9.5608 - val_mean_squared_error: 9.5608\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 11.2427 - mean_squared_error: 11.2427 - val_loss: 9.4907 - val_mean_squared_error: 9.4907\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 10.9441 - mean_squared_error: 10.9441 - val_loss: 10.0137 - val_mean_squared_error: 10.0137\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 10.7553 - mean_squared_error: 10.7553 - val_loss: 10.7888 - val_mean_squared_error: 10.7888\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 10.6610 - mean_squared_error: 10.6610 - val_loss: 9.5028 - val_mean_squared_error: 9.5028\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 10.2864 - mean_squared_error: 10.2864 - val_loss: 9.4255 - val_mean_squared_error: 9.4255\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 10.2228 - mean_squared_error: 10.2228 - val_loss: 9.0920 - val_mean_squared_error: 9.0920\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 9.8343 - mean_squared_error: 9.8343 - val_loss: 9.3493 - val_mean_squared_error: 9.3493\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 9.7122 - mean_squared_error: 9.7122 - val_loss: 10.4172 - val_mean_squared_error: 10.4172\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 9.6133 - mean_squared_error: 9.6133 - val_loss: 9.4148 - val_mean_squared_error: 9.4148\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 10.2105 - mean_squared_error: 10.2105 - val_loss: 8.9641 - val_mean_squared_error: 8.9641\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 9.6545 - mean_squared_error: 9.6545 - val_loss: 9.2787 - val_mean_squared_error: 9.2787\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 9.3773 - mean_squared_error: 9.3773 - val_loss: 8.8413 - val_mean_squared_error: 8.8413\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 9.2718 - mean_squared_error: 9.2718 - val_loss: 12.4210 - val_mean_squared_error: 12.4209\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 9.3429 - mean_squared_error: 9.3429 - val_loss: 8.7193 - val_mean_squared_error: 8.7193\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 8.9501 - mean_squared_error: 8.9501 - val_loss: 8.9896 - val_mean_squared_error: 8.9896\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 8.8090 - mean_squared_error: 8.8090 - val_loss: 9.1230 - val_mean_squared_error: 9.1230\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 8.7551 - mean_squared_error: 8.7551 - val_loss: 9.0520 - val_mean_squared_error: 9.0520\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 8.6072 - mean_squared_error: 8.6072 - val_loss: 8.7011 - val_mean_squared_error: 8.7011\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 8.3269 - mean_squared_error: 8.3268 - val_loss: 26.9002 - val_mean_squared_error: 26.9002\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 8.2517 - mean_squared_error: 8.2517 - val_loss: 9.3184 - val_mean_squared_error: 9.3184\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 8.0717 - mean_squared_error: 8.0717 - val_loss: 9.2732 - val_mean_squared_error: 9.2732\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 8.0484 - mean_squared_error: 8.0484 - val_loss: 8.8530 - val_mean_squared_error: 8.8530\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 7.9560 - mean_squared_error: 7.9560 - val_loss: 81.2449 - val_mean_squared_error: 81.2449\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 7.8158 - mean_squared_error: 7.8158 - val_loss: 8.8695 - val_mean_squared_error: 8.8695\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 7.6252 - mean_squared_error: 7.6252 - val_loss: 7.8840 - val_mean_squared_error: 7.8840\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 7.3747 - mean_squared_error: 7.3747 - val_loss: 485.1601 - val_mean_squared_error: 485.1600\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 7.3291 - mean_squared_error: 7.3291 - val_loss: 978.8440 - val_mean_squared_error: 978.8439\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 7.2692 - mean_squared_error: 7.2692 - val_loss: 7.9133 - val_mean_squared_error: 7.9133\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 805us/sample - loss: 7.0582 - mean_squared_error: 7.0582 - val_loss: 7.9514 - val_mean_squared_error: 7.9514\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 6.7937 - mean_squared_error: 6.7937 - val_loss: 7.7165 - val_mean_squared_error: 7.7165\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 6.7274 - mean_squared_error: 6.7274 - val_loss: 7.6869 - val_mean_squared_error: 7.6869\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 6.7357 - mean_squared_error: 6.7357 - val_loss: 7.0314 - val_mean_squared_error: 7.0314\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 6.1675 - mean_squared_error: 6.1675 - val_loss: 7.1733 - val_mean_squared_error: 7.1733\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 6.2564 - mean_squared_error: 6.2564 - val_loss: 7.5155 - val_mean_squared_error: 7.5155\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 5.9459 - mean_squared_error: 5.9459 - val_loss: 7.3448 - val_mean_squared_error: 7.3448\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 5.9539 - mean_squared_error: 5.9539 - val_loss: 7.4376 - val_mean_squared_error: 7.4376\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 5.6518 - mean_squared_error: 5.6518 - val_loss: 6.9594 - val_mean_squared_error: 6.9594\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 5.5051 - mean_squared_error: 5.5051 - val_loss: 2871.5798 - val_mean_squared_error: 2871.5798\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 5.2735 - mean_squared_error: 5.2735 - val_loss: 8.7249 - val_mean_squared_error: 8.7249\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 5.0612 - mean_squared_error: 5.0612 - val_loss: 6.8785 - val_mean_squared_error: 6.8785\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 4.9676 - mean_squared_error: 4.9676 - val_loss: 5.7489 - val_mean_squared_error: 5.7489\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 4.6296 - mean_squared_error: 4.6296 - val_loss: 10.9758 - val_mean_squared_error: 10.9758\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 4.6037 - mean_squared_error: 4.6037 - val_loss: 5.7868 - val_mean_squared_error: 5.7868\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 4.4795 - mean_squared_error: 4.4795 - val_loss: 5.4703 - val_mean_squared_error: 5.4703\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 4.3290 - mean_squared_error: 4.3290 - val_loss: 28.1657 - val_mean_squared_error: 28.1657\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 4.1575 - mean_squared_error: 4.1575 - val_loss: 182.0652 - val_mean_squared_error: 182.0652\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 4.0377 - mean_squared_error: 4.0377 - val_loss: 5.0841 - val_mean_squared_error: 5.0841\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 3.9422 - mean_squared_error: 3.9422 - val_loss: 6.0375 - val_mean_squared_error: 6.0375\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 805us/sample - loss: 3.9375 - mean_squared_error: 3.9375 - val_loss: 5.1940 - val_mean_squared_error: 5.1940\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 3.7475 - mean_squared_error: 3.7475 - val_loss: 5.3226 - val_mean_squared_error: 5.3226\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 806us/sample - loss: 3.7494 - mean_squared_error: 3.7494 - val_loss: 5.0314 - val_mean_squared_error: 5.0314\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 3.5258 - mean_squared_error: 3.5258 - val_loss: 5.1432 - val_mean_squared_error: 5.1432\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 3.3675 - mean_squared_error: 3.3675 - val_loss: 4.6394 - val_mean_squared_error: 4.6394\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 3.3322 - mean_squared_error: 3.3322 - val_loss: 4.9493 - val_mean_squared_error: 4.9493\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 3.3040 - mean_squared_error: 3.3040 - val_loss: 4.7530 - val_mean_squared_error: 4.7530\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 3.1435 - mean_squared_error: 3.1435 - val_loss: 4.3810 - val_mean_squared_error: 4.3810\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 3.1149 - mean_squared_error: 3.1149 - val_loss: 4.8144 - val_mean_squared_error: 4.8144\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 3.2862 - mean_squared_error: 3.2862 - val_loss: 4.5134 - val_mean_squared_error: 4.5134\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 3.1180 - mean_squared_error: 3.1180 - val_loss: 40272.0061 - val_mean_squared_error: 40272.0078\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 2.9768 - mean_squared_error: 2.9768 - val_loss: 16755.9229 - val_mean_squared_error: 16755.9219\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 2.8702 - mean_squared_error: 2.8702 - val_loss: 4.1928 - val_mean_squared_error: 4.1928\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 804us/sample - loss: 2.8014 - mean_squared_error: 2.8014 - val_loss: 193.8874 - val_mean_squared_error: 193.8875\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 2.7207 - mean_squared_error: 2.7207 - val_loss: 12678.5881 - val_mean_squared_error: 12678.5879\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 2.7297 - mean_squared_error: 2.7297 - val_loss: 4.3595 - val_mean_squared_error: 4.3595\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 2.6599 - mean_squared_error: 2.6599 - val_loss: 49.6183 - val_mean_squared_error: 49.6183\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 2.6371 - mean_squared_error: 2.6371 - val_loss: 5239.4426 - val_mean_squared_error: 5239.4424\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 2.6952 - mean_squared_error: 2.6952 - val_loss: 29508.5145 - val_mean_squared_error: 29508.5117\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 2.4789 - mean_squared_error: 2.4789 - val_loss: 4.0677 - val_mean_squared_error: 4.0677\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 805us/sample - loss: 2.5136 - mean_squared_error: 2.5136 - val_loss: 3.8773 - val_mean_squared_error: 3.8773\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 2.3767 - mean_squared_error: 2.3767 - val_loss: 42355.9956 - val_mean_squared_error: 42355.9922\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 2.5694 - mean_squared_error: 2.5694 - val_loss: 3.9865 - val_mean_squared_error: 3.9865\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 804us/sample - loss: 2.2514 - mean_squared_error: 2.2514 - val_loss: 4.1423 - val_mean_squared_error: 4.1423\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 2.2853 - mean_squared_error: 2.2853 - val_loss: 3.6669 - val_mean_squared_error: 3.6669\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 2.3193 - mean_squared_error: 2.3193 - val_loss: 3.7815 - val_mean_squared_error: 3.7815\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 2.1275 - mean_squared_error: 2.1275 - val_loss: 10369.5801 - val_mean_squared_error: 10369.5801\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 2.1623 - mean_squared_error: 2.1623 - val_loss: 3.9116 - val_mean_squared_error: 3.9116\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 2.1581 - mean_squared_error: 2.1581 - val_loss: 17982.1677 - val_mean_squared_error: 17982.1680\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 2.0833 - mean_squared_error: 2.0833 - val_loss: 7.6578 - val_mean_squared_error: 7.6578\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 3.1498 - mean_squared_error: 3.1498 - val_loss: 8.5762 - val_mean_squared_error: 8.5762\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 2.2130 - mean_squared_error: 2.2130 - val_loss: 4.0532 - val_mean_squared_error: 4.0532\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 2.1079 - mean_squared_error: 2.1079 - val_loss: 3.9206 - val_mean_squared_error: 3.9206\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 2.0022 - mean_squared_error: 2.0022 - val_loss: 3.6469 - val_mean_squared_error: 3.6469\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 2.0477 - mean_squared_error: 2.0477 - val_loss: 3.6387 - val_mean_squared_error: 3.6387\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 806us/sample - loss: 2.0465 - mean_squared_error: 2.0465 - val_loss: 3.6914 - val_mean_squared_error: 3.6914\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.9952 - mean_squared_error: 1.9952 - val_loss: 3.7544 - val_mean_squared_error: 3.7544\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.8688 - mean_squared_error: 1.8688 - val_loss: 3.3838 - val_mean_squared_error: 3.3838\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 1.8893 - mean_squared_error: 1.8893 - val_loss: 6322.8100 - val_mean_squared_error: 6322.8101\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 1.7807 - mean_squared_error: 1.7807 - val_loss: 3.3714 - val_mean_squared_error: 3.3714\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 805us/sample - loss: 1.8176 - mean_squared_error: 1.8176 - val_loss: 3.7909 - val_mean_squared_error: 3.7909\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.8755 - mean_squared_error: 1.8755 - val_loss: 3.5771 - val_mean_squared_error: 3.5771\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 1.7374 - mean_squared_error: 1.7374 - val_loss: 3.3112 - val_mean_squared_error: 3.3112\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 1.7798 - mean_squared_error: 1.7798 - val_loss: 3.4745 - val_mean_squared_error: 3.4745\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.8301 - mean_squared_error: 1.8301 - val_loss: 3.6345 - val_mean_squared_error: 3.6345\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.8124 - mean_squared_error: 1.8124 - val_loss: 3.7147 - val_mean_squared_error: 3.7147\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.8126 - mean_squared_error: 1.8126 - val_loss: 3.7235 - val_mean_squared_error: 3.7235\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 1.6952 - mean_squared_error: 1.6952 - val_loss: 3.3468 - val_mean_squared_error: 3.3468\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.7736 - mean_squared_error: 1.7736 - val_loss: 63370.4893 - val_mean_squared_error: 63370.4844\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.7456 - mean_squared_error: 1.7456 - val_loss: 3.5555 - val_mean_squared_error: 3.5555\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.5327 - mean_squared_error: 1.5327 - val_loss: 3.5579 - val_mean_squared_error: 3.5579\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 1.6633 - mean_squared_error: 1.6633 - val_loss: 31878.9279 - val_mean_squared_error: 31878.9277\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.6060 - mean_squared_error: 1.6060 - val_loss: 1308.5057 - val_mean_squared_error: 1308.5055\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.5293 - mean_squared_error: 1.5293 - val_loss: 3.2251 - val_mean_squared_error: 3.2251\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 806us/sample - loss: 1.5359 - mean_squared_error: 1.5359 - val_loss: 3.3853 - val_mean_squared_error: 3.3853\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.6021 - mean_squared_error: 1.6021 - val_loss: 3.1906 - val_mean_squared_error: 3.1906\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.6218 - mean_squared_error: 1.6218 - val_loss: 3.3666 - val_mean_squared_error: 3.3666\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.5537 - mean_squared_error: 1.5537 - val_loss: 3.2930 - val_mean_squared_error: 3.2930\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.5147 - mean_squared_error: 1.5147 - val_loss: 189.5401 - val_mean_squared_error: 189.5401\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.5102 - mean_squared_error: 1.5102 - val_loss: 3.5612 - val_mean_squared_error: 3.5612\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.6404 - mean_squared_error: 1.6404 - val_loss: 93.3893 - val_mean_squared_error: 93.3893\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.6809 - mean_squared_error: 1.6809 - val_loss: 3.3630 - val_mean_squared_error: 3.3630\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.4270 - mean_squared_error: 1.4270 - val_loss: 3.2582 - val_mean_squared_error: 3.2582\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.4828 - mean_squared_error: 1.4828 - val_loss: 3.0475 - val_mean_squared_error: 3.0475\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.4277 - mean_squared_error: 1.4277 - val_loss: 3.2704 - val_mean_squared_error: 3.2704\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.4535 - mean_squared_error: 1.4535 - val_loss: 3.3623 - val_mean_squared_error: 3.3623\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.4467 - mean_squared_error: 1.4467 - val_loss: 3.2592 - val_mean_squared_error: 3.2592\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.4261 - mean_squared_error: 1.4261 - val_loss: 23.0925 - val_mean_squared_error: 23.0925\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.4164 - mean_squared_error: 1.4164 - val_loss: 3.6868 - val_mean_squared_error: 3.6868\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.3747 - mean_squared_error: 1.3747 - val_loss: 3.0768 - val_mean_squared_error: 3.0768\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.2822 - mean_squared_error: 1.2822 - val_loss: 3.3127 - val_mean_squared_error: 3.3127\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 800us/sample - loss: 1.4179 - mean_squared_error: 1.4179 - val_loss: 3.3088 - val_mean_squared_error: 3.3088\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.3185 - mean_squared_error: 1.3185 - val_loss: 2.9790 - val_mean_squared_error: 2.9790\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.3469 - mean_squared_error: 1.3469 - val_loss: 3.1244 - val_mean_squared_error: 3.1244\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 806us/sample - loss: 1.3125 - mean_squared_error: 1.3125 - val_loss: 3.1596 - val_mean_squared_error: 3.1596\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.3027 - mean_squared_error: 1.3027 - val_loss: 3.1042 - val_mean_squared_error: 3.1042\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.4066 - mean_squared_error: 1.4066 - val_loss: 3.4124 - val_mean_squared_error: 3.4124\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.3539 - mean_squared_error: 1.3539 - val_loss: 3.4650 - val_mean_squared_error: 3.4650\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.3601 - mean_squared_error: 1.3601 - val_loss: 3.2816 - val_mean_squared_error: 3.2816\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.1958 - mean_squared_error: 1.1958 - val_loss: 3.1614 - val_mean_squared_error: 3.1614\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.1845 - mean_squared_error: 1.1845 - val_loss: 2.8858 - val_mean_squared_error: 2.8858\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.2907 - mean_squared_error: 1.2907 - val_loss: 3.2707 - val_mean_squared_error: 3.2707\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 804us/sample - loss: 1.1965 - mean_squared_error: 1.1965 - val_loss: 3.1280 - val_mean_squared_error: 3.1280\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.2913 - mean_squared_error: 1.2913 - val_loss: 2.9614 - val_mean_squared_error: 2.9614\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.2817 - mean_squared_error: 1.2817 - val_loss: 3.4856 - val_mean_squared_error: 3.4856\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.3095 - mean_squared_error: 1.3095 - val_loss: 3.4866 - val_mean_squared_error: 3.4866\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.2890 - mean_squared_error: 1.2890 - val_loss: 3.1878 - val_mean_squared_error: 3.1878\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.1995 - mean_squared_error: 1.1995 - val_loss: 2.9959 - val_mean_squared_error: 2.9959\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.2233 - mean_squared_error: 1.2233 - val_loss: 3.0650 - val_mean_squared_error: 3.0650\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1787 - mean_squared_error: 1.1787 - val_loss: 3.0646 - val_mean_squared_error: 3.0646\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.2594 - mean_squared_error: 1.2594 - val_loss: 3.0179 - val_mean_squared_error: 3.0179\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.1424 - mean_squared_error: 1.1424 - val_loss: 3.1534 - val_mean_squared_error: 3.1534\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1096 - mean_squared_error: 1.1096 - val_loss: 3.4802 - val_mean_squared_error: 3.4802\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 802us/sample - loss: 1.2354 - mean_squared_error: 1.2354 - val_loss: 3.0880 - val_mean_squared_error: 3.0880\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 1.2006 - mean_squared_error: 1.2006 - val_loss: 3.1025 - val_mean_squared_error: 3.1025\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.0859 - mean_squared_error: 1.0859 - val_loss: 3.0455 - val_mean_squared_error: 3.0455\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1751 - mean_squared_error: 1.1751 - val_loss: 23866.1084 - val_mean_squared_error: 23866.1074\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.0797 - mean_squared_error: 1.0797 - val_loss: 3.2037 - val_mean_squared_error: 3.2037\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 1.1085 - mean_squared_error: 1.1085 - val_loss: 3.0217 - val_mean_squared_error: 3.0217\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.1737 - mean_squared_error: 1.1737 - val_loss: 3.3149 - val_mean_squared_error: 3.3149\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1195 - mean_squared_error: 1.1195 - val_loss: 3.4969 - val_mean_squared_error: 3.4969\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.0210 - mean_squared_error: 1.0210 - val_loss: 3.0631 - val_mean_squared_error: 3.0631\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.1136 - mean_squared_error: 1.1136 - val_loss: 35476.6929 - val_mean_squared_error: 35476.6953\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.1708 - mean_squared_error: 1.1708 - val_loss: 36637.4344 - val_mean_squared_error: 36637.4375\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.0966 - mean_squared_error: 1.0966 - val_loss: 26816.9741 - val_mean_squared_error: 26816.9785\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 802us/sample - loss: 1.0882 - mean_squared_error: 1.0882 - val_loss: 10741.5252 - val_mean_squared_error: 10741.5254\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.0596 - mean_squared_error: 1.0596 - val_loss: 871.0453 - val_mean_squared_error: 871.0453\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.1238 - mean_squared_error: 1.1238 - val_loss: 47610.7632 - val_mean_squared_error: 47610.7656\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 1.0222 - mean_squared_error: 1.0222 - val_loss: 2168.6409 - val_mean_squared_error: 2168.6411\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.0480 - mean_squared_error: 1.0480 - val_loss: 3.0590 - val_mean_squared_error: 3.0590\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.0419 - mean_squared_error: 1.0419 - val_loss: 1526.0547 - val_mean_squared_error: 1526.0546\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.1167 - mean_squared_error: 1.1167 - val_loss: 74284.3815 - val_mean_squared_error: 74284.3828\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 1.0451 - mean_squared_error: 1.0451 - val_loss: 653.7344 - val_mean_squared_error: 653.7343\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 1.0269 - mean_squared_error: 1.0269 - val_loss: 803.2656 - val_mean_squared_error: 803.2655\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1005 - mean_squared_error: 1.1005 - val_loss: 8632.4789 - val_mean_squared_error: 8632.4785\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.0656 - mean_squared_error: 1.0656 - val_loss: 39107.0405 - val_mean_squared_error: 39107.0391\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 0.9551 - mean_squared_error: 0.9551 - val_loss: 109455.5919 - val_mean_squared_error: 109455.5859\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 1.0466 - mean_squared_error: 1.0466 - val_loss: 157.0315 - val_mean_squared_error: 157.0315\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.0861 - mean_squared_error: 1.0861 - val_loss: 45.6992 - val_mean_squared_error: 45.6992\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.0820 - mean_squared_error: 1.0820 - val_loss: 69309.5230 - val_mean_squared_error: 69309.5234\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 1.1094 - mean_squared_error: 1.1094 - val_loss: 374.2673 - val_mean_squared_error: 374.2673\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 808us/sample - loss: 0.9435 - mean_squared_error: 0.9435 - val_loss: 13.2202 - val_mean_squared_error: 13.2202\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.0778 - mean_squared_error: 1.0778 - val_loss: 77229.9601 - val_mean_squared_error: 77229.9609\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 804us/sample - loss: 0.9833 - mean_squared_error: 0.9833 - val_loss: 14.4667 - val_mean_squared_error: 14.4667\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 805us/sample - loss: 1.0565 - mean_squared_error: 1.0565 - val_loss: 2.9156 - val_mean_squared_error: 2.9156\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 0.9660 - mean_squared_error: 0.9660 - val_loss: 194.7321 - val_mean_squared_error: 194.7321\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 0.9015 - mean_squared_error: 0.9015 - val_loss: 47831.3784 - val_mean_squared_error: 47831.3789\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 0.9231 - mean_squared_error: 0.9231 - val_loss: 243.2538 - val_mean_squared_error: 243.2538\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 3.0351 - val_mean_squared_error: 3.0351\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 806us/sample - loss: 0.8930 - mean_squared_error: 0.8930 - val_loss: 3.2066 - val_mean_squared_error: 3.2066\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 0.9374 - mean_squared_error: 0.9374 - val_loss: 16631.5550 - val_mean_squared_error: 16631.5527\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 0.9476 - mean_squared_error: 0.9476 - val_loss: 136685.4978 - val_mean_squared_error: 136685.5000\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 0.9906 - mean_squared_error: 0.9906 - val_loss: 3.0103 - val_mean_squared_error: 3.0103\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 0.9564 - mean_squared_error: 0.9564 - val_loss: 2.8796 - val_mean_squared_error: 2.8796\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 0.9408 - mean_squared_error: 0.9408 - val_loss: 3.9437 - val_mean_squared_error: 3.9437\n",
            "==================================================\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_26 (Conv2D)           (None, 96, 96, 12)        108       \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 96, 96, 12)        1296      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 96, 96, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 48, 48, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 48, 48, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 48, 48, 24)        2592      \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 48, 48, 24)        5184      \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 48, 48, 24)        5184      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 48, 48, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 24, 24, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 24, 24, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 24, 24, 48)        10368     \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 24, 24, 48)        20736     \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 24, 24, 48)        20736     \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 24, 24, 48)        20736     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 24, 24, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 12, 12, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 12, 12, 48)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 12, 12, 96)        41472     \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 12, 12, 96)        82944     \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 12, 12, 96)        82944     \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 12, 12, 96)        82944     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 12, 12, 96)        384       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 6, 6, 96)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 3456)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 500)               1728500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 2,375,994\n",
            "Trainable params: 2,373,634\n",
            "Non-trainable params: 2,360\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 2s 1ms/sample - loss: 295.3867 - mean_squared_error: 295.3867 - val_loss: 9236.9352 - val_mean_squared_error: 9236.9355\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 34.3450 - mean_squared_error: 34.3450 - val_loss: 307.6920 - val_mean_squared_error: 307.6920\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 22.6853 - mean_squared_error: 22.6853 - val_loss: 60.3483 - val_mean_squared_error: 60.3483\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 22.9730 - mean_squared_error: 22.9730 - val_loss: 41.9219 - val_mean_squared_error: 41.9219\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 21.3694 - mean_squared_error: 21.3694 - val_loss: 18.3245 - val_mean_squared_error: 18.3245\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 698us/sample - loss: 17.5316 - mean_squared_error: 17.5316 - val_loss: 14.5705 - val_mean_squared_error: 14.5705\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 14.8686 - mean_squared_error: 14.8686 - val_loss: 11.3434 - val_mean_squared_error: 11.3434\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 15.5418 - mean_squared_error: 15.5418 - val_loss: 10.4975 - val_mean_squared_error: 10.4975\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 14.9424 - mean_squared_error: 14.9424 - val_loss: 16.9278 - val_mean_squared_error: 16.9278\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 14.0260 - mean_squared_error: 14.0260 - val_loss: 11.4719 - val_mean_squared_error: 11.4719\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 13.5565 - mean_squared_error: 13.5565 - val_loss: 10.1585 - val_mean_squared_error: 10.1585\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 13.3641 - mean_squared_error: 13.3641 - val_loss: 10.8497 - val_mean_squared_error: 10.8497\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 12.5227 - mean_squared_error: 12.5227 - val_loss: 10.4622 - val_mean_squared_error: 10.4622\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 12.4065 - mean_squared_error: 12.4065 - val_loss: 10.4789 - val_mean_squared_error: 10.4789\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 11.6474 - mean_squared_error: 11.6474 - val_loss: 9.7779 - val_mean_squared_error: 9.7779\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 674us/sample - loss: 11.5767 - mean_squared_error: 11.5767 - val_loss: 10.5764 - val_mean_squared_error: 10.5764\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 11.7368 - mean_squared_error: 11.7368 - val_loss: 10.4414 - val_mean_squared_error: 10.4414\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 11.1159 - mean_squared_error: 11.1159 - val_loss: 10.0077 - val_mean_squared_error: 10.0077\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 10.8677 - mean_squared_error: 10.8677 - val_loss: 10.0170 - val_mean_squared_error: 10.0170\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 10.6900 - mean_squared_error: 10.6900 - val_loss: 9.8660 - val_mean_squared_error: 9.8660\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 10.4956 - mean_squared_error: 10.4956 - val_loss: 10.1080 - val_mean_squared_error: 10.1080\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 676us/sample - loss: 10.4803 - mean_squared_error: 10.4803 - val_loss: 9.4664 - val_mean_squared_error: 9.4664\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 10.1762 - mean_squared_error: 10.1762 - val_loss: 9.6752 - val_mean_squared_error: 9.6753\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 10.3740 - mean_squared_error: 10.3740 - val_loss: 9.8362 - val_mean_squared_error: 9.8362\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 10.3226 - mean_squared_error: 10.3226 - val_loss: 9.6971 - val_mean_squared_error: 9.6971\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 9.9601 - mean_squared_error: 9.9601 - val_loss: 9.5303 - val_mean_squared_error: 9.5303\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 9.7363 - mean_squared_error: 9.7363 - val_loss: 9.9116 - val_mean_squared_error: 9.9116\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 9.7315 - mean_squared_error: 9.7315 - val_loss: 9.7130 - val_mean_squared_error: 9.7130\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 9.6488 - mean_squared_error: 9.6488 - val_loss: 9.3109 - val_mean_squared_error: 9.3109\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 9.3817 - mean_squared_error: 9.3817 - val_loss: 9.2722 - val_mean_squared_error: 9.2722\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 9.2516 - mean_squared_error: 9.2516 - val_loss: 9.2699 - val_mean_squared_error: 9.2699\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 9.2528 - mean_squared_error: 9.2528 - val_loss: 9.4771 - val_mean_squared_error: 9.4771\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 9.4395 - mean_squared_error: 9.4395 - val_loss: 9.8675 - val_mean_squared_error: 9.8675\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 9.2201 - mean_squared_error: 9.2201 - val_loss: 9.4991 - val_mean_squared_error: 9.4991\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 8.9449 - mean_squared_error: 8.9449 - val_loss: 8.7681 - val_mean_squared_error: 8.7681\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 8.7441 - mean_squared_error: 8.7441 - val_loss: 8.7866 - val_mean_squared_error: 8.7866\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 8.5029 - mean_squared_error: 8.5029 - val_loss: 9.9076 - val_mean_squared_error: 9.9076\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 8.6304 - mean_squared_error: 8.6304 - val_loss: 9.7449 - val_mean_squared_error: 9.7449\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 8.4540 - mean_squared_error: 8.4540 - val_loss: 9.0485 - val_mean_squared_error: 9.0485\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 8.2488 - mean_squared_error: 8.2488 - val_loss: 8.6751 - val_mean_squared_error: 8.6751\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 8.1513 - mean_squared_error: 8.1513 - val_loss: 8.9309 - val_mean_squared_error: 8.9309\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 7.9815 - mean_squared_error: 7.9815 - val_loss: 8.9913 - val_mean_squared_error: 8.9913\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 7.8932 - mean_squared_error: 7.8932 - val_loss: 8.6362 - val_mean_squared_error: 8.6362\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 7.7659 - mean_squared_error: 7.7659 - val_loss: 8.1110 - val_mean_squared_error: 8.1110\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 7.8150 - mean_squared_error: 7.8150 - val_loss: 8.3800 - val_mean_squared_error: 8.3800\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 7.7551 - mean_squared_error: 7.7551 - val_loss: 7.9684 - val_mean_squared_error: 7.9684\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 7.7872 - mean_squared_error: 7.7872 - val_loss: 8.3191 - val_mean_squared_error: 8.3191\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 7.3031 - mean_squared_error: 7.3031 - val_loss: 8.7178 - val_mean_squared_error: 8.7178\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 7.1055 - mean_squared_error: 7.1055 - val_loss: 7.8495 - val_mean_squared_error: 7.8495\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 7.1257 - mean_squared_error: 7.1257 - val_loss: 8.1509 - val_mean_squared_error: 8.1509\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 6.9093 - mean_squared_error: 6.9093 - val_loss: 234.1343 - val_mean_squared_error: 234.1343\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 6.6858 - mean_squared_error: 6.6858 - val_loss: 8.2309 - val_mean_squared_error: 8.2309\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 6.6403 - mean_squared_error: 6.6403 - val_loss: 7.7484 - val_mean_squared_error: 7.7484\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 6.3503 - mean_squared_error: 6.3503 - val_loss: 8.0442 - val_mean_squared_error: 8.0442\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 6.2079 - mean_squared_error: 6.2079 - val_loss: 7.4301 - val_mean_squared_error: 7.4301\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 5.9782 - mean_squared_error: 5.9782 - val_loss: 15.7927 - val_mean_squared_error: 15.7927\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 5.7692 - mean_squared_error: 5.7692 - val_loss: 19.1691 - val_mean_squared_error: 19.1691\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 5.6887 - mean_squared_error: 5.6887 - val_loss: 7.2128 - val_mean_squared_error: 7.2128\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 5.4483 - mean_squared_error: 5.4483 - val_loss: 6.5582 - val_mean_squared_error: 6.5582\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 5.2268 - mean_squared_error: 5.2268 - val_loss: 5.7805 - val_mean_squared_error: 5.7805\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 5.0578 - mean_squared_error: 5.0578 - val_loss: 5.9147 - val_mean_squared_error: 5.9147\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 4.8533 - mean_squared_error: 4.8533 - val_loss: 6.2439 - val_mean_squared_error: 6.2439\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 4.6874 - mean_squared_error: 4.6874 - val_loss: 6.1701 - val_mean_squared_error: 6.1701\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 4.5953 - mean_squared_error: 4.5953 - val_loss: 5.3672 - val_mean_squared_error: 5.3672\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 679us/sample - loss: 4.5002 - mean_squared_error: 4.5002 - val_loss: 4.9741 - val_mean_squared_error: 4.9741\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 4.2151 - mean_squared_error: 4.2151 - val_loss: 5.1882 - val_mean_squared_error: 5.1882\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 4.0288 - mean_squared_error: 4.0288 - val_loss: 5.2679 - val_mean_squared_error: 5.2679\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 4.0107 - mean_squared_error: 4.0107 - val_loss: 5.0223 - val_mean_squared_error: 5.0223\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 3.9471 - mean_squared_error: 3.9471 - val_loss: 4.8451 - val_mean_squared_error: 4.8451\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 3.9406 - mean_squared_error: 3.9406 - val_loss: 4.9823 - val_mean_squared_error: 4.9823\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 678us/sample - loss: 3.9192 - mean_squared_error: 3.9192 - val_loss: 4.7936 - val_mean_squared_error: 4.7936\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 3.6442 - mean_squared_error: 3.6442 - val_loss: 5.1146 - val_mean_squared_error: 5.1146\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 675us/sample - loss: 3.5257 - mean_squared_error: 3.5257 - val_loss: 4.6329 - val_mean_squared_error: 4.6329\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 3.4635 - mean_squared_error: 3.4635 - val_loss: 4.8771 - val_mean_squared_error: 4.8771\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 3.3157 - mean_squared_error: 3.3157 - val_loss: 4.5077 - val_mean_squared_error: 4.5077\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 3.3428 - mean_squared_error: 3.3428 - val_loss: 4.8979 - val_mean_squared_error: 4.8979\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 3.5848 - mean_squared_error: 3.5848 - val_loss: 4.5981 - val_mean_squared_error: 4.5981\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 3.2131 - mean_squared_error: 3.2131 - val_loss: 4.3424 - val_mean_squared_error: 4.3424\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 3.1149 - mean_squared_error: 3.1149 - val_loss: 6.1179 - val_mean_squared_error: 6.1179\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 3.1213 - mean_squared_error: 3.1213 - val_loss: 4.4562 - val_mean_squared_error: 4.4562\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 2.8890 - mean_squared_error: 2.8890 - val_loss: 4.2072 - val_mean_squared_error: 4.2072\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.9170 - mean_squared_error: 2.9170 - val_loss: 7.2147 - val_mean_squared_error: 7.2147\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 2.7476 - mean_squared_error: 2.7476 - val_loss: 4.3545 - val_mean_squared_error: 4.3545\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 2.7857 - mean_squared_error: 2.7857 - val_loss: 4.3531 - val_mean_squared_error: 4.3531\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.7746 - mean_squared_error: 2.7746 - val_loss: 4.3065 - val_mean_squared_error: 4.3065\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.7317 - mean_squared_error: 2.7317 - val_loss: 4.2460 - val_mean_squared_error: 4.2460\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 2.6136 - mean_squared_error: 2.6136 - val_loss: 4.1005 - val_mean_squared_error: 4.1005\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.5347 - mean_squared_error: 2.5347 - val_loss: 4.3891 - val_mean_squared_error: 4.3891\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 2.5078 - mean_squared_error: 2.5078 - val_loss: 4.3924 - val_mean_squared_error: 4.3924\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.6462 - mean_squared_error: 2.6462 - val_loss: 4.1673 - val_mean_squared_error: 4.1673\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 2.5379 - mean_squared_error: 2.5379 - val_loss: 4.0192 - val_mean_squared_error: 4.0192\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.4444 - mean_squared_error: 2.4444 - val_loss: 4.5141 - val_mean_squared_error: 4.5141\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 2.4897 - mean_squared_error: 2.4897 - val_loss: 3.9326 - val_mean_squared_error: 3.9326\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 2.3469 - mean_squared_error: 2.3469 - val_loss: 4.3309 - val_mean_squared_error: 4.3309\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.2185 - mean_squared_error: 2.2185 - val_loss: 4.0538 - val_mean_squared_error: 4.0538\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 2.2107 - mean_squared_error: 2.2107 - val_loss: 3.8282 - val_mean_squared_error: 3.8282\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 2.1780 - mean_squared_error: 2.1780 - val_loss: 3.7901 - val_mean_squared_error: 3.7901\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 2.2037 - mean_squared_error: 2.2037 - val_loss: 3.9106 - val_mean_squared_error: 3.9106\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 2.2573 - mean_squared_error: 2.2573 - val_loss: 4.0447 - val_mean_squared_error: 4.0447\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 2.2284 - mean_squared_error: 2.2284 - val_loss: 4.0855 - val_mean_squared_error: 4.0855\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.1440 - mean_squared_error: 2.1440 - val_loss: 4.0782 - val_mean_squared_error: 4.0782\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 2.0369 - mean_squared_error: 2.0369 - val_loss: 81.6499 - val_mean_squared_error: 81.6499\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 1.9784 - mean_squared_error: 1.9784 - val_loss: 4.0817 - val_mean_squared_error: 4.0817\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 664us/sample - loss: 1.9802 - mean_squared_error: 1.9802 - val_loss: 65.9302 - val_mean_squared_error: 65.9302\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 2.0484 - mean_squared_error: 2.0484 - val_loss: 7808.8561 - val_mean_squared_error: 7808.8560\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 2.0533 - mean_squared_error: 2.0533 - val_loss: 441.2715 - val_mean_squared_error: 441.2714\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.8967 - mean_squared_error: 1.8967 - val_loss: 4.0916 - val_mean_squared_error: 4.0916\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.8581 - mean_squared_error: 1.8581 - val_loss: 3.8710 - val_mean_squared_error: 3.8710\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.9011 - mean_squared_error: 1.9011 - val_loss: 3.7950 - val_mean_squared_error: 3.7950\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.8702 - mean_squared_error: 1.8702 - val_loss: 3.8003 - val_mean_squared_error: 3.8003\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.8868 - mean_squared_error: 1.8868 - val_loss: 4.1000 - val_mean_squared_error: 4.1000\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.8666 - mean_squared_error: 1.8666 - val_loss: 4.0004 - val_mean_squared_error: 4.0004\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.9491 - mean_squared_error: 1.9491 - val_loss: 3.8031 - val_mean_squared_error: 3.8031\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 674us/sample - loss: 1.8564 - mean_squared_error: 1.8564 - val_loss: 4.0019 - val_mean_squared_error: 4.0019\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.8291 - mean_squared_error: 1.8291 - val_loss: 3.8625 - val_mean_squared_error: 3.8625\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.8748 - mean_squared_error: 1.8748 - val_loss: 3.8756 - val_mean_squared_error: 3.8756\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.8084 - mean_squared_error: 1.8084 - val_loss: 3.7936 - val_mean_squared_error: 3.7936\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.8669 - mean_squared_error: 1.8669 - val_loss: 2633.0763 - val_mean_squared_error: 2633.0762\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.7266 - mean_squared_error: 1.7266 - val_loss: 3.8110 - val_mean_squared_error: 3.8110\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.6579 - mean_squared_error: 1.6579 - val_loss: 3.5687 - val_mean_squared_error: 3.5687\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.6801 - mean_squared_error: 1.6801 - val_loss: 3.6569 - val_mean_squared_error: 3.6569\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.5550 - mean_squared_error: 1.5550 - val_loss: 3.6782 - val_mean_squared_error: 3.6782\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 1.5991 - mean_squared_error: 1.5991 - val_loss: 3.7250 - val_mean_squared_error: 3.7250\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 674us/sample - loss: 1.6562 - mean_squared_error: 1.6562 - val_loss: 3.7960 - val_mean_squared_error: 3.7960\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.6527 - mean_squared_error: 1.6527 - val_loss: 3.7688 - val_mean_squared_error: 3.7688\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.6494 - mean_squared_error: 1.6494 - val_loss: 3.5124 - val_mean_squared_error: 3.5124\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.6948 - mean_squared_error: 1.6948 - val_loss: 3.6836 - val_mean_squared_error: 3.6836\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.7256 - mean_squared_error: 1.7256 - val_loss: 86.6729 - val_mean_squared_error: 86.6729\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5743 - mean_squared_error: 1.5743 - val_loss: 3.6062 - val_mean_squared_error: 3.6062\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.6584 - mean_squared_error: 1.6584 - val_loss: 3.5668 - val_mean_squared_error: 3.5668\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.6191 - mean_squared_error: 1.6191 - val_loss: 3.7927 - val_mean_squared_error: 3.7927\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.5214 - mean_squared_error: 1.5214 - val_loss: 3.7203 - val_mean_squared_error: 3.7203\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.6150 - mean_squared_error: 1.6150 - val_loss: 3.5951 - val_mean_squared_error: 3.5951\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5064 - mean_squared_error: 1.5064 - val_loss: 3.7012 - val_mean_squared_error: 3.7012\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.4827 - mean_squared_error: 1.4827 - val_loss: 9.7069 - val_mean_squared_error: 9.7069\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.4519 - mean_squared_error: 1.4519 - val_loss: 3.7172 - val_mean_squared_error: 3.7172\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.4206 - mean_squared_error: 1.4206 - val_loss: 3.5244 - val_mean_squared_error: 3.5244\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.5409 - mean_squared_error: 1.5409 - val_loss: 3.3099 - val_mean_squared_error: 3.3099\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.3786 - mean_squared_error: 1.3786 - val_loss: 3.8577 - val_mean_squared_error: 3.8577\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.4273 - mean_squared_error: 1.4273 - val_loss: 3.6116 - val_mean_squared_error: 3.6116\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 1.3729 - mean_squared_error: 1.3729 - val_loss: 3.7245 - val_mean_squared_error: 3.7245\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.4169 - mean_squared_error: 1.4169 - val_loss: 2717.3588 - val_mean_squared_error: 2717.3591\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.3880 - mean_squared_error: 1.3880 - val_loss: 3.4216 - val_mean_squared_error: 3.4216\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2965 - mean_squared_error: 1.2965 - val_loss: 1753.7319 - val_mean_squared_error: 1753.7317\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.3783 - mean_squared_error: 1.3783 - val_loss: 141.2584 - val_mean_squared_error: 141.2584\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.3444 - mean_squared_error: 1.3444 - val_loss: 3.3636 - val_mean_squared_error: 3.3636\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.2446 - mean_squared_error: 1.2446 - val_loss: 3.3369 - val_mean_squared_error: 3.3369\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.3954 - mean_squared_error: 1.3954 - val_loss: 3.4686 - val_mean_squared_error: 3.4686\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.3657 - mean_squared_error: 1.3657 - val_loss: 3.5965 - val_mean_squared_error: 3.5965\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2750 - mean_squared_error: 1.2750 - val_loss: 3.7833 - val_mean_squared_error: 3.7833\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.3049 - mean_squared_error: 1.3049 - val_loss: 3.2883 - val_mean_squared_error: 3.2883\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.2864 - mean_squared_error: 1.2864 - val_loss: 3.4148 - val_mean_squared_error: 3.4148\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2661 - mean_squared_error: 1.2661 - val_loss: 3.3693 - val_mean_squared_error: 3.3693\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.2629 - mean_squared_error: 1.2629 - val_loss: 3.3451 - val_mean_squared_error: 3.3451\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.2083 - mean_squared_error: 1.2083 - val_loss: 3.4906 - val_mean_squared_error: 3.4906\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.1774 - mean_squared_error: 1.1774 - val_loss: 3.2732 - val_mean_squared_error: 3.2732\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.2880 - mean_squared_error: 1.2880 - val_loss: 3.4267 - val_mean_squared_error: 3.4267\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.2194 - mean_squared_error: 1.2194 - val_loss: 3.7043 - val_mean_squared_error: 3.7043\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1976 - mean_squared_error: 1.1976 - val_loss: 3.4881 - val_mean_squared_error: 3.4881\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.2316 - mean_squared_error: 1.2316 - val_loss: 3.4473 - val_mean_squared_error: 3.4473\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.2108 - mean_squared_error: 1.2108 - val_loss: 3.5971 - val_mean_squared_error: 3.5971\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.1832 - mean_squared_error: 1.1832 - val_loss: 3.3377 - val_mean_squared_error: 3.3377\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.2426 - mean_squared_error: 1.2426 - val_loss: 3.4465 - val_mean_squared_error: 3.4465\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.1531 - mean_squared_error: 1.1531 - val_loss: 3.4300 - val_mean_squared_error: 3.4300\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.1578 - mean_squared_error: 1.1578 - val_loss: 3.3806 - val_mean_squared_error: 3.3806\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.1616 - mean_squared_error: 1.1616 - val_loss: 3.5081 - val_mean_squared_error: 3.5081\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.1591 - mean_squared_error: 1.1591 - val_loss: 3.2522 - val_mean_squared_error: 3.2522\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1844 - mean_squared_error: 1.1844 - val_loss: 3.4088 - val_mean_squared_error: 3.4088\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.1224 - mean_squared_error: 1.1224 - val_loss: 3.3923 - val_mean_squared_error: 3.3923\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1540 - mean_squared_error: 1.1540 - val_loss: 3.2753 - val_mean_squared_error: 3.2753\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1488 - mean_squared_error: 1.1488 - val_loss: 3.4675 - val_mean_squared_error: 3.4675\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 1.0757 - mean_squared_error: 1.0757 - val_loss: 3.3525 - val_mean_squared_error: 3.3525\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.1425 - mean_squared_error: 1.1425 - val_loss: 3.2989 - val_mean_squared_error: 3.2989\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0608 - mean_squared_error: 1.0608 - val_loss: 3.4753 - val_mean_squared_error: 3.4753\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 1.1655 - mean_squared_error: 1.1655 - val_loss: 3.2971 - val_mean_squared_error: 3.2971\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 665us/sample - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 3.3095 - val_mean_squared_error: 3.3095\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.0244 - mean_squared_error: 1.0244 - val_loss: 3.2989 - val_mean_squared_error: 3.2989\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.0577 - mean_squared_error: 1.0577 - val_loss: 3.3215 - val_mean_squared_error: 3.3215\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.1047 - mean_squared_error: 1.1047 - val_loss: 63.3033 - val_mean_squared_error: 63.3033\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 672us/sample - loss: 1.1211 - mean_squared_error: 1.1211 - val_loss: 3.3903 - val_mean_squared_error: 3.3903\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 679us/sample - loss: 1.0809 - mean_squared_error: 1.0809 - val_loss: 3.3136 - val_mean_squared_error: 3.3136\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.1437 - mean_squared_error: 1.1437 - val_loss: 3.3838 - val_mean_squared_error: 3.3838\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.0242 - mean_squared_error: 1.0242 - val_loss: 3.1915 - val_mean_squared_error: 3.1915\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.1158 - mean_squared_error: 1.1158 - val_loss: 3.5232 - val_mean_squared_error: 3.5232\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.0650 - mean_squared_error: 1.0650 - val_loss: 3.2551 - val_mean_squared_error: 3.2551\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.1304 - mean_squared_error: 1.1304 - val_loss: 4.1868 - val_mean_squared_error: 4.1868\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 0.9985 - mean_squared_error: 0.9985 - val_loss: 3.1764 - val_mean_squared_error: 3.1764\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 0.9713 - mean_squared_error: 0.9713 - val_loss: 3.3239 - val_mean_squared_error: 3.3239\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 673us/sample - loss: 1.0584 - mean_squared_error: 1.0584 - val_loss: 3.2343 - val_mean_squared_error: 3.2343\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 1.0020 - mean_squared_error: 1.0020 - val_loss: 3.2821 - val_mean_squared_error: 3.2821\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.0206 - mean_squared_error: 1.0206 - val_loss: 3.2058 - val_mean_squared_error: 3.2058\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0511 - mean_squared_error: 1.0511 - val_loss: 3.1859 - val_mean_squared_error: 3.1859\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 670us/sample - loss: 1.0539 - mean_squared_error: 1.0539 - val_loss: 3.3138 - val_mean_squared_error: 3.3138\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 1.0323 - mean_squared_error: 1.0323 - val_loss: 3.3585 - val_mean_squared_error: 3.3585\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 0.9931 - mean_squared_error: 0.9931 - val_loss: 3.3982 - val_mean_squared_error: 3.3982\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 1s 671us/sample - loss: 0.9934 - mean_squared_error: 0.9934 - val_loss: 3.6045 - val_mean_squared_error: 3.6045\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 668us/sample - loss: 1.0941 - mean_squared_error: 1.0941 - val_loss: 3.3856 - val_mean_squared_error: 3.3856\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 669us/sample - loss: 0.9587 - mean_squared_error: 0.9587 - val_loss: 3.1559 - val_mean_squared_error: 3.1559\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 667us/sample - loss: 1.0729 - mean_squared_error: 1.0729 - val_loss: 3.5053 - val_mean_squared_error: 3.5053\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 666us/sample - loss: 1.0862 - mean_squared_error: 1.0862 - val_loss: 3.4565 - val_mean_squared_error: 3.4565\n",
            "==================================================\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_39 (Conv2D)           (None, 96, 96, 16)        144       \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 96, 96, 16)        2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 96, 96, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 48, 48, 32)        4608      \n",
            "_________________________________________________________________\n",
            "conv2d_42 (Conv2D)           (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 24, 24, 64)        18432     \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 12, 12, 128)       73728     \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 500)               2304500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,245,598\n",
            "Trainable params: 3,243,118\n",
            "Non-trainable params: 2,480\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 1819 samples, validate on 321 samples\n",
            "Epoch 1/200\n",
            "1819/1819 [==============================] - 3s 1ms/sample - loss: 273.8910 - mean_squared_error: 273.8911 - val_loss: 13333.6746 - val_mean_squared_error: 13333.6748\n",
            "Epoch 2/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 24.4881 - mean_squared_error: 24.4881 - val_loss: 109.3339 - val_mean_squared_error: 109.3339\n",
            "Epoch 3/200\n",
            "1819/1819 [==============================] - 2s 826us/sample - loss: 16.6651 - mean_squared_error: 16.6651 - val_loss: 60.4410 - val_mean_squared_error: 60.4410\n",
            "Epoch 4/200\n",
            "1819/1819 [==============================] - 2s 836us/sample - loss: 17.0565 - mean_squared_error: 17.0565 - val_loss: 33.8199 - val_mean_squared_error: 33.8199\n",
            "Epoch 5/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 15.1106 - mean_squared_error: 15.1106 - val_loss: 22.3857 - val_mean_squared_error: 22.3857\n",
            "Epoch 6/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 14.1056 - mean_squared_error: 14.1056 - val_loss: 23.4611 - val_mean_squared_error: 23.4611\n",
            "Epoch 7/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 15.3339 - mean_squared_error: 15.3339 - val_loss: 14.3752 - val_mean_squared_error: 14.3752\n",
            "Epoch 8/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 13.6443 - mean_squared_error: 13.6443 - val_loss: 13.4047 - val_mean_squared_error: 13.4047\n",
            "Epoch 9/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 12.8736 - mean_squared_error: 12.8736 - val_loss: 14.8578 - val_mean_squared_error: 14.8578\n",
            "Epoch 10/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 12.6316 - mean_squared_error: 12.6316 - val_loss: 11.7870 - val_mean_squared_error: 11.7870\n",
            "Epoch 11/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 12.4650 - mean_squared_error: 12.4650 - val_loss: 11.8320 - val_mean_squared_error: 11.8320\n",
            "Epoch 12/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 12.4836 - mean_squared_error: 12.4836 - val_loss: 13.8243 - val_mean_squared_error: 13.8243\n",
            "Epoch 13/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 11.8246 - mean_squared_error: 11.8246 - val_loss: 14.0607 - val_mean_squared_error: 14.0606\n",
            "Epoch 14/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 11.5956 - mean_squared_error: 11.5956 - val_loss: 11.9650 - val_mean_squared_error: 11.9650\n",
            "Epoch 15/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 11.6570 - mean_squared_error: 11.6570 - val_loss: 10.5140 - val_mean_squared_error: 10.5140\n",
            "Epoch 16/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 11.6803 - mean_squared_error: 11.6803 - val_loss: 9.8737 - val_mean_squared_error: 9.8737\n",
            "Epoch 17/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 11.3569 - mean_squared_error: 11.3569 - val_loss: 10.3258 - val_mean_squared_error: 10.3258\n",
            "Epoch 18/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 11.3322 - mean_squared_error: 11.3322 - val_loss: 10.3475 - val_mean_squared_error: 10.3475\n",
            "Epoch 19/200\n",
            "1819/1819 [==============================] - 2s 825us/sample - loss: 11.1314 - mean_squared_error: 11.1314 - val_loss: 11.4749 - val_mean_squared_error: 11.4749\n",
            "Epoch 20/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 10.7471 - mean_squared_error: 10.7471 - val_loss: 10.4630 - val_mean_squared_error: 10.4630\n",
            "Epoch 21/200\n",
            "1819/1819 [==============================] - 2s 830us/sample - loss: 11.2733 - mean_squared_error: 11.2733 - val_loss: 10.1211 - val_mean_squared_error: 10.1211\n",
            "Epoch 22/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 10.7494 - mean_squared_error: 10.7494 - val_loss: 10.4536 - val_mean_squared_error: 10.4536\n",
            "Epoch 23/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 10.2762 - mean_squared_error: 10.2762 - val_loss: 9.6162 - val_mean_squared_error: 9.6162\n",
            "Epoch 24/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 11.2734 - mean_squared_error: 11.2734 - val_loss: 11.8124 - val_mean_squared_error: 11.8124\n",
            "Epoch 25/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 10.4387 - mean_squared_error: 10.4387 - val_loss: 10.6317 - val_mean_squared_error: 10.6317\n",
            "Epoch 26/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 10.3559 - mean_squared_error: 10.3559 - val_loss: 9.6675 - val_mean_squared_error: 9.6675\n",
            "Epoch 27/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 10.0784 - mean_squared_error: 10.0784 - val_loss: 9.5796 - val_mean_squared_error: 9.5796\n",
            "Epoch 28/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 10.0547 - mean_squared_error: 10.0547 - val_loss: 9.6371 - val_mean_squared_error: 9.6371\n",
            "Epoch 29/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 10.0353 - mean_squared_error: 10.0353 - val_loss: 10.0584 - val_mean_squared_error: 10.0584\n",
            "Epoch 30/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 9.6667 - mean_squared_error: 9.6667 - val_loss: 9.5338 - val_mean_squared_error: 9.5338\n",
            "Epoch 31/200\n",
            "1819/1819 [==============================] - 2s 827us/sample - loss: 9.8177 - mean_squared_error: 9.8177 - val_loss: 9.6866 - val_mean_squared_error: 9.6866\n",
            "Epoch 32/200\n",
            "1819/1819 [==============================] - 2s 829us/sample - loss: 9.9144 - mean_squared_error: 9.9144 - val_loss: 9.7697 - val_mean_squared_error: 9.7697\n",
            "Epoch 33/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 9.5503 - mean_squared_error: 9.5503 - val_loss: 11.0688 - val_mean_squared_error: 11.0688\n",
            "Epoch 34/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 9.2082 - mean_squared_error: 9.2082 - val_loss: 9.3016 - val_mean_squared_error: 9.3016\n",
            "Epoch 35/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 9.2054 - mean_squared_error: 9.2054 - val_loss: 8.8322 - val_mean_squared_error: 8.8322\n",
            "Epoch 36/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 9.1511 - mean_squared_error: 9.1511 - val_loss: 9.3421 - val_mean_squared_error: 9.3421\n",
            "Epoch 37/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 8.8838 - mean_squared_error: 8.8838 - val_loss: 9.5361 - val_mean_squared_error: 9.5361\n",
            "Epoch 38/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 8.8126 - mean_squared_error: 8.8126 - val_loss: 11.6237 - val_mean_squared_error: 11.6237\n",
            "Epoch 39/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 8.6780 - mean_squared_error: 8.6780 - val_loss: 8.5768 - val_mean_squared_error: 8.5768\n",
            "Epoch 40/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 8.8054 - mean_squared_error: 8.8054 - val_loss: 11.8476 - val_mean_squared_error: 11.8476\n",
            "Epoch 41/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 8.3949 - mean_squared_error: 8.3949 - val_loss: 22.4104 - val_mean_squared_error: 22.4104\n",
            "Epoch 42/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 8.2722 - mean_squared_error: 8.2722 - val_loss: 12.2989 - val_mean_squared_error: 12.2989\n",
            "Epoch 43/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 8.0750 - mean_squared_error: 8.0750 - val_loss: 9.7273 - val_mean_squared_error: 9.7273\n",
            "Epoch 44/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 8.3334 - mean_squared_error: 8.3334 - val_loss: 9.5266 - val_mean_squared_error: 9.5266\n",
            "Epoch 45/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 7.8824 - mean_squared_error: 7.8824 - val_loss: 11.5723 - val_mean_squared_error: 11.5723\n",
            "Epoch 46/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 7.7350 - mean_squared_error: 7.7350 - val_loss: 8.5104 - val_mean_squared_error: 8.5104\n",
            "Epoch 47/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 7.7732 - mean_squared_error: 7.7732 - val_loss: 8.4390 - val_mean_squared_error: 8.4390\n",
            "Epoch 48/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 7.5493 - mean_squared_error: 7.5493 - val_loss: 9.8222 - val_mean_squared_error: 9.8222\n",
            "Epoch 49/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 7.2115 - mean_squared_error: 7.2115 - val_loss: 8.4051 - val_mean_squared_error: 8.4051\n",
            "Epoch 50/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 7.1819 - mean_squared_error: 7.1819 - val_loss: 19.3905 - val_mean_squared_error: 19.3905\n",
            "Epoch 51/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 8.1758 - mean_squared_error: 8.1758 - val_loss: 15.4524 - val_mean_squared_error: 15.4524\n",
            "Epoch 52/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 7.3415 - mean_squared_error: 7.3415 - val_loss: 8.2965 - val_mean_squared_error: 8.2965\n",
            "Epoch 53/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 7.0305 - mean_squared_error: 7.0305 - val_loss: 8.4319 - val_mean_squared_error: 8.4319\n",
            "Epoch 54/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 7.0299 - mean_squared_error: 7.0299 - val_loss: 7.8114 - val_mean_squared_error: 7.8114\n",
            "Epoch 55/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 6.4924 - mean_squared_error: 6.4924 - val_loss: 7.3388 - val_mean_squared_error: 7.3388\n",
            "Epoch 56/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 6.5927 - mean_squared_error: 6.5927 - val_loss: 7.4257 - val_mean_squared_error: 7.4257\n",
            "Epoch 57/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 6.4847 - mean_squared_error: 6.4847 - val_loss: 6.8693 - val_mean_squared_error: 6.8693\n",
            "Epoch 58/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 6.2490 - mean_squared_error: 6.2490 - val_loss: 7.2853 - val_mean_squared_error: 7.2853\n",
            "Epoch 59/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 6.0613 - mean_squared_error: 6.0613 - val_loss: 6.4082 - val_mean_squared_error: 6.4082\n",
            "Epoch 60/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 5.6935 - mean_squared_error: 5.6935 - val_loss: 6.7505 - val_mean_squared_error: 6.7505\n",
            "Epoch 61/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 5.6614 - mean_squared_error: 5.6614 - val_loss: 6.6185 - val_mean_squared_error: 6.6185\n",
            "Epoch 62/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 5.4064 - mean_squared_error: 5.4064 - val_loss: 5.9116 - val_mean_squared_error: 5.9116\n",
            "Epoch 63/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 5.1849 - mean_squared_error: 5.1849 - val_loss: 5.8456 - val_mean_squared_error: 5.8456\n",
            "Epoch 64/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 5.1730 - mean_squared_error: 5.1730 - val_loss: 6.6560 - val_mean_squared_error: 6.6560\n",
            "Epoch 65/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 4.9872 - mean_squared_error: 4.9872 - val_loss: 6.2057 - val_mean_squared_error: 6.2057\n",
            "Epoch 66/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 4.7201 - mean_squared_error: 4.7201 - val_loss: 6.0045 - val_mean_squared_error: 6.0045\n",
            "Epoch 67/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 4.6621 - mean_squared_error: 4.6621 - val_loss: 7.2491 - val_mean_squared_error: 7.2491\n",
            "Epoch 68/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 4.4081 - mean_squared_error: 4.4081 - val_loss: 4.9894 - val_mean_squared_error: 4.9894\n",
            "Epoch 69/200\n",
            "1819/1819 [==============================] - 1s 809us/sample - loss: 4.4326 - mean_squared_error: 4.4326 - val_loss: 5.0959 - val_mean_squared_error: 5.0959\n",
            "Epoch 70/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 4.2986 - mean_squared_error: 4.2986 - val_loss: 6.6536 - val_mean_squared_error: 6.6536\n",
            "Epoch 71/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 4.0634 - mean_squared_error: 4.0634 - val_loss: 11.5008 - val_mean_squared_error: 11.5008\n",
            "Epoch 72/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 7.6980 - mean_squared_error: 7.6980 - val_loss: 10917.2391 - val_mean_squared_error: 10917.2393\n",
            "Epoch 73/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 6.7963 - mean_squared_error: 6.7963 - val_loss: 197.6106 - val_mean_squared_error: 197.6106\n",
            "Epoch 74/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 5.9647 - mean_squared_error: 5.9647 - val_loss: 10.9257 - val_mean_squared_error: 10.9257\n",
            "Epoch 75/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 5.1556 - mean_squared_error: 5.1556 - val_loss: 6.4147 - val_mean_squared_error: 6.4147\n",
            "Epoch 76/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 4.8592 - mean_squared_error: 4.8591 - val_loss: 7.2277 - val_mean_squared_error: 7.2277\n",
            "Epoch 77/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 4.6324 - mean_squared_error: 4.6324 - val_loss: 6.1691 - val_mean_squared_error: 6.1691\n",
            "Epoch 78/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 4.1926 - mean_squared_error: 4.1926 - val_loss: 5.2707 - val_mean_squared_error: 5.2707\n",
            "Epoch 79/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 4.0394 - mean_squared_error: 4.0394 - val_loss: 5.6335 - val_mean_squared_error: 5.6335\n",
            "Epoch 80/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 3.9595 - mean_squared_error: 3.9595 - val_loss: 4.9833 - val_mean_squared_error: 4.9833\n",
            "Epoch 81/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 3.9636 - mean_squared_error: 3.9636 - val_loss: 5.5737 - val_mean_squared_error: 5.5737\n",
            "Epoch 82/200\n",
            "1819/1819 [==============================] - 2s 825us/sample - loss: 3.8397 - mean_squared_error: 3.8397 - val_loss: 4.7955 - val_mean_squared_error: 4.7955\n",
            "Epoch 83/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 3.5402 - mean_squared_error: 3.5402 - val_loss: 4.8618 - val_mean_squared_error: 4.8618\n",
            "Epoch 84/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 3.6123 - mean_squared_error: 3.6123 - val_loss: 4.3778 - val_mean_squared_error: 4.3778\n",
            "Epoch 85/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 3.4475 - mean_squared_error: 3.4475 - val_loss: 4.6123 - val_mean_squared_error: 4.6123\n",
            "Epoch 86/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 3.4831 - mean_squared_error: 3.4831 - val_loss: 5.3407 - val_mean_squared_error: 5.3407\n",
            "Epoch 87/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 3.3289 - mean_squared_error: 3.3289 - val_loss: 4.3764 - val_mean_squared_error: 4.3764\n",
            "Epoch 88/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 3.1760 - mean_squared_error: 3.1760 - val_loss: 4.8212 - val_mean_squared_error: 4.8212\n",
            "Epoch 89/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 3.2209 - mean_squared_error: 3.2209 - val_loss: 4.5773 - val_mean_squared_error: 4.5773\n",
            "Epoch 90/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 3.1871 - mean_squared_error: 3.1871 - val_loss: 4.5220 - val_mean_squared_error: 4.5220\n",
            "Epoch 91/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 3.1591 - mean_squared_error: 3.1591 - val_loss: 4.2387 - val_mean_squared_error: 4.2387\n",
            "Epoch 92/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 2.9948 - mean_squared_error: 2.9948 - val_loss: 4.3131 - val_mean_squared_error: 4.3131\n",
            "Epoch 93/200\n",
            "1819/1819 [==============================] - 2s 832us/sample - loss: 3.0435 - mean_squared_error: 3.0435 - val_loss: 4.6640 - val_mean_squared_error: 4.6640\n",
            "Epoch 94/200\n",
            "1819/1819 [==============================] - 2s 832us/sample - loss: 2.9250 - mean_squared_error: 2.9250 - val_loss: 4.5259 - val_mean_squared_error: 4.5259\n",
            "Epoch 95/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 2.9304 - mean_squared_error: 2.9304 - val_loss: 4.6000 - val_mean_squared_error: 4.6000\n",
            "Epoch 96/200\n",
            "1819/1819 [==============================] - 2s 825us/sample - loss: 2.7874 - mean_squared_error: 2.7874 - val_loss: 4.1387 - val_mean_squared_error: 4.1387\n",
            "Epoch 97/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 2.7502 - mean_squared_error: 2.7502 - val_loss: 4.1759 - val_mean_squared_error: 4.1759\n",
            "Epoch 98/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 2.6403 - mean_squared_error: 2.6403 - val_loss: 4.3508 - val_mean_squared_error: 4.3508\n",
            "Epoch 99/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 2.6599 - mean_squared_error: 2.6599 - val_loss: 4.1684 - val_mean_squared_error: 4.1684\n",
            "Epoch 100/200\n",
            "1819/1819 [==============================] - 2s 825us/sample - loss: 2.6152 - mean_squared_error: 2.6152 - val_loss: 3.8895 - val_mean_squared_error: 3.8895\n",
            "Epoch 101/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 2.5333 - mean_squared_error: 2.5333 - val_loss: 4.3108 - val_mean_squared_error: 4.3108\n",
            "Epoch 102/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 2.6745 - mean_squared_error: 2.6745 - val_loss: 4.2967 - val_mean_squared_error: 4.2967\n",
            "Epoch 103/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 2.4837 - mean_squared_error: 2.4837 - val_loss: 4.2289 - val_mean_squared_error: 4.2289\n",
            "Epoch 104/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 2.3564 - mean_squared_error: 2.3564 - val_loss: 4.1657 - val_mean_squared_error: 4.1657\n",
            "Epoch 105/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 2.3907 - mean_squared_error: 2.3907 - val_loss: 4.0339 - val_mean_squared_error: 4.0339\n",
            "Epoch 106/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 2.3327 - mean_squared_error: 2.3327 - val_loss: 3.9424 - val_mean_squared_error: 3.9424\n",
            "Epoch 107/200\n",
            "1819/1819 [==============================] - 2s 825us/sample - loss: 2.3506 - mean_squared_error: 2.3506 - val_loss: 4.2217 - val_mean_squared_error: 4.2217\n",
            "Epoch 108/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 2.3082 - mean_squared_error: 2.3082 - val_loss: 3.6749 - val_mean_squared_error: 3.6749\n",
            "Epoch 109/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 2.2588 - mean_squared_error: 2.2588 - val_loss: 4.1502 - val_mean_squared_error: 4.1502\n",
            "Epoch 110/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 2.2909 - mean_squared_error: 2.2909 - val_loss: 4.0312 - val_mean_squared_error: 4.0312\n",
            "Epoch 111/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 2.1897 - mean_squared_error: 2.1897 - val_loss: 3.8155 - val_mean_squared_error: 3.8155\n",
            "Epoch 112/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 2.1337 - mean_squared_error: 2.1337 - val_loss: 3.8442 - val_mean_squared_error: 3.8442\n",
            "Epoch 113/200\n",
            "1819/1819 [==============================] - 2s 827us/sample - loss: 2.1198 - mean_squared_error: 2.1198 - val_loss: 3.6436 - val_mean_squared_error: 3.6436\n",
            "Epoch 114/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 2.1128 - mean_squared_error: 2.1128 - val_loss: 3.6758 - val_mean_squared_error: 3.6758\n",
            "Epoch 115/200\n",
            "1819/1819 [==============================] - 2s 826us/sample - loss: 2.0894 - mean_squared_error: 2.0894 - val_loss: 4.6254 - val_mean_squared_error: 4.6254\n",
            "Epoch 116/200\n",
            "1819/1819 [==============================] - 2s 826us/sample - loss: 2.1500 - mean_squared_error: 2.1500 - val_loss: 3.9511 - val_mean_squared_error: 3.9511\n",
            "Epoch 117/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 2.0744 - mean_squared_error: 2.0744 - val_loss: 3.7353 - val_mean_squared_error: 3.7353\n",
            "Epoch 118/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.9879 - mean_squared_error: 1.9879 - val_loss: 3.9884 - val_mean_squared_error: 3.9884\n",
            "Epoch 119/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 1.9354 - mean_squared_error: 1.9354 - val_loss: 3.7784 - val_mean_squared_error: 3.7784\n",
            "Epoch 120/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.9549 - mean_squared_error: 1.9549 - val_loss: 3.7659 - val_mean_squared_error: 3.7659\n",
            "Epoch 121/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.9644 - mean_squared_error: 1.9644 - val_loss: 3.6961 - val_mean_squared_error: 3.6961\n",
            "Epoch 122/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.8480 - mean_squared_error: 1.8480 - val_loss: 3.6988 - val_mean_squared_error: 3.6988\n",
            "Epoch 123/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 2.0589 - mean_squared_error: 2.0589 - val_loss: 3.7776 - val_mean_squared_error: 3.7776\n",
            "Epoch 124/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.8892 - mean_squared_error: 1.8892 - val_loss: 3.6362 - val_mean_squared_error: 3.6362\n",
            "Epoch 125/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.8942 - mean_squared_error: 1.8942 - val_loss: 3.7585 - val_mean_squared_error: 3.7585\n",
            "Epoch 126/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.7542 - mean_squared_error: 1.7542 - val_loss: 3.5024 - val_mean_squared_error: 3.5024\n",
            "Epoch 127/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.7620 - mean_squared_error: 1.7620 - val_loss: 3.4828 - val_mean_squared_error: 3.4828\n",
            "Epoch 128/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.7448 - mean_squared_error: 1.7448 - val_loss: 3.8156 - val_mean_squared_error: 3.8156\n",
            "Epoch 129/200\n",
            "1819/1819 [==============================] - 2s 825us/sample - loss: 1.7020 - mean_squared_error: 1.7020 - val_loss: 3.6079 - val_mean_squared_error: 3.6079\n",
            "Epoch 130/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 1.7542 - mean_squared_error: 1.7542 - val_loss: 3.7148 - val_mean_squared_error: 3.7148\n",
            "Epoch 131/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.8044 - mean_squared_error: 1.8044 - val_loss: 3.4887 - val_mean_squared_error: 3.4887\n",
            "Epoch 132/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.7336 - mean_squared_error: 1.7336 - val_loss: 3.5980 - val_mean_squared_error: 3.5980\n",
            "Epoch 133/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.6432 - mean_squared_error: 1.6432 - val_loss: 3.6076 - val_mean_squared_error: 3.6076\n",
            "Epoch 134/200\n",
            "1819/1819 [==============================] - 1s 819us/sample - loss: 1.6076 - mean_squared_error: 1.6076 - val_loss: 3.5458 - val_mean_squared_error: 3.5458\n",
            "Epoch 135/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.7363 - mean_squared_error: 1.7363 - val_loss: 3.4503 - val_mean_squared_error: 3.4503\n",
            "Epoch 136/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.7249 - mean_squared_error: 1.7249 - val_loss: 3.4889 - val_mean_squared_error: 3.4889\n",
            "Epoch 137/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.6305 - mean_squared_error: 1.6305 - val_loss: 3.4702 - val_mean_squared_error: 3.4702\n",
            "Epoch 138/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.6334 - mean_squared_error: 1.6334 - val_loss: 3.4289 - val_mean_squared_error: 3.4289\n",
            "Epoch 139/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.6024 - mean_squared_error: 1.6024 - val_loss: 3.2870 - val_mean_squared_error: 3.2870\n",
            "Epoch 140/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.7017 - mean_squared_error: 1.7017 - val_loss: 3.4329 - val_mean_squared_error: 3.4329\n",
            "Epoch 141/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.5557 - mean_squared_error: 1.5557 - val_loss: 3.3226 - val_mean_squared_error: 3.3226\n",
            "Epoch 142/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.5974 - mean_squared_error: 1.5974 - val_loss: 3.2645 - val_mean_squared_error: 3.2645\n",
            "Epoch 143/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.5944 - mean_squared_error: 1.5944 - val_loss: 3.6490 - val_mean_squared_error: 3.6490\n",
            "Epoch 144/200\n",
            "1819/1819 [==============================] - 1s 822us/sample - loss: 1.5206 - mean_squared_error: 1.5206 - val_loss: 3.5198 - val_mean_squared_error: 3.5198\n",
            "Epoch 145/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.5171 - mean_squared_error: 1.5171 - val_loss: 3.6909 - val_mean_squared_error: 3.6909\n",
            "Epoch 146/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.5203 - mean_squared_error: 1.5203 - val_loss: 3.5700 - val_mean_squared_error: 3.5700\n",
            "Epoch 147/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.4154 - mean_squared_error: 1.4154 - val_loss: 3.3190 - val_mean_squared_error: 3.3190\n",
            "Epoch 148/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.4630 - mean_squared_error: 1.4630 - val_loss: 3.6245 - val_mean_squared_error: 3.6245\n",
            "Epoch 149/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.5264 - mean_squared_error: 1.5264 - val_loss: 3.4491 - val_mean_squared_error: 3.4491\n",
            "Epoch 150/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 1.4918 - mean_squared_error: 1.4918 - val_loss: 3.9935 - val_mean_squared_error: 3.9935\n",
            "Epoch 151/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.4411 - mean_squared_error: 1.4411 - val_loss: 3.3297 - val_mean_squared_error: 3.3297\n",
            "Epoch 152/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.4579 - mean_squared_error: 1.4579 - val_loss: 3.4507 - val_mean_squared_error: 3.4507\n",
            "Epoch 153/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.5740 - mean_squared_error: 1.5740 - val_loss: 3.4145 - val_mean_squared_error: 3.4145\n",
            "Epoch 154/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.4781 - mean_squared_error: 1.4781 - val_loss: 3.4283 - val_mean_squared_error: 3.4283\n",
            "Epoch 155/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.4463 - mean_squared_error: 1.4463 - val_loss: 3.5097 - val_mean_squared_error: 3.5097\n",
            "Epoch 156/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.5053 - mean_squared_error: 1.5053 - val_loss: 4.3172 - val_mean_squared_error: 4.3172\n",
            "Epoch 157/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 1.4624 - mean_squared_error: 1.4624 - val_loss: 3.6864 - val_mean_squared_error: 3.6864\n",
            "Epoch 158/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.4063 - mean_squared_error: 1.4063 - val_loss: 3.2873 - val_mean_squared_error: 3.2873\n",
            "Epoch 159/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.3429 - mean_squared_error: 1.3429 - val_loss: 3.0990 - val_mean_squared_error: 3.0990\n",
            "Epoch 160/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.4060 - mean_squared_error: 1.4060 - val_loss: 3.3785 - val_mean_squared_error: 3.3785\n",
            "Epoch 161/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.3134 - mean_squared_error: 1.3134 - val_loss: 3.3915 - val_mean_squared_error: 3.3915\n",
            "Epoch 162/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.3209 - mean_squared_error: 1.3209 - val_loss: 3.4536 - val_mean_squared_error: 3.4536\n",
            "Epoch 163/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 1.3964 - mean_squared_error: 1.3964 - val_loss: 79758179283.5190 - val_mean_squared_error: 79758180352.0000\n",
            "Epoch 164/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 1.5134 - mean_squared_error: 1.5134 - val_loss: 13.8210 - val_mean_squared_error: 13.8210\n",
            "Epoch 165/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.6741 - mean_squared_error: 1.6741 - val_loss: 7.0179 - val_mean_squared_error: 7.0179\n",
            "Epoch 166/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.5340 - mean_squared_error: 1.5340 - val_loss: 4.4947 - val_mean_squared_error: 4.4947\n",
            "Epoch 167/200\n",
            "1819/1819 [==============================] - 1s 823us/sample - loss: 1.4784 - mean_squared_error: 1.4784 - val_loss: 3.6940 - val_mean_squared_error: 3.6940\n",
            "Epoch 168/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.3580 - mean_squared_error: 1.3580 - val_loss: 3.5127 - val_mean_squared_error: 3.5127\n",
            "Epoch 169/200\n",
            "1819/1819 [==============================] - 1s 821us/sample - loss: 1.4098 - mean_squared_error: 1.4098 - val_loss: 3.3660 - val_mean_squared_error: 3.3660\n",
            "Epoch 170/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.3083 - mean_squared_error: 1.3083 - val_loss: 3.6279 - val_mean_squared_error: 3.6279\n",
            "Epoch 171/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.2671 - mean_squared_error: 1.2671 - val_loss: 3.5515 - val_mean_squared_error: 3.5515\n",
            "Epoch 172/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.2957 - mean_squared_error: 1.2957 - val_loss: 3.3659 - val_mean_squared_error: 3.3659\n",
            "Epoch 173/200\n",
            "1819/1819 [==============================] - 1s 818us/sample - loss: 1.3123 - mean_squared_error: 1.3123 - val_loss: 3.3649 - val_mean_squared_error: 3.3649\n",
            "Epoch 174/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.3105 - mean_squared_error: 1.3105 - val_loss: 3.1632 - val_mean_squared_error: 3.1632\n",
            "Epoch 175/200\n",
            "1819/1819 [==============================] - 1s 811us/sample - loss: 1.2903 - mean_squared_error: 1.2903 - val_loss: 3.2489 - val_mean_squared_error: 3.2489\n",
            "Epoch 176/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.2793 - mean_squared_error: 1.2793 - val_loss: 3.1540 - val_mean_squared_error: 3.1540\n",
            "Epoch 177/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.2290 - mean_squared_error: 1.2290 - val_loss: 3.3380 - val_mean_squared_error: 3.3380\n",
            "Epoch 178/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.2655 - mean_squared_error: 1.2655 - val_loss: 3.5578 - val_mean_squared_error: 3.5578\n",
            "Epoch 179/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.3512 - mean_squared_error: 1.3512 - val_loss: 3.3107 - val_mean_squared_error: 3.3107\n",
            "Epoch 180/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.2257 - mean_squared_error: 1.2257 - val_loss: 3.1597 - val_mean_squared_error: 3.1597\n",
            "Epoch 181/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.1889 - mean_squared_error: 1.1889 - val_loss: 3.1543 - val_mean_squared_error: 3.1543\n",
            "Epoch 182/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.2376 - mean_squared_error: 1.2376 - val_loss: 3.3840 - val_mean_squared_error: 3.3840\n",
            "Epoch 183/200\n",
            "1819/1819 [==============================] - 1s 812us/sample - loss: 1.1475 - mean_squared_error: 1.1475 - val_loss: 3.2270 - val_mean_squared_error: 3.2270\n",
            "Epoch 184/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.2298 - mean_squared_error: 1.2298 - val_loss: 3.4888 - val_mean_squared_error: 3.4888\n",
            "Epoch 185/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.3637 - mean_squared_error: 1.3637 - val_loss: 3.2380 - val_mean_squared_error: 3.2380\n",
            "Epoch 186/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1865 - mean_squared_error: 1.1865 - val_loss: 3.3730 - val_mean_squared_error: 3.3730\n",
            "Epoch 187/200\n",
            "1819/1819 [==============================] - 1s 824us/sample - loss: 1.1385 - mean_squared_error: 1.1385 - val_loss: 3.5759 - val_mean_squared_error: 3.5759\n",
            "Epoch 188/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.1464 - mean_squared_error: 1.1464 - val_loss: 3.9510 - val_mean_squared_error: 3.9510\n",
            "Epoch 189/200\n",
            "1819/1819 [==============================] - 1s 807us/sample - loss: 1.1407 - mean_squared_error: 1.1407 - val_loss: 3.1385 - val_mean_squared_error: 3.1385\n",
            "Epoch 190/200\n",
            "1819/1819 [==============================] - 1s 816us/sample - loss: 1.2263 - mean_squared_error: 1.2263 - val_loss: 3.3294 - val_mean_squared_error: 3.3294\n",
            "Epoch 191/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.2090 - mean_squared_error: 1.2090 - val_loss: 3.2951 - val_mean_squared_error: 3.2951\n",
            "Epoch 192/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.1855 - mean_squared_error: 1.1855 - val_loss: 3.1169 - val_mean_squared_error: 3.1169\n",
            "Epoch 193/200\n",
            "1819/1819 [==============================] - 1s 820us/sample - loss: 1.1130 - mean_squared_error: 1.1130 - val_loss: 3.1755 - val_mean_squared_error: 3.1755\n",
            "Epoch 194/200\n",
            "1819/1819 [==============================] - 1s 813us/sample - loss: 1.1263 - mean_squared_error: 1.1263 - val_loss: 3.2059 - val_mean_squared_error: 3.2059\n",
            "Epoch 195/200\n",
            "1819/1819 [==============================] - 1s 817us/sample - loss: 1.0466 - mean_squared_error: 1.0466 - val_loss: 3.4569 - val_mean_squared_error: 3.4569\n",
            "Epoch 196/200\n",
            "1819/1819 [==============================] - 2s 827us/sample - loss: 1.1067 - mean_squared_error: 1.1067 - val_loss: 3.3188 - val_mean_squared_error: 3.3188\n",
            "Epoch 197/200\n",
            "1819/1819 [==============================] - 1s 814us/sample - loss: 1.1936 - mean_squared_error: 1.1936 - val_loss: 3.3846 - val_mean_squared_error: 3.3846\n",
            "Epoch 198/200\n",
            "1819/1819 [==============================] - 1s 810us/sample - loss: 1.0955 - mean_squared_error: 1.0955 - val_loss: 2.9744 - val_mean_squared_error: 2.9744\n",
            "Epoch 199/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.0458 - mean_squared_error: 1.0458 - val_loss: 3.2041 - val_mean_squared_error: 3.2041\n",
            "Epoch 200/200\n",
            "1819/1819 [==============================] - 1s 815us/sample - loss: 1.0928 - mean_squared_error: 1.0928 - val_loss: 3.2250 - val_mean_squared_error: 3.2250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSsk092Pw83H",
        "colab_type": "text"
      },
      "source": [
        "## Augmentation of Training Data\n",
        "\n",
        "As discussed in the overview of our dataset, many of our training images are not currently being used due to the lack of labels for some keypoints. This unfortunately leaves us with only ~2100 images to split between training and development sets. In order to attempt to make a more robust training set that will hopefully generalize better to the test data, we will augment our images by simply flipping them across the columns (flip across the y-axis). This flip was implemented in the DataExploration.ipynb notebook at the [team's GitHub repo](https://github.com/tomgoter/w207_finalproject). Essentially flipping the pixel data is easy enough. We simply reverse the columns for all of the x-coordinate keypoints (i.e., those that end with \"_x\"). The only tricky part in this is that when we flip an image the labels for keypoints that are oriented by left/right directions are now reversed. So we need to go through the keypoint column names and relabel our columns. This amount of manipulation of our training data can be error prone, so as a quality assurance check the image below (and several others like it) was generated from the original and flipped datasets. All of the keypoints are identified by the blue dots. The original image is on the right, and the flipped image is on the left. One can see that the keypoints have been mirrored appropriately. After creating a flipped dataset we merged it with the original training set and shuffled the data. This dataset (i.e. pandas dataframe) was then pickled for easy porting to Google Drive. At this point, we are ready to use the expanded dataset.\n",
        "\n",
        "Great! We have doubled the size of our data from which we can train\n",
        "\n",
        "![alt text](https://github.com/tomgoter/w207_finalproject/raw/master/Images/flipped.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z91PoSp-QgVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the augmented dataframe from the pickle file\n",
        "df_nostache_nonan_w_flip = pd.read_pickle(drive_path + \"df_nostache_nonan_w_flip.pkl\")\n",
        "\n",
        "# Grab the last column - that is our image data for X matrix\n",
        "flipped_X = df_nostache_nonan_w_flip.iloc[:, -1]\n",
        "\n",
        "# Convert from a series of arrays to an NDarray\n",
        "flipped_X = np.array([x.reshape(96,96,1) for x in flipped_X])\n",
        "\n",
        "# Grab the keypoints and stick into our y-variable\n",
        "flipped_y = np.array(df_nostache_nonan_w_flip.iloc[:,:-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdeY38Znxhl4",
        "colab_type": "code",
        "outputId": "2eb1a756-a3a7-47be-f294-b0d58d94ceab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# We have doubled the size of our training/development data\n",
        "flipped_X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4280, 96, 96, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5DT58Q-x3Zo",
        "colab_type": "text"
      },
      "source": [
        "### Revisit old sensitivities with new data\n",
        "\n",
        "Now that we have more training data to play with, let's run some old sensitivities again.  We start by running with 12 and 16 starting filter depth, two learning rate factors and two dropout algorithms.\n",
        "\n",
        "The image below shows the boost in performance we get through this data augmentation. Unfortunately, as expected, it also comes with a significant run time penalty as we have doubled our training set size (specifially for starting filter depth of 12 and dropout rate of 0 initially increasing by 0.02 each layer).\n",
        "\n",
        "![alt text](https://github.com/tomgoter/w207_finalproject/raw/master/Images/flipped_performance.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDZSGXOxx3hY",
        "colab_type": "code",
        "outputId": "83e87693-7e33-4999-fcce-5aa62d1645a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Use an early stopping callback and our timing callback\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                              patience=100, mode='auto')\n",
        "time_callback = TimeHistory()\n",
        "\n",
        "# Initialize a new data frame to hold our output data\n",
        "cnn_flipped_df = pd.DataFrame()\n",
        "\n",
        "# Start filter list\n",
        "start_filters = [12, 16]\n",
        "\n",
        "# Create a list of initial dropout values and steps to increase\n",
        "dropouts = [(0.00,0.00), (0.00,0.02)]\n",
        "\n",
        "# Run a parametric study\n",
        "for lr_factor in [10, 15]: \n",
        "    for start_filter in start_filters:\n",
        "        for d in dropouts:\n",
        "          \n",
        "            # Create the model with the specified parameters\n",
        "            model = create_bn_cnn_model(start_filter, d[0], d[1])\n",
        "            \n",
        "            # Use the adam optimizer with the default learning rate\n",
        "            adam = optimizers.Adam(lr=0.001*lr_factor, beta_1=0.9, beta_2=0.999)\n",
        "            \n",
        "            # Compile our model\n",
        "            model.compile(\n",
        "                  optimizer=adam,\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_squared_error'])\n",
        "            \n",
        "            # Save the output of the model - also implement both the timining and early stop\n",
        "            # callbacks\n",
        "            history = model.fit(\n",
        "                flipped_X.astype(np.float32), flipped_y.astype(np.float32),\n",
        "                epochs=200,\n",
        "                validation_split=0.15, callbacks=[time_callback, early_stop])\n",
        "            \n",
        "            # Snag the times\n",
        "            times = time_callback.times\n",
        "\n",
        "            # Convert model output data to dataframe\n",
        "            hist = pd.DataFrame(history.history)\n",
        "            hist['epoch'] = history.epoch\n",
        "            hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "            hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "            hist['times'] = times\n",
        "            \n",
        "            # Add model specific metadata to differentiate between models\n",
        "            hist['starting_filter'] = start_filter\n",
        "            hist['layers'] = 3\n",
        "            hist['pooling'] = 'yes'\n",
        "            hist['fc_layer1'] = 500\n",
        "            hist['fc_layer2'] = 500\n",
        "            hist['activation'] = 'relu'\n",
        "            hist['optimizer'] = 'adam'\n",
        "            hist['lrate'] = adam.get_config()['learning_rate']\n",
        "            hist['dropout_initial'] = d[0]\n",
        "            hist['dropout_step'] = d[1]\n",
        "            hist['batch_norm'] = 1\n",
        "            hist['bias'] = 0\n",
        "            hist['stride'] = 1\n",
        "            hist['flipped'] = 1.0\n",
        "\n",
        "            # Keep concatenating to dataframe\n",
        "            cnn_flipped_df = pd.concat([cnn_flipped_df,hist])\n",
        "\n",
        "            # Re-pickle after every model to retain progress\n",
        "            cnn_flipped_df.to_pickle(drive_path+\"OutputData/cnn_flipped_df2.pkl\")\n",
        "\n",
        "            # Save models.\n",
        "            filename = \"cnn_flipped2_{}_d{}_s{}_sf{}_lrfactor{}_flipped_100\".format('adam', d[0], d[1], start_filter, lr_factor)\n",
        "            model.save(drive_path+\"Models/\"+filename+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_30 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_32 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 718us/sample - loss: 223.3142 - mean_squared_error: 223.3142 - val_loss: 151.4509 - val_mean_squared_error: 151.4509\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 20.5596 - mean_squared_error: 20.5596 - val_loss: 43.3977 - val_mean_squared_error: 43.3977\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 16.9446 - mean_squared_error: 16.9446 - val_loss: 21.4646 - val_mean_squared_error: 21.4646\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 15.1629 - mean_squared_error: 15.1629 - val_loss: 23.7014 - val_mean_squared_error: 23.7014\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 13.8136 - mean_squared_error: 13.8136 - val_loss: 13.9691 - val_mean_squared_error: 13.9691\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 14.2498 - mean_squared_error: 14.2498 - val_loss: 11.8729 - val_mean_squared_error: 11.8729\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 11.4777 - mean_squared_error: 11.4777 - val_loss: 9.4305 - val_mean_squared_error: 9.4305\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 9.9448 - mean_squared_error: 9.9448 - val_loss: 8.8529 - val_mean_squared_error: 8.8529\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 8.1301 - mean_squared_error: 8.1301 - val_loss: 6.7840 - val_mean_squared_error: 6.7840\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 6.8384 - mean_squared_error: 6.8384 - val_loss: 8.9403 - val_mean_squared_error: 8.9403\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 6.0532 - mean_squared_error: 6.0532 - val_loss: 7.0981 - val_mean_squared_error: 7.0981\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 6.0849 - mean_squared_error: 6.0849 - val_loss: 6.0997 - val_mean_squared_error: 6.0997\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 5.3542 - mean_squared_error: 5.3542 - val_loss: 7.3873 - val_mean_squared_error: 7.3873\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 4.5127 - mean_squared_error: 4.5127 - val_loss: 4.9114 - val_mean_squared_error: 4.9114\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 4.1213 - mean_squared_error: 4.1213 - val_loss: 4.3341 - val_mean_squared_error: 4.3341\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 3.7479 - mean_squared_error: 3.7479 - val_loss: 4.2510 - val_mean_squared_error: 4.2510\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 3.4966 - mean_squared_error: 3.4966 - val_loss: 3.5520 - val_mean_squared_error: 3.5520\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 3.5349 - mean_squared_error: 3.5349 - val_loss: 3.6550 - val_mean_squared_error: 3.6550\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 3.1214 - mean_squared_error: 3.1214 - val_loss: 4.1181 - val_mean_squared_error: 4.1181\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 2.9157 - mean_squared_error: 2.9157 - val_loss: 3.3085 - val_mean_squared_error: 3.3085\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 2.7540 - mean_squared_error: 2.7540 - val_loss: 3.1140 - val_mean_squared_error: 3.1140\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 2.5722 - mean_squared_error: 2.5722 - val_loss: 3.2011 - val_mean_squared_error: 3.2011\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 2.6377 - mean_squared_error: 2.6377 - val_loss: 3.6096 - val_mean_squared_error: 3.6096\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 2.6453 - mean_squared_error: 2.6453 - val_loss: 3.2134 - val_mean_squared_error: 3.2134\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 2.4185 - mean_squared_error: 2.4185 - val_loss: 2.7516 - val_mean_squared_error: 2.7516\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 2.2970 - mean_squared_error: 2.2970 - val_loss: 3.3797 - val_mean_squared_error: 3.3797\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 2.3633 - mean_squared_error: 2.3633 - val_loss: 3.0593 - val_mean_squared_error: 3.0593\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 2.3117 - mean_squared_error: 2.3117 - val_loss: 2.8924 - val_mean_squared_error: 2.8924\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 2.2194 - mean_squared_error: 2.2194 - val_loss: 3.6893 - val_mean_squared_error: 3.6893\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 2.0648 - mean_squared_error: 2.0648 - val_loss: 2.6712 - val_mean_squared_error: 2.6712\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 2.0753 - mean_squared_error: 2.0753 - val_loss: 2.9479 - val_mean_squared_error: 2.9479\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.9289 - mean_squared_error: 1.9289 - val_loss: 2.8194 - val_mean_squared_error: 2.8194\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 2.0931 - mean_squared_error: 2.0931 - val_loss: 3.9831 - val_mean_squared_error: 3.9831\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 2.0360 - mean_squared_error: 2.0360 - val_loss: 2.6401 - val_mean_squared_error: 2.6401\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.9018 - mean_squared_error: 1.9018 - val_loss: 2.7092 - val_mean_squared_error: 2.7092\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.7849 - mean_squared_error: 1.7849 - val_loss: 2.5822 - val_mean_squared_error: 2.5822\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.7435 - mean_squared_error: 1.7435 - val_loss: 2.6659 - val_mean_squared_error: 2.6659\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.7805 - mean_squared_error: 1.7805 - val_loss: 3.0314 - val_mean_squared_error: 3.0314\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 367us/sample - loss: 1.9482 - mean_squared_error: 1.9482 - val_loss: 3.3140 - val_mean_squared_error: 3.3140\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.9136 - mean_squared_error: 1.9136 - val_loss: 2.4409 - val_mean_squared_error: 2.4409\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.6602 - mean_squared_error: 1.6602 - val_loss: 2.3167 - val_mean_squared_error: 2.3167\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.6723 - mean_squared_error: 1.6723 - val_loss: 2.4204 - val_mean_squared_error: 2.4204\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.6155 - mean_squared_error: 1.6155 - val_loss: 2.5112 - val_mean_squared_error: 2.5112\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.6777 - mean_squared_error: 1.6777 - val_loss: 2.6747 - val_mean_squared_error: 2.6747\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.6113 - mean_squared_error: 1.6113 - val_loss: 2.3743 - val_mean_squared_error: 2.3743\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.5410 - mean_squared_error: 1.5410 - val_loss: 2.5118 - val_mean_squared_error: 2.5118\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.4501 - mean_squared_error: 1.4501 - val_loss: 2.7500 - val_mean_squared_error: 2.7500\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.5445 - mean_squared_error: 1.5445 - val_loss: 2.7402 - val_mean_squared_error: 2.7402\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.4764 - mean_squared_error: 1.4764 - val_loss: 2.4106 - val_mean_squared_error: 2.4106\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.6240 - mean_squared_error: 1.6240 - val_loss: 2.6251 - val_mean_squared_error: 2.6251\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.4339 - mean_squared_error: 1.4339 - val_loss: 2.3960 - val_mean_squared_error: 2.3960\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.5710 - mean_squared_error: 1.5710 - val_loss: 2.4993 - val_mean_squared_error: 2.4993\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.3914 - mean_squared_error: 1.3914 - val_loss: 2.3304 - val_mean_squared_error: 2.3304\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.3163 - mean_squared_error: 1.3163 - val_loss: 2.2393 - val_mean_squared_error: 2.2393\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.4256 - mean_squared_error: 1.4256 - val_loss: 2.5220 - val_mean_squared_error: 2.5220\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.3658 - mean_squared_error: 1.3658 - val_loss: 2.0791 - val_mean_squared_error: 2.0791\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.2934 - mean_squared_error: 1.2934 - val_loss: 2.7063 - val_mean_squared_error: 2.7063\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 1.3094 - mean_squared_error: 1.3094 - val_loss: 2.5179 - val_mean_squared_error: 2.5179\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.2699 - mean_squared_error: 1.2699 - val_loss: 2.2488 - val_mean_squared_error: 2.2488\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.2101 - mean_squared_error: 1.2101 - val_loss: 2.2026 - val_mean_squared_error: 2.2026\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.2877 - mean_squared_error: 1.2877 - val_loss: 2.4205 - val_mean_squared_error: 2.4205\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.2132 - mean_squared_error: 1.2132 - val_loss: 2.4084 - val_mean_squared_error: 2.4084\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.3299 - mean_squared_error: 1.3299 - val_loss: 2.4909 - val_mean_squared_error: 2.4909\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.1113 - mean_squared_error: 1.1113 - val_loss: 2.4012 - val_mean_squared_error: 2.4012\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.3266 - mean_squared_error: 1.3266 - val_loss: 2.1011 - val_mean_squared_error: 2.1011\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.1735 - mean_squared_error: 1.1735 - val_loss: 2.6774 - val_mean_squared_error: 2.6774\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.1599 - mean_squared_error: 1.1599 - val_loss: 2.1230 - val_mean_squared_error: 2.1230\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.1990 - mean_squared_error: 1.1990 - val_loss: 2.2819 - val_mean_squared_error: 2.2819\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.0939 - mean_squared_error: 1.0939 - val_loss: 2.3117 - val_mean_squared_error: 2.3117\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.1494 - mean_squared_error: 1.1494 - val_loss: 2.7000 - val_mean_squared_error: 2.7000\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.1534 - mean_squared_error: 1.1534 - val_loss: 2.2981 - val_mean_squared_error: 2.2981\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.0702 - mean_squared_error: 1.0702 - val_loss: 2.3494 - val_mean_squared_error: 2.3494\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.0750 - mean_squared_error: 1.0750 - val_loss: 2.2360 - val_mean_squared_error: 2.2360\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.0713 - mean_squared_error: 1.0713 - val_loss: 2.1147 - val_mean_squared_error: 2.1147\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.0295 - mean_squared_error: 1.0295 - val_loss: 2.3198 - val_mean_squared_error: 2.3198\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.1420 - mean_squared_error: 1.1420 - val_loss: 2.0474 - val_mean_squared_error: 2.0474\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9440 - mean_squared_error: 0.9440 - val_loss: 2.2433 - val_mean_squared_error: 2.2433\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.9625 - mean_squared_error: 0.9625 - val_loss: 1.9975 - val_mean_squared_error: 1.9975\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.9879 - mean_squared_error: 0.9879 - val_loss: 2.3657 - val_mean_squared_error: 2.3657\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.0710 - mean_squared_error: 1.0710 - val_loss: 2.2988 - val_mean_squared_error: 2.2988\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.1285 - mean_squared_error: 1.1285 - val_loss: 2.4436 - val_mean_squared_error: 2.4436\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 2.0747 - val_mean_squared_error: 2.0747\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.9401 - mean_squared_error: 0.9401 - val_loss: 2.2439 - val_mean_squared_error: 2.2439\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.0704 - mean_squared_error: 1.0704 - val_loss: 2.3139 - val_mean_squared_error: 2.3139\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.9510 - mean_squared_error: 0.9510 - val_loss: 2.2904 - val_mean_squared_error: 2.2904\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.0977 - mean_squared_error: 1.0977 - val_loss: 2.1401 - val_mean_squared_error: 2.1401\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.9336 - mean_squared_error: 0.9336 - val_loss: 2.6698 - val_mean_squared_error: 2.6698\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.9041 - mean_squared_error: 0.9041 - val_loss: 2.1297 - val_mean_squared_error: 2.1297\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.9144 - mean_squared_error: 0.9144 - val_loss: 2.1403 - val_mean_squared_error: 2.1403\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.9380 - mean_squared_error: 0.9380 - val_loss: 2.3060 - val_mean_squared_error: 2.3060\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.9936 - mean_squared_error: 0.9936 - val_loss: 2.0913 - val_mean_squared_error: 2.0913\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9313 - mean_squared_error: 0.9313 - val_loss: 2.3471 - val_mean_squared_error: 2.3471\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.9841 - mean_squared_error: 0.9841 - val_loss: 2.6081 - val_mean_squared_error: 2.6081\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.8000 - mean_squared_error: 0.8000 - val_loss: 1.9621 - val_mean_squared_error: 1.9621\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.9531 - mean_squared_error: 0.9531 - val_loss: 2.1049 - val_mean_squared_error: 2.1049\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9180 - mean_squared_error: 0.9180 - val_loss: 2.1533 - val_mean_squared_error: 2.1533\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.9288 - mean_squared_error: 0.9288 - val_loss: 2.1362 - val_mean_squared_error: 2.1362\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9304 - mean_squared_error: 0.9304 - val_loss: 2.5241 - val_mean_squared_error: 2.5241\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.8880 - mean_squared_error: 0.8880 - val_loss: 2.1001 - val_mean_squared_error: 2.1001\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.8759 - mean_squared_error: 0.8759 - val_loss: 2.3414 - val_mean_squared_error: 2.3414\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.8775 - mean_squared_error: 0.8775 - val_loss: 1.9096 - val_mean_squared_error: 1.9096\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.8010 - mean_squared_error: 0.8010 - val_loss: 2.0138 - val_mean_squared_error: 2.0138\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8591 - mean_squared_error: 0.8591 - val_loss: 2.2449 - val_mean_squared_error: 2.2449\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.8426 - mean_squared_error: 0.8426 - val_loss: 1.9604 - val_mean_squared_error: 1.9604\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.8256 - mean_squared_error: 0.8256 - val_loss: 2.1749 - val_mean_squared_error: 2.1749\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.8663 - mean_squared_error: 0.8663 - val_loss: 2.1019 - val_mean_squared_error: 2.1019\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.7866 - mean_squared_error: 0.7866 - val_loss: 1.9520 - val_mean_squared_error: 1.9520\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.7261 - mean_squared_error: 0.7261 - val_loss: 2.2257 - val_mean_squared_error: 2.2257\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.8033 - mean_squared_error: 0.8033 - val_loss: 1.9886 - val_mean_squared_error: 1.9886\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7307 - mean_squared_error: 0.7307 - val_loss: 1.9284 - val_mean_squared_error: 1.9284\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8307 - mean_squared_error: 0.8307 - val_loss: 1.8873 - val_mean_squared_error: 1.8873\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.8227 - mean_squared_error: 0.8227 - val_loss: 2.1702 - val_mean_squared_error: 2.1702\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.8067 - mean_squared_error: 0.8067 - val_loss: 2.1121 - val_mean_squared_error: 2.1121\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7889 - mean_squared_error: 0.7889 - val_loss: 2.0214 - val_mean_squared_error: 2.0214\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.7891 - mean_squared_error: 0.7891 - val_loss: 2.0418 - val_mean_squared_error: 2.0418\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.8913 - mean_squared_error: 0.8913 - val_loss: 2.0427 - val_mean_squared_error: 2.0427\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8588 - mean_squared_error: 0.8588 - val_loss: 1.9744 - val_mean_squared_error: 1.9744\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.7547 - mean_squared_error: 0.7547 - val_loss: 2.1174 - val_mean_squared_error: 2.1174\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.8103 - mean_squared_error: 0.8103 - val_loss: 1.8900 - val_mean_squared_error: 1.8900\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.6924 - mean_squared_error: 0.6924 - val_loss: 2.0629 - val_mean_squared_error: 2.0629\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.7699 - mean_squared_error: 0.7699 - val_loss: 2.0657 - val_mean_squared_error: 2.0657\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7055 - mean_squared_error: 0.7055 - val_loss: 1.9317 - val_mean_squared_error: 1.9317\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7190 - mean_squared_error: 0.7190 - val_loss: 1.8247 - val_mean_squared_error: 1.8247\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.7175 - mean_squared_error: 0.7175 - val_loss: 1.9752 - val_mean_squared_error: 1.9752\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.7713 - mean_squared_error: 0.7713 - val_loss: 2.1596 - val_mean_squared_error: 2.1596\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.7584 - mean_squared_error: 0.7584 - val_loss: 2.2134 - val_mean_squared_error: 2.2134\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.7366 - mean_squared_error: 0.7366 - val_loss: 2.0033 - val_mean_squared_error: 2.0033\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.6595 - mean_squared_error: 0.6595 - val_loss: 1.9114 - val_mean_squared_error: 1.9114\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.6706 - mean_squared_error: 0.6706 - val_loss: 2.2313 - val_mean_squared_error: 2.2313\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.6997 - mean_squared_error: 0.6997 - val_loss: 1.9032 - val_mean_squared_error: 1.9032\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.6401 - mean_squared_error: 0.6401 - val_loss: 1.9113 - val_mean_squared_error: 1.9113\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.6358 - mean_squared_error: 0.6358 - val_loss: 1.9011 - val_mean_squared_error: 1.9011\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.6446 - mean_squared_error: 0.6446 - val_loss: 2.0946 - val_mean_squared_error: 2.0946\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.6927 - mean_squared_error: 0.6927 - val_loss: 2.2089 - val_mean_squared_error: 2.2089\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.6655 - mean_squared_error: 0.6655 - val_loss: 2.0435 - val_mean_squared_error: 2.0435\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.6672 - mean_squared_error: 0.6672 - val_loss: 1.9154 - val_mean_squared_error: 1.9154\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.6473 - mean_squared_error: 0.6473 - val_loss: 1.9620 - val_mean_squared_error: 1.9620\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.6838 - mean_squared_error: 0.6838 - val_loss: 1.9309 - val_mean_squared_error: 1.9309\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.6528 - mean_squared_error: 0.6528 - val_loss: 1.9668 - val_mean_squared_error: 1.9668\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.6231 - mean_squared_error: 0.6231 - val_loss: 1.9896 - val_mean_squared_error: 1.9896\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.6420 - mean_squared_error: 0.6420 - val_loss: 2.0535 - val_mean_squared_error: 2.0535\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6747 - mean_squared_error: 0.6747 - val_loss: 2.3597 - val_mean_squared_error: 2.3597\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.7425 - mean_squared_error: 0.7425 - val_loss: 1.9535 - val_mean_squared_error: 1.9535\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.6161 - mean_squared_error: 0.6161 - val_loss: 2.2181 - val_mean_squared_error: 2.2181\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5753 - mean_squared_error: 0.5753 - val_loss: 2.0099 - val_mean_squared_error: 2.0099\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.6937 - mean_squared_error: 0.6937 - val_loss: 2.0139 - val_mean_squared_error: 2.0139\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.6186 - mean_squared_error: 0.6186 - val_loss: 2.1634 - val_mean_squared_error: 2.1634\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6118 - mean_squared_error: 0.6118 - val_loss: 2.0973 - val_mean_squared_error: 2.0973\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.6464 - mean_squared_error: 0.6464 - val_loss: 2.0081 - val_mean_squared_error: 2.0081\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.5609 - mean_squared_error: 0.5609 - val_loss: 2.0123 - val_mean_squared_error: 2.0123\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.6380 - mean_squared_error: 0.6380 - val_loss: 2.1273 - val_mean_squared_error: 2.1273\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.5836 - mean_squared_error: 0.5836 - val_loss: 2.3937 - val_mean_squared_error: 2.3937\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.6429 - mean_squared_error: 0.6429 - val_loss: 1.8810 - val_mean_squared_error: 1.8810\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.5827 - mean_squared_error: 0.5827 - val_loss: 1.8954 - val_mean_squared_error: 1.8954\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.5847 - mean_squared_error: 0.5847 - val_loss: 2.0820 - val_mean_squared_error: 2.0820\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.5602 - mean_squared_error: 0.5602 - val_loss: 2.0053 - val_mean_squared_error: 2.0053\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.6679 - mean_squared_error: 0.6679 - val_loss: 1.8735 - val_mean_squared_error: 1.8735\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5411 - mean_squared_error: 0.5411 - val_loss: 1.8401 - val_mean_squared_error: 1.8401\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6135 - mean_squared_error: 0.6135 - val_loss: 1.9807 - val_mean_squared_error: 1.9807\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5874 - mean_squared_error: 0.5874 - val_loss: 1.9205 - val_mean_squared_error: 1.9205\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.5647 - mean_squared_error: 0.5647 - val_loss: 1.9536 - val_mean_squared_error: 1.9536\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5554 - mean_squared_error: 0.5554 - val_loss: 1.9571 - val_mean_squared_error: 1.9571\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.5595 - mean_squared_error: 0.5595 - val_loss: 2.0131 - val_mean_squared_error: 2.0131\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.6310 - mean_squared_error: 0.6310 - val_loss: 2.0444 - val_mean_squared_error: 2.0444\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.5683 - mean_squared_error: 0.5683 - val_loss: 2.1230 - val_mean_squared_error: 2.1230\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.5146 - mean_squared_error: 0.5146 - val_loss: 1.9313 - val_mean_squared_error: 1.9313\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.5711 - mean_squared_error: 0.5711 - val_loss: 1.9771 - val_mean_squared_error: 1.9771\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.5530 - mean_squared_error: 0.5530 - val_loss: 1.8885 - val_mean_squared_error: 1.8885\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.5393 - mean_squared_error: 0.5393 - val_loss: 1.9761 - val_mean_squared_error: 1.9761\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.6088 - mean_squared_error: 0.6088 - val_loss: 1.9876 - val_mean_squared_error: 1.9876\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.5455 - mean_squared_error: 0.5455 - val_loss: 2.0475 - val_mean_squared_error: 2.0475\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5421 - mean_squared_error: 0.5421 - val_loss: 1.8987 - val_mean_squared_error: 1.8987\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.5166 - mean_squared_error: 0.5166 - val_loss: 1.9749 - val_mean_squared_error: 1.9749\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.5458 - mean_squared_error: 0.5458 - val_loss: 1.8759 - val_mean_squared_error: 1.8759\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5163 - mean_squared_error: 0.5163 - val_loss: 1.8475 - val_mean_squared_error: 1.8475\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.5541 - mean_squared_error: 0.5541 - val_loss: 2.0343 - val_mean_squared_error: 2.0343\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.5607 - mean_squared_error: 0.5607 - val_loss: 1.9594 - val_mean_squared_error: 1.9594\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.4932 - mean_squared_error: 0.4932 - val_loss: 1.9775 - val_mean_squared_error: 1.9775\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.5317 - mean_squared_error: 0.5317 - val_loss: 1.9225 - val_mean_squared_error: 1.9225\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.5123 - mean_squared_error: 0.5123 - val_loss: 1.9009 - val_mean_squared_error: 1.9009\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.5182 - mean_squared_error: 0.5182 - val_loss: 1.9025 - val_mean_squared_error: 1.9025\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5453 - mean_squared_error: 0.5453 - val_loss: 1.8842 - val_mean_squared_error: 1.8842\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.5840 - mean_squared_error: 0.5840 - val_loss: 2.0067 - val_mean_squared_error: 2.0067\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5106 - mean_squared_error: 0.5106 - val_loss: 1.9368 - val_mean_squared_error: 1.9368\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.4618 - mean_squared_error: 0.4618 - val_loss: 1.9255 - val_mean_squared_error: 1.9255\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.5191 - mean_squared_error: 0.5191 - val_loss: 1.8955 - val_mean_squared_error: 1.8955\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.5825 - mean_squared_error: 0.5825 - val_loss: 2.0288 - val_mean_squared_error: 2.0288\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.5164 - mean_squared_error: 0.5164 - val_loss: 1.9289 - val_mean_squared_error: 1.9289\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.4917 - mean_squared_error: 0.4917 - val_loss: 1.9868 - val_mean_squared_error: 1.9868\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.5026 - mean_squared_error: 0.5026 - val_loss: 1.8252 - val_mean_squared_error: 1.8252\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.5130 - mean_squared_error: 0.5130 - val_loss: 1.8088 - val_mean_squared_error: 1.8088\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.4775 - mean_squared_error: 0.4775 - val_loss: 1.9680 - val_mean_squared_error: 1.9680\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.4900 - mean_squared_error: 0.4900 - val_loss: 1.9477 - val_mean_squared_error: 1.9477\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.5499 - mean_squared_error: 0.5499 - val_loss: 2.0119 - val_mean_squared_error: 2.0119\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.4807 - mean_squared_error: 0.4807 - val_loss: 1.8205 - val_mean_squared_error: 1.8205\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.4615 - mean_squared_error: 0.4615 - val_loss: 1.9731 - val_mean_squared_error: 1.9731\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.4862 - mean_squared_error: 0.4862 - val_loss: 1.9261 - val_mean_squared_error: 1.9261\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.4908 - mean_squared_error: 0.4908 - val_loss: 1.9343 - val_mean_squared_error: 1.9343\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.4593 - mean_squared_error: 0.4593 - val_loss: 2.0098 - val_mean_squared_error: 2.0098\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.4762 - mean_squared_error: 0.4762 - val_loss: 1.9949 - val_mean_squared_error: 1.9949\n",
            "==================================================\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_33 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_34 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_57 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_35 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_58 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_59 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 771us/sample - loss: 221.6025 - mean_squared_error: 221.6024 - val_loss: 227.3726 - val_mean_squared_error: 227.3727\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 370us/sample - loss: 19.7476 - mean_squared_error: 19.7476 - val_loss: 60.6190 - val_mean_squared_error: 60.6189\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 20.3314 - mean_squared_error: 20.3314 - val_loss: 27.6148 - val_mean_squared_error: 27.6148\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 367us/sample - loss: 15.8000 - mean_squared_error: 15.8000 - val_loss: 13.3160 - val_mean_squared_error: 13.3160\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 13.4612 - mean_squared_error: 13.4612 - val_loss: 10.6330 - val_mean_squared_error: 10.6330\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 12.9284 - mean_squared_error: 12.9284 - val_loss: 11.6784 - val_mean_squared_error: 11.6784\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 12.4543 - mean_squared_error: 12.4543 - val_loss: 13.6958 - val_mean_squared_error: 13.6958\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 10.8434 - mean_squared_error: 10.8434 - val_loss: 8.6489 - val_mean_squared_error: 8.6489\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 9.1943 - mean_squared_error: 9.1943 - val_loss: 8.6522 - val_mean_squared_error: 8.6522\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 7.9864 - mean_squared_error: 7.9864 - val_loss: 8.0180 - val_mean_squared_error: 8.0180\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 7.4352 - mean_squared_error: 7.4352 - val_loss: 7.5511 - val_mean_squared_error: 7.5511\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 6.5116 - mean_squared_error: 6.5117 - val_loss: 6.4300 - val_mean_squared_error: 6.4300\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 5.9682 - mean_squared_error: 5.9682 - val_loss: 7.3879 - val_mean_squared_error: 7.3879\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 5.1588 - mean_squared_error: 5.1588 - val_loss: 4.8262 - val_mean_squared_error: 4.8262\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 366us/sample - loss: 5.1391 - mean_squared_error: 5.1391 - val_loss: 4.8603 - val_mean_squared_error: 4.8603\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 4.4286 - mean_squared_error: 4.4286 - val_loss: 4.1359 - val_mean_squared_error: 4.1359\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 4.2189 - mean_squared_error: 4.2189 - val_loss: 3.7851 - val_mean_squared_error: 3.7851\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 370us/sample - loss: 3.8932 - mean_squared_error: 3.8932 - val_loss: 4.6495 - val_mean_squared_error: 4.6495\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 3.6589 - mean_squared_error: 3.6589 - val_loss: 3.4071 - val_mean_squared_error: 3.4071\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 3.5756 - mean_squared_error: 3.5756 - val_loss: 3.3670 - val_mean_squared_error: 3.3670\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 3.3830 - mean_squared_error: 3.3830 - val_loss: 3.3937 - val_mean_squared_error: 3.3937\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 3.1621 - mean_squared_error: 3.1621 - val_loss: 3.2724 - val_mean_squared_error: 3.2724\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 3.2104 - mean_squared_error: 3.2104 - val_loss: 3.5057 - val_mean_squared_error: 3.5057\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 367us/sample - loss: 2.9965 - mean_squared_error: 2.9965 - val_loss: 3.1932 - val_mean_squared_error: 3.1932\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 367us/sample - loss: 2.8825 - mean_squared_error: 2.8825 - val_loss: 2.9046 - val_mean_squared_error: 2.9046\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 2.7604 - mean_squared_error: 2.7604 - val_loss: 2.9763 - val_mean_squared_error: 2.9763\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 2.6107 - mean_squared_error: 2.6107 - val_loss: 2.9539 - val_mean_squared_error: 2.9539\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 2.5684 - mean_squared_error: 2.5684 - val_loss: 2.7813 - val_mean_squared_error: 2.7813\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 2.4756 - mean_squared_error: 2.4756 - val_loss: 2.3675 - val_mean_squared_error: 2.3675\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 2.3637 - mean_squared_error: 2.3637 - val_loss: 2.6846 - val_mean_squared_error: 2.6846\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 2.3593 - mean_squared_error: 2.3593 - val_loss: 2.3719 - val_mean_squared_error: 2.3719\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 2.2324 - mean_squared_error: 2.2324 - val_loss: 2.7633 - val_mean_squared_error: 2.7633\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 2.1698 - mean_squared_error: 2.1698 - val_loss: 2.8551 - val_mean_squared_error: 2.8551\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 2.0757 - mean_squared_error: 2.0757 - val_loss: 2.4447 - val_mean_squared_error: 2.4447\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 2.1409 - mean_squared_error: 2.1409 - val_loss: 2.4970 - val_mean_squared_error: 2.4970\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 2.1470 - mean_squared_error: 2.1470 - val_loss: 2.5473 - val_mean_squared_error: 2.5473\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 1.9769 - mean_squared_error: 1.9769 - val_loss: 2.4904 - val_mean_squared_error: 2.4904\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 1.9365 - mean_squared_error: 1.9365 - val_loss: 2.3647 - val_mean_squared_error: 2.3647\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 1.8937 - mean_squared_error: 1.8937 - val_loss: 3.0622 - val_mean_squared_error: 3.0622\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 2.0452 - mean_squared_error: 2.0452 - val_loss: 2.3682 - val_mean_squared_error: 2.3682\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.7737 - mean_squared_error: 1.7737 - val_loss: 2.2316 - val_mean_squared_error: 2.2316\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.8588 - mean_squared_error: 1.8588 - val_loss: 2.1940 - val_mean_squared_error: 2.1940\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.8077 - mean_squared_error: 1.8077 - val_loss: 3.2743 - val_mean_squared_error: 3.2743\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.7807 - mean_squared_error: 1.7807 - val_loss: 2.2920 - val_mean_squared_error: 2.2920\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.6905 - mean_squared_error: 1.6905 - val_loss: 2.2380 - val_mean_squared_error: 2.2380\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 374us/sample - loss: 1.6821 - mean_squared_error: 1.6821 - val_loss: 2.3450 - val_mean_squared_error: 2.3450\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 373us/sample - loss: 1.6672 - mean_squared_error: 1.6672 - val_loss: 2.0466 - val_mean_squared_error: 2.0466\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 1.6594 - mean_squared_error: 1.6594 - val_loss: 2.3397 - val_mean_squared_error: 2.3397\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 1.6521 - mean_squared_error: 1.6521 - val_loss: 2.3910 - val_mean_squared_error: 2.3910\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 1.7321 - mean_squared_error: 1.7321 - val_loss: 2.2035 - val_mean_squared_error: 2.2035\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 1.5945 - mean_squared_error: 1.5945 - val_loss: 1.9905 - val_mean_squared_error: 1.9905\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.4775 - mean_squared_error: 1.4775 - val_loss: 2.1149 - val_mean_squared_error: 2.1149\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 374us/sample - loss: 1.5198 - mean_squared_error: 1.5198 - val_loss: 2.1661 - val_mean_squared_error: 2.1661\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 1.4381 - mean_squared_error: 1.4381 - val_loss: 2.2614 - val_mean_squared_error: 2.2614\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 1.4401 - mean_squared_error: 1.4401 - val_loss: 2.1666 - val_mean_squared_error: 2.1666\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 368us/sample - loss: 1.4641 - mean_squared_error: 1.4641 - val_loss: 1.9062 - val_mean_squared_error: 1.9062\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 1.3508 - mean_squared_error: 1.3508 - val_loss: 2.2991 - val_mean_squared_error: 2.2991\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 366us/sample - loss: 1.3713 - mean_squared_error: 1.3713 - val_loss: 1.8822 - val_mean_squared_error: 1.8822\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 1.4453 - mean_squared_error: 1.4453 - val_loss: 1.9410 - val_mean_squared_error: 1.9410\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 1.3302 - mean_squared_error: 1.3302 - val_loss: 1.9566 - val_mean_squared_error: 1.9566\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 1.3536 - mean_squared_error: 1.3536 - val_loss: 1.9525 - val_mean_squared_error: 1.9525\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.3354 - mean_squared_error: 1.3354 - val_loss: 2.1001 - val_mean_squared_error: 2.1001\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.2544 - mean_squared_error: 1.2544 - val_loss: 2.2215 - val_mean_squared_error: 2.2215\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 1.2482 - mean_squared_error: 1.2482 - val_loss: 1.9875 - val_mean_squared_error: 1.9875\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.2772 - mean_squared_error: 1.2772 - val_loss: 2.2759 - val_mean_squared_error: 2.2759\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 1.3111 - mean_squared_error: 1.3111 - val_loss: 2.1708 - val_mean_squared_error: 2.1708\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 1.2364 - mean_squared_error: 1.2364 - val_loss: 1.9291 - val_mean_squared_error: 1.9291\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 1.2616 - mean_squared_error: 1.2616 - val_loss: 1.9908 - val_mean_squared_error: 1.9908\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 1.2402 - mean_squared_error: 1.2402 - val_loss: 1.9761 - val_mean_squared_error: 1.9761\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 1.2149 - mean_squared_error: 1.2149 - val_loss: 1.7774 - val_mean_squared_error: 1.7774\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.2557 - mean_squared_error: 1.2557 - val_loss: 2.0352 - val_mean_squared_error: 2.0352\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.1878 - mean_squared_error: 1.1878 - val_loss: 1.9605 - val_mean_squared_error: 1.9605\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 1.2149 - mean_squared_error: 1.2149 - val_loss: 2.0787 - val_mean_squared_error: 2.0787\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.1446 - mean_squared_error: 1.1446 - val_loss: 1.8644 - val_mean_squared_error: 1.8644\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 1.1358 - mean_squared_error: 1.1358 - val_loss: 1.9509 - val_mean_squared_error: 1.9509\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 1.1360 - mean_squared_error: 1.1360 - val_loss: 2.1203 - val_mean_squared_error: 2.1203\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.2274 - mean_squared_error: 1.2274 - val_loss: 2.1694 - val_mean_squared_error: 2.1694\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.0546 - mean_squared_error: 1.0546 - val_loss: 1.8583 - val_mean_squared_error: 1.8583\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 1.0938 - mean_squared_error: 1.0938 - val_loss: 1.9703 - val_mean_squared_error: 1.9703\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 1.1514 - mean_squared_error: 1.1514 - val_loss: 2.2485 - val_mean_squared_error: 2.2485\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.0763 - mean_squared_error: 1.0763 - val_loss: 2.0639 - val_mean_squared_error: 2.0639\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.0912 - mean_squared_error: 1.0912 - val_loss: 1.8615 - val_mean_squared_error: 1.8615\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 1.1701 - mean_squared_error: 1.1701 - val_loss: 2.0403 - val_mean_squared_error: 2.0403\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 1.0916 - mean_squared_error: 1.0916 - val_loss: 2.1065 - val_mean_squared_error: 2.1065\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.6972 - val_mean_squared_error: 1.6972\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.0523 - mean_squared_error: 1.0523 - val_loss: 1.9853 - val_mean_squared_error: 1.9853\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 1.0100 - mean_squared_error: 1.0100 - val_loss: 1.9357 - val_mean_squared_error: 1.9357\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.0984 - mean_squared_error: 1.0984 - val_loss: 2.2212 - val_mean_squared_error: 2.2212\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.9891 - mean_squared_error: 0.9891 - val_loss: 1.8991 - val_mean_squared_error: 1.8991\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 1.0304 - mean_squared_error: 1.0304 - val_loss: 1.7783 - val_mean_squared_error: 1.7783\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 1.0487 - mean_squared_error: 1.0487 - val_loss: 2.0280 - val_mean_squared_error: 2.0280\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.0601 - mean_squared_error: 1.0601 - val_loss: 1.7345 - val_mean_squared_error: 1.7345\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.9862 - mean_squared_error: 0.9862 - val_loss: 1.9418 - val_mean_squared_error: 1.9418\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 366us/sample - loss: 0.9964 - mean_squared_error: 0.9964 - val_loss: 2.6774 - val_mean_squared_error: 2.6774\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.9043 - mean_squared_error: 0.9043 - val_loss: 1.8772 - val_mean_squared_error: 1.8772\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.9682 - mean_squared_error: 0.9682 - val_loss: 1.8259 - val_mean_squared_error: 1.8259\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.0187 - mean_squared_error: 1.0187 - val_loss: 1.8440 - val_mean_squared_error: 1.8440\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.9723 - mean_squared_error: 0.9723 - val_loss: 1.9173 - val_mean_squared_error: 1.9173\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 1.0344 - mean_squared_error: 1.0344 - val_loss: 1.9498 - val_mean_squared_error: 1.9498\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 0.9720 - mean_squared_error: 0.9720 - val_loss: 2.0510 - val_mean_squared_error: 2.0510\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.9498 - mean_squared_error: 0.9498 - val_loss: 1.8654 - val_mean_squared_error: 1.8654\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.8876 - mean_squared_error: 0.8876 - val_loss: 1.8607 - val_mean_squared_error: 1.8607\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 0.9133 - mean_squared_error: 0.9133 - val_loss: 1.8150 - val_mean_squared_error: 1.8150\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.8823 - mean_squared_error: 0.8823 - val_loss: 1.8259 - val_mean_squared_error: 1.8259\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.8569 - mean_squared_error: 0.8569 - val_loss: 1.8988 - val_mean_squared_error: 1.8988\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.9270 - mean_squared_error: 0.9270 - val_loss: 1.7834 - val_mean_squared_error: 1.7834\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 367us/sample - loss: 0.8927 - mean_squared_error: 0.8927 - val_loss: 2.0019 - val_mean_squared_error: 2.0019\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 0.9144 - mean_squared_error: 0.9144 - val_loss: 2.0123 - val_mean_squared_error: 2.0123\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.8256 - mean_squared_error: 0.8256 - val_loss: 1.8207 - val_mean_squared_error: 1.8207\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.9534 - mean_squared_error: 0.9534 - val_loss: 1.7460 - val_mean_squared_error: 1.7460\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.8621 - mean_squared_error: 0.8621 - val_loss: 1.9514 - val_mean_squared_error: 1.9514\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.8225 - mean_squared_error: 0.8225 - val_loss: 1.8279 - val_mean_squared_error: 1.8279\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.9122 - mean_squared_error: 0.9122 - val_loss: 1.7780 - val_mean_squared_error: 1.7780\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.7687 - mean_squared_error: 0.7687 - val_loss: 1.8196 - val_mean_squared_error: 1.8196\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.8734 - mean_squared_error: 0.8734 - val_loss: 1.7348 - val_mean_squared_error: 1.7348\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.8073 - mean_squared_error: 0.8073 - val_loss: 1.7233 - val_mean_squared_error: 1.7233\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 366us/sample - loss: 0.8301 - mean_squared_error: 0.8301 - val_loss: 2.0158 - val_mean_squared_error: 2.0158\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 366us/sample - loss: 0.8060 - mean_squared_error: 0.8060 - val_loss: 1.8002 - val_mean_squared_error: 1.8002\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.8365 - mean_squared_error: 0.8365 - val_loss: 1.7581 - val_mean_squared_error: 1.7581\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.8371 - mean_squared_error: 0.8371 - val_loss: 1.9399 - val_mean_squared_error: 1.9399\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.8470 - mean_squared_error: 0.8470 - val_loss: 1.8073 - val_mean_squared_error: 1.8073\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.7267 - mean_squared_error: 0.7267 - val_loss: 1.7605 - val_mean_squared_error: 1.7605\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.7993 - mean_squared_error: 0.7993 - val_loss: 1.8234 - val_mean_squared_error: 1.8234\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.7503 - mean_squared_error: 0.7503 - val_loss: 1.7884 - val_mean_squared_error: 1.7884\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.7839 - mean_squared_error: 0.7839 - val_loss: 1.7918 - val_mean_squared_error: 1.7918\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.7443 - mean_squared_error: 0.7443 - val_loss: 1.8417 - val_mean_squared_error: 1.8417\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.7898 - mean_squared_error: 0.7898 - val_loss: 1.8900 - val_mean_squared_error: 1.8900\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 365us/sample - loss: 0.8113 - mean_squared_error: 0.8113 - val_loss: 1.8316 - val_mean_squared_error: 1.8316\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.7831 - mean_squared_error: 0.7831 - val_loss: 1.7693 - val_mean_squared_error: 1.7693\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.7950 - mean_squared_error: 0.7950 - val_loss: 2.2905 - val_mean_squared_error: 2.2905\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.7084 - mean_squared_error: 0.7084 - val_loss: 1.8167 - val_mean_squared_error: 1.8167\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.7457 - mean_squared_error: 0.7457 - val_loss: 1.7374 - val_mean_squared_error: 1.7374\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.6682 - mean_squared_error: 0.6682 - val_loss: 1.9024 - val_mean_squared_error: 1.9024\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 0.7935 - mean_squared_error: 0.7935 - val_loss: 1.9666 - val_mean_squared_error: 1.9666\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.7348 - mean_squared_error: 0.7348 - val_loss: 1.7885 - val_mean_squared_error: 1.7885\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7259 - mean_squared_error: 0.7259 - val_loss: 1.7880 - val_mean_squared_error: 1.7880\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.6913 - mean_squared_error: 0.6913 - val_loss: 1.9704 - val_mean_squared_error: 1.9704\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.6727 - mean_squared_error: 0.6727 - val_loss: 1.7336 - val_mean_squared_error: 1.7336\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.7312 - mean_squared_error: 0.7312 - val_loss: 1.8379 - val_mean_squared_error: 1.8379\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.6662 - mean_squared_error: 0.6662 - val_loss: 1.7676 - val_mean_squared_error: 1.7676\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.6608 - mean_squared_error: 0.6608 - val_loss: 1.7621 - val_mean_squared_error: 1.7621\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.6891 - mean_squared_error: 0.6891 - val_loss: 1.8534 - val_mean_squared_error: 1.8534\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.7013 - mean_squared_error: 0.7013 - val_loss: 1.8317 - val_mean_squared_error: 1.8317\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.7350 - mean_squared_error: 0.7350 - val_loss: 1.7860 - val_mean_squared_error: 1.7860\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.7343 - mean_squared_error: 0.7343 - val_loss: 1.9021 - val_mean_squared_error: 1.9021\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.6884 - mean_squared_error: 0.6884 - val_loss: 1.7396 - val_mean_squared_error: 1.7396\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.6660 - mean_squared_error: 0.6660 - val_loss: 1.9153 - val_mean_squared_error: 1.9153\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7059 - mean_squared_error: 0.7059 - val_loss: 1.9798 - val_mean_squared_error: 1.9798\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.6750 - mean_squared_error: 0.6750 - val_loss: 1.6696 - val_mean_squared_error: 1.6696\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.6462 - mean_squared_error: 0.6462 - val_loss: 1.8894 - val_mean_squared_error: 1.8894\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.6938 - mean_squared_error: 0.6938 - val_loss: 1.7799 - val_mean_squared_error: 1.7799\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.6609 - mean_squared_error: 0.6609 - val_loss: 1.7358 - val_mean_squared_error: 1.7358\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.6236 - mean_squared_error: 0.6236 - val_loss: 1.6884 - val_mean_squared_error: 1.6884\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.6071 - mean_squared_error: 0.6071 - val_loss: 1.8805 - val_mean_squared_error: 1.8805\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.6642 - mean_squared_error: 0.6642 - val_loss: 1.9436 - val_mean_squared_error: 1.9436\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.6715 - mean_squared_error: 0.6715 - val_loss: 1.9138 - val_mean_squared_error: 1.9138\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 366us/sample - loss: 0.6781 - mean_squared_error: 0.6781 - val_loss: 1.7464 - val_mean_squared_error: 1.7464\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.6341 - mean_squared_error: 0.6341 - val_loss: 1.9360 - val_mean_squared_error: 1.9360\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 361us/sample - loss: 0.6147 - mean_squared_error: 0.6147 - val_loss: 1.7906 - val_mean_squared_error: 1.7906\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.6182 - mean_squared_error: 0.6182 - val_loss: 1.7074 - val_mean_squared_error: 1.7074\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 368us/sample - loss: 0.6387 - mean_squared_error: 0.6387 - val_loss: 1.8758 - val_mean_squared_error: 1.8758\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.6257 - mean_squared_error: 0.6257 - val_loss: 1.7948 - val_mean_squared_error: 1.7948\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6268 - mean_squared_error: 0.6268 - val_loss: 1.6966 - val_mean_squared_error: 1.6966\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.6202 - mean_squared_error: 0.6202 - val_loss: 1.7558 - val_mean_squared_error: 1.7558\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.5746 - mean_squared_error: 0.5746 - val_loss: 1.9701 - val_mean_squared_error: 1.9701\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6020 - mean_squared_error: 0.6020 - val_loss: 1.7537 - val_mean_squared_error: 1.7537\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.5863 - mean_squared_error: 0.5863 - val_loss: 1.7308 - val_mean_squared_error: 1.7308\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.5985 - mean_squared_error: 0.5985 - val_loss: 1.7026 - val_mean_squared_error: 1.7026\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5890 - mean_squared_error: 0.5890 - val_loss: 1.7344 - val_mean_squared_error: 1.7344\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.6255 - mean_squared_error: 0.6255 - val_loss: 1.8355 - val_mean_squared_error: 1.8355\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.6354 - mean_squared_error: 0.6354 - val_loss: 1.7898 - val_mean_squared_error: 1.7898\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.5674 - mean_squared_error: 0.5674 - val_loss: 1.8936 - val_mean_squared_error: 1.8936\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.5988 - mean_squared_error: 0.5988 - val_loss: 1.6354 - val_mean_squared_error: 1.6354\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5830 - mean_squared_error: 0.5830 - val_loss: 1.8507 - val_mean_squared_error: 1.8507\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.5655 - mean_squared_error: 0.5655 - val_loss: 1.8417 - val_mean_squared_error: 1.8417\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.6177 - mean_squared_error: 0.6177 - val_loss: 1.9029 - val_mean_squared_error: 1.9029\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.5715 - mean_squared_error: 0.5715 - val_loss: 1.7679 - val_mean_squared_error: 1.7679\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.6185 - mean_squared_error: 0.6185 - val_loss: 1.9228 - val_mean_squared_error: 1.9228\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.5699 - mean_squared_error: 0.5699 - val_loss: 1.7686 - val_mean_squared_error: 1.7686\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.5671 - mean_squared_error: 0.5671 - val_loss: 1.9486 - val_mean_squared_error: 1.9486\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.5773 - mean_squared_error: 0.5773 - val_loss: 1.9411 - val_mean_squared_error: 1.9411\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5941 - mean_squared_error: 0.5941 - val_loss: 1.6953 - val_mean_squared_error: 1.6953\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.5346 - mean_squared_error: 0.5346 - val_loss: 1.7803 - val_mean_squared_error: 1.7803\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.5102 - mean_squared_error: 0.5102 - val_loss: 1.7422 - val_mean_squared_error: 1.7422\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.5397 - mean_squared_error: 0.5397 - val_loss: 1.7037 - val_mean_squared_error: 1.7037\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.5567 - mean_squared_error: 0.5567 - val_loss: 1.7221 - val_mean_squared_error: 1.7221\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.5394 - mean_squared_error: 0.5394 - val_loss: 1.7403 - val_mean_squared_error: 1.7403\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.5713 - mean_squared_error: 0.5713 - val_loss: 1.7796 - val_mean_squared_error: 1.7796\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5637 - mean_squared_error: 0.5637 - val_loss: 1.7519 - val_mean_squared_error: 1.7519\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.5676 - mean_squared_error: 0.5676 - val_loss: 1.7836 - val_mean_squared_error: 1.7836\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.5475 - mean_squared_error: 0.5475 - val_loss: 1.8316 - val_mean_squared_error: 1.8316\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.5601 - mean_squared_error: 0.5601 - val_loss: 1.7641 - val_mean_squared_error: 1.7641\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.5646 - mean_squared_error: 0.5646 - val_loss: 1.9031 - val_mean_squared_error: 1.9031\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.5394 - mean_squared_error: 0.5394 - val_loss: 1.7252 - val_mean_squared_error: 1.7252\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.5635 - mean_squared_error: 0.5635 - val_loss: 1.9499 - val_mean_squared_error: 1.9499\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.5051 - mean_squared_error: 0.5051 - val_loss: 1.7279 - val_mean_squared_error: 1.7279\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.5299 - mean_squared_error: 0.5299 - val_loss: 1.8161 - val_mean_squared_error: 1.8161\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5381 - mean_squared_error: 0.5381 - val_loss: 1.8425 - val_mean_squared_error: 1.8425\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5841 - mean_squared_error: 0.5841 - val_loss: 1.8028 - val_mean_squared_error: 1.8028\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.5486 - mean_squared_error: 0.5486 - val_loss: 1.7211 - val_mean_squared_error: 1.7211\n",
            "==================================================\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_36 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_60 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_37 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_38 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_50 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_51 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 757us/sample - loss: 225.8227 - mean_squared_error: 225.8227 - val_loss: 130.7595 - val_mean_squared_error: 130.7595\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 19.0552 - mean_squared_error: 19.0552 - val_loss: 49.5999 - val_mean_squared_error: 49.5999\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 19.2934 - mean_squared_error: 19.2934 - val_loss: 35.7771 - val_mean_squared_error: 35.7772\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 16.5490 - mean_squared_error: 16.5490 - val_loss: 19.1632 - val_mean_squared_error: 19.1632\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 14.0985 - mean_squared_error: 14.0985 - val_loss: 12.8358 - val_mean_squared_error: 12.8358\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 11.8114 - mean_squared_error: 11.8114 - val_loss: 10.2091 - val_mean_squared_error: 10.2091\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 11.8897 - mean_squared_error: 11.8897 - val_loss: 9.4801 - val_mean_squared_error: 9.4801\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 10.7165 - mean_squared_error: 10.7165 - val_loss: 10.3078 - val_mean_squared_error: 10.3078\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 9.3796 - mean_squared_error: 9.3796 - val_loss: 9.0058 - val_mean_squared_error: 9.0058\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 8.6063 - mean_squared_error: 8.6063 - val_loss: 7.7113 - val_mean_squared_error: 7.7113\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 6.6955 - mean_squared_error: 6.6955 - val_loss: 7.2252 - val_mean_squared_error: 7.2252\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 5.9014 - mean_squared_error: 5.9014 - val_loss: 5.8462 - val_mean_squared_error: 5.8462\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 5.0595 - mean_squared_error: 5.0595 - val_loss: 4.7280 - val_mean_squared_error: 4.7280\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 4.5997 - mean_squared_error: 4.5997 - val_loss: 4.3710 - val_mean_squared_error: 4.3710\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 4.0616 - mean_squared_error: 4.0616 - val_loss: 5.6640 - val_mean_squared_error: 5.6640\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 3.8872 - mean_squared_error: 3.8872 - val_loss: 4.2812 - val_mean_squared_error: 4.2812\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 374us/sample - loss: 3.8207 - mean_squared_error: 3.8207 - val_loss: 3.8618 - val_mean_squared_error: 3.8618\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 3.5365 - mean_squared_error: 3.5365 - val_loss: 3.5279 - val_mean_squared_error: 3.5279\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 3.1992 - mean_squared_error: 3.1992 - val_loss: 3.7513 - val_mean_squared_error: 3.7513\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 3.1808 - mean_squared_error: 3.1808 - val_loss: 3.8716 - val_mean_squared_error: 3.8716\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 2.9019 - mean_squared_error: 2.9019 - val_loss: 3.7506 - val_mean_squared_error: 3.7506\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 2.7783 - mean_squared_error: 2.7783 - val_loss: 4.2286 - val_mean_squared_error: 4.2286\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 2.6318 - mean_squared_error: 2.6318 - val_loss: 3.5830 - val_mean_squared_error: 3.5830\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 2.4644 - mean_squared_error: 2.4644 - val_loss: 3.1486 - val_mean_squared_error: 3.1486\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 2.5338 - mean_squared_error: 2.5338 - val_loss: 3.0858 - val_mean_squared_error: 3.0858\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 2.3739 - mean_squared_error: 2.3739 - val_loss: 3.0140 - val_mean_squared_error: 3.0140\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 2.2337 - mean_squared_error: 2.2337 - val_loss: 2.9079 - val_mean_squared_error: 2.9079\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 2.3145 - mean_squared_error: 2.3145 - val_loss: 2.9468 - val_mean_squared_error: 2.9468\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 2.3149 - mean_squared_error: 2.3149 - val_loss: 2.9369 - val_mean_squared_error: 2.9369\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 2.2053 - mean_squared_error: 2.2053 - val_loss: 3.1826 - val_mean_squared_error: 3.1826\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 2.0275 - mean_squared_error: 2.0275 - val_loss: 2.7885 - val_mean_squared_error: 2.7885\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 2.0278 - mean_squared_error: 2.0278 - val_loss: 2.6932 - val_mean_squared_error: 2.6932\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 2.1309 - mean_squared_error: 2.1309 - val_loss: 3.5828 - val_mean_squared_error: 3.5828\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 1.9467 - mean_squared_error: 1.9467 - val_loss: 2.7809 - val_mean_squared_error: 2.7809\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.8235 - mean_squared_error: 1.8235 - val_loss: 2.6282 - val_mean_squared_error: 2.6282\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 1.8413 - mean_squared_error: 1.8413 - val_loss: 2.6485 - val_mean_squared_error: 2.6485\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 1.8589 - mean_squared_error: 1.8589 - val_loss: 2.5173 - val_mean_squared_error: 2.5173\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.6248 - mean_squared_error: 1.6248 - val_loss: 2.4400 - val_mean_squared_error: 2.4400\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.7370 - mean_squared_error: 1.7370 - val_loss: 3.0081 - val_mean_squared_error: 3.0081\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 1.6782 - mean_squared_error: 1.6782 - val_loss: 2.5597 - val_mean_squared_error: 2.5597\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 1.5275 - mean_squared_error: 1.5275 - val_loss: 2.2279 - val_mean_squared_error: 2.2279\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 1.5370 - mean_squared_error: 1.5370 - val_loss: 2.5136 - val_mean_squared_error: 2.5136\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.5792 - mean_squared_error: 1.5792 - val_loss: 2.4019 - val_mean_squared_error: 2.4019\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 375us/sample - loss: 1.5308 - mean_squared_error: 1.5308 - val_loss: 2.3414 - val_mean_squared_error: 2.3414\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 375us/sample - loss: 1.4914 - mean_squared_error: 1.4914 - val_loss: 2.8932 - val_mean_squared_error: 2.8932\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 1.5943 - mean_squared_error: 1.5943 - val_loss: 2.2673 - val_mean_squared_error: 2.2673\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 1.5859 - mean_squared_error: 1.5859 - val_loss: 2.3597 - val_mean_squared_error: 2.3597\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 1.4156 - mean_squared_error: 1.4156 - val_loss: 2.5452 - val_mean_squared_error: 2.5452\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.4203 - mean_squared_error: 1.4202 - val_loss: 2.6457 - val_mean_squared_error: 2.6457\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 1.2999 - mean_squared_error: 1.2999 - val_loss: 2.5381 - val_mean_squared_error: 2.5381\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.2957 - mean_squared_error: 1.2957 - val_loss: 2.3595 - val_mean_squared_error: 2.3595\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.2827 - mean_squared_error: 1.2827 - val_loss: 2.3456 - val_mean_squared_error: 2.3456\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.2781 - mean_squared_error: 1.2781 - val_loss: 2.1993 - val_mean_squared_error: 2.1993\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 1.4258 - mean_squared_error: 1.4258 - val_loss: 2.7445 - val_mean_squared_error: 2.7445\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.3646 - mean_squared_error: 1.3646 - val_loss: 2.2809 - val_mean_squared_error: 2.2809\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 1.3166 - mean_squared_error: 1.3166 - val_loss: 2.2389 - val_mean_squared_error: 2.2389\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 1.3197 - mean_squared_error: 1.3197 - val_loss: 2.3156 - val_mean_squared_error: 2.3156\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 1.2257 - mean_squared_error: 1.2257 - val_loss: 2.2961 - val_mean_squared_error: 2.2961\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.1679 - mean_squared_error: 1.1679 - val_loss: 2.2416 - val_mean_squared_error: 2.2416\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 1.2314 - mean_squared_error: 1.2314 - val_loss: 2.3409 - val_mean_squared_error: 2.3409\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 1.1387 - mean_squared_error: 1.1387 - val_loss: 2.2918 - val_mean_squared_error: 2.2918\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 1.2152 - mean_squared_error: 1.2152 - val_loss: 2.3108 - val_mean_squared_error: 2.3108\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 1.2054 - mean_squared_error: 1.2054 - val_loss: 2.3571 - val_mean_squared_error: 2.3571\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 1.3126 - mean_squared_error: 1.3126 - val_loss: 2.0611 - val_mean_squared_error: 2.0611\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.1284 - mean_squared_error: 1.1284 - val_loss: 2.2383 - val_mean_squared_error: 2.2383\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.0960 - mean_squared_error: 1.0960 - val_loss: 2.1170 - val_mean_squared_error: 2.1170\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 1.0159 - mean_squared_error: 1.0159 - val_loss: 2.3024 - val_mean_squared_error: 2.3024\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.1060 - mean_squared_error: 1.1060 - val_loss: 2.1530 - val_mean_squared_error: 2.1530\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.1589 - mean_squared_error: 1.1589 - val_loss: 2.3425 - val_mean_squared_error: 2.3425\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.1664 - mean_squared_error: 1.1664 - val_loss: 2.1970 - val_mean_squared_error: 2.1970\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 1.0221 - mean_squared_error: 1.0221 - val_loss: 2.0465 - val_mean_squared_error: 2.0465\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 1.2728 - mean_squared_error: 1.2728 - val_loss: 2.2395 - val_mean_squared_error: 2.2395\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.0963 - mean_squared_error: 1.0963 - val_loss: 1.9384 - val_mean_squared_error: 1.9384\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.0721 - mean_squared_error: 1.0721 - val_loss: 2.2155 - val_mean_squared_error: 2.2155\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0530 - mean_squared_error: 1.0530 - val_loss: 1.9652 - val_mean_squared_error: 1.9652\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.0233 - mean_squared_error: 1.0233 - val_loss: 2.1043 - val_mean_squared_error: 2.1043\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.0416 - mean_squared_error: 1.0416 - val_loss: 2.2996 - val_mean_squared_error: 2.2996\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.9557 - mean_squared_error: 0.9557 - val_loss: 2.1557 - val_mean_squared_error: 2.1557\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0006 - mean_squared_error: 1.0006 - val_loss: 2.0275 - val_mean_squared_error: 2.0275\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.9825 - mean_squared_error: 0.9825 - val_loss: 2.4176 - val_mean_squared_error: 2.4176\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 1.0464 - mean_squared_error: 1.0464 - val_loss: 2.1126 - val_mean_squared_error: 2.1126\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.9274 - mean_squared_error: 0.9274 - val_loss: 2.4050 - val_mean_squared_error: 2.4050\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.9065 - mean_squared_error: 0.9065 - val_loss: 1.9634 - val_mean_squared_error: 1.9634\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.9248 - mean_squared_error: 0.9248 - val_loss: 2.1863 - val_mean_squared_error: 2.1863\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.9425 - mean_squared_error: 0.9425 - val_loss: 2.1182 - val_mean_squared_error: 2.1182\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.9921 - mean_squared_error: 0.9921 - val_loss: 2.2488 - val_mean_squared_error: 2.2488\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.8485 - mean_squared_error: 0.8485 - val_loss: 2.1995 - val_mean_squared_error: 2.1995\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.9387 - mean_squared_error: 0.9387 - val_loss: 1.8563 - val_mean_squared_error: 1.8563\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.9167 - mean_squared_error: 0.9167 - val_loss: 2.1563 - val_mean_squared_error: 2.1563\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 1.0513 - mean_squared_error: 1.0513 - val_loss: 2.1305 - val_mean_squared_error: 2.1305\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.9557 - mean_squared_error: 0.9557 - val_loss: 2.5664 - val_mean_squared_error: 2.5664\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.8773 - mean_squared_error: 0.8773 - val_loss: 2.4792 - val_mean_squared_error: 2.4792\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.9710 - mean_squared_error: 0.9710 - val_loss: 2.0125 - val_mean_squared_error: 2.0125\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.9205 - mean_squared_error: 0.9205 - val_loss: 2.1790 - val_mean_squared_error: 2.1790\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.8470 - mean_squared_error: 0.8470 - val_loss: 1.9871 - val_mean_squared_error: 1.9871\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 1.9772 - val_mean_squared_error: 1.9772\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.8180 - mean_squared_error: 0.8180 - val_loss: 2.1000 - val_mean_squared_error: 2.1000\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.8140 - mean_squared_error: 0.8140 - val_loss: 1.9567 - val_mean_squared_error: 1.9567\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.8729 - mean_squared_error: 0.8729 - val_loss: 2.2966 - val_mean_squared_error: 2.2966\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.9300 - mean_squared_error: 0.9300 - val_loss: 1.9467 - val_mean_squared_error: 1.9467\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.7294 - mean_squared_error: 0.7294 - val_loss: 2.0092 - val_mean_squared_error: 2.0092\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.7535 - mean_squared_error: 0.7535 - val_loss: 1.8998 - val_mean_squared_error: 1.8998\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.7939 - mean_squared_error: 0.7939 - val_loss: 2.3174 - val_mean_squared_error: 2.3174\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.8717 - mean_squared_error: 0.8717 - val_loss: 1.8618 - val_mean_squared_error: 1.8618\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.8063 - mean_squared_error: 0.8063 - val_loss: 2.0794 - val_mean_squared_error: 2.0794\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 0.8879 - mean_squared_error: 0.8879 - val_loss: 1.9610 - val_mean_squared_error: 1.9610\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.8227 - mean_squared_error: 0.8227 - val_loss: 1.9727 - val_mean_squared_error: 1.9727\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.8092 - mean_squared_error: 0.8092 - val_loss: 1.9958 - val_mean_squared_error: 1.9958\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 0.7228 - mean_squared_error: 0.7228 - val_loss: 2.1896 - val_mean_squared_error: 2.1896\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.7828 - mean_squared_error: 0.7828 - val_loss: 1.9809 - val_mean_squared_error: 1.9809\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.7791 - mean_squared_error: 0.7791 - val_loss: 1.9842 - val_mean_squared_error: 1.9842\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.7288 - mean_squared_error: 0.7288 - val_loss: 2.1132 - val_mean_squared_error: 2.1132\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.7146 - mean_squared_error: 0.7146 - val_loss: 2.0183 - val_mean_squared_error: 2.0183\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.7608 - mean_squared_error: 0.7608 - val_loss: 1.9635 - val_mean_squared_error: 1.9635\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.7604 - mean_squared_error: 0.7604 - val_loss: 1.8311 - val_mean_squared_error: 1.8311\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.7867 - mean_squared_error: 0.7867 - val_loss: 1.9153 - val_mean_squared_error: 1.9153\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.6881 - mean_squared_error: 0.6881 - val_loss: 2.1397 - val_mean_squared_error: 2.1397\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.7247 - mean_squared_error: 0.7247 - val_loss: 1.9898 - val_mean_squared_error: 1.9898\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.7126 - mean_squared_error: 0.7126 - val_loss: 2.0121 - val_mean_squared_error: 2.0121\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.7063 - mean_squared_error: 0.7063 - val_loss: 2.0380 - val_mean_squared_error: 2.0380\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.7402 - mean_squared_error: 0.7402 - val_loss: 1.9669 - val_mean_squared_error: 1.9669\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.7413 - mean_squared_error: 0.7413 - val_loss: 1.7890 - val_mean_squared_error: 1.7890\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.7124 - mean_squared_error: 0.7124 - val_loss: 1.9680 - val_mean_squared_error: 1.9680\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.6869 - mean_squared_error: 0.6869 - val_loss: 1.8768 - val_mean_squared_error: 1.8768\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 375us/sample - loss: 0.6469 - mean_squared_error: 0.6469 - val_loss: 1.8883 - val_mean_squared_error: 1.8883\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.7054 - mean_squared_error: 0.7054 - val_loss: 2.2049 - val_mean_squared_error: 2.2049\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 374us/sample - loss: 0.7082 - mean_squared_error: 0.7082 - val_loss: 2.1446 - val_mean_squared_error: 2.1446\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.7095 - mean_squared_error: 0.7095 - val_loss: 1.9959 - val_mean_squared_error: 1.9959\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.7411 - mean_squared_error: 0.7411 - val_loss: 1.8716 - val_mean_squared_error: 1.8716\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.7004 - mean_squared_error: 0.7004 - val_loss: 1.8634 - val_mean_squared_error: 1.8634\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.7412 - mean_squared_error: 0.7412 - val_loss: 1.9627 - val_mean_squared_error: 1.9627\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.6720 - mean_squared_error: 0.6720 - val_loss: 1.8233 - val_mean_squared_error: 1.8233\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.6061 - mean_squared_error: 0.6061 - val_loss: 1.9098 - val_mean_squared_error: 1.9098\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.6299 - mean_squared_error: 0.6299 - val_loss: 1.8833 - val_mean_squared_error: 1.8833\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.6202 - mean_squared_error: 0.6202 - val_loss: 1.8838 - val_mean_squared_error: 1.8838\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.6363 - mean_squared_error: 0.6363 - val_loss: 1.8995 - val_mean_squared_error: 1.8995\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.6216 - mean_squared_error: 0.6216 - val_loss: 1.8825 - val_mean_squared_error: 1.8825\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.6170 - mean_squared_error: 0.6170 - val_loss: 2.0579 - val_mean_squared_error: 2.0579\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.6204 - mean_squared_error: 0.6204 - val_loss: 1.8559 - val_mean_squared_error: 1.8559\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.7575 - mean_squared_error: 0.7575 - val_loss: 1.8597 - val_mean_squared_error: 1.8597\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.6351 - mean_squared_error: 0.6351 - val_loss: 1.9092 - val_mean_squared_error: 1.9092\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.5918 - mean_squared_error: 0.5918 - val_loss: 1.8431 - val_mean_squared_error: 1.8431\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.6388 - mean_squared_error: 0.6388 - val_loss: 1.7952 - val_mean_squared_error: 1.7952\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.6241 - mean_squared_error: 0.6241 - val_loss: 1.9517 - val_mean_squared_error: 1.9517\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.6463 - mean_squared_error: 0.6463 - val_loss: 2.1648 - val_mean_squared_error: 2.1648\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.6084 - mean_squared_error: 0.6084 - val_loss: 1.9984 - val_mean_squared_error: 1.9984\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.6260 - mean_squared_error: 0.6260 - val_loss: 1.8813 - val_mean_squared_error: 1.8813\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.6523 - mean_squared_error: 0.6523 - val_loss: 1.8839 - val_mean_squared_error: 1.8839\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5619 - mean_squared_error: 0.5619 - val_loss: 1.9455 - val_mean_squared_error: 1.9455\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.5800 - mean_squared_error: 0.5800 - val_loss: 2.2475 - val_mean_squared_error: 2.2475\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.6523 - mean_squared_error: 0.6523 - val_loss: 1.8473 - val_mean_squared_error: 1.8473\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.6273 - mean_squared_error: 0.6273 - val_loss: 1.8266 - val_mean_squared_error: 1.8266\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.6204 - mean_squared_error: 0.6204 - val_loss: 1.9673 - val_mean_squared_error: 1.9673\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.5667 - mean_squared_error: 0.5667 - val_loss: 1.8290 - val_mean_squared_error: 1.8290\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.5353 - mean_squared_error: 0.5353 - val_loss: 1.9024 - val_mean_squared_error: 1.9024\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.6306 - mean_squared_error: 0.6306 - val_loss: 1.9882 - val_mean_squared_error: 1.9882\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.6433 - mean_squared_error: 0.6433 - val_loss: 1.9470 - val_mean_squared_error: 1.9470\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.5997 - mean_squared_error: 0.5997 - val_loss: 1.9111 - val_mean_squared_error: 1.9111\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.5958 - mean_squared_error: 0.5958 - val_loss: 1.8246 - val_mean_squared_error: 1.8246\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.5992 - mean_squared_error: 0.5992 - val_loss: 1.9598 - val_mean_squared_error: 1.9598\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5757 - mean_squared_error: 0.5757 - val_loss: 1.9763 - val_mean_squared_error: 1.9763\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.6494 - mean_squared_error: 0.6494 - val_loss: 1.9165 - val_mean_squared_error: 1.9165\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.5749 - mean_squared_error: 0.5749 - val_loss: 1.8309 - val_mean_squared_error: 1.8309\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 375us/sample - loss: 0.5484 - mean_squared_error: 0.5484 - val_loss: 1.8977 - val_mean_squared_error: 1.8977\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.5572 - mean_squared_error: 0.5572 - val_loss: 1.8618 - val_mean_squared_error: 1.8618\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.6108 - mean_squared_error: 0.6108 - val_loss: 1.8833 - val_mean_squared_error: 1.8833\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.5902 - mean_squared_error: 0.5902 - val_loss: 2.1113 - val_mean_squared_error: 2.1113\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.5829 - mean_squared_error: 0.5829 - val_loss: 1.8056 - val_mean_squared_error: 1.8056\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5918 - mean_squared_error: 0.5918 - val_loss: 1.9108 - val_mean_squared_error: 1.9108\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5759 - mean_squared_error: 0.5759 - val_loss: 1.9169 - val_mean_squared_error: 1.9169\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 0.5811 - mean_squared_error: 0.5811 - val_loss: 1.9524 - val_mean_squared_error: 1.9524\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5896 - mean_squared_error: 0.5896 - val_loss: 1.8325 - val_mean_squared_error: 1.8325\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 375us/sample - loss: 0.5486 - mean_squared_error: 0.5486 - val_loss: 1.8629 - val_mean_squared_error: 1.8629\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.6143 - mean_squared_error: 0.6143 - val_loss: 2.0627 - val_mean_squared_error: 2.0627\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.6106 - mean_squared_error: 0.6106 - val_loss: 2.0001 - val_mean_squared_error: 2.0001\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.5011 - mean_squared_error: 0.5011 - val_loss: 1.8873 - val_mean_squared_error: 1.8873\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5057 - mean_squared_error: 0.5057 - val_loss: 1.9193 - val_mean_squared_error: 1.9193\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 376us/sample - loss: 0.5299 - mean_squared_error: 0.5299 - val_loss: 1.8344 - val_mean_squared_error: 1.8344\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 373us/sample - loss: 0.5719 - mean_squared_error: 0.5719 - val_loss: 1.9515 - val_mean_squared_error: 1.9515\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.4852 - mean_squared_error: 0.4852 - val_loss: 1.9812 - val_mean_squared_error: 1.9812\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.4852 - mean_squared_error: 0.4852 - val_loss: 1.9149 - val_mean_squared_error: 1.9149\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.5572 - mean_squared_error: 0.5572 - val_loss: 1.9018 - val_mean_squared_error: 1.9018\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.5187 - mean_squared_error: 0.5187 - val_loss: 1.8117 - val_mean_squared_error: 1.8117\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.5519 - mean_squared_error: 0.5519 - val_loss: 1.9390 - val_mean_squared_error: 1.9390\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.4858 - mean_squared_error: 0.4858 - val_loss: 2.0167 - val_mean_squared_error: 2.0167\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.5322 - mean_squared_error: 0.5322 - val_loss: 1.8898 - val_mean_squared_error: 1.8898\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.5016 - mean_squared_error: 0.5016 - val_loss: 1.8587 - val_mean_squared_error: 1.8587\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 377us/sample - loss: 0.4777 - mean_squared_error: 0.4777 - val_loss: 1.9032 - val_mean_squared_error: 1.9032\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.5525 - mean_squared_error: 0.5525 - val_loss: 1.8208 - val_mean_squared_error: 1.8208\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 379us/sample - loss: 0.4929 - mean_squared_error: 0.4929 - val_loss: 1.8860 - val_mean_squared_error: 1.8860\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.5059 - mean_squared_error: 0.5059 - val_loss: 1.9017 - val_mean_squared_error: 1.9017\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.5059 - mean_squared_error: 0.5059 - val_loss: 1.8903 - val_mean_squared_error: 1.8903\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.4918 - mean_squared_error: 0.4918 - val_loss: 1.9294 - val_mean_squared_error: 1.9294\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5027 - mean_squared_error: 0.5027 - val_loss: 1.8046 - val_mean_squared_error: 1.8046\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 0.4826 - mean_squared_error: 0.4826 - val_loss: 1.9769 - val_mean_squared_error: 1.9769\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.4580 - mean_squared_error: 0.4580 - val_loss: 1.8496 - val_mean_squared_error: 1.8496\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 380us/sample - loss: 0.4643 - mean_squared_error: 0.4643 - val_loss: 1.8178 - val_mean_squared_error: 1.8178\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 0.4790 - mean_squared_error: 0.4790 - val_loss: 1.8891 - val_mean_squared_error: 1.8891\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.4998 - mean_squared_error: 0.4998 - val_loss: 1.8495 - val_mean_squared_error: 1.8495\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 378us/sample - loss: 0.4844 - mean_squared_error: 0.4844 - val_loss: 1.8576 - val_mean_squared_error: 1.8576\n",
            "==================================================\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_39 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_39 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_52 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_40 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_53 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_41 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_54 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_68 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_55 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_69 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 226.7116 - mean_squared_error: 226.7116 - val_loss: 195.3012 - val_mean_squared_error: 195.3012\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 21.7994 - mean_squared_error: 21.7994 - val_loss: 40.3885 - val_mean_squared_error: 40.3885\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 17.1563 - mean_squared_error: 17.1563 - val_loss: 19.4497 - val_mean_squared_error: 19.4497\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 15.1951 - mean_squared_error: 15.1951 - val_loss: 11.3941 - val_mean_squared_error: 11.3941\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 13.1415 - mean_squared_error: 13.1415 - val_loss: 12.0542 - val_mean_squared_error: 12.0542\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 12.3726 - mean_squared_error: 12.3726 - val_loss: 10.6672 - val_mean_squared_error: 10.6672\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 10.6595 - mean_squared_error: 10.6595 - val_loss: 8.6612 - val_mean_squared_error: 8.6612\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 10.7983 - mean_squared_error: 10.7983 - val_loss: 9.5732 - val_mean_squared_error: 9.5732\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 9.1950 - mean_squared_error: 9.1950 - val_loss: 7.5182 - val_mean_squared_error: 7.5182\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 7.4789 - mean_squared_error: 7.4789 - val_loss: 6.6889 - val_mean_squared_error: 6.6889\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 6.6434 - mean_squared_error: 6.6434 - val_loss: 8.2187 - val_mean_squared_error: 8.2187\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 5.9679 - mean_squared_error: 5.9679 - val_loss: 6.1300 - val_mean_squared_error: 6.1300\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 5.7389 - mean_squared_error: 5.7389 - val_loss: 4.6019 - val_mean_squared_error: 4.6019\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 4.5371 - mean_squared_error: 4.5371 - val_loss: 4.1410 - val_mean_squared_error: 4.1410\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 4.6694 - mean_squared_error: 4.6694 - val_loss: 4.4732 - val_mean_squared_error: 4.4732\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 4.0016 - mean_squared_error: 4.0016 - val_loss: 4.4798 - val_mean_squared_error: 4.4798\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 3.8194 - mean_squared_error: 3.8194 - val_loss: 3.6181 - val_mean_squared_error: 3.6181\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 3.6642 - mean_squared_error: 3.6642 - val_loss: 3.3103 - val_mean_squared_error: 3.3103\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 3.3399 - mean_squared_error: 3.3399 - val_loss: 3.4885 - val_mean_squared_error: 3.4885\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 3.2487 - mean_squared_error: 3.2487 - val_loss: 3.0981 - val_mean_squared_error: 3.0981\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 3.0539 - mean_squared_error: 3.0539 - val_loss: 2.8957 - val_mean_squared_error: 2.8957\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 2.8795 - mean_squared_error: 2.8795 - val_loss: 2.7503 - val_mean_squared_error: 2.7503\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 2.8486 - mean_squared_error: 2.8486 - val_loss: 2.9517 - val_mean_squared_error: 2.9517\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 2.7904 - mean_squared_error: 2.7904 - val_loss: 2.7883 - val_mean_squared_error: 2.7883\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 2.7141 - mean_squared_error: 2.7141 - val_loss: 3.1325 - val_mean_squared_error: 3.1325\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.5780 - mean_squared_error: 2.5780 - val_loss: 2.9156 - val_mean_squared_error: 2.9156\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 2.6586 - mean_squared_error: 2.6586 - val_loss: 2.9203 - val_mean_squared_error: 2.9203\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 2.4915 - mean_squared_error: 2.4915 - val_loss: 3.2851 - val_mean_squared_error: 3.2851\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 2.5124 - mean_squared_error: 2.5124 - val_loss: 2.3334 - val_mean_squared_error: 2.3334\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 2.3314 - mean_squared_error: 2.3314 - val_loss: 2.9559 - val_mean_squared_error: 2.9559\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 2.2596 - mean_squared_error: 2.2596 - val_loss: 2.3835 - val_mean_squared_error: 2.3835\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 2.1398 - mean_squared_error: 2.1398 - val_loss: 2.6147 - val_mean_squared_error: 2.6147\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 2.1419 - mean_squared_error: 2.1419 - val_loss: 4.4464 - val_mean_squared_error: 4.4464\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 2.2843 - mean_squared_error: 2.2843 - val_loss: 2.3654 - val_mean_squared_error: 2.3654\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 2.0964 - mean_squared_error: 2.0964 - val_loss: 2.4153 - val_mean_squared_error: 2.4153\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 2.0567 - mean_squared_error: 2.0567 - val_loss: 2.2309 - val_mean_squared_error: 2.2309\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.9651 - mean_squared_error: 1.9651 - val_loss: 2.5773 - val_mean_squared_error: 2.5773\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.9904 - mean_squared_error: 1.9904 - val_loss: 2.1848 - val_mean_squared_error: 2.1848\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.8489 - mean_squared_error: 1.8489 - val_loss: 2.2348 - val_mean_squared_error: 2.2348\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.9079 - mean_squared_error: 1.9079 - val_loss: 2.4494 - val_mean_squared_error: 2.4494\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.8465 - mean_squared_error: 1.8465 - val_loss: 2.4689 - val_mean_squared_error: 2.4689\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 1.8292 - mean_squared_error: 1.8292 - val_loss: 2.2833 - val_mean_squared_error: 2.2833\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.9476 - mean_squared_error: 1.9476 - val_loss: 1.9874 - val_mean_squared_error: 1.9874\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.7314 - mean_squared_error: 1.7314 - val_loss: 2.0916 - val_mean_squared_error: 2.0916\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 1.8289 - mean_squared_error: 1.8289 - val_loss: 2.0800 - val_mean_squared_error: 2.0800\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.6677 - mean_squared_error: 1.6677 - val_loss: 2.3743 - val_mean_squared_error: 2.3743\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.6456 - mean_squared_error: 1.6456 - val_loss: 2.1810 - val_mean_squared_error: 2.1810\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.5934 - mean_squared_error: 1.5934 - val_loss: 2.0341 - val_mean_squared_error: 2.0341\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.6526 - mean_squared_error: 1.6526 - val_loss: 2.5021 - val_mean_squared_error: 2.5021\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 1.5714 - mean_squared_error: 1.5714 - val_loss: 2.4837 - val_mean_squared_error: 2.4837\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.5069 - mean_squared_error: 1.5069 - val_loss: 1.7929 - val_mean_squared_error: 1.7929\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.5373 - mean_squared_error: 1.5373 - val_loss: 2.8489 - val_mean_squared_error: 2.8489\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.5570 - mean_squared_error: 1.5570 - val_loss: 2.1694 - val_mean_squared_error: 2.1694\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.5306 - mean_squared_error: 1.5306 - val_loss: 2.1478 - val_mean_squared_error: 2.1478\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.5259 - mean_squared_error: 1.5259 - val_loss: 1.9519 - val_mean_squared_error: 1.9519\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.4139 - mean_squared_error: 1.4139 - val_loss: 1.8335 - val_mean_squared_error: 1.8335\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.4017 - mean_squared_error: 1.4017 - val_loss: 1.9740 - val_mean_squared_error: 1.9740\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.3732 - mean_squared_error: 1.3732 - val_loss: 1.9342 - val_mean_squared_error: 1.9342\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.4366 - mean_squared_error: 1.4366 - val_loss: 2.3596 - val_mean_squared_error: 2.3596\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.3350 - mean_squared_error: 1.3350 - val_loss: 2.5524 - val_mean_squared_error: 2.5524\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.3000 - mean_squared_error: 1.3000 - val_loss: 1.9104 - val_mean_squared_error: 1.9104\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.1956 - mean_squared_error: 1.1956 - val_loss: 1.9959 - val_mean_squared_error: 1.9959\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.3309 - mean_squared_error: 1.3309 - val_loss: 2.1704 - val_mean_squared_error: 2.1704\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.2812 - mean_squared_error: 1.2812 - val_loss: 2.1520 - val_mean_squared_error: 2.1520\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.2827 - mean_squared_error: 1.2827 - val_loss: 1.8521 - val_mean_squared_error: 1.8521\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1765 - mean_squared_error: 1.1765 - val_loss: 1.7024 - val_mean_squared_error: 1.7024\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.1573 - mean_squared_error: 1.1573 - val_loss: 1.7137 - val_mean_squared_error: 1.7137\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.3585 - mean_squared_error: 1.3585 - val_loss: 2.0026 - val_mean_squared_error: 2.0026\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.2925 - mean_squared_error: 1.2925 - val_loss: 1.9840 - val_mean_squared_error: 1.9840\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.2186 - mean_squared_error: 1.2186 - val_loss: 1.7089 - val_mean_squared_error: 1.7089\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.2838 - mean_squared_error: 1.2838 - val_loss: 1.9552 - val_mean_squared_error: 1.9552\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.2212 - mean_squared_error: 1.2212 - val_loss: 1.9813 - val_mean_squared_error: 1.9813\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 1.2192 - mean_squared_error: 1.2192 - val_loss: 1.7397 - val_mean_squared_error: 1.7397\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.2031 - mean_squared_error: 1.2031 - val_loss: 1.8565 - val_mean_squared_error: 1.8565\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.2051 - mean_squared_error: 1.2051 - val_loss: 2.1886 - val_mean_squared_error: 2.1886\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.1394 - mean_squared_error: 1.1394 - val_loss: 1.7788 - val_mean_squared_error: 1.7788\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.0911 - mean_squared_error: 1.0911 - val_loss: 1.8009 - val_mean_squared_error: 1.8009\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.0115 - mean_squared_error: 1.0115 - val_loss: 2.0089 - val_mean_squared_error: 2.0089\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.1372 - mean_squared_error: 1.1372 - val_loss: 2.0776 - val_mean_squared_error: 2.0776\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.0931 - mean_squared_error: 1.0931 - val_loss: 1.8340 - val_mean_squared_error: 1.8340\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0180 - mean_squared_error: 1.0180 - val_loss: 1.6330 - val_mean_squared_error: 1.6330\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1203 - mean_squared_error: 1.1203 - val_loss: 1.5894 - val_mean_squared_error: 1.5894\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.0927 - mean_squared_error: 1.0927 - val_loss: 1.7501 - val_mean_squared_error: 1.7501\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.0600 - mean_squared_error: 1.0600 - val_loss: 1.7558 - val_mean_squared_error: 1.7558\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0039 - mean_squared_error: 1.0039 - val_loss: 1.9341 - val_mean_squared_error: 1.9341\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.0890 - mean_squared_error: 1.0890 - val_loss: 1.8289 - val_mean_squared_error: 1.8289\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.1661 - mean_squared_error: 1.1661 - val_loss: 1.7534 - val_mean_squared_error: 1.7534\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 1.0236 - mean_squared_error: 1.0236 - val_loss: 1.8866 - val_mean_squared_error: 1.8866\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.9834 - mean_squared_error: 0.9834 - val_loss: 1.7048 - val_mean_squared_error: 1.7048\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 403us/sample - loss: 0.9682 - mean_squared_error: 0.9682 - val_loss: 1.6071 - val_mean_squared_error: 1.6071\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.9102 - mean_squared_error: 0.9102 - val_loss: 1.7074 - val_mean_squared_error: 1.7074\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.0408 - mean_squared_error: 1.0408 - val_loss: 1.6851 - val_mean_squared_error: 1.6851\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 0.9689 - mean_squared_error: 0.9689 - val_loss: 1.7795 - val_mean_squared_error: 1.7795\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.0174 - mean_squared_error: 1.0174 - val_loss: 2.3171 - val_mean_squared_error: 2.3171\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.0545 - mean_squared_error: 1.0545 - val_loss: 1.7365 - val_mean_squared_error: 1.7365\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.0382 - mean_squared_error: 1.0382 - val_loss: 1.6673 - val_mean_squared_error: 1.6673\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.9033 - mean_squared_error: 0.9033 - val_loss: 1.5928 - val_mean_squared_error: 1.5928\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.8872 - mean_squared_error: 0.8872 - val_loss: 1.8052 - val_mean_squared_error: 1.8052\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.9458 - mean_squared_error: 0.9458 - val_loss: 1.6996 - val_mean_squared_error: 1.6996\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.8841 - mean_squared_error: 0.8841 - val_loss: 1.6860 - val_mean_squared_error: 1.6860\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.8385 - mean_squared_error: 0.8385 - val_loss: 1.6044 - val_mean_squared_error: 1.6044\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8695 - mean_squared_error: 0.8695 - val_loss: 1.7177 - val_mean_squared_error: 1.7177\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.9479 - mean_squared_error: 0.9479 - val_loss: 1.7193 - val_mean_squared_error: 1.7193\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.8740 - mean_squared_error: 0.8740 - val_loss: 1.8277 - val_mean_squared_error: 1.8277\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8578 - mean_squared_error: 0.8578 - val_loss: 1.9621 - val_mean_squared_error: 1.9621\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9407 - mean_squared_error: 0.9407 - val_loss: 1.6854 - val_mean_squared_error: 1.6854\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8837 - mean_squared_error: 0.8837 - val_loss: 1.7270 - val_mean_squared_error: 1.7270\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.8474 - mean_squared_error: 0.8474 - val_loss: 1.6807 - val_mean_squared_error: 1.6807\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8965 - mean_squared_error: 0.8965 - val_loss: 1.6421 - val_mean_squared_error: 1.6421\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8499 - mean_squared_error: 0.8499 - val_loss: 1.9451 - val_mean_squared_error: 1.9451\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8133 - mean_squared_error: 0.8133 - val_loss: 1.5337 - val_mean_squared_error: 1.5337\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8684 - mean_squared_error: 0.8684 - val_loss: 1.7019 - val_mean_squared_error: 1.7019\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.7949 - mean_squared_error: 0.7949 - val_loss: 1.8548 - val_mean_squared_error: 1.8548\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8335 - mean_squared_error: 0.8335 - val_loss: 1.5307 - val_mean_squared_error: 1.5307\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8683 - mean_squared_error: 0.8683 - val_loss: 1.6430 - val_mean_squared_error: 1.6430\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8730 - mean_squared_error: 0.8730 - val_loss: 1.6957 - val_mean_squared_error: 1.6957\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7870 - mean_squared_error: 0.7870 - val_loss: 1.7350 - val_mean_squared_error: 1.7350\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8169 - mean_squared_error: 0.8169 - val_loss: 1.6904 - val_mean_squared_error: 1.6904\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7615 - mean_squared_error: 0.7615 - val_loss: 1.8338 - val_mean_squared_error: 1.8338\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7703 - mean_squared_error: 0.7703 - val_loss: 1.7521 - val_mean_squared_error: 1.7521\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.7534 - mean_squared_error: 0.7534 - val_loss: 1.6225 - val_mean_squared_error: 1.6225\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7552 - mean_squared_error: 0.7552 - val_loss: 1.7617 - val_mean_squared_error: 1.7617\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7589 - mean_squared_error: 0.7589 - val_loss: 1.9205 - val_mean_squared_error: 1.9205\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.8305 - mean_squared_error: 0.8305 - val_loss: 1.7864 - val_mean_squared_error: 1.7864\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8581 - mean_squared_error: 0.8581 - val_loss: 2.3539 - val_mean_squared_error: 2.3539\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.8290 - mean_squared_error: 0.8290 - val_loss: 1.5973 - val_mean_squared_error: 1.5973\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7233 - mean_squared_error: 0.7233 - val_loss: 1.7401 - val_mean_squared_error: 1.7401\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.7089 - mean_squared_error: 0.7089 - val_loss: 1.7109 - val_mean_squared_error: 1.7109\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7316 - mean_squared_error: 0.7316 - val_loss: 1.6614 - val_mean_squared_error: 1.6614\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.7057 - mean_squared_error: 0.7057 - val_loss: 1.7409 - val_mean_squared_error: 1.7409\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7644 - mean_squared_error: 0.7644 - val_loss: 1.7944 - val_mean_squared_error: 1.7944\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.8621 - mean_squared_error: 0.8621 - val_loss: 1.6923 - val_mean_squared_error: 1.6923\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.6763 - mean_squared_error: 0.6763 - val_loss: 1.6992 - val_mean_squared_error: 1.6992\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.7297 - mean_squared_error: 0.7297 - val_loss: 1.6208 - val_mean_squared_error: 1.6208\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7410 - mean_squared_error: 0.7410 - val_loss: 1.7825 - val_mean_squared_error: 1.7825\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.7384 - mean_squared_error: 0.7384 - val_loss: 1.7003 - val_mean_squared_error: 1.7003\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.7242 - mean_squared_error: 0.7242 - val_loss: 1.6580 - val_mean_squared_error: 1.6580\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7704 - mean_squared_error: 0.7704 - val_loss: 1.6357 - val_mean_squared_error: 1.6357\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.7416 - mean_squared_error: 0.7416 - val_loss: 1.8125 - val_mean_squared_error: 1.8125\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7236 - mean_squared_error: 0.7236 - val_loss: 1.7391 - val_mean_squared_error: 1.7391\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6986 - mean_squared_error: 0.6986 - val_loss: 1.5861 - val_mean_squared_error: 1.5861\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.6517 - mean_squared_error: 0.6517 - val_loss: 1.5971 - val_mean_squared_error: 1.5971\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6562 - mean_squared_error: 0.6562 - val_loss: 1.7245 - val_mean_squared_error: 1.7245\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.7078 - mean_squared_error: 0.7078 - val_loss: 1.8360 - val_mean_squared_error: 1.8360\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7006 - mean_squared_error: 0.7006 - val_loss: 1.8200 - val_mean_squared_error: 1.8200\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.7171 - mean_squared_error: 0.7171 - val_loss: 1.8786 - val_mean_squared_error: 1.8786\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.6634 - mean_squared_error: 0.6634 - val_loss: 1.6644 - val_mean_squared_error: 1.6644\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7927 - mean_squared_error: 0.7927 - val_loss: 2.0496 - val_mean_squared_error: 2.0496\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7191 - mean_squared_error: 0.7191 - val_loss: 1.8018 - val_mean_squared_error: 1.8018\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6535 - mean_squared_error: 0.6535 - val_loss: 1.6056 - val_mean_squared_error: 1.6056\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6832 - mean_squared_error: 0.6832 - val_loss: 1.9954 - val_mean_squared_error: 1.9954\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6278 - mean_squared_error: 0.6278 - val_loss: 1.8557 - val_mean_squared_error: 1.8557\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6204 - mean_squared_error: 0.6204 - val_loss: 1.6120 - val_mean_squared_error: 1.6120\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6398 - mean_squared_error: 0.6398 - val_loss: 1.7612 - val_mean_squared_error: 1.7612\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6386 - mean_squared_error: 0.6386 - val_loss: 1.6761 - val_mean_squared_error: 1.6761\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6318 - mean_squared_error: 0.6318 - val_loss: 1.6745 - val_mean_squared_error: 1.6745\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6381 - mean_squared_error: 0.6381 - val_loss: 1.7307 - val_mean_squared_error: 1.7307\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6529 - mean_squared_error: 0.6529 - val_loss: 1.8846 - val_mean_squared_error: 1.8846\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6298 - mean_squared_error: 0.6298 - val_loss: 1.8538 - val_mean_squared_error: 1.8538\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6749 - mean_squared_error: 0.6749 - val_loss: 1.6846 - val_mean_squared_error: 1.6846\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.5807 - mean_squared_error: 0.5807 - val_loss: 1.6724 - val_mean_squared_error: 1.6724\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6086 - mean_squared_error: 0.6086 - val_loss: 1.6359 - val_mean_squared_error: 1.6359\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.6243 - mean_squared_error: 0.6243 - val_loss: 1.6429 - val_mean_squared_error: 1.6429\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.6179 - mean_squared_error: 0.6179 - val_loss: 1.8083 - val_mean_squared_error: 1.8083\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.6231 - mean_squared_error: 0.6231 - val_loss: 1.7720 - val_mean_squared_error: 1.7720\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5983 - mean_squared_error: 0.5983 - val_loss: 1.6673 - val_mean_squared_error: 1.6673\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.5781 - mean_squared_error: 0.5781 - val_loss: 1.7970 - val_mean_squared_error: 1.7970\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5823 - mean_squared_error: 0.5823 - val_loss: 1.7078 - val_mean_squared_error: 1.7078\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6242 - mean_squared_error: 0.6242 - val_loss: 1.8457 - val_mean_squared_error: 1.8457\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6184 - mean_squared_error: 0.6184 - val_loss: 1.7061 - val_mean_squared_error: 1.7061\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5805 - mean_squared_error: 0.5805 - val_loss: 1.7608 - val_mean_squared_error: 1.7608\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.5653 - mean_squared_error: 0.5653 - val_loss: 1.7555 - val_mean_squared_error: 1.7555\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.5480 - mean_squared_error: 0.5480 - val_loss: 1.6350 - val_mean_squared_error: 1.6350\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5638 - mean_squared_error: 0.5638 - val_loss: 1.7144 - val_mean_squared_error: 1.7144\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6130 - mean_squared_error: 0.6130 - val_loss: 1.8270 - val_mean_squared_error: 1.8270\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.6138 - mean_squared_error: 0.6138 - val_loss: 1.6684 - val_mean_squared_error: 1.6684\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5883 - mean_squared_error: 0.5883 - val_loss: 1.7462 - val_mean_squared_error: 1.7462\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.5523 - mean_squared_error: 0.5523 - val_loss: 1.6906 - val_mean_squared_error: 1.6906\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 0.6035 - mean_squared_error: 0.6035 - val_loss: 1.6901 - val_mean_squared_error: 1.6901\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.5339 - mean_squared_error: 0.5339 - val_loss: 1.7461 - val_mean_squared_error: 1.7461\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6239 - mean_squared_error: 0.6239 - val_loss: 1.6709 - val_mean_squared_error: 1.6709\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5451 - mean_squared_error: 0.5451 - val_loss: 1.7992 - val_mean_squared_error: 1.7992\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5701 - mean_squared_error: 0.5701 - val_loss: 1.8067 - val_mean_squared_error: 1.8067\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 0.5580 - mean_squared_error: 0.5580 - val_loss: 1.6402 - val_mean_squared_error: 1.6402\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5976 - mean_squared_error: 0.5976 - val_loss: 1.6478 - val_mean_squared_error: 1.6478\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6027 - mean_squared_error: 0.6027 - val_loss: 1.7721 - val_mean_squared_error: 1.7721\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6061 - mean_squared_error: 0.6061 - val_loss: 1.7162 - val_mean_squared_error: 1.7162\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.5998 - mean_squared_error: 0.5998 - val_loss: 1.7008 - val_mean_squared_error: 1.7008\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5655 - mean_squared_error: 0.5655 - val_loss: 1.7972 - val_mean_squared_error: 1.7972\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.5670 - mean_squared_error: 0.5670 - val_loss: 1.8280 - val_mean_squared_error: 1.8280\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5578 - mean_squared_error: 0.5578 - val_loss: 1.8550 - val_mean_squared_error: 1.8550\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.5524 - mean_squared_error: 0.5524 - val_loss: 1.7676 - val_mean_squared_error: 1.7676\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5563 - mean_squared_error: 0.5563 - val_loss: 1.6515 - val_mean_squared_error: 1.6515\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.5268 - mean_squared_error: 0.5268 - val_loss: 1.6682 - val_mean_squared_error: 1.6682\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.5688 - mean_squared_error: 0.5688 - val_loss: 1.8779 - val_mean_squared_error: 1.8779\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5103 - mean_squared_error: 0.5103 - val_loss: 1.7405 - val_mean_squared_error: 1.7405\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5200 - mean_squared_error: 0.5200 - val_loss: 1.7556 - val_mean_squared_error: 1.7556\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5491 - mean_squared_error: 0.5491 - val_loss: 1.7033 - val_mean_squared_error: 1.7033\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5289 - mean_squared_error: 0.5289 - val_loss: 1.6752 - val_mean_squared_error: 1.6752\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.5052 - mean_squared_error: 0.5052 - val_loss: 1.8117 - val_mean_squared_error: 1.8117\n",
            "==================================================\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_42 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_42 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_56 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_43 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_57 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_58 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_73 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_59 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_74 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 773us/sample - loss: 171.5404 - mean_squared_error: 171.5405 - val_loss: 181.1619 - val_mean_squared_error: 181.1619\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 19.4808 - mean_squared_error: 19.4808 - val_loss: 57.7885 - val_mean_squared_error: 57.7885\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 17.2639 - mean_squared_error: 17.2639 - val_loss: 23.6416 - val_mean_squared_error: 23.6416\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 15.5013 - mean_squared_error: 15.5013 - val_loss: 18.6819 - val_mean_squared_error: 18.6819\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 13.8931 - mean_squared_error: 13.8931 - val_loss: 14.9824 - val_mean_squared_error: 14.9824\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 12.1536 - mean_squared_error: 12.1536 - val_loss: 14.1705 - val_mean_squared_error: 14.1705\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 12.2923 - mean_squared_error: 12.2923 - val_loss: 11.1057 - val_mean_squared_error: 11.1057\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 10.0742 - mean_squared_error: 10.0742 - val_loss: 10.4033 - val_mean_squared_error: 10.4033\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 8.7966 - mean_squared_error: 8.7966 - val_loss: 8.6144 - val_mean_squared_error: 8.6144\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 335us/sample - loss: 7.2644 - mean_squared_error: 7.2644 - val_loss: 8.6460 - val_mean_squared_error: 8.6460\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 335us/sample - loss: 6.1993 - mean_squared_error: 6.1993 - val_loss: 6.2802 - val_mean_squared_error: 6.2802\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 5.3166 - mean_squared_error: 5.3166 - val_loss: 5.7791 - val_mean_squared_error: 5.7791\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 4.6440 - mean_squared_error: 4.6440 - val_loss: 4.8422 - val_mean_squared_error: 4.8422\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 4.2184 - mean_squared_error: 4.2184 - val_loss: 4.7371 - val_mean_squared_error: 4.7371\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 333us/sample - loss: 3.7779 - mean_squared_error: 3.7779 - val_loss: 4.6109 - val_mean_squared_error: 4.6109\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 3.6085 - mean_squared_error: 3.6085 - val_loss: 4.4770 - val_mean_squared_error: 4.4770\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 3.3666 - mean_squared_error: 3.3666 - val_loss: 3.8256 - val_mean_squared_error: 3.8256\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 3.2268 - mean_squared_error: 3.2268 - val_loss: 4.2022 - val_mean_squared_error: 4.2022\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.9315 - mean_squared_error: 2.9315 - val_loss: 3.5476 - val_mean_squared_error: 3.5476\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 2.7567 - mean_squared_error: 2.7567 - val_loss: 3.0520 - val_mean_squared_error: 3.0520\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 2.8528 - mean_squared_error: 2.8528 - val_loss: 3.4776 - val_mean_squared_error: 3.4776\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 2.5053 - mean_squared_error: 2.5053 - val_loss: 2.8251 - val_mean_squared_error: 2.8251\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 2.5661 - mean_squared_error: 2.5661 - val_loss: 4.1363 - val_mean_squared_error: 4.1363\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.4408 - mean_squared_error: 2.4408 - val_loss: 2.8235 - val_mean_squared_error: 2.8235\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 2.4169 - mean_squared_error: 2.4169 - val_loss: 2.9146 - val_mean_squared_error: 2.9146\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.3518 - mean_squared_error: 2.3518 - val_loss: 2.9335 - val_mean_squared_error: 2.9335\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 2.1268 - mean_squared_error: 2.1268 - val_loss: 2.8541 - val_mean_squared_error: 2.8541\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.2651 - mean_squared_error: 2.2651 - val_loss: 3.5486 - val_mean_squared_error: 3.5486\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.1431 - mean_squared_error: 2.1431 - val_loss: 2.9024 - val_mean_squared_error: 2.9024\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 2.0162 - mean_squared_error: 2.0162 - val_loss: 2.6763 - val_mean_squared_error: 2.6763\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 2.0300 - mean_squared_error: 2.0300 - val_loss: 2.6624 - val_mean_squared_error: 2.6624\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.9178 - mean_squared_error: 1.9178 - val_loss: 2.5591 - val_mean_squared_error: 2.5591\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.8178 - mean_squared_error: 1.8178 - val_loss: 2.3958 - val_mean_squared_error: 2.3958\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 2.0908 - mean_squared_error: 2.0908 - val_loss: 2.6564 - val_mean_squared_error: 2.6564\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.8840 - mean_squared_error: 1.8840 - val_loss: 3.0869 - val_mean_squared_error: 3.0869\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.8442 - mean_squared_error: 1.8442 - val_loss: 2.7430 - val_mean_squared_error: 2.7430\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.8335 - mean_squared_error: 1.8335 - val_loss: 2.4883 - val_mean_squared_error: 2.4883\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.6246 - mean_squared_error: 1.6246 - val_loss: 2.2987 - val_mean_squared_error: 2.2987\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 1.5330 - mean_squared_error: 1.5330 - val_loss: 2.1632 - val_mean_squared_error: 2.1632\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.6668 - mean_squared_error: 1.6668 - val_loss: 2.4251 - val_mean_squared_error: 2.4251\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.8200 - mean_squared_error: 1.8200 - val_loss: 2.3905 - val_mean_squared_error: 2.3905\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.5378 - mean_squared_error: 1.5378 - val_loss: 2.3540 - val_mean_squared_error: 2.3540\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.6408 - mean_squared_error: 1.6408 - val_loss: 2.6868 - val_mean_squared_error: 2.6868\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 1.5319 - mean_squared_error: 1.5319 - val_loss: 2.0988 - val_mean_squared_error: 2.0988\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.5862 - mean_squared_error: 1.5862 - val_loss: 2.6334 - val_mean_squared_error: 2.6334\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 1.6568 - mean_squared_error: 1.6568 - val_loss: 2.2715 - val_mean_squared_error: 2.2715\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 1.4760 - mean_squared_error: 1.4760 - val_loss: 3.2187 - val_mean_squared_error: 3.2187\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.5358 - mean_squared_error: 1.5358 - val_loss: 2.2656 - val_mean_squared_error: 2.2656\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.4594 - mean_squared_error: 1.4594 - val_loss: 2.1045 - val_mean_squared_error: 2.1045\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.4397 - mean_squared_error: 1.4397 - val_loss: 2.1464 - val_mean_squared_error: 2.1464\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.3090 - mean_squared_error: 1.3090 - val_loss: 2.3231 - val_mean_squared_error: 2.3231\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.2583 - mean_squared_error: 1.2583 - val_loss: 2.3623 - val_mean_squared_error: 2.3623\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.3768 - mean_squared_error: 1.3768 - val_loss: 2.1756 - val_mean_squared_error: 2.1756\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.2905 - mean_squared_error: 1.2905 - val_loss: 2.3466 - val_mean_squared_error: 2.3466\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2681 - mean_squared_error: 1.2681 - val_loss: 2.5466 - val_mean_squared_error: 2.5466\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.2399 - mean_squared_error: 1.2399 - val_loss: 1.9911 - val_mean_squared_error: 1.9911\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 1.3601 - mean_squared_error: 1.3601 - val_loss: 2.3461 - val_mean_squared_error: 2.3461\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.2490 - mean_squared_error: 1.2490 - val_loss: 3.2938 - val_mean_squared_error: 3.2938\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.2995 - mean_squared_error: 1.2995 - val_loss: 2.5295 - val_mean_squared_error: 2.5295\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.1937 - mean_squared_error: 1.1937 - val_loss: 2.3381 - val_mean_squared_error: 2.3381\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.1138 - mean_squared_error: 1.1138 - val_loss: 3.0353 - val_mean_squared_error: 3.0353\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.1498 - mean_squared_error: 1.1498 - val_loss: 2.3183 - val_mean_squared_error: 2.3183\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2445 - mean_squared_error: 1.2445 - val_loss: 2.2272 - val_mean_squared_error: 2.2272\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.2727 - mean_squared_error: 1.2727 - val_loss: 2.6147 - val_mean_squared_error: 2.6147\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.1120 - mean_squared_error: 1.1120 - val_loss: 2.6823 - val_mean_squared_error: 2.6823\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.2044 - mean_squared_error: 1.2044 - val_loss: 2.1192 - val_mean_squared_error: 2.1192\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.0666 - mean_squared_error: 1.0666 - val_loss: 2.2004 - val_mean_squared_error: 2.2004\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.0711 - mean_squared_error: 1.0711 - val_loss: 2.4231 - val_mean_squared_error: 2.4231\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.1947 - mean_squared_error: 1.1947 - val_loss: 2.2371 - val_mean_squared_error: 2.2371\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 1.1112 - mean_squared_error: 1.1112 - val_loss: 2.7487 - val_mean_squared_error: 2.7487\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 1.1942 - mean_squared_error: 1.1942 - val_loss: 2.1762 - val_mean_squared_error: 2.1762\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.0931 - mean_squared_error: 1.0931 - val_loss: 2.1498 - val_mean_squared_error: 2.1498\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.1325 - mean_squared_error: 1.1325 - val_loss: 2.6020 - val_mean_squared_error: 2.6020\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.0642 - mean_squared_error: 1.0642 - val_loss: 2.3280 - val_mean_squared_error: 2.3280\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.1608 - mean_squared_error: 1.1608 - val_loss: 1.9472 - val_mean_squared_error: 1.9472\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.0837 - mean_squared_error: 1.0837 - val_loss: 2.1180 - val_mean_squared_error: 2.1180\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.0474 - mean_squared_error: 1.0474 - val_loss: 2.4383 - val_mean_squared_error: 2.4383\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.0104 - mean_squared_error: 1.0104 - val_loss: 2.0961 - val_mean_squared_error: 2.0961\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 1.0010 - mean_squared_error: 1.0010 - val_loss: 2.1817 - val_mean_squared_error: 2.1817\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 1.0053 - mean_squared_error: 1.0053 - val_loss: 1.9925 - val_mean_squared_error: 1.9925\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.0339 - mean_squared_error: 1.0339 - val_loss: 2.3407 - val_mean_squared_error: 2.3407\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.9377 - mean_squared_error: 0.9377 - val_loss: 2.0681 - val_mean_squared_error: 2.0681\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.0721 - mean_squared_error: 1.0721 - val_loss: 1.9872 - val_mean_squared_error: 1.9872\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.9020 - mean_squared_error: 0.9020 - val_loss: 2.2114 - val_mean_squared_error: 2.2114\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.9410 - mean_squared_error: 0.9410 - val_loss: 1.9056 - val_mean_squared_error: 1.9056\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.8665 - mean_squared_error: 0.8665 - val_loss: 2.2483 - val_mean_squared_error: 2.2483\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.9215 - mean_squared_error: 0.9215 - val_loss: 2.2530 - val_mean_squared_error: 2.2530\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.9458 - mean_squared_error: 0.9458 - val_loss: 2.1068 - val_mean_squared_error: 2.1068\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9479 - mean_squared_error: 0.9479 - val_loss: 2.0903 - val_mean_squared_error: 2.0903\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.9895 - mean_squared_error: 0.9895 - val_loss: 2.2738 - val_mean_squared_error: 2.2738\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.9254 - mean_squared_error: 0.9254 - val_loss: 2.2146 - val_mean_squared_error: 2.2146\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.8878 - mean_squared_error: 0.8878 - val_loss: 1.9877 - val_mean_squared_error: 1.9877\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.8722 - mean_squared_error: 0.8722 - val_loss: 2.4371 - val_mean_squared_error: 2.4371\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.8551 - mean_squared_error: 0.8551 - val_loss: 2.0628 - val_mean_squared_error: 2.0628\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.9380 - mean_squared_error: 0.9380 - val_loss: 2.2171 - val_mean_squared_error: 2.2171\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.8576 - mean_squared_error: 0.8576 - val_loss: 1.9407 - val_mean_squared_error: 1.9407\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.8244 - mean_squared_error: 0.8244 - val_loss: 2.2372 - val_mean_squared_error: 2.2372\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.8728 - mean_squared_error: 0.8728 - val_loss: 2.0467 - val_mean_squared_error: 2.0467\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.9395 - mean_squared_error: 0.9395 - val_loss: 2.1678 - val_mean_squared_error: 2.1678\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.9731 - mean_squared_error: 0.9731 - val_loss: 2.3618 - val_mean_squared_error: 2.3618\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.8480 - mean_squared_error: 0.8480 - val_loss: 2.0581 - val_mean_squared_error: 2.0581\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.7983 - mean_squared_error: 0.7983 - val_loss: 2.0277 - val_mean_squared_error: 2.0277\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.8664 - mean_squared_error: 0.8664 - val_loss: 2.2349 - val_mean_squared_error: 2.2349\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.8104 - mean_squared_error: 0.8104 - val_loss: 2.0732 - val_mean_squared_error: 2.0732\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.7351 - mean_squared_error: 0.7351 - val_loss: 1.9180 - val_mean_squared_error: 1.9180\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.8126 - mean_squared_error: 0.8126 - val_loss: 2.4472 - val_mean_squared_error: 2.4472\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.9047 - mean_squared_error: 0.9047 - val_loss: 2.5355 - val_mean_squared_error: 2.5355\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.7601 - mean_squared_error: 0.7601 - val_loss: 1.9058 - val_mean_squared_error: 1.9058\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.7318 - mean_squared_error: 0.7318 - val_loss: 2.1204 - val_mean_squared_error: 2.1204\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.8243 - mean_squared_error: 0.8243 - val_loss: 2.2268 - val_mean_squared_error: 2.2268\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.7771 - mean_squared_error: 0.7771 - val_loss: 2.3191 - val_mean_squared_error: 2.3191\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.7187 - mean_squared_error: 0.7187 - val_loss: 2.1767 - val_mean_squared_error: 2.1767\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 0.7451 - mean_squared_error: 0.7451 - val_loss: 1.9685 - val_mean_squared_error: 1.9685\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7522 - mean_squared_error: 0.7522 - val_loss: 2.1161 - val_mean_squared_error: 2.1161\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.8762 - mean_squared_error: 0.8762 - val_loss: 1.8834 - val_mean_squared_error: 1.8834\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7432 - mean_squared_error: 0.7432 - val_loss: 2.3319 - val_mean_squared_error: 2.3319\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.7581 - mean_squared_error: 0.7581 - val_loss: 1.9438 - val_mean_squared_error: 1.9438\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7041 - mean_squared_error: 0.7041 - val_loss: 1.8755 - val_mean_squared_error: 1.8755\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.7289 - mean_squared_error: 0.7289 - val_loss: 2.0133 - val_mean_squared_error: 2.0133\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.7076 - mean_squared_error: 0.7076 - val_loss: 2.1084 - val_mean_squared_error: 2.1084\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.7304 - mean_squared_error: 0.7304 - val_loss: 2.1053 - val_mean_squared_error: 2.1053\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7250 - mean_squared_error: 0.7250 - val_loss: 2.0369 - val_mean_squared_error: 2.0369\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.7294 - mean_squared_error: 0.7294 - val_loss: 2.1099 - val_mean_squared_error: 2.1099\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.7673 - mean_squared_error: 0.7673 - val_loss: 1.9831 - val_mean_squared_error: 1.9831\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.7392 - mean_squared_error: 0.7392 - val_loss: 1.9840 - val_mean_squared_error: 1.9840\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.6974 - mean_squared_error: 0.6974 - val_loss: 2.0550 - val_mean_squared_error: 2.0550\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.7109 - mean_squared_error: 0.7109 - val_loss: 1.9088 - val_mean_squared_error: 1.9088\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.6523 - mean_squared_error: 0.6523 - val_loss: 1.8857 - val_mean_squared_error: 1.8857\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.8069 - mean_squared_error: 0.8069 - val_loss: 1.9254 - val_mean_squared_error: 1.9254\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.6676 - mean_squared_error: 0.6676 - val_loss: 1.8595 - val_mean_squared_error: 1.8595\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.5801 - mean_squared_error: 0.5801 - val_loss: 1.9638 - val_mean_squared_error: 1.9638\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.6894 - mean_squared_error: 0.6894 - val_loss: 1.9175 - val_mean_squared_error: 1.9175\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 0.6532 - mean_squared_error: 0.6532 - val_loss: 1.8617 - val_mean_squared_error: 1.8617\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.6543 - mean_squared_error: 0.6543 - val_loss: 1.9402 - val_mean_squared_error: 1.9402\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.6155 - mean_squared_error: 0.6155 - val_loss: 1.9639 - val_mean_squared_error: 1.9639\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.6560 - mean_squared_error: 0.6560 - val_loss: 2.2188 - val_mean_squared_error: 2.2188\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.6309 - mean_squared_error: 0.6309 - val_loss: 1.9293 - val_mean_squared_error: 1.9293\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 0.6745 - mean_squared_error: 0.6745 - val_loss: 2.1203 - val_mean_squared_error: 2.1203\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.6512 - mean_squared_error: 0.6512 - val_loss: 1.9925 - val_mean_squared_error: 1.9925\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.6879 - mean_squared_error: 0.6879 - val_loss: 1.8398 - val_mean_squared_error: 1.8398\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.6807 - mean_squared_error: 0.6807 - val_loss: 2.2472 - val_mean_squared_error: 2.2472\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.6495 - mean_squared_error: 0.6495 - val_loss: 2.1745 - val_mean_squared_error: 2.1745\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.6926 - mean_squared_error: 0.6926 - val_loss: 1.9937 - val_mean_squared_error: 1.9937\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.6001 - mean_squared_error: 0.6001 - val_loss: 1.9377 - val_mean_squared_error: 1.9377\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 335us/sample - loss: 0.6572 - mean_squared_error: 0.6572 - val_loss: 1.9536 - val_mean_squared_error: 1.9536\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.6075 - mean_squared_error: 0.6075 - val_loss: 2.0498 - val_mean_squared_error: 2.0498\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.6208 - mean_squared_error: 0.6208 - val_loss: 1.8925 - val_mean_squared_error: 1.8925\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.6508 - mean_squared_error: 0.6508 - val_loss: 1.9178 - val_mean_squared_error: 1.9178\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.6082 - mean_squared_error: 0.6082 - val_loss: 2.0120 - val_mean_squared_error: 2.0120\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.6203 - mean_squared_error: 0.6203 - val_loss: 2.0062 - val_mean_squared_error: 2.0062\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.6260 - mean_squared_error: 0.6260 - val_loss: 1.8835 - val_mean_squared_error: 1.8835\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.6019 - mean_squared_error: 0.6019 - val_loss: 2.0634 - val_mean_squared_error: 2.0634\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.5663 - mean_squared_error: 0.5663 - val_loss: 1.9450 - val_mean_squared_error: 1.9450\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5943 - mean_squared_error: 0.5943 - val_loss: 1.8241 - val_mean_squared_error: 1.8241\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5964 - mean_squared_error: 0.5964 - val_loss: 1.8331 - val_mean_squared_error: 1.8331\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.6104 - mean_squared_error: 0.6104 - val_loss: 1.8937 - val_mean_squared_error: 1.8937\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.6108 - mean_squared_error: 0.6108 - val_loss: 1.9638 - val_mean_squared_error: 1.9638\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5893 - mean_squared_error: 0.5893 - val_loss: 1.9130 - val_mean_squared_error: 1.9130\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.6049 - mean_squared_error: 0.6049 - val_loss: 2.0015 - val_mean_squared_error: 2.0015\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.5565 - mean_squared_error: 0.5565 - val_loss: 2.4140 - val_mean_squared_error: 2.4140\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.5975 - mean_squared_error: 0.5975 - val_loss: 2.2354 - val_mean_squared_error: 2.2354\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.6147 - mean_squared_error: 0.6147 - val_loss: 2.1118 - val_mean_squared_error: 2.1118\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.5540 - mean_squared_error: 0.5540 - val_loss: 1.9650 - val_mean_squared_error: 1.9650\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.5823 - mean_squared_error: 0.5823 - val_loss: 1.8549 - val_mean_squared_error: 1.8549\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 0.5165 - mean_squared_error: 0.5165 - val_loss: 1.9252 - val_mean_squared_error: 1.9252\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 0.5306 - mean_squared_error: 0.5306 - val_loss: 1.8720 - val_mean_squared_error: 1.8720\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 0.5030 - mean_squared_error: 0.5030 - val_loss: 1.8563 - val_mean_squared_error: 1.8563\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5519 - mean_squared_error: 0.5519 - val_loss: 2.0999 - val_mean_squared_error: 2.0999\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.6015 - mean_squared_error: 0.6015 - val_loss: 1.9736 - val_mean_squared_error: 1.9736\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.5245 - mean_squared_error: 0.5245 - val_loss: 1.9653 - val_mean_squared_error: 1.9653\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.5780 - mean_squared_error: 0.5780 - val_loss: 1.9301 - val_mean_squared_error: 1.9302\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.5417 - mean_squared_error: 0.5417 - val_loss: 1.8441 - val_mean_squared_error: 1.8441\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.4966 - mean_squared_error: 0.4966 - val_loss: 1.8969 - val_mean_squared_error: 1.8969\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.5113 - mean_squared_error: 0.5113 - val_loss: 2.2265 - val_mean_squared_error: 2.2265\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.5569 - mean_squared_error: 0.5569 - val_loss: 1.8910 - val_mean_squared_error: 1.8910\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.5251 - mean_squared_error: 0.5251 - val_loss: 1.9007 - val_mean_squared_error: 1.9007\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 0.5355 - mean_squared_error: 0.5355 - val_loss: 2.0246 - val_mean_squared_error: 2.0246\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.5218 - mean_squared_error: 0.5218 - val_loss: 1.8789 - val_mean_squared_error: 1.8789\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5341 - mean_squared_error: 0.5341 - val_loss: 1.8193 - val_mean_squared_error: 1.8193\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.5504 - mean_squared_error: 0.5504 - val_loss: 1.9335 - val_mean_squared_error: 1.9335\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5354 - mean_squared_error: 0.5354 - val_loss: 1.9636 - val_mean_squared_error: 1.9636\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 0.5072 - mean_squared_error: 0.5072 - val_loss: 1.9769 - val_mean_squared_error: 1.9769\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.5689 - mean_squared_error: 0.5689 - val_loss: 2.1626 - val_mean_squared_error: 2.1626\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.5285 - mean_squared_error: 0.5285 - val_loss: 2.0330 - val_mean_squared_error: 2.0330\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.5122 - mean_squared_error: 0.5122 - val_loss: 1.9923 - val_mean_squared_error: 1.9923\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 0.5098 - mean_squared_error: 0.5098 - val_loss: 1.8694 - val_mean_squared_error: 1.8694\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 2.1655 - val_mean_squared_error: 2.1655\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.4773 - mean_squared_error: 0.4773 - val_loss: 1.8871 - val_mean_squared_error: 1.8871\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 0.5418 - mean_squared_error: 0.5418 - val_loss: 1.8549 - val_mean_squared_error: 1.8549\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.4990 - mean_squared_error: 0.4990 - val_loss: 1.9710 - val_mean_squared_error: 1.9710\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 0.5055 - mean_squared_error: 0.5055 - val_loss: 1.9474 - val_mean_squared_error: 1.9474\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 335us/sample - loss: 0.5248 - mean_squared_error: 0.5248 - val_loss: 2.0001 - val_mean_squared_error: 2.0001\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 337us/sample - loss: 0.4737 - mean_squared_error: 0.4737 - val_loss: 1.8012 - val_mean_squared_error: 1.8012\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.4855 - mean_squared_error: 0.4855 - val_loss: 1.9064 - val_mean_squared_error: 1.9064\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 0.4732 - mean_squared_error: 0.4732 - val_loss: 2.0041 - val_mean_squared_error: 2.0041\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.4954 - mean_squared_error: 0.4954 - val_loss: 1.8927 - val_mean_squared_error: 1.8927\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 0.4920 - mean_squared_error: 0.4920 - val_loss: 1.8338 - val_mean_squared_error: 1.8338\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 0.5467 - mean_squared_error: 0.5467 - val_loss: 1.8238 - val_mean_squared_error: 1.8238\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.4607 - mean_squared_error: 0.4607 - val_loss: 1.8363 - val_mean_squared_error: 1.8363\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 336us/sample - loss: 0.4667 - mean_squared_error: 0.4667 - val_loss: 1.8841 - val_mean_squared_error: 1.8841\n",
            "==================================================\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_45 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_75 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_60 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_76 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_61 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_77 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_62 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_78 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_63 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_79 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 837us/sample - loss: 178.0851 - mean_squared_error: 178.0851 - val_loss: 148.5042 - val_mean_squared_error: 148.5042\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 370us/sample - loss: 20.1247 - mean_squared_error: 20.1247 - val_loss: 41.2281 - val_mean_squared_error: 41.2281\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 18.4188 - mean_squared_error: 18.4188 - val_loss: 35.7206 - val_mean_squared_error: 35.7206\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 15.7851 - mean_squared_error: 15.7851 - val_loss: 16.1071 - val_mean_squared_error: 16.1071\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 12.9661 - mean_squared_error: 12.9661 - val_loss: 13.4622 - val_mean_squared_error: 13.4622\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 11.3201 - mean_squared_error: 11.3201 - val_loss: 10.2582 - val_mean_squared_error: 10.2582\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 10.4952 - mean_squared_error: 10.4952 - val_loss: 8.9945 - val_mean_squared_error: 8.9945\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 9.6000 - mean_squared_error: 9.6000 - val_loss: 8.6243 - val_mean_squared_error: 8.6243\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 7.9814 - mean_squared_error: 7.9814 - val_loss: 7.8942 - val_mean_squared_error: 7.8942\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 7.0495 - mean_squared_error: 7.0495 - val_loss: 6.2896 - val_mean_squared_error: 6.2896\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 5.9933 - mean_squared_error: 5.9933 - val_loss: 7.8542 - val_mean_squared_error: 7.8542\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 5.3036 - mean_squared_error: 5.3036 - val_loss: 6.5327 - val_mean_squared_error: 6.5327\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 4.6180 - mean_squared_error: 4.6180 - val_loss: 5.2348 - val_mean_squared_error: 5.2348\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 4.3621 - mean_squared_error: 4.3621 - val_loss: 5.5262 - val_mean_squared_error: 5.5262\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 3.8386 - mean_squared_error: 3.8386 - val_loss: 3.9531 - val_mean_squared_error: 3.9531\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 3.6521 - mean_squared_error: 3.6521 - val_loss: 5.1774 - val_mean_squared_error: 5.1774\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 3.6224 - mean_squared_error: 3.6224 - val_loss: 3.5195 - val_mean_squared_error: 3.5195\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 3.4532 - mean_squared_error: 3.4532 - val_loss: 4.3198 - val_mean_squared_error: 4.3198\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 3.0374 - mean_squared_error: 3.0374 - val_loss: 3.2855 - val_mean_squared_error: 3.2855\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 2.9802 - mean_squared_error: 2.9802 - val_loss: 3.2655 - val_mean_squared_error: 3.2655\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 2.9327 - mean_squared_error: 2.9327 - val_loss: 3.5552 - val_mean_squared_error: 3.5552\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 2.7050 - mean_squared_error: 2.7050 - val_loss: 3.1767 - val_mean_squared_error: 3.1767\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 2.6728 - mean_squared_error: 2.6728 - val_loss: 2.9702 - val_mean_squared_error: 2.9702\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 2.7487 - mean_squared_error: 2.7487 - val_loss: 3.7258 - val_mean_squared_error: 3.7258\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 2.5678 - mean_squared_error: 2.5678 - val_loss: 2.8895 - val_mean_squared_error: 2.8895\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 2.4430 - mean_squared_error: 2.4430 - val_loss: 2.6468 - val_mean_squared_error: 2.6468\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 2.4085 - mean_squared_error: 2.4085 - val_loss: 3.6279 - val_mean_squared_error: 3.6279\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 2.3947 - mean_squared_error: 2.3947 - val_loss: 2.7703 - val_mean_squared_error: 2.7703\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 2.3267 - mean_squared_error: 2.3267 - val_loss: 2.9517 - val_mean_squared_error: 2.9517\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 2.2242 - mean_squared_error: 2.2242 - val_loss: 3.6671 - val_mean_squared_error: 3.6671\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 2.1569 - mean_squared_error: 2.1569 - val_loss: 2.8192 - val_mean_squared_error: 2.8192\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 2.3343 - mean_squared_error: 2.3343 - val_loss: 3.8979 - val_mean_squared_error: 3.8979\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 2.0744 - mean_squared_error: 2.0744 - val_loss: 2.5899 - val_mean_squared_error: 2.5899\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 2.1079 - mean_squared_error: 2.1079 - val_loss: 2.7143 - val_mean_squared_error: 2.7143\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.8544 - mean_squared_error: 1.8544 - val_loss: 2.3849 - val_mean_squared_error: 2.3849\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.9785 - mean_squared_error: 1.9785 - val_loss: 2.1509 - val_mean_squared_error: 2.1509\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.8266 - mean_squared_error: 1.8266 - val_loss: 2.3047 - val_mean_squared_error: 2.3047\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.8295 - mean_squared_error: 1.8295 - val_loss: 2.6598 - val_mean_squared_error: 2.6598\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.9318 - mean_squared_error: 1.9318 - val_loss: 2.8790 - val_mean_squared_error: 2.8790\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.8768 - mean_squared_error: 1.8768 - val_loss: 2.8763 - val_mean_squared_error: 2.8763\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.8715 - mean_squared_error: 1.8715 - val_loss: 2.7580 - val_mean_squared_error: 2.7580\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.7993 - mean_squared_error: 1.7993 - val_loss: 2.1013 - val_mean_squared_error: 2.1013\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.6274 - mean_squared_error: 1.6274 - val_loss: 2.8878 - val_mean_squared_error: 2.8878\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.8280 - mean_squared_error: 1.8280 - val_loss: 2.1670 - val_mean_squared_error: 2.1670\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.7318 - mean_squared_error: 1.7318 - val_loss: 2.1597 - val_mean_squared_error: 2.1597\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.5495 - mean_squared_error: 1.5495 - val_loss: 2.5449 - val_mean_squared_error: 2.5449\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.5507 - mean_squared_error: 1.5507 - val_loss: 2.6335 - val_mean_squared_error: 2.6335\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.6467 - mean_squared_error: 1.6467 - val_loss: 2.1918 - val_mean_squared_error: 2.1918\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.4772 - mean_squared_error: 1.4772 - val_loss: 2.5946 - val_mean_squared_error: 2.5946\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.5929 - mean_squared_error: 1.5929 - val_loss: 2.6489 - val_mean_squared_error: 2.6489\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.7322 - mean_squared_error: 1.7322 - val_loss: 2.6757 - val_mean_squared_error: 2.6757\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.5005 - mean_squared_error: 1.5005 - val_loss: 2.0306 - val_mean_squared_error: 2.0306\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.5012 - mean_squared_error: 1.5012 - val_loss: 2.3327 - val_mean_squared_error: 2.3327\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.4992 - mean_squared_error: 1.4992 - val_loss: 2.1206 - val_mean_squared_error: 2.1206\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.4469 - mean_squared_error: 1.4469 - val_loss: 2.2072 - val_mean_squared_error: 2.2072\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.3199 - mean_squared_error: 1.3199 - val_loss: 1.9218 - val_mean_squared_error: 1.9218\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.5262 - mean_squared_error: 1.5262 - val_loss: 1.9284 - val_mean_squared_error: 1.9284\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.3908 - mean_squared_error: 1.3908 - val_loss: 2.1868 - val_mean_squared_error: 2.1868\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.3030 - mean_squared_error: 1.3030 - val_loss: 2.5566 - val_mean_squared_error: 2.5566\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 1.3048 - mean_squared_error: 1.3048 - val_loss: 2.0037 - val_mean_squared_error: 2.0037\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.4347 - mean_squared_error: 1.4347 - val_loss: 1.9478 - val_mean_squared_error: 1.9478\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.3349 - mean_squared_error: 1.3349 - val_loss: 2.2156 - val_mean_squared_error: 2.2156\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.3170 - mean_squared_error: 1.3170 - val_loss: 2.6160 - val_mean_squared_error: 2.6160\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.3466 - mean_squared_error: 1.3466 - val_loss: 1.8770 - val_mean_squared_error: 1.8770\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.2767 - mean_squared_error: 1.2767 - val_loss: 1.9859 - val_mean_squared_error: 1.9859\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 1.2500 - mean_squared_error: 1.2500 - val_loss: 1.8916 - val_mean_squared_error: 1.8916\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.2305 - mean_squared_error: 1.2305 - val_loss: 2.0366 - val_mean_squared_error: 2.0366\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.4337 - mean_squared_error: 1.4337 - val_loss: 2.0267 - val_mean_squared_error: 2.0267\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.3245 - mean_squared_error: 1.3245 - val_loss: 2.1414 - val_mean_squared_error: 2.1414\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 1.2515 - mean_squared_error: 1.2515 - val_loss: 1.8924 - val_mean_squared_error: 1.8924\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.2743 - mean_squared_error: 1.2743 - val_loss: 1.7486 - val_mean_squared_error: 1.7486\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 1.2996 - mean_squared_error: 1.2996 - val_loss: 1.9719 - val_mean_squared_error: 1.9719\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.3303 - mean_squared_error: 1.3303 - val_loss: 1.9518 - val_mean_squared_error: 1.9518\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.1037 - mean_squared_error: 1.1037 - val_loss: 2.0055 - val_mean_squared_error: 2.0055\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.2004 - mean_squared_error: 1.2004 - val_loss: 1.8164 - val_mean_squared_error: 1.8164\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.1602 - mean_squared_error: 1.1602 - val_loss: 1.9054 - val_mean_squared_error: 1.9054\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.2691 - mean_squared_error: 1.2691 - val_loss: 2.3512 - val_mean_squared_error: 2.3512\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.1759 - mean_squared_error: 1.1759 - val_loss: 1.8839 - val_mean_squared_error: 1.8839\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.1355 - mean_squared_error: 1.1355 - val_loss: 2.0270 - val_mean_squared_error: 2.0270\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.0834 - mean_squared_error: 1.0834 - val_loss: 1.9633 - val_mean_squared_error: 1.9633\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.1122 - mean_squared_error: 1.1122 - val_loss: 1.9780 - val_mean_squared_error: 1.9780\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.0258 - mean_squared_error: 1.0258 - val_loss: 1.9188 - val_mean_squared_error: 1.9188\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.0579 - mean_squared_error: 1.0579 - val_loss: 1.8294 - val_mean_squared_error: 1.8294\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 1.1557 - mean_squared_error: 1.1557 - val_loss: 1.8868 - val_mean_squared_error: 1.8868\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.0524 - mean_squared_error: 1.0524 - val_loss: 2.0133 - val_mean_squared_error: 2.0133\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.1158 - mean_squared_error: 1.1158 - val_loss: 1.8081 - val_mean_squared_error: 1.8081\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.0721 - mean_squared_error: 1.0721 - val_loss: 2.3246 - val_mean_squared_error: 2.3246\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.9329 - mean_squared_error: 0.9329 - val_loss: 2.0484 - val_mean_squared_error: 2.0484\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.9658 - mean_squared_error: 0.9658 - val_loss: 1.9722 - val_mean_squared_error: 1.9722\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9953 - mean_squared_error: 0.9953 - val_loss: 1.8598 - val_mean_squared_error: 1.8598\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 1.0124 - mean_squared_error: 1.0124 - val_loss: 2.5065 - val_mean_squared_error: 2.5065\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.0222 - mean_squared_error: 1.0222 - val_loss: 2.2666 - val_mean_squared_error: 2.2666\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9914 - mean_squared_error: 0.9914 - val_loss: 2.0823 - val_mean_squared_error: 2.0823\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 1.8173 - val_mean_squared_error: 1.8173\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.0345 - mean_squared_error: 1.0345 - val_loss: 2.0230 - val_mean_squared_error: 2.0230\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.9323 - mean_squared_error: 0.9323 - val_loss: 1.7611 - val_mean_squared_error: 1.7611\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.9138 - mean_squared_error: 0.9138 - val_loss: 1.8406 - val_mean_squared_error: 1.8406\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.8936 - mean_squared_error: 0.8936 - val_loss: 1.9517 - val_mean_squared_error: 1.9517\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.9271 - mean_squared_error: 0.9271 - val_loss: 1.9389 - val_mean_squared_error: 1.9389\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.9258 - mean_squared_error: 0.9258 - val_loss: 1.7838 - val_mean_squared_error: 1.7838\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.9110 - mean_squared_error: 0.9110 - val_loss: 1.8438 - val_mean_squared_error: 1.8438\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.8959 - mean_squared_error: 0.8959 - val_loss: 1.9256 - val_mean_squared_error: 1.9256\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8456 - mean_squared_error: 0.8456 - val_loss: 1.7884 - val_mean_squared_error: 1.7884\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.8483 - mean_squared_error: 0.8483 - val_loss: 1.8468 - val_mean_squared_error: 1.8468\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.9371 - mean_squared_error: 0.9371 - val_loss: 1.9846 - val_mean_squared_error: 1.9846\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8647 - mean_squared_error: 0.8647 - val_loss: 1.8282 - val_mean_squared_error: 1.8282\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.8686 - mean_squared_error: 0.8686 - val_loss: 1.7074 - val_mean_squared_error: 1.7074\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.8615 - mean_squared_error: 0.8615 - val_loss: 2.1633 - val_mean_squared_error: 2.1633\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.8912 - mean_squared_error: 0.8912 - val_loss: 2.0329 - val_mean_squared_error: 2.0329\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.8892 - mean_squared_error: 0.8892 - val_loss: 1.8038 - val_mean_squared_error: 1.8038\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.7987 - mean_squared_error: 0.7987 - val_loss: 1.9928 - val_mean_squared_error: 1.9928\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.9062 - mean_squared_error: 0.9062 - val_loss: 2.0411 - val_mean_squared_error: 2.0411\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.8330 - mean_squared_error: 0.8330 - val_loss: 1.8388 - val_mean_squared_error: 1.8388\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.8942 - mean_squared_error: 0.8942 - val_loss: 2.0263 - val_mean_squared_error: 2.0263\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.8428 - mean_squared_error: 0.8428 - val_loss: 1.8600 - val_mean_squared_error: 1.8600\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.8048 - mean_squared_error: 0.8048 - val_loss: 2.1216 - val_mean_squared_error: 2.1216\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8245 - mean_squared_error: 0.8245 - val_loss: 1.9683 - val_mean_squared_error: 1.9683\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.8552 - mean_squared_error: 0.8552 - val_loss: 1.9904 - val_mean_squared_error: 1.9904\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.7891 - mean_squared_error: 0.7891 - val_loss: 1.9978 - val_mean_squared_error: 1.9978\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.7431 - mean_squared_error: 0.7431 - val_loss: 1.8239 - val_mean_squared_error: 1.8239\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.8046 - mean_squared_error: 0.8046 - val_loss: 1.7673 - val_mean_squared_error: 1.7673\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.7694 - mean_squared_error: 0.7694 - val_loss: 2.3857 - val_mean_squared_error: 2.3857\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.8423 - mean_squared_error: 0.8423 - val_loss: 1.9382 - val_mean_squared_error: 1.9382\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.7501 - mean_squared_error: 0.7501 - val_loss: 1.7810 - val_mean_squared_error: 1.7810\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8240 - mean_squared_error: 0.8240 - val_loss: 1.8067 - val_mean_squared_error: 1.8067\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.7647 - mean_squared_error: 0.7647 - val_loss: 1.7681 - val_mean_squared_error: 1.7681\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.8530 - mean_squared_error: 0.8530 - val_loss: 1.8788 - val_mean_squared_error: 1.8788\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.7932 - mean_squared_error: 0.7932 - val_loss: 1.9215 - val_mean_squared_error: 1.9215\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.7754 - mean_squared_error: 0.7754 - val_loss: 1.8802 - val_mean_squared_error: 1.8802\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.7465 - mean_squared_error: 0.7465 - val_loss: 2.0590 - val_mean_squared_error: 2.0590\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.8104 - mean_squared_error: 0.8104 - val_loss: 1.7184 - val_mean_squared_error: 1.7184\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.7552 - mean_squared_error: 0.7552 - val_loss: 1.8422 - val_mean_squared_error: 1.8422\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.7216 - mean_squared_error: 0.7216 - val_loss: 1.8669 - val_mean_squared_error: 1.8669\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.7929 - mean_squared_error: 0.7929 - val_loss: 1.9585 - val_mean_squared_error: 1.9585\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.7444 - mean_squared_error: 0.7444 - val_loss: 1.8236 - val_mean_squared_error: 1.8236\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7265 - mean_squared_error: 0.7265 - val_loss: 1.9017 - val_mean_squared_error: 1.9017\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.6987 - mean_squared_error: 0.6987 - val_loss: 1.8455 - val_mean_squared_error: 1.8455\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7917 - mean_squared_error: 0.7917 - val_loss: 1.9941 - val_mean_squared_error: 1.9941\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.7281 - mean_squared_error: 0.7281 - val_loss: 1.7053 - val_mean_squared_error: 1.7053\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7184 - mean_squared_error: 0.7184 - val_loss: 2.0485 - val_mean_squared_error: 2.0485\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.7040 - mean_squared_error: 0.7040 - val_loss: 1.8743 - val_mean_squared_error: 1.8743\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.7450 - mean_squared_error: 0.7450 - val_loss: 2.0488 - val_mean_squared_error: 2.0488\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 358us/sample - loss: 0.7354 - mean_squared_error: 0.7354 - val_loss: 1.9146 - val_mean_squared_error: 1.9146\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6599 - mean_squared_error: 0.6599 - val_loss: 2.1125 - val_mean_squared_error: 2.1125\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.6973 - mean_squared_error: 0.6973 - val_loss: 1.8983 - val_mean_squared_error: 1.8983\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.7101 - mean_squared_error: 0.7101 - val_loss: 1.8343 - val_mean_squared_error: 1.8343\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 0.6842 - mean_squared_error: 0.6842 - val_loss: 2.0298 - val_mean_squared_error: 2.0298\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7115 - mean_squared_error: 0.7115 - val_loss: 1.7624 - val_mean_squared_error: 1.7624\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.7255 - mean_squared_error: 0.7255 - val_loss: 2.0789 - val_mean_squared_error: 2.0789\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 364us/sample - loss: 0.6806 - mean_squared_error: 0.6806 - val_loss: 1.7437 - val_mean_squared_error: 1.7437\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.6707 - mean_squared_error: 0.6707 - val_loss: 1.8245 - val_mean_squared_error: 1.8245\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.6994 - mean_squared_error: 0.6994 - val_loss: 2.0165 - val_mean_squared_error: 2.0165\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 359us/sample - loss: 0.7458 - mean_squared_error: 0.7458 - val_loss: 1.9757 - val_mean_squared_error: 1.9757\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.6932 - mean_squared_error: 0.6932 - val_loss: 2.0140 - val_mean_squared_error: 2.0140\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.7024 - mean_squared_error: 0.7024 - val_loss: 1.7214 - val_mean_squared_error: 1.7214\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 363us/sample - loss: 0.8014 - mean_squared_error: 0.8014 - val_loss: 1.8351 - val_mean_squared_error: 1.8351\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 362us/sample - loss: 0.7560 - mean_squared_error: 0.7560 - val_loss: 1.9612 - val_mean_squared_error: 1.9612\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 360us/sample - loss: 0.6878 - mean_squared_error: 0.6878 - val_loss: 2.0998 - val_mean_squared_error: 2.0998\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.7205 - mean_squared_error: 0.7205 - val_loss: 1.8266 - val_mean_squared_error: 1.8266\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.6513 - mean_squared_error: 0.6513 - val_loss: 1.8108 - val_mean_squared_error: 1.8108\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.6344 - mean_squared_error: 0.6344 - val_loss: 1.7291 - val_mean_squared_error: 1.7291\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.7087 - mean_squared_error: 0.7087 - val_loss: 1.7977 - val_mean_squared_error: 1.7977\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.6339 - mean_squared_error: 0.6339 - val_loss: 1.8069 - val_mean_squared_error: 1.8069\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.6971 - mean_squared_error: 0.6971 - val_loss: 1.8670 - val_mean_squared_error: 1.8670\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 0.6738 - mean_squared_error: 0.6738 - val_loss: 1.8543 - val_mean_squared_error: 1.8543\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.6463 - mean_squared_error: 0.6463 - val_loss: 1.9795 - val_mean_squared_error: 1.9795\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.5936 - mean_squared_error: 0.5936 - val_loss: 1.8090 - val_mean_squared_error: 1.8090\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 353us/sample - loss: 0.6764 - mean_squared_error: 0.6764 - val_loss: 1.9229 - val_mean_squared_error: 1.9229\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 0.6705 - mean_squared_error: 0.6705 - val_loss: 2.0018 - val_mean_squared_error: 2.0018\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.6600 - mean_squared_error: 0.6600 - val_loss: 1.8115 - val_mean_squared_error: 1.8115\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 0.6101 - mean_squared_error: 0.6101 - val_loss: 2.0285 - val_mean_squared_error: 2.0285\n",
            "==================================================\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_48 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_80 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_48 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_64 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_81 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_49 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_65 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_82 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_50 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_66 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_16 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_83 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_67 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_84 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 898us/sample - loss: 174.1677 - mean_squared_error: 174.1677 - val_loss: 134.1559 - val_mean_squared_error: 134.1559\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 23.4646 - mean_squared_error: 23.4646 - val_loss: 84.7452 - val_mean_squared_error: 84.7452\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 15.2138 - mean_squared_error: 15.2138 - val_loss: 20.4356 - val_mean_squared_error: 20.4356\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 14.6220 - mean_squared_error: 14.6220 - val_loss: 12.4339 - val_mean_squared_error: 12.4339\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 12.8264 - mean_squared_error: 12.8264 - val_loss: 11.4635 - val_mean_squared_error: 11.4635\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 12.6991 - mean_squared_error: 12.6991 - val_loss: 10.4148 - val_mean_squared_error: 10.4148\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 10.8907 - mean_squared_error: 10.8907 - val_loss: 8.5082 - val_mean_squared_error: 8.5082\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 9.8573 - mean_squared_error: 9.8573 - val_loss: 8.3731 - val_mean_squared_error: 8.3731\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 8.2013 - mean_squared_error: 8.2013 - val_loss: 8.3531 - val_mean_squared_error: 8.3531\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 6.5947 - mean_squared_error: 6.5947 - val_loss: 6.3401 - val_mean_squared_error: 6.3401\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 5.5573 - mean_squared_error: 5.5573 - val_loss: 5.5322 - val_mean_squared_error: 5.5322\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 4.9015 - mean_squared_error: 4.9015 - val_loss: 5.1233 - val_mean_squared_error: 5.1233\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 4.3747 - mean_squared_error: 4.3747 - val_loss: 5.0620 - val_mean_squared_error: 5.0620\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 4.1520 - mean_squared_error: 4.1520 - val_loss: 5.0779 - val_mean_squared_error: 5.0779\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 3.6882 - mean_squared_error: 3.6882 - val_loss: 4.3590 - val_mean_squared_error: 4.3590\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 3.7763 - mean_squared_error: 3.7763 - val_loss: 4.2154 - val_mean_squared_error: 4.2154\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 3.2190 - mean_squared_error: 3.2190 - val_loss: 3.5004 - val_mean_squared_error: 3.5004\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 3.0563 - mean_squared_error: 3.0563 - val_loss: 3.5212 - val_mean_squared_error: 3.5212\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 2.9965 - mean_squared_error: 2.9965 - val_loss: 3.4645 - val_mean_squared_error: 3.4645\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 2.6261 - mean_squared_error: 2.6261 - val_loss: 2.9378 - val_mean_squared_error: 2.9378\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.5076 - mean_squared_error: 2.5076 - val_loss: 3.9568 - val_mean_squared_error: 3.9568\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 2.6676 - mean_squared_error: 2.6676 - val_loss: 2.9840 - val_mean_squared_error: 2.9840\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 2.4633 - mean_squared_error: 2.4633 - val_loss: 3.5598 - val_mean_squared_error: 3.5598\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 2.3592 - mean_squared_error: 2.3592 - val_loss: 3.0356 - val_mean_squared_error: 3.0356\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 2.3338 - mean_squared_error: 2.3338 - val_loss: 3.1454 - val_mean_squared_error: 3.1454\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 2.1244 - mean_squared_error: 2.1244 - val_loss: 2.5379 - val_mean_squared_error: 2.5379\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 2.1021 - mean_squared_error: 2.1021 - val_loss: 2.8456 - val_mean_squared_error: 2.8456\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 2.0325 - mean_squared_error: 2.0325 - val_loss: 2.8212 - val_mean_squared_error: 2.8212\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 2.0068 - mean_squared_error: 2.0068 - val_loss: 2.2991 - val_mean_squared_error: 2.2991\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.9776 - mean_squared_error: 1.9776 - val_loss: 3.5048 - val_mean_squared_error: 3.5048\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 1.9620 - mean_squared_error: 1.9620 - val_loss: 2.5513 - val_mean_squared_error: 2.5513\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.8738 - mean_squared_error: 1.8738 - val_loss: 2.4296 - val_mean_squared_error: 2.4296\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.9274 - mean_squared_error: 1.9274 - val_loss: 2.7593 - val_mean_squared_error: 2.7593\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.7464 - mean_squared_error: 1.7464 - val_loss: 2.6340 - val_mean_squared_error: 2.6340\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 1.7600 - mean_squared_error: 1.7600 - val_loss: 2.3562 - val_mean_squared_error: 2.3562\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 1.7148 - mean_squared_error: 1.7148 - val_loss: 2.3442 - val_mean_squared_error: 2.3442\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.7143 - mean_squared_error: 1.7143 - val_loss: 2.4762 - val_mean_squared_error: 2.4762\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 1.6602 - mean_squared_error: 1.6602 - val_loss: 2.6602 - val_mean_squared_error: 2.6602\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.6888 - mean_squared_error: 1.6888 - val_loss: 2.7029 - val_mean_squared_error: 2.7029\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 1.6367 - mean_squared_error: 1.6367 - val_loss: 2.1956 - val_mean_squared_error: 2.1956\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.5779 - mean_squared_error: 1.5779 - val_loss: 2.4344 - val_mean_squared_error: 2.4344\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.5768 - mean_squared_error: 1.5768 - val_loss: 2.3754 - val_mean_squared_error: 2.3754\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.5232 - mean_squared_error: 1.5232 - val_loss: 2.4237 - val_mean_squared_error: 2.4237\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.4874 - mean_squared_error: 1.4874 - val_loss: 2.4629 - val_mean_squared_error: 2.4629\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.5292 - mean_squared_error: 1.5292 - val_loss: 2.7408 - val_mean_squared_error: 2.7408\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.4696 - mean_squared_error: 1.4696 - val_loss: 2.3391 - val_mean_squared_error: 2.3391\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.4820 - mean_squared_error: 1.4820 - val_loss: 2.1973 - val_mean_squared_error: 2.1973\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.3058 - mean_squared_error: 1.3058 - val_loss: 2.4358 - val_mean_squared_error: 2.4358\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.3037 - mean_squared_error: 1.3037 - val_loss: 2.6362 - val_mean_squared_error: 2.6362\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 381us/sample - loss: 1.5418 - mean_squared_error: 1.5418 - val_loss: 2.8992 - val_mean_squared_error: 2.8992\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.6035 - mean_squared_error: 1.6035 - val_loss: 2.6172 - val_mean_squared_error: 2.6172\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 1.3871 - mean_squared_error: 1.3871 - val_loss: 2.1233 - val_mean_squared_error: 2.1233\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 383us/sample - loss: 1.4139 - mean_squared_error: 1.4139 - val_loss: 2.0416 - val_mean_squared_error: 2.0416\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.2358 - mean_squared_error: 1.2358 - val_loss: 2.1585 - val_mean_squared_error: 2.1585\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.3106 - mean_squared_error: 1.3106 - val_loss: 2.1467 - val_mean_squared_error: 2.1467\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.1715 - mean_squared_error: 1.1715 - val_loss: 2.2061 - val_mean_squared_error: 2.2061\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.2473 - mean_squared_error: 1.2473 - val_loss: 2.4022 - val_mean_squared_error: 2.4022\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.2465 - mean_squared_error: 1.2465 - val_loss: 2.2972 - val_mean_squared_error: 2.2972\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.2812 - mean_squared_error: 1.2812 - val_loss: 2.2607 - val_mean_squared_error: 2.2607\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.2363 - mean_squared_error: 1.2363 - val_loss: 2.1080 - val_mean_squared_error: 2.1080\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.2068 - mean_squared_error: 1.2068 - val_loss: 2.3561 - val_mean_squared_error: 2.3561\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1706 - mean_squared_error: 1.1706 - val_loss: 2.2048 - val_mean_squared_error: 2.2048\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.1098 - mean_squared_error: 1.1098 - val_loss: 1.8711 - val_mean_squared_error: 1.8711\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0803 - mean_squared_error: 1.0803 - val_loss: 2.1692 - val_mean_squared_error: 2.1692\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1325 - mean_squared_error: 1.1325 - val_loss: 3.3002 - val_mean_squared_error: 3.3002\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.2004 - mean_squared_error: 1.2004 - val_loss: 2.1093 - val_mean_squared_error: 2.1093\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.1356 - mean_squared_error: 1.1356 - val_loss: 2.0587 - val_mean_squared_error: 2.0587\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1144 - mean_squared_error: 1.1144 - val_loss: 1.8743 - val_mean_squared_error: 1.8743\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.9654 - mean_squared_error: 0.9654 - val_loss: 2.0646 - val_mean_squared_error: 2.0646\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.0529 - mean_squared_error: 1.0529 - val_loss: 2.0270 - val_mean_squared_error: 2.0270\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.1846 - mean_squared_error: 1.1846 - val_loss: 2.2396 - val_mean_squared_error: 2.2396\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.0881 - mean_squared_error: 1.0881 - val_loss: 2.5457 - val_mean_squared_error: 2.5457\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.1239 - mean_squared_error: 1.1239 - val_loss: 2.1901 - val_mean_squared_error: 2.1901\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.0225 - mean_squared_error: 1.0225 - val_loss: 2.1498 - val_mean_squared_error: 2.1498\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 1.0257 - mean_squared_error: 1.0257 - val_loss: 1.8613 - val_mean_squared_error: 1.8613\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 382us/sample - loss: 0.9657 - mean_squared_error: 0.9657 - val_loss: 2.1414 - val_mean_squared_error: 2.1414\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.9111 - mean_squared_error: 0.9111 - val_loss: 2.2314 - val_mean_squared_error: 2.2314\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.1308 - mean_squared_error: 1.1308 - val_loss: 1.9005 - val_mean_squared_error: 1.9005\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8980 - mean_squared_error: 0.8980 - val_loss: 2.0802 - val_mean_squared_error: 2.0802\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8438 - mean_squared_error: 0.8438 - val_loss: 1.9721 - val_mean_squared_error: 1.9721\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.9060 - mean_squared_error: 0.9060 - val_loss: 1.8838 - val_mean_squared_error: 1.8838\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8705 - mean_squared_error: 0.8705 - val_loss: 2.1696 - val_mean_squared_error: 2.1696\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0268 - mean_squared_error: 1.0268 - val_loss: 1.9640 - val_mean_squared_error: 1.9640\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.0217 - mean_squared_error: 1.0217 - val_loss: 2.0332 - val_mean_squared_error: 2.0332\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.0076 - mean_squared_error: 1.0076 - val_loss: 1.9628 - val_mean_squared_error: 1.9628\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9387 - mean_squared_error: 0.9387 - val_loss: 2.0516 - val_mean_squared_error: 2.0516\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8106 - mean_squared_error: 0.8106 - val_loss: 2.5792 - val_mean_squared_error: 2.5792\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0238 - mean_squared_error: 1.0238 - val_loss: 1.9212 - val_mean_squared_error: 1.9212\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.9247 - mean_squared_error: 0.9247 - val_loss: 2.0030 - val_mean_squared_error: 2.0030\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.9150 - mean_squared_error: 0.9150 - val_loss: 1.9962 - val_mean_squared_error: 1.9962\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.8764 - mean_squared_error: 0.8764 - val_loss: 1.9530 - val_mean_squared_error: 1.9530\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.8463 - mean_squared_error: 0.8463 - val_loss: 2.0002 - val_mean_squared_error: 2.0002\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.8618 - mean_squared_error: 0.8618 - val_loss: 2.0343 - val_mean_squared_error: 2.0343\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8333 - mean_squared_error: 0.8333 - val_loss: 1.9462 - val_mean_squared_error: 1.9462\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.9550 - mean_squared_error: 0.9550 - val_loss: 2.2280 - val_mean_squared_error: 2.2280\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.9716 - mean_squared_error: 0.9716 - val_loss: 2.0840 - val_mean_squared_error: 2.0840\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.7956 - mean_squared_error: 0.7956 - val_loss: 1.8753 - val_mean_squared_error: 1.8753\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.7756 - mean_squared_error: 0.7756 - val_loss: 2.0634 - val_mean_squared_error: 2.0634\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8570 - mean_squared_error: 0.8570 - val_loss: 1.8874 - val_mean_squared_error: 1.8874\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8343 - mean_squared_error: 0.8343 - val_loss: 1.9644 - val_mean_squared_error: 1.9644\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8106 - mean_squared_error: 0.8106 - val_loss: 1.8067 - val_mean_squared_error: 1.8067\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.7905 - mean_squared_error: 0.7905 - val_loss: 2.6233 - val_mean_squared_error: 2.6233\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.8529 - mean_squared_error: 0.8529 - val_loss: 1.7809 - val_mean_squared_error: 1.7809\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8863 - mean_squared_error: 0.8863 - val_loss: 1.9333 - val_mean_squared_error: 1.9333\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.7556 - mean_squared_error: 0.7556 - val_loss: 2.0824 - val_mean_squared_error: 2.0824\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.7408 - mean_squared_error: 0.7408 - val_loss: 1.8877 - val_mean_squared_error: 1.8877\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7074 - mean_squared_error: 0.7074 - val_loss: 2.0764 - val_mean_squared_error: 2.0764\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.8176 - mean_squared_error: 0.8176 - val_loss: 1.8510 - val_mean_squared_error: 1.8510\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7479 - mean_squared_error: 0.7479 - val_loss: 2.2155 - val_mean_squared_error: 2.2155\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8176 - mean_squared_error: 0.8176 - val_loss: 2.0087 - val_mean_squared_error: 2.0087\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8613 - mean_squared_error: 0.8613 - val_loss: 2.0473 - val_mean_squared_error: 2.0473\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7693 - mean_squared_error: 0.7693 - val_loss: 2.0648 - val_mean_squared_error: 2.0648\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7659 - mean_squared_error: 0.7659 - val_loss: 2.0283 - val_mean_squared_error: 2.0283\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.7635 - mean_squared_error: 0.7635 - val_loss: 2.2167 - val_mean_squared_error: 2.2167\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.7512 - mean_squared_error: 0.7512 - val_loss: 2.1302 - val_mean_squared_error: 2.1302\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7457 - mean_squared_error: 0.7457 - val_loss: 1.9213 - val_mean_squared_error: 1.9213\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7500 - mean_squared_error: 0.7500 - val_loss: 1.8515 - val_mean_squared_error: 1.8515\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8018 - mean_squared_error: 0.8018 - val_loss: 1.9181 - val_mean_squared_error: 1.9181\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.8209 - mean_squared_error: 0.8209 - val_loss: 1.8657 - val_mean_squared_error: 1.8657\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6982 - mean_squared_error: 0.6982 - val_loss: 1.8518 - val_mean_squared_error: 1.8518\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.7326 - mean_squared_error: 0.7326 - val_loss: 1.9332 - val_mean_squared_error: 1.9332\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.7702 - mean_squared_error: 0.7702 - val_loss: 2.5164 - val_mean_squared_error: 2.5164\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.7360 - mean_squared_error: 0.7360 - val_loss: 2.0374 - val_mean_squared_error: 2.0374\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7003 - mean_squared_error: 0.7003 - val_loss: 1.8768 - val_mean_squared_error: 1.8768\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7742 - mean_squared_error: 0.7742 - val_loss: 1.8646 - val_mean_squared_error: 1.8646\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.7210 - mean_squared_error: 0.7210 - val_loss: 1.8484 - val_mean_squared_error: 1.8484\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.7505 - mean_squared_error: 0.7505 - val_loss: 1.7416 - val_mean_squared_error: 1.7416\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7063 - mean_squared_error: 0.7063 - val_loss: 1.7361 - val_mean_squared_error: 1.7361\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.6367 - mean_squared_error: 0.6367 - val_loss: 1.8336 - val_mean_squared_error: 1.8336\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6774 - mean_squared_error: 0.6774 - val_loss: 1.9578 - val_mean_squared_error: 1.9578\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.7097 - mean_squared_error: 0.7097 - val_loss: 1.8134 - val_mean_squared_error: 1.8134\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.6559 - mean_squared_error: 0.6559 - val_loss: 1.8808 - val_mean_squared_error: 1.8808\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6806 - mean_squared_error: 0.6806 - val_loss: 1.8016 - val_mean_squared_error: 1.8016\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6249 - mean_squared_error: 0.6249 - val_loss: 1.8732 - val_mean_squared_error: 1.8732\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.5852 - mean_squared_error: 0.5852 - val_loss: 1.8172 - val_mean_squared_error: 1.8172\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.6798 - mean_squared_error: 0.6798 - val_loss: 2.2154 - val_mean_squared_error: 2.2154\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6395 - mean_squared_error: 0.6395 - val_loss: 2.0028 - val_mean_squared_error: 2.0028\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6285 - mean_squared_error: 0.6285 - val_loss: 1.8453 - val_mean_squared_error: 1.8453\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.7067 - mean_squared_error: 0.7067 - val_loss: 1.7997 - val_mean_squared_error: 1.7997\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6428 - mean_squared_error: 0.6428 - val_loss: 1.7615 - val_mean_squared_error: 1.7615\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6309 - mean_squared_error: 0.6309 - val_loss: 2.3196 - val_mean_squared_error: 2.3196\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.7148 - mean_squared_error: 0.7148 - val_loss: 2.1559 - val_mean_squared_error: 2.1559\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.6657 - mean_squared_error: 0.6657 - val_loss: 1.8221 - val_mean_squared_error: 1.8221\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6462 - mean_squared_error: 0.6462 - val_loss: 2.3135 - val_mean_squared_error: 2.3135\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6065 - mean_squared_error: 0.6065 - val_loss: 2.0182 - val_mean_squared_error: 2.0182\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.6156 - mean_squared_error: 0.6156 - val_loss: 1.9412 - val_mean_squared_error: 1.9412\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5835 - mean_squared_error: 0.5835 - val_loss: 1.9860 - val_mean_squared_error: 1.9860\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.6591 - mean_squared_error: 0.6591 - val_loss: 1.8120 - val_mean_squared_error: 1.8120\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.6636 - mean_squared_error: 0.6636 - val_loss: 1.9604 - val_mean_squared_error: 1.9604\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.5858 - mean_squared_error: 0.5858 - val_loss: 1.7519 - val_mean_squared_error: 1.7519\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.6471 - mean_squared_error: 0.6471 - val_loss: 1.9192 - val_mean_squared_error: 1.9192\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6305 - mean_squared_error: 0.6305 - val_loss: 2.0953 - val_mean_squared_error: 2.0953\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5999 - mean_squared_error: 0.5999 - val_loss: 1.9526 - val_mean_squared_error: 1.9526\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5756 - mean_squared_error: 0.5756 - val_loss: 1.9420 - val_mean_squared_error: 1.9420\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5883 - mean_squared_error: 0.5883 - val_loss: 1.9980 - val_mean_squared_error: 1.9980\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5682 - mean_squared_error: 0.5682 - val_loss: 1.8161 - val_mean_squared_error: 1.8161\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.5699 - mean_squared_error: 0.5699 - val_loss: 1.7874 - val_mean_squared_error: 1.7874\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5801 - mean_squared_error: 0.5801 - val_loss: 1.9343 - val_mean_squared_error: 1.9343\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.6139 - mean_squared_error: 0.6139 - val_loss: 1.8832 - val_mean_squared_error: 1.8832\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5929 - mean_squared_error: 0.5929 - val_loss: 1.8341 - val_mean_squared_error: 1.8341\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.5683 - mean_squared_error: 0.5683 - val_loss: 1.8994 - val_mean_squared_error: 1.8994\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.5459 - mean_squared_error: 0.5459 - val_loss: 1.7874 - val_mean_squared_error: 1.7874\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 384us/sample - loss: 0.5703 - mean_squared_error: 0.5703 - val_loss: 1.8850 - val_mean_squared_error: 1.8850\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5581 - mean_squared_error: 0.5581 - val_loss: 2.0897 - val_mean_squared_error: 2.0897\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 0.5653 - mean_squared_error: 0.5653 - val_loss: 1.7380 - val_mean_squared_error: 1.7380\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5406 - mean_squared_error: 0.5406 - val_loss: 1.8931 - val_mean_squared_error: 1.8931\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5947 - mean_squared_error: 0.5947 - val_loss: 1.8709 - val_mean_squared_error: 1.8709\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.6388 - mean_squared_error: 0.6388 - val_loss: 1.7818 - val_mean_squared_error: 1.7818\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5405 - mean_squared_error: 0.5405 - val_loss: 1.7256 - val_mean_squared_error: 1.7256\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6078 - mean_squared_error: 0.6078 - val_loss: 1.9756 - val_mean_squared_error: 1.9756\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.6223 - mean_squared_error: 0.6223 - val_loss: 1.9866 - val_mean_squared_error: 1.9866\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.5793 - mean_squared_error: 0.5793 - val_loss: 1.9015 - val_mean_squared_error: 1.9015\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.5274 - mean_squared_error: 0.5274 - val_loss: 1.7996 - val_mean_squared_error: 1.7996\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.5693 - mean_squared_error: 0.5693 - val_loss: 1.8804 - val_mean_squared_error: 1.8804\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5512 - mean_squared_error: 0.5512 - val_loss: 1.8228 - val_mean_squared_error: 1.8228\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.5993 - mean_squared_error: 0.5993 - val_loss: 1.7444 - val_mean_squared_error: 1.7444\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.5509 - mean_squared_error: 0.5509 - val_loss: 1.8202 - val_mean_squared_error: 1.8202\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.5113 - mean_squared_error: 0.5113 - val_loss: 1.8513 - val_mean_squared_error: 1.8513\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.5185 - mean_squared_error: 0.5185 - val_loss: 1.8217 - val_mean_squared_error: 1.8217\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.5536 - mean_squared_error: 0.5536 - val_loss: 1.7814 - val_mean_squared_error: 1.7814\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.5588 - mean_squared_error: 0.5588 - val_loss: 1.8394 - val_mean_squared_error: 1.8394\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5111 - mean_squared_error: 0.5111 - val_loss: 1.8452 - val_mean_squared_error: 1.8452\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.4929 - mean_squared_error: 0.4929 - val_loss: 1.7842 - val_mean_squared_error: 1.7842\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.5083 - mean_squared_error: 0.5083 - val_loss: 1.7484 - val_mean_squared_error: 1.7484\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5963 - mean_squared_error: 0.5963 - val_loss: 1.9151 - val_mean_squared_error: 1.9151\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5003 - mean_squared_error: 0.5003 - val_loss: 1.8257 - val_mean_squared_error: 1.8257\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.5657 - mean_squared_error: 0.5657 - val_loss: 1.7892 - val_mean_squared_error: 1.7892\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.4729 - mean_squared_error: 0.4729 - val_loss: 1.7466 - val_mean_squared_error: 1.7466\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.5228 - mean_squared_error: 0.5228 - val_loss: 1.8057 - val_mean_squared_error: 1.8057\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.4464 - mean_squared_error: 0.4464 - val_loss: 1.8086 - val_mean_squared_error: 1.8086\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 0.4914 - mean_squared_error: 0.4914 - val_loss: 1.7148 - val_mean_squared_error: 1.7148\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.5203 - mean_squared_error: 0.5203 - val_loss: 1.6896 - val_mean_squared_error: 1.6896\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.5063 - mean_squared_error: 0.5063 - val_loss: 1.8056 - val_mean_squared_error: 1.8056\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.5088 - mean_squared_error: 0.5088 - val_loss: 1.7510 - val_mean_squared_error: 1.7510\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 405us/sample - loss: 0.4640 - mean_squared_error: 0.4640 - val_loss: 1.8264 - val_mean_squared_error: 1.8264\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 0.5419 - mean_squared_error: 0.5419 - val_loss: 1.7943 - val_mean_squared_error: 1.7943\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 0.5547 - mean_squared_error: 0.5547 - val_loss: 1.8916 - val_mean_squared_error: 1.8916\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.5513 - mean_squared_error: 0.5513 - val_loss: 1.8420 - val_mean_squared_error: 1.8420\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.4908 - mean_squared_error: 0.4908 - val_loss: 1.7970 - val_mean_squared_error: 1.7970\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.4912 - mean_squared_error: 0.4912 - val_loss: 1.8407 - val_mean_squared_error: 1.8407\n",
            "==================================================\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_51 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_85 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_51 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_68 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_86 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_52 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_69 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_87 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_53 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_70 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_17 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_88 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_71 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_89 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 962us/sample - loss: 175.7548 - mean_squared_error: 175.7548 - val_loss: 184.6983 - val_mean_squared_error: 184.6983\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 18.0932 - mean_squared_error: 18.0932 - val_loss: 42.3898 - val_mean_squared_error: 42.3898\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 15.0298 - mean_squared_error: 15.0298 - val_loss: 15.4117 - val_mean_squared_error: 15.4117\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 13.2383 - mean_squared_error: 13.2383 - val_loss: 10.8765 - val_mean_squared_error: 10.8765\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 11.8380 - mean_squared_error: 11.8380 - val_loss: 12.2057 - val_mean_squared_error: 12.2057\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 11.2210 - mean_squared_error: 11.2210 - val_loss: 9.6636 - val_mean_squared_error: 9.6636\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 9.7814 - mean_squared_error: 9.7814 - val_loss: 9.2379 - val_mean_squared_error: 9.2379\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 8.8784 - mean_squared_error: 8.8784 - val_loss: 8.5248 - val_mean_squared_error: 8.5248\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 7.3313 - mean_squared_error: 7.3313 - val_loss: 7.0702 - val_mean_squared_error: 7.0702\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 6.3316 - mean_squared_error: 6.3316 - val_loss: 9.2696 - val_mean_squared_error: 9.2696\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 5.7886 - mean_squared_error: 5.7886 - val_loss: 6.2462 - val_mean_squared_error: 6.2462\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 405us/sample - loss: 5.0124 - mean_squared_error: 5.0124 - val_loss: 5.3610 - val_mean_squared_error: 5.3610\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 403us/sample - loss: 4.8843 - mean_squared_error: 4.8843 - val_loss: 6.1606 - val_mean_squared_error: 6.1606\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 4.3352 - mean_squared_error: 4.3352 - val_loss: 6.1841 - val_mean_squared_error: 6.1841\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 4.1334 - mean_squared_error: 4.1334 - val_loss: 4.8764 - val_mean_squared_error: 4.8764\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 3.6760 - mean_squared_error: 3.6760 - val_loss: 3.5600 - val_mean_squared_error: 3.5600\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 3.5002 - mean_squared_error: 3.5002 - val_loss: 4.2830 - val_mean_squared_error: 4.2830\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 3.5143 - mean_squared_error: 3.5143 - val_loss: 4.2615 - val_mean_squared_error: 4.2615\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 408us/sample - loss: 3.1286 - mean_squared_error: 3.1286 - val_loss: 3.2947 - val_mean_squared_error: 3.2947\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 403us/sample - loss: 3.0248 - mean_squared_error: 3.0248 - val_loss: 3.9911 - val_mean_squared_error: 3.9911\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 403us/sample - loss: 3.0598 - mean_squared_error: 3.0598 - val_loss: 3.1733 - val_mean_squared_error: 3.1733\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 2.7373 - mean_squared_error: 2.7373 - val_loss: 2.8047 - val_mean_squared_error: 2.8047\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 2.6651 - mean_squared_error: 2.6651 - val_loss: 2.6243 - val_mean_squared_error: 2.6243\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 2.5788 - mean_squared_error: 2.5788 - val_loss: 2.7293 - val_mean_squared_error: 2.7293\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 2.6109 - mean_squared_error: 2.6109 - val_loss: 2.7882 - val_mean_squared_error: 2.7882\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 2.3919 - mean_squared_error: 2.3919 - val_loss: 2.6195 - val_mean_squared_error: 2.6195\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 2.4005 - mean_squared_error: 2.4005 - val_loss: 3.2495 - val_mean_squared_error: 3.2495\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 2.3251 - mean_squared_error: 2.3251 - val_loss: 3.1919 - val_mean_squared_error: 3.1919\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 403us/sample - loss: 2.2203 - mean_squared_error: 2.2203 - val_loss: 2.8754 - val_mean_squared_error: 2.8754\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 2.1855 - mean_squared_error: 2.1855 - val_loss: 2.5250 - val_mean_squared_error: 2.5250\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 2.1955 - mean_squared_error: 2.1955 - val_loss: 2.5627 - val_mean_squared_error: 2.5627\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 2.0545 - mean_squared_error: 2.0545 - val_loss: 2.6894 - val_mean_squared_error: 2.6894\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 2.0084 - mean_squared_error: 2.0084 - val_loss: 2.9582 - val_mean_squared_error: 2.9582\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.9637 - mean_squared_error: 1.9637 - val_loss: 2.3727 - val_mean_squared_error: 2.3727\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.9479 - mean_squared_error: 1.9479 - val_loss: 2.0513 - val_mean_squared_error: 2.0513\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.9386 - mean_squared_error: 1.9386 - val_loss: 2.3631 - val_mean_squared_error: 2.3631\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 1.8477 - mean_squared_error: 1.8477 - val_loss: 2.1436 - val_mean_squared_error: 2.1436\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 1.8077 - mean_squared_error: 1.8077 - val_loss: 2.6437 - val_mean_squared_error: 2.6437\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.7500 - mean_squared_error: 1.7500 - val_loss: 2.1670 - val_mean_squared_error: 2.1670\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.7951 - mean_squared_error: 1.7951 - val_loss: 2.1883 - val_mean_squared_error: 2.1883\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.6850 - mean_squared_error: 1.6850 - val_loss: 2.4617 - val_mean_squared_error: 2.4617\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.8985 - mean_squared_error: 1.8985 - val_loss: 2.9073 - val_mean_squared_error: 2.9073\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.6460 - mean_squared_error: 1.6460 - val_loss: 2.8960 - val_mean_squared_error: 2.8960\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.6442 - mean_squared_error: 1.6442 - val_loss: 2.4684 - val_mean_squared_error: 2.4684\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.6144 - mean_squared_error: 1.6144 - val_loss: 2.8482 - val_mean_squared_error: 2.8482\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.6573 - mean_squared_error: 1.6573 - val_loss: 2.2006 - val_mean_squared_error: 2.2006\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.6843 - mean_squared_error: 1.6843 - val_loss: 2.2293 - val_mean_squared_error: 2.2293\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.4220 - mean_squared_error: 1.4220 - val_loss: 2.1190 - val_mean_squared_error: 2.1190\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.5463 - mean_squared_error: 1.5463 - val_loss: 2.6614 - val_mean_squared_error: 2.6614\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.4843 - mean_squared_error: 1.4843 - val_loss: 2.2111 - val_mean_squared_error: 2.2111\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.4431 - mean_squared_error: 1.4431 - val_loss: 2.1980 - val_mean_squared_error: 2.1980\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.4389 - mean_squared_error: 1.4389 - val_loss: 2.0407 - val_mean_squared_error: 2.0407\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.3820 - mean_squared_error: 1.3820 - val_loss: 1.9872 - val_mean_squared_error: 1.9872\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.4508 - mean_squared_error: 1.4508 - val_loss: 2.2747 - val_mean_squared_error: 2.2747\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 1.4129 - mean_squared_error: 1.4129 - val_loss: 1.8879 - val_mean_squared_error: 1.8879\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.3324 - mean_squared_error: 1.3324 - val_loss: 1.9457 - val_mean_squared_error: 1.9457\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.3575 - mean_squared_error: 1.3575 - val_loss: 2.1455 - val_mean_squared_error: 2.1455\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.3412 - mean_squared_error: 1.3412 - val_loss: 2.0380 - val_mean_squared_error: 2.0380\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.3236 - mean_squared_error: 1.3236 - val_loss: 1.9563 - val_mean_squared_error: 1.9563\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.3675 - mean_squared_error: 1.3675 - val_loss: 2.5719 - val_mean_squared_error: 2.5719\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.4244 - mean_squared_error: 1.4244 - val_loss: 2.1401 - val_mean_squared_error: 2.1401\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 1.2686 - mean_squared_error: 1.2686 - val_loss: 1.9149 - val_mean_squared_error: 1.9149\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 1.3596 - mean_squared_error: 1.3596 - val_loss: 2.2526 - val_mean_squared_error: 2.2526\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 1.3334 - mean_squared_error: 1.3334 - val_loss: 2.2999 - val_mean_squared_error: 2.2999\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.3657 - mean_squared_error: 1.3657 - val_loss: 2.3232 - val_mean_squared_error: 2.3232\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.1975 - mean_squared_error: 1.1975 - val_loss: 1.9495 - val_mean_squared_error: 1.9495\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.3413 - mean_squared_error: 1.3413 - val_loss: 2.1746 - val_mean_squared_error: 2.1746\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.1536 - mean_squared_error: 1.1536 - val_loss: 1.9767 - val_mean_squared_error: 1.9767\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.1905 - mean_squared_error: 1.1905 - val_loss: 2.5113 - val_mean_squared_error: 2.5113\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.2909 - mean_squared_error: 1.2909 - val_loss: 2.1605 - val_mean_squared_error: 2.1605\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.2848 - mean_squared_error: 1.2848 - val_loss: 1.7580 - val_mean_squared_error: 1.7580\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.2562 - mean_squared_error: 1.2562 - val_loss: 2.2283 - val_mean_squared_error: 2.2283\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.1330 - mean_squared_error: 1.1330 - val_loss: 2.1527 - val_mean_squared_error: 2.1527\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.2377 - mean_squared_error: 1.2377 - val_loss: 1.9833 - val_mean_squared_error: 1.9833\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.2525 - mean_squared_error: 1.2525 - val_loss: 1.7448 - val_mean_squared_error: 1.7448\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.1573 - mean_squared_error: 1.1573 - val_loss: 2.2030 - val_mean_squared_error: 2.2030\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.1623 - mean_squared_error: 1.1623 - val_loss: 2.0017 - val_mean_squared_error: 2.0017\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.1393 - mean_squared_error: 1.1393 - val_loss: 2.0533 - val_mean_squared_error: 2.0533\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.0980 - mean_squared_error: 1.0980 - val_loss: 2.0798 - val_mean_squared_error: 2.0798\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.1824 - mean_squared_error: 1.1824 - val_loss: 2.1237 - val_mean_squared_error: 2.1237\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.1512 - mean_squared_error: 1.1512 - val_loss: 1.9005 - val_mean_squared_error: 1.9005\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.0067 - mean_squared_error: 1.0067 - val_loss: 1.9954 - val_mean_squared_error: 1.9954\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.0799 - mean_squared_error: 1.0799 - val_loss: 1.9509 - val_mean_squared_error: 1.9509\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 405us/sample - loss: 1.0892 - mean_squared_error: 1.0892 - val_loss: 1.9272 - val_mean_squared_error: 1.9272\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 1.0711 - mean_squared_error: 1.0711 - val_loss: 2.1126 - val_mean_squared_error: 2.1126\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 405us/sample - loss: 1.0171 - mean_squared_error: 1.0171 - val_loss: 1.9875 - val_mean_squared_error: 1.9875\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 405us/sample - loss: 1.0834 - mean_squared_error: 1.0834 - val_loss: 1.7842 - val_mean_squared_error: 1.7842\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 404us/sample - loss: 1.0537 - mean_squared_error: 1.0537 - val_loss: 1.8498 - val_mean_squared_error: 1.8498\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 1.0620 - mean_squared_error: 1.0620 - val_loss: 2.0510 - val_mean_squared_error: 2.0510\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.9918 - mean_squared_error: 0.9918 - val_loss: 1.7243 - val_mean_squared_error: 1.7243\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 1.0197 - mean_squared_error: 1.0197 - val_loss: 1.9174 - val_mean_squared_error: 1.9174\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.0005 - mean_squared_error: 1.0005 - val_loss: 1.7545 - val_mean_squared_error: 1.7545\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.9970 - mean_squared_error: 0.9970 - val_loss: 1.8458 - val_mean_squared_error: 1.8458\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 1.0037 - mean_squared_error: 1.0037 - val_loss: 1.9446 - val_mean_squared_error: 1.9446\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.9849 - mean_squared_error: 0.9849 - val_loss: 1.8512 - val_mean_squared_error: 1.8512\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 401us/sample - loss: 1.0449 - mean_squared_error: 1.0449 - val_loss: 1.9882 - val_mean_squared_error: 1.9882\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.9567 - mean_squared_error: 0.9567 - val_loss: 1.8960 - val_mean_squared_error: 1.8960\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.0940 - mean_squared_error: 1.0940 - val_loss: 1.9284 - val_mean_squared_error: 1.9284\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.9951 - mean_squared_error: 0.9951 - val_loss: 1.8609 - val_mean_squared_error: 1.8609\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.9961 - mean_squared_error: 0.9961 - val_loss: 1.9053 - val_mean_squared_error: 1.9053\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0181 - mean_squared_error: 1.0181 - val_loss: 2.0262 - val_mean_squared_error: 2.0262\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.9521 - mean_squared_error: 0.9521 - val_loss: 1.8112 - val_mean_squared_error: 1.8112\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.9342 - mean_squared_error: 0.9342 - val_loss: 2.3787 - val_mean_squared_error: 2.3787\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0875 - mean_squared_error: 1.0875 - val_loss: 1.8687 - val_mean_squared_error: 1.8687\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8453 - mean_squared_error: 0.8453 - val_loss: 1.7680 - val_mean_squared_error: 1.7680\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.9208 - mean_squared_error: 0.9208 - val_loss: 1.8727 - val_mean_squared_error: 1.8727\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8801 - mean_squared_error: 0.8801 - val_loss: 1.7865 - val_mean_squared_error: 1.7865\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.9099 - mean_squared_error: 0.9099 - val_loss: 1.8082 - val_mean_squared_error: 1.8082\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.9271 - mean_squared_error: 0.9271 - val_loss: 2.1607 - val_mean_squared_error: 2.1607\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.9199 - mean_squared_error: 0.9199 - val_loss: 1.9323 - val_mean_squared_error: 1.9323\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8210 - mean_squared_error: 0.8210 - val_loss: 1.8432 - val_mean_squared_error: 1.8432\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.8429 - mean_squared_error: 0.8429 - val_loss: 1.7555 - val_mean_squared_error: 1.7555\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8254 - mean_squared_error: 0.8254 - val_loss: 1.7667 - val_mean_squared_error: 1.7667\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8249 - mean_squared_error: 0.8249 - val_loss: 1.7660 - val_mean_squared_error: 1.7660\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8459 - mean_squared_error: 0.8459 - val_loss: 1.6755 - val_mean_squared_error: 1.6755\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.9065 - mean_squared_error: 0.9065 - val_loss: 1.9898 - val_mean_squared_error: 1.9898\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8763 - mean_squared_error: 0.8763 - val_loss: 1.9095 - val_mean_squared_error: 1.9095\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8680 - mean_squared_error: 0.8680 - val_loss: 1.9858 - val_mean_squared_error: 1.9858\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8457 - mean_squared_error: 0.8457 - val_loss: 1.7331 - val_mean_squared_error: 1.7331\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.8837 - mean_squared_error: 0.8837 - val_loss: 2.0502 - val_mean_squared_error: 2.0502\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8157 - mean_squared_error: 0.8157 - val_loss: 1.7960 - val_mean_squared_error: 1.7960\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.9402 - mean_squared_error: 0.9402 - val_loss: 1.9295 - val_mean_squared_error: 1.9295\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8100 - mean_squared_error: 0.8100 - val_loss: 1.7528 - val_mean_squared_error: 1.7528\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7776 - mean_squared_error: 0.7776 - val_loss: 1.7169 - val_mean_squared_error: 1.7169\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7944 - mean_squared_error: 0.7944 - val_loss: 1.9079 - val_mean_squared_error: 1.9079\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7720 - mean_squared_error: 0.7720 - val_loss: 1.7122 - val_mean_squared_error: 1.7122\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7870 - mean_squared_error: 0.7870 - val_loss: 1.7442 - val_mean_squared_error: 1.7442\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.7789 - mean_squared_error: 0.7789 - val_loss: 1.7128 - val_mean_squared_error: 1.7128\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8706 - mean_squared_error: 0.8706 - val_loss: 2.1115 - val_mean_squared_error: 2.1115\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.7435 - mean_squared_error: 0.7435 - val_loss: 1.8635 - val_mean_squared_error: 1.8635\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.7536 - mean_squared_error: 0.7536 - val_loss: 1.7437 - val_mean_squared_error: 1.7437\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.7704 - mean_squared_error: 0.7704 - val_loss: 1.9012 - val_mean_squared_error: 1.9012\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.7946 - mean_squared_error: 0.7946 - val_loss: 1.9471 - val_mean_squared_error: 1.9471\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7463 - mean_squared_error: 0.7463 - val_loss: 1.7795 - val_mean_squared_error: 1.7795\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.7473 - mean_squared_error: 0.7473 - val_loss: 1.6500 - val_mean_squared_error: 1.6500\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.8184 - mean_squared_error: 0.8184 - val_loss: 2.1023 - val_mean_squared_error: 2.1023\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7880 - mean_squared_error: 0.7880 - val_loss: 1.9716 - val_mean_squared_error: 1.9716\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.7565 - mean_squared_error: 0.7565 - val_loss: 1.7150 - val_mean_squared_error: 1.7150\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.7983 - mean_squared_error: 0.7983 - val_loss: 1.9417 - val_mean_squared_error: 1.9417\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7088 - mean_squared_error: 0.7088 - val_loss: 1.6990 - val_mean_squared_error: 1.6990\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.7322 - mean_squared_error: 0.7322 - val_loss: 1.9264 - val_mean_squared_error: 1.9264\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.7621 - mean_squared_error: 0.7621 - val_loss: 2.0781 - val_mean_squared_error: 2.0781\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.7395 - mean_squared_error: 0.7395 - val_loss: 1.7335 - val_mean_squared_error: 1.7335\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.6831 - mean_squared_error: 0.6831 - val_loss: 1.7742 - val_mean_squared_error: 1.7742\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.7409 - mean_squared_error: 0.7409 - val_loss: 1.8058 - val_mean_squared_error: 1.8058\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.7034 - mean_squared_error: 0.7034 - val_loss: 1.6930 - val_mean_squared_error: 1.6930\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.7468 - mean_squared_error: 0.7468 - val_loss: 1.7547 - val_mean_squared_error: 1.7547\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.7196 - mean_squared_error: 0.7196 - val_loss: 1.8169 - val_mean_squared_error: 1.8169\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.7145 - mean_squared_error: 0.7145 - val_loss: 1.7096 - val_mean_squared_error: 1.7096\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6882 - mean_squared_error: 0.6882 - val_loss: 1.7408 - val_mean_squared_error: 1.7408\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.7175 - mean_squared_error: 0.7175 - val_loss: 1.7958 - val_mean_squared_error: 1.7958\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6823 - mean_squared_error: 0.6823 - val_loss: 1.6946 - val_mean_squared_error: 1.6946\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6886 - mean_squared_error: 0.6886 - val_loss: 2.2996 - val_mean_squared_error: 2.2996\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.6904 - mean_squared_error: 0.6904 - val_loss: 1.8631 - val_mean_squared_error: 1.8631\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.6870 - mean_squared_error: 0.6870 - val_loss: 1.7272 - val_mean_squared_error: 1.7272\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.7101 - mean_squared_error: 0.7101 - val_loss: 1.7397 - val_mean_squared_error: 1.7397\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.6918 - mean_squared_error: 0.6918 - val_loss: 1.7316 - val_mean_squared_error: 1.7316\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6944 - mean_squared_error: 0.6944 - val_loss: 1.6536 - val_mean_squared_error: 1.6536\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6384 - mean_squared_error: 0.6384 - val_loss: 1.8943 - val_mean_squared_error: 1.8943\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.6455 - mean_squared_error: 0.6455 - val_loss: 1.7085 - val_mean_squared_error: 1.7085\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.6239 - mean_squared_error: 0.6239 - val_loss: 1.7473 - val_mean_squared_error: 1.7473\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.7007 - mean_squared_error: 0.7007 - val_loss: 1.7239 - val_mean_squared_error: 1.7239\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.6941 - mean_squared_error: 0.6941 - val_loss: 1.8019 - val_mean_squared_error: 1.8019\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.7101 - mean_squared_error: 0.7101 - val_loss: 1.8302 - val_mean_squared_error: 1.8302\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.7253 - mean_squared_error: 0.7253 - val_loss: 1.6442 - val_mean_squared_error: 1.6442\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6912 - mean_squared_error: 0.6912 - val_loss: 1.6100 - val_mean_squared_error: 1.6100\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.6529 - mean_squared_error: 0.6529 - val_loss: 1.7780 - val_mean_squared_error: 1.7780\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6150 - mean_squared_error: 0.6150 - val_loss: 1.7249 - val_mean_squared_error: 1.7249\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.6255 - mean_squared_error: 0.6255 - val_loss: 1.7892 - val_mean_squared_error: 1.7892\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.5963 - mean_squared_error: 0.5963 - val_loss: 1.7583 - val_mean_squared_error: 1.7583\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.5868 - mean_squared_error: 0.5868 - val_loss: 1.7253 - val_mean_squared_error: 1.7253\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.6298 - mean_squared_error: 0.6298 - val_loss: 1.9002 - val_mean_squared_error: 1.9002\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6031 - mean_squared_error: 0.6031 - val_loss: 1.8727 - val_mean_squared_error: 1.8727\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6720 - mean_squared_error: 0.6720 - val_loss: 1.8223 - val_mean_squared_error: 1.8223\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.6356 - mean_squared_error: 0.6356 - val_loss: 1.7548 - val_mean_squared_error: 1.7548\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6142 - mean_squared_error: 0.6142 - val_loss: 1.6258 - val_mean_squared_error: 1.6258\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.6732 - mean_squared_error: 0.6732 - val_loss: 1.8753 - val_mean_squared_error: 1.8753\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6397 - mean_squared_error: 0.6397 - val_loss: 1.6997 - val_mean_squared_error: 1.6997\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6036 - mean_squared_error: 0.6036 - val_loss: 1.8023 - val_mean_squared_error: 1.8023\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6151 - mean_squared_error: 0.6151 - val_loss: 1.7313 - val_mean_squared_error: 1.7313\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6188 - mean_squared_error: 0.6188 - val_loss: 1.8753 - val_mean_squared_error: 1.8753\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6084 - mean_squared_error: 0.6084 - val_loss: 1.8374 - val_mean_squared_error: 1.8374\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.6434 - mean_squared_error: 0.6434 - val_loss: 1.7924 - val_mean_squared_error: 1.7924\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.5675 - mean_squared_error: 0.5675 - val_loss: 1.7033 - val_mean_squared_error: 1.7033\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 405us/sample - loss: 0.5909 - mean_squared_error: 0.5909 - val_loss: 1.8037 - val_mean_squared_error: 1.8037\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.6053 - mean_squared_error: 0.6053 - val_loss: 1.6995 - val_mean_squared_error: 1.6995\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.6180 - mean_squared_error: 0.6180 - val_loss: 1.7295 - val_mean_squared_error: 1.7295\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6133 - mean_squared_error: 0.6133 - val_loss: 1.8860 - val_mean_squared_error: 1.8860\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5558 - mean_squared_error: 0.5558 - val_loss: 1.7225 - val_mean_squared_error: 1.7225\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.5623 - mean_squared_error: 0.5623 - val_loss: 1.7350 - val_mean_squared_error: 1.7350\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.6140 - mean_squared_error: 0.6140 - val_loss: 1.7875 - val_mean_squared_error: 1.7875\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.5653 - mean_squared_error: 0.5653 - val_loss: 1.6392 - val_mean_squared_error: 1.6392\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6463 - mean_squared_error: 0.6463 - val_loss: 1.7681 - val_mean_squared_error: 1.7681\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.6120 - mean_squared_error: 0.6120 - val_loss: 1.7785 - val_mean_squared_error: 1.7785\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.6096 - mean_squared_error: 0.6096 - val_loss: 1.6898 - val_mean_squared_error: 1.6898\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.5668 - mean_squared_error: 0.5668 - val_loss: 1.7671 - val_mean_squared_error: 1.7671\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.5760 - mean_squared_error: 0.5760 - val_loss: 1.8832 - val_mean_squared_error: 1.8832\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.6125 - mean_squared_error: 0.6125 - val_loss: 1.8093 - val_mean_squared_error: 1.8093\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.5615 - mean_squared_error: 0.5615 - val_loss: 1.6633 - val_mean_squared_error: 1.6633\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 411us/sample - loss: 0.6094 - mean_squared_error: 0.6094 - val_loss: 1.7399 - val_mean_squared_error: 1.7399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsVp-fvCqlcd",
        "colab_type": "code",
        "outputId": "175e9e0a-1e1d-442f-819b-b9cb822d4f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Use an early stopping callback and our timing callback\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                              patience=100, mode='auto')\n",
        "time_callback = TimeHistory()\n",
        "\n",
        "# Initialize a new data frame to hold our output data\n",
        "cnn_flipped_df = pd.DataFrame()\n",
        "\n",
        "# Start filter list\n",
        "start_filters = [12, 16]\n",
        "\n",
        "# Create a list of initial dropout values and steps to increase\n",
        "dropouts = [(0.00,0.10)]\n",
        "\n",
        "for lr_factor in [10]: \n",
        "    for start_filter in start_filters:\n",
        "        for d in dropouts:\n",
        "            model = create_bn_cnn_model(start_filter, d[0], d[1], fc1=500, fc2=500)\n",
        "            # Use the adam optimizer with the default learning rate\n",
        "            adam = optimizers.Adam(lr=0.001*lr_factor, beta_1=0.9, beta_2=0.999)\n",
        "            model.compile(\n",
        "                  optimizer=adam,\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_squared_error'])\n",
        "            history = model.fit(\n",
        "                flipped_X.astype(np.float32), flipped_y.astype(np.float32),\n",
        "                epochs=200,\n",
        "                validation_split=0.15, callbacks=[time_callback, early_stop])\n",
        "            times = time_callback.times\n",
        "\n",
        "            # Convert to dataframe\n",
        "            hist = pd.DataFrame(history.history)\n",
        "            hist['epoch'] = history.epoch\n",
        "            hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "            hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "            hist['times'] = times\n",
        "            hist['starting_filter'] = start_filter\n",
        "            hist['layers'] = 3\n",
        "            hist['pooling'] = 'yes'\n",
        "            hist['fc_layer1'] = 500\n",
        "            hist['fc_layer2'] = 500\n",
        "            hist['activation'] = 'relu'\n",
        "            hist['optimizer'] = 'adam'\n",
        "            hist['lrate'] = adam.get_config()['learning_rate']\n",
        "            hist['dropout_initial'] = d[0]\n",
        "            hist['dropout_step'] = d[1]\n",
        "            hist['batch_norm'] = 1\n",
        "            hist['bias'] = 0\n",
        "            hist['stride'] = 1\n",
        "            hist['flipped'] = 1.0\n",
        "\n",
        "            # Keep concatenating to dataframe\n",
        "            cnn_flipped_df = pd.concat([cnn_flipped_df,hist])\n",
        "\n",
        "            # Re-pickle after every model to retain progress\n",
        "            cnn_flipped_df.to_pickle(drive_path+\"OutputData/cnn_flipped_df6.pkl\")\n",
        "\n",
        "            # Save models.\n",
        "            filename = \"cnn_flipped6_{}_d{}_s{}_sf{}_lrfactor{}_flipped_100\".format('adam', d[0], d[1], start_filter, lr_factor)\n",
        "            model.save(drive_path+\"Models/\"+filename+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_42 (Conv2D)           (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_42 (MaxPooling (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_56 (Dropout)         (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_43 (MaxPooling (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_57 (Dropout)         (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_58 (Dropout)         (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_73 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_59 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_74 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,180,234\n",
            "Trainable params: 3,178,066\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 837us/sample - loss: 230.1333 - mean_squared_error: 230.1334 - val_loss: 233.5911 - val_mean_squared_error: 233.5911\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 21.9216 - mean_squared_error: 21.9216 - val_loss: 68.3068 - val_mean_squared_error: 68.3068\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 18.3456 - mean_squared_error: 18.3456 - val_loss: 23.3899 - val_mean_squared_error: 23.3899\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 16.2826 - mean_squared_error: 16.2826 - val_loss: 15.9336 - val_mean_squared_error: 15.9336\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 15.0244 - mean_squared_error: 15.0244 - val_loss: 23.1279 - val_mean_squared_error: 23.1279\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 14.3419 - mean_squared_error: 14.3419 - val_loss: 15.0854 - val_mean_squared_error: 15.0854\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 12.0839 - mean_squared_error: 12.0839 - val_loss: 10.8079 - val_mean_squared_error: 10.8079\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 11.4763 - mean_squared_error: 11.4763 - val_loss: 11.5478 - val_mean_squared_error: 11.5478\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 10.0495 - mean_squared_error: 10.0495 - val_loss: 9.2513 - val_mean_squared_error: 9.2513\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 9.8827 - mean_squared_error: 9.8827 - val_loss: 8.0024 - val_mean_squared_error: 8.0024\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 8.4692 - mean_squared_error: 8.4692 - val_loss: 7.4662 - val_mean_squared_error: 7.4662\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 7.4583 - mean_squared_error: 7.4583 - val_loss: 6.2666 - val_mean_squared_error: 6.2666\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 6.7544 - mean_squared_error: 6.7544 - val_loss: 6.5478 - val_mean_squared_error: 6.5478\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 6.0218 - mean_squared_error: 6.0218 - val_loss: 5.9917 - val_mean_squared_error: 5.9917\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 5.7824 - mean_squared_error: 5.7824 - val_loss: 4.3376 - val_mean_squared_error: 4.3376\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 5.1199 - mean_squared_error: 5.1199 - val_loss: 4.9593 - val_mean_squared_error: 4.9593\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 4.7682 - mean_squared_error: 4.7682 - val_loss: 3.9804 - val_mean_squared_error: 3.9804\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 4.5882 - mean_squared_error: 4.5882 - val_loss: 4.0979 - val_mean_squared_error: 4.0979\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 4.4420 - mean_squared_error: 4.4420 - val_loss: 3.7960 - val_mean_squared_error: 3.7960\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 4.3229 - mean_squared_error: 4.3229 - val_loss: 4.2997 - val_mean_squared_error: 4.2997\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 4.2955 - mean_squared_error: 4.2955 - val_loss: 3.4209 - val_mean_squared_error: 3.4209\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 3.9560 - mean_squared_error: 3.9560 - val_loss: 4.4654 - val_mean_squared_error: 4.4654\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 3.7738 - mean_squared_error: 3.7738 - val_loss: 2.9772 - val_mean_squared_error: 2.9772\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 3.6051 - mean_squared_error: 3.6051 - val_loss: 2.9260 - val_mean_squared_error: 2.9260\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 3.6060 - mean_squared_error: 3.6060 - val_loss: 2.8402 - val_mean_squared_error: 2.8402\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 3.2797 - mean_squared_error: 3.2797 - val_loss: 3.2450 - val_mean_squared_error: 3.2450\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 3.3472 - mean_squared_error: 3.3472 - val_loss: 2.9707 - val_mean_squared_error: 2.9707\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 3.1748 - mean_squared_error: 3.1748 - val_loss: 3.1572 - val_mean_squared_error: 3.1572\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 3.1017 - mean_squared_error: 3.1017 - val_loss: 2.9390 - val_mean_squared_error: 2.9390\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 3.1163 - mean_squared_error: 3.1163 - val_loss: 2.6118 - val_mean_squared_error: 2.6118\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 2.9944 - mean_squared_error: 2.9944 - val_loss: 2.5662 - val_mean_squared_error: 2.5662\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 338us/sample - loss: 2.9646 - mean_squared_error: 2.9646 - val_loss: 2.7880 - val_mean_squared_error: 2.7880\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 2.7362 - mean_squared_error: 2.7362 - val_loss: 2.3848 - val_mean_squared_error: 2.3848\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 2.7499 - mean_squared_error: 2.7499 - val_loss: 2.4331 - val_mean_squared_error: 2.4331\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 2.6782 - mean_squared_error: 2.6782 - val_loss: 2.2909 - val_mean_squared_error: 2.2909\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 2.5849 - mean_squared_error: 2.5849 - val_loss: 2.3235 - val_mean_squared_error: 2.3235\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 2.5506 - mean_squared_error: 2.5506 - val_loss: 2.7084 - val_mean_squared_error: 2.7084\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 2.4069 - mean_squared_error: 2.4069 - val_loss: 2.2721 - val_mean_squared_error: 2.2721\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 2.6217 - mean_squared_error: 2.6217 - val_loss: 2.6510 - val_mean_squared_error: 2.6510\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 2.3428 - mean_squared_error: 2.3428 - val_loss: 2.2836 - val_mean_squared_error: 2.2836\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.3437 - mean_squared_error: 2.3437 - val_loss: 2.7037 - val_mean_squared_error: 2.7037\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 2.3959 - mean_squared_error: 2.3959 - val_loss: 2.6272 - val_mean_squared_error: 2.6272\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.1745 - mean_squared_error: 2.1745 - val_loss: 1.9364 - val_mean_squared_error: 1.9364\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 2.1412 - mean_squared_error: 2.1412 - val_loss: 2.5140 - val_mean_squared_error: 2.5140\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 2.0327 - mean_squared_error: 2.0327 - val_loss: 2.1235 - val_mean_squared_error: 2.1235\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 2.0727 - mean_squared_error: 2.0727 - val_loss: 1.8860 - val_mean_squared_error: 1.8860\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 2.0700 - mean_squared_error: 2.0700 - val_loss: 2.2739 - val_mean_squared_error: 2.2739\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 2.0760 - mean_squared_error: 2.0760 - val_loss: 2.0013 - val_mean_squared_error: 2.0013\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 2.0290 - mean_squared_error: 2.0290 - val_loss: 1.8519 - val_mean_squared_error: 1.8519\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 340us/sample - loss: 2.0492 - mean_squared_error: 2.0492 - val_loss: 1.9240 - val_mean_squared_error: 1.9240\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.9780 - mean_squared_error: 1.9780 - val_loss: 2.1972 - val_mean_squared_error: 2.1972\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.9742 - mean_squared_error: 1.9742 - val_loss: 2.0443 - val_mean_squared_error: 2.0443\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.8391 - mean_squared_error: 1.8391 - val_loss: 1.8686 - val_mean_squared_error: 1.8686\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.8194 - mean_squared_error: 1.8194 - val_loss: 1.8867 - val_mean_squared_error: 1.8867\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.7523 - mean_squared_error: 1.7523 - val_loss: 2.1721 - val_mean_squared_error: 2.1721\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.7926 - mean_squared_error: 1.7926 - val_loss: 2.3080 - val_mean_squared_error: 2.3080\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.6867 - mean_squared_error: 1.6867 - val_loss: 1.9891 - val_mean_squared_error: 1.9891\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.7126 - mean_squared_error: 1.7126 - val_loss: 1.9220 - val_mean_squared_error: 1.9220\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.7713 - mean_squared_error: 1.7713 - val_loss: 1.9013 - val_mean_squared_error: 1.9013\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.6939 - mean_squared_error: 1.6939 - val_loss: 1.7365 - val_mean_squared_error: 1.7365\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.6358 - mean_squared_error: 1.6358 - val_loss: 1.9299 - val_mean_squared_error: 1.9299\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.7074 - mean_squared_error: 1.7074 - val_loss: 2.0788 - val_mean_squared_error: 2.0788\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.6331 - mean_squared_error: 1.6331 - val_loss: 1.9074 - val_mean_squared_error: 1.9074\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.5850 - mean_squared_error: 1.5850 - val_loss: 2.0268 - val_mean_squared_error: 2.0268\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.6403 - mean_squared_error: 1.6403 - val_loss: 1.7257 - val_mean_squared_error: 1.7257\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.6302 - mean_squared_error: 1.6302 - val_loss: 1.7299 - val_mean_squared_error: 1.7299\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.6388 - mean_squared_error: 1.6388 - val_loss: 1.6430 - val_mean_squared_error: 1.6430\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.6239 - mean_squared_error: 1.6239 - val_loss: 1.6821 - val_mean_squared_error: 1.6821\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.7037 - mean_squared_error: 1.7037 - val_loss: 1.7526 - val_mean_squared_error: 1.7526\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.5393 - mean_squared_error: 1.5393 - val_loss: 1.7286 - val_mean_squared_error: 1.7286\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.5689 - mean_squared_error: 1.5689 - val_loss: 1.9809 - val_mean_squared_error: 1.9809\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.6811 - mean_squared_error: 1.6811 - val_loss: 1.8676 - val_mean_squared_error: 1.8676\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.4348 - mean_squared_error: 1.4348 - val_loss: 2.1427 - val_mean_squared_error: 2.1427\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.5170 - mean_squared_error: 1.5170 - val_loss: 1.5674 - val_mean_squared_error: 1.5674\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.5529 - mean_squared_error: 1.5529 - val_loss: 1.6775 - val_mean_squared_error: 1.6775\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.4476 - mean_squared_error: 1.4476 - val_loss: 1.7336 - val_mean_squared_error: 1.7336\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.4454 - mean_squared_error: 1.4454 - val_loss: 1.6502 - val_mean_squared_error: 1.6502\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.3833 - mean_squared_error: 1.3833 - val_loss: 1.8646 - val_mean_squared_error: 1.8646\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.5215 - mean_squared_error: 1.5215 - val_loss: 1.8217 - val_mean_squared_error: 1.8217\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.4268 - mean_squared_error: 1.4268 - val_loss: 1.5672 - val_mean_squared_error: 1.5672\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.4122 - mean_squared_error: 1.4122 - val_loss: 1.6736 - val_mean_squared_error: 1.6736\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.4270 - mean_squared_error: 1.4270 - val_loss: 2.0412 - val_mean_squared_error: 2.0412\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.4469 - mean_squared_error: 1.4469 - val_loss: 1.7207 - val_mean_squared_error: 1.7207\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.3674 - mean_squared_error: 1.3674 - val_loss: 1.8242 - val_mean_squared_error: 1.8242\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.3397 - mean_squared_error: 1.3397 - val_loss: 1.6644 - val_mean_squared_error: 1.6644\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.3633 - mean_squared_error: 1.3633 - val_loss: 2.1426 - val_mean_squared_error: 2.1426\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.3847 - mean_squared_error: 1.3847 - val_loss: 2.1218 - val_mean_squared_error: 2.1218\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.3668 - mean_squared_error: 1.3668 - val_loss: 1.8632 - val_mean_squared_error: 1.8632\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 1.2940 - mean_squared_error: 1.2940 - val_loss: 1.6955 - val_mean_squared_error: 1.6955\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.4067 - mean_squared_error: 1.4067 - val_loss: 1.7115 - val_mean_squared_error: 1.7115\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.3319 - mean_squared_error: 1.3319 - val_loss: 1.5796 - val_mean_squared_error: 1.5796\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.3089 - mean_squared_error: 1.3089 - val_loss: 1.7287 - val_mean_squared_error: 1.7287\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.4024 - mean_squared_error: 1.4024 - val_loss: 1.6545 - val_mean_squared_error: 1.6545\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.3825 - mean_squared_error: 1.3825 - val_loss: 1.6912 - val_mean_squared_error: 1.6912\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.1905 - mean_squared_error: 1.1905 - val_loss: 1.6798 - val_mean_squared_error: 1.6798\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.2898 - mean_squared_error: 1.2898 - val_loss: 1.6114 - val_mean_squared_error: 1.6114\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.2900 - mean_squared_error: 1.2900 - val_loss: 1.7266 - val_mean_squared_error: 1.7266\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2038 - mean_squared_error: 1.2038 - val_loss: 1.7081 - val_mean_squared_error: 1.7081\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.2512 - mean_squared_error: 1.2512 - val_loss: 1.7629 - val_mean_squared_error: 1.7629\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.2166 - mean_squared_error: 1.2166 - val_loss: 1.7268 - val_mean_squared_error: 1.7268\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.2339 - mean_squared_error: 1.2339 - val_loss: 2.0743 - val_mean_squared_error: 2.0743\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 351us/sample - loss: 1.2355 - mean_squared_error: 1.2355 - val_loss: 1.5951 - val_mean_squared_error: 1.5951\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.1839 - mean_squared_error: 1.1839 - val_loss: 1.6800 - val_mean_squared_error: 1.6800\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2124 - mean_squared_error: 1.2124 - val_loss: 1.6656 - val_mean_squared_error: 1.6656\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.2188 - mean_squared_error: 1.2188 - val_loss: 1.7096 - val_mean_squared_error: 1.7096\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2061 - mean_squared_error: 1.2061 - val_loss: 1.6643 - val_mean_squared_error: 1.6643\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.3163 - mean_squared_error: 1.3163 - val_loss: 1.5102 - val_mean_squared_error: 1.5102\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.1490 - mean_squared_error: 1.1490 - val_loss: 1.6717 - val_mean_squared_error: 1.6717\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.2362 - mean_squared_error: 1.2362 - val_loss: 1.5599 - val_mean_squared_error: 1.5599\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2200 - mean_squared_error: 1.2200 - val_loss: 1.6092 - val_mean_squared_error: 1.6092\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.1409 - mean_squared_error: 1.1409 - val_loss: 1.6113 - val_mean_squared_error: 1.6113\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.1858 - mean_squared_error: 1.1858 - val_loss: 1.7435 - val_mean_squared_error: 1.7435\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.1111 - mean_squared_error: 1.1111 - val_loss: 1.8302 - val_mean_squared_error: 1.8302\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.2412 - mean_squared_error: 1.2412 - val_loss: 1.9499 - val_mean_squared_error: 1.9499\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.1633 - mean_squared_error: 1.1633 - val_loss: 2.0933 - val_mean_squared_error: 2.0933\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.1098 - mean_squared_error: 1.1098 - val_loss: 1.6425 - val_mean_squared_error: 1.6425\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.1016 - mean_squared_error: 1.1016 - val_loss: 1.5535 - val_mean_squared_error: 1.5535\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.1675 - mean_squared_error: 1.1675 - val_loss: 1.7340 - val_mean_squared_error: 1.7340\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.1717 - mean_squared_error: 1.1717 - val_loss: 1.7893 - val_mean_squared_error: 1.7893\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 343us/sample - loss: 1.1659 - mean_squared_error: 1.1659 - val_loss: 1.7112 - val_mean_squared_error: 1.7112\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.1757 - mean_squared_error: 1.1757 - val_loss: 1.6818 - val_mean_squared_error: 1.6818\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.1517 - mean_squared_error: 1.1517 - val_loss: 1.7289 - val_mean_squared_error: 1.7289\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 1.1225 - mean_squared_error: 1.1225 - val_loss: 2.0171 - val_mean_squared_error: 2.0171\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 1.1909 - mean_squared_error: 1.1909 - val_loss: 1.6635 - val_mean_squared_error: 1.6635\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 347us/sample - loss: 1.0919 - mean_squared_error: 1.0919 - val_loss: 1.8945 - val_mean_squared_error: 1.8945\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 355us/sample - loss: 1.1147 - mean_squared_error: 1.1147 - val_loss: 1.8370 - val_mean_squared_error: 1.8370\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.1479 - mean_squared_error: 1.1479 - val_loss: 1.6168 - val_mean_squared_error: 1.6168\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.0928 - mean_squared_error: 1.0928 - val_loss: 1.5816 - val_mean_squared_error: 1.5816\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.1376 - mean_squared_error: 1.1376 - val_loss: 1.7296 - val_mean_squared_error: 1.7296\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0853 - mean_squared_error: 1.0853 - val_loss: 1.6499 - val_mean_squared_error: 1.6499\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.1159 - mean_squared_error: 1.1159 - val_loss: 1.7532 - val_mean_squared_error: 1.7532\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 1.0329 - mean_squared_error: 1.0329 - val_loss: 2.0669 - val_mean_squared_error: 2.0669\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0898 - mean_squared_error: 1.0898 - val_loss: 1.7655 - val_mean_squared_error: 1.7655\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0830 - mean_squared_error: 1.0830 - val_loss: 1.7227 - val_mean_squared_error: 1.7227\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.9943 - mean_squared_error: 0.9943 - val_loss: 1.5620 - val_mean_squared_error: 1.5620\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0233 - mean_squared_error: 1.0233 - val_loss: 1.6684 - val_mean_squared_error: 1.6684\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0776 - mean_squared_error: 1.0776 - val_loss: 1.6472 - val_mean_squared_error: 1.6472\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.0800 - mean_squared_error: 1.0800 - val_loss: 1.6479 - val_mean_squared_error: 1.6479\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0808 - mean_squared_error: 1.0808 - val_loss: 1.5830 - val_mean_squared_error: 1.5830\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.0521 - mean_squared_error: 1.0521 - val_loss: 1.7081 - val_mean_squared_error: 1.7081\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.0597 - mean_squared_error: 1.0597 - val_loss: 1.7934 - val_mean_squared_error: 1.7934\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 349us/sample - loss: 1.0280 - mean_squared_error: 1.0280 - val_loss: 1.6118 - val_mean_squared_error: 1.6118\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 346us/sample - loss: 1.0669 - mean_squared_error: 1.0669 - val_loss: 1.6094 - val_mean_squared_error: 1.6094\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 350us/sample - loss: 0.9788 - mean_squared_error: 0.9788 - val_loss: 1.5895 - val_mean_squared_error: 1.5895\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.0376 - mean_squared_error: 1.0376 - val_loss: 1.6589 - val_mean_squared_error: 1.6589\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.6533 - val_mean_squared_error: 1.6533\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 1.0537 - mean_squared_error: 1.0537 - val_loss: 1.6206 - val_mean_squared_error: 1.6206\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.0031 - mean_squared_error: 1.0031 - val_loss: 1.7377 - val_mean_squared_error: 1.7377\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 0.9811 - mean_squared_error: 0.9811 - val_loss: 1.5836 - val_mean_squared_error: 1.5836\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 357us/sample - loss: 0.9901 - mean_squared_error: 0.9901 - val_loss: 1.8639 - val_mean_squared_error: 1.8639\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 354us/sample - loss: 1.0468 - mean_squared_error: 1.0468 - val_loss: 1.6596 - val_mean_squared_error: 1.6596\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 356us/sample - loss: 1.0324 - mean_squared_error: 1.0324 - val_loss: 1.6531 - val_mean_squared_error: 1.6531\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.9491 - mean_squared_error: 0.9491 - val_loss: 1.6728 - val_mean_squared_error: 1.6728\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9835 - mean_squared_error: 0.9835 - val_loss: 1.5604 - val_mean_squared_error: 1.5604\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.9906 - mean_squared_error: 0.9906 - val_loss: 1.9184 - val_mean_squared_error: 1.9184\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.0249 - mean_squared_error: 1.0249 - val_loss: 1.6947 - val_mean_squared_error: 1.6947\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9851 - mean_squared_error: 0.9851 - val_loss: 1.6645 - val_mean_squared_error: 1.6645\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.9551 - mean_squared_error: 0.9551 - val_loss: 1.8248 - val_mean_squared_error: 1.8248\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.9474 - mean_squared_error: 0.9474 - val_loss: 1.7632 - val_mean_squared_error: 1.7632\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 342us/sample - loss: 1.0372 - mean_squared_error: 1.0372 - val_loss: 1.8196 - val_mean_squared_error: 1.8196\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 1.0569 - mean_squared_error: 1.0569 - val_loss: 1.6064 - val_mean_squared_error: 1.6064\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 1.0102 - mean_squared_error: 1.0102 - val_loss: 1.6420 - val_mean_squared_error: 1.6420\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.9448 - mean_squared_error: 0.9448 - val_loss: 1.6172 - val_mean_squared_error: 1.6172\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9852 - mean_squared_error: 0.9852 - val_loss: 1.6628 - val_mean_squared_error: 1.6628\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 339us/sample - loss: 0.9115 - mean_squared_error: 0.9115 - val_loss: 1.6043 - val_mean_squared_error: 1.6043\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.9615 - mean_squared_error: 0.9615 - val_loss: 1.7190 - val_mean_squared_error: 1.7190\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9347 - mean_squared_error: 0.9347 - val_loss: 1.6517 - val_mean_squared_error: 1.6517\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9258 - mean_squared_error: 0.9258 - val_loss: 1.6772 - val_mean_squared_error: 1.6772\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.9261 - mean_squared_error: 0.9261 - val_loss: 1.5908 - val_mean_squared_error: 1.5908\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 344us/sample - loss: 0.9360 - mean_squared_error: 0.9360 - val_loss: 1.6432 - val_mean_squared_error: 1.6432\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 341us/sample - loss: 0.9418 - mean_squared_error: 0.9418 - val_loss: 1.6754 - val_mean_squared_error: 1.6754\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 345us/sample - loss: 0.9941 - mean_squared_error: 0.9941 - val_loss: 1.7864 - val_mean_squared_error: 1.7864\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 348us/sample - loss: 0.9246 - mean_squared_error: 0.9246 - val_loss: 1.7682 - val_mean_squared_error: 1.7682\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 352us/sample - loss: 0.9023 - mean_squared_error: 0.9023 - val_loss: 1.5170 - val_mean_squared_error: 1.5170\n",
            "==================================================\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_45 (Conv2D)           (None, 94, 94, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_75 (Batc (None, 94, 94, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_60 (Dropout)         (None, 47, 47, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 46, 46, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_76 (Batc (None, 46, 46, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_61 (Dropout)         (None, 23, 23, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 22, 22, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_77 (Batc (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_62 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 500)               3872500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_78 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_63 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_79 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 4,152,862\n",
            "Trainable params: 4,150,638\n",
            "Non-trainable params: 2,224\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 3s 916us/sample - loss: 228.3768 - mean_squared_error: 228.3769 - val_loss: 174.2572 - val_mean_squared_error: 174.2572\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 20.5152 - mean_squared_error: 20.5152 - val_loss: 43.4856 - val_mean_squared_error: 43.4856\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 15.7205 - mean_squared_error: 15.7205 - val_loss: 12.7312 - val_mean_squared_error: 12.7312\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 14.9349 - mean_squared_error: 14.9349 - val_loss: 11.5483 - val_mean_squared_error: 11.5483\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 13.3080 - mean_squared_error: 13.3080 - val_loss: 11.0735 - val_mean_squared_error: 11.0735\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 12.1526 - mean_squared_error: 12.1526 - val_loss: 12.0810 - val_mean_squared_error: 12.0810\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 11.6030 - mean_squared_error: 11.6030 - val_loss: 10.5068 - val_mean_squared_error: 10.5068\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 10.4850 - mean_squared_error: 10.4850 - val_loss: 9.7831 - val_mean_squared_error: 9.7831\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 9.4118 - mean_squared_error: 9.4118 - val_loss: 11.4880 - val_mean_squared_error: 11.4880\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 8.5468 - mean_squared_error: 8.5468 - val_loss: 8.7952 - val_mean_squared_error: 8.7952\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 7.6789 - mean_squared_error: 7.6789 - val_loss: 7.3553 - val_mean_squared_error: 7.3553\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 6.6784 - mean_squared_error: 6.6784 - val_loss: 5.6911 - val_mean_squared_error: 5.6911\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 6.1154 - mean_squared_error: 6.1154 - val_loss: 5.1822 - val_mean_squared_error: 5.1822\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 5.5963 - mean_squared_error: 5.5963 - val_loss: 4.5712 - val_mean_squared_error: 4.5712\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 5.4388 - mean_squared_error: 5.4388 - val_loss: 4.4158 - val_mean_squared_error: 4.4158\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 4.9673 - mean_squared_error: 4.9673 - val_loss: 3.9888 - val_mean_squared_error: 3.9888\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 4.5428 - mean_squared_error: 4.5428 - val_loss: 3.7005 - val_mean_squared_error: 3.7005\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 4.4148 - mean_squared_error: 4.4148 - val_loss: 5.6247 - val_mean_squared_error: 5.6247\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 4.3763 - mean_squared_error: 4.3763 - val_loss: 3.9845 - val_mean_squared_error: 3.9845\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 4.0323 - mean_squared_error: 4.0323 - val_loss: 3.6174 - val_mean_squared_error: 3.6174\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 3.9770 - mean_squared_error: 3.9770 - val_loss: 3.1888 - val_mean_squared_error: 3.1888\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 3.6883 - mean_squared_error: 3.6883 - val_loss: 2.8964 - val_mean_squared_error: 2.8964\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 3.7223 - mean_squared_error: 3.7223 - val_loss: 2.8386 - val_mean_squared_error: 2.8386\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 3.5818 - mean_squared_error: 3.5818 - val_loss: 3.0218 - val_mean_squared_error: 3.0218\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 3.4399 - mean_squared_error: 3.4399 - val_loss: 3.2340 - val_mean_squared_error: 3.2340\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 3.1958 - mean_squared_error: 3.1958 - val_loss: 3.0831 - val_mean_squared_error: 3.0831\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 3.1393 - mean_squared_error: 3.1393 - val_loss: 2.7872 - val_mean_squared_error: 2.7872\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 2.9904 - mean_squared_error: 2.9904 - val_loss: 2.7058 - val_mean_squared_error: 2.7058\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 3.0022 - mean_squared_error: 3.0022 - val_loss: 3.0049 - val_mean_squared_error: 3.0049\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.9642 - mean_squared_error: 2.9642 - val_loss: 2.8634 - val_mean_squared_error: 2.8634\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 2.8122 - mean_squared_error: 2.8122 - val_loss: 2.9611 - val_mean_squared_error: 2.9611\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.7238 - mean_squared_error: 2.7238 - val_loss: 2.6597 - val_mean_squared_error: 2.6597\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 2.6528 - mean_squared_error: 2.6528 - val_loss: 2.5337 - val_mean_squared_error: 2.5337\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 2.7306 - mean_squared_error: 2.7306 - val_loss: 2.5820 - val_mean_squared_error: 2.5820\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 2.6636 - mean_squared_error: 2.6636 - val_loss: 2.4101 - val_mean_squared_error: 2.4101\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 2.5729 - mean_squared_error: 2.5729 - val_loss: 2.0959 - val_mean_squared_error: 2.0959\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 2.4347 - mean_squared_error: 2.4347 - val_loss: 2.1637 - val_mean_squared_error: 2.1637\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.4209 - mean_squared_error: 2.4209 - val_loss: 2.1538 - val_mean_squared_error: 2.1538\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 2.3599 - mean_squared_error: 2.3599 - val_loss: 2.1472 - val_mean_squared_error: 2.1472\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 2.2854 - mean_squared_error: 2.2854 - val_loss: 1.9835 - val_mean_squared_error: 1.9835\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 2.4214 - mean_squared_error: 2.4214 - val_loss: 2.0987 - val_mean_squared_error: 2.0987\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.2267 - mean_squared_error: 2.2267 - val_loss: 1.9505 - val_mean_squared_error: 1.9505\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 2.1891 - mean_squared_error: 2.1891 - val_loss: 2.1218 - val_mean_squared_error: 2.1218\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 2.0994 - mean_squared_error: 2.0994 - val_loss: 2.2900 - val_mean_squared_error: 2.2900\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 2.1343 - mean_squared_error: 2.1343 - val_loss: 1.9685 - val_mean_squared_error: 1.9685\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 2.0921 - mean_squared_error: 2.0921 - val_loss: 1.9044 - val_mean_squared_error: 1.9044\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 2.0195 - mean_squared_error: 2.0195 - val_loss: 1.9561 - val_mean_squared_error: 1.9561\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 2.0469 - mean_squared_error: 2.0469 - val_loss: 2.4396 - val_mean_squared_error: 2.4396\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.9089 - mean_squared_error: 1.9089 - val_loss: 1.8075 - val_mean_squared_error: 1.8075\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.9438 - mean_squared_error: 1.9438 - val_loss: 2.0380 - val_mean_squared_error: 2.0380\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.9641 - mean_squared_error: 1.9641 - val_loss: 1.8972 - val_mean_squared_error: 1.8972\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.8149 - mean_squared_error: 1.8149 - val_loss: 1.9728 - val_mean_squared_error: 1.9728\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.9811 - mean_squared_error: 1.9811 - val_loss: 2.6854 - val_mean_squared_error: 2.6854\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.8247 - mean_squared_error: 1.8248 - val_loss: 1.7529 - val_mean_squared_error: 1.7529\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.8756 - mean_squared_error: 1.8756 - val_loss: 1.9952 - val_mean_squared_error: 1.9952\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.7205 - mean_squared_error: 1.7205 - val_loss: 2.1848 - val_mean_squared_error: 2.1848\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.6976 - mean_squared_error: 1.6976 - val_loss: 2.0938 - val_mean_squared_error: 2.0938\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.7157 - mean_squared_error: 1.7157 - val_loss: 2.0553 - val_mean_squared_error: 2.0553\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.7042 - mean_squared_error: 1.7042 - val_loss: 1.8310 - val_mean_squared_error: 1.8310\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.6506 - mean_squared_error: 1.6506 - val_loss: 1.8740 - val_mean_squared_error: 1.8740\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.6945 - mean_squared_error: 1.6945 - val_loss: 1.7346 - val_mean_squared_error: 1.7346\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.5484 - mean_squared_error: 1.5484 - val_loss: 2.0006 - val_mean_squared_error: 2.0006\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.5901 - mean_squared_error: 1.5901 - val_loss: 1.8754 - val_mean_squared_error: 1.8754\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.7344 - mean_squared_error: 1.7344 - val_loss: 1.7961 - val_mean_squared_error: 1.7961\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.5655 - mean_squared_error: 1.5655 - val_loss: 2.0739 - val_mean_squared_error: 2.0739\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.5689 - mean_squared_error: 1.5689 - val_loss: 1.6457 - val_mean_squared_error: 1.6457\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.5034 - mean_squared_error: 1.5034 - val_loss: 1.8409 - val_mean_squared_error: 1.8409\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.6020 - mean_squared_error: 1.6020 - val_loss: 1.7562 - val_mean_squared_error: 1.7562\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.5026 - mean_squared_error: 1.5026 - val_loss: 1.9685 - val_mean_squared_error: 1.9685\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.5269 - mean_squared_error: 1.5269 - val_loss: 1.8400 - val_mean_squared_error: 1.8400\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.4497 - mean_squared_error: 1.4497 - val_loss: 1.6737 - val_mean_squared_error: 1.6737\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.4686 - mean_squared_error: 1.4686 - val_loss: 1.7037 - val_mean_squared_error: 1.7037\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.4664 - mean_squared_error: 1.4664 - val_loss: 1.6971 - val_mean_squared_error: 1.6971\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.4512 - mean_squared_error: 1.4512 - val_loss: 1.7890 - val_mean_squared_error: 1.7890\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.4176 - mean_squared_error: 1.4176 - val_loss: 1.6882 - val_mean_squared_error: 1.6882\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.4880 - mean_squared_error: 1.4880 - val_loss: 1.6898 - val_mean_squared_error: 1.6898\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.4638 - mean_squared_error: 1.4638 - val_loss: 1.9206 - val_mean_squared_error: 1.9206\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.4667 - mean_squared_error: 1.4667 - val_loss: 1.9450 - val_mean_squared_error: 1.9450\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.3868 - mean_squared_error: 1.3868 - val_loss: 1.9832 - val_mean_squared_error: 1.9832\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.4332 - mean_squared_error: 1.4332 - val_loss: 1.5886 - val_mean_squared_error: 1.5886\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.3983 - mean_squared_error: 1.3983 - val_loss: 1.6899 - val_mean_squared_error: 1.6899\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.3199 - mean_squared_error: 1.3199 - val_loss: 1.8004 - val_mean_squared_error: 1.8004\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.3254 - mean_squared_error: 1.3254 - val_loss: 1.6511 - val_mean_squared_error: 1.6511\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.4849 - mean_squared_error: 1.4849 - val_loss: 1.9747 - val_mean_squared_error: 1.9747\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.3196 - mean_squared_error: 1.3196 - val_loss: 1.7058 - val_mean_squared_error: 1.7058\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.3178 - mean_squared_error: 1.3178 - val_loss: 1.7598 - val_mean_squared_error: 1.7598\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.2864 - mean_squared_error: 1.2864 - val_loss: 1.6917 - val_mean_squared_error: 1.6917\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.2890 - mean_squared_error: 1.2890 - val_loss: 1.9304 - val_mean_squared_error: 1.9304\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.3113 - mean_squared_error: 1.3113 - val_loss: 1.6147 - val_mean_squared_error: 1.6147\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.3384 - mean_squared_error: 1.3384 - val_loss: 1.6460 - val_mean_squared_error: 1.6460\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.3209 - mean_squared_error: 1.3209 - val_loss: 1.8709 - val_mean_squared_error: 1.8709\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 1s 385us/sample - loss: 1.3150 - mean_squared_error: 1.3150 - val_loss: 1.6069 - val_mean_squared_error: 1.6069\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 1s 386us/sample - loss: 1.3929 - mean_squared_error: 1.3929 - val_loss: 1.7801 - val_mean_squared_error: 1.7801\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1890 - mean_squared_error: 1.1890 - val_loss: 1.8147 - val_mean_squared_error: 1.8147\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.3775 - mean_squared_error: 1.3775 - val_loss: 1.6183 - val_mean_squared_error: 1.6183\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1625 - mean_squared_error: 1.1625 - val_loss: 1.6499 - val_mean_squared_error: 1.6499\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.2160 - mean_squared_error: 1.2160 - val_loss: 1.6104 - val_mean_squared_error: 1.6104\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.2085 - mean_squared_error: 1.2085 - val_loss: 1.6568 - val_mean_squared_error: 1.6568\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.3010 - mean_squared_error: 1.3010 - val_loss: 1.6315 - val_mean_squared_error: 1.6315\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.2632 - mean_squared_error: 1.2632 - val_loss: 1.7427 - val_mean_squared_error: 1.7427\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 1.1958 - mean_squared_error: 1.1958 - val_loss: 1.5431 - val_mean_squared_error: 1.5431\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.2375 - mean_squared_error: 1.2375 - val_loss: 1.8113 - val_mean_squared_error: 1.8113\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.2114 - mean_squared_error: 1.2114 - val_loss: 1.7882 - val_mean_squared_error: 1.7882\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.2230 - mean_squared_error: 1.2230 - val_loss: 1.7286 - val_mean_squared_error: 1.7286\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.1646 - mean_squared_error: 1.1646 - val_loss: 1.6315 - val_mean_squared_error: 1.6315\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.1129 - mean_squared_error: 1.1129 - val_loss: 1.6899 - val_mean_squared_error: 1.6899\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.1918 - mean_squared_error: 1.1918 - val_loss: 1.8828 - val_mean_squared_error: 1.8828\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.1905 - mean_squared_error: 1.1905 - val_loss: 1.6197 - val_mean_squared_error: 1.6197\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.2612 - mean_squared_error: 1.2612 - val_loss: 2.0361 - val_mean_squared_error: 2.0361\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.0967 - mean_squared_error: 1.0967 - val_loss: 1.7677 - val_mean_squared_error: 1.7677\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.1511 - mean_squared_error: 1.1511 - val_loss: 2.0853 - val_mean_squared_error: 2.0853\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.1144 - mean_squared_error: 1.1144 - val_loss: 1.6055 - val_mean_squared_error: 1.6055\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 1.1358 - mean_squared_error: 1.1358 - val_loss: 1.7142 - val_mean_squared_error: 1.7142\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.1317 - mean_squared_error: 1.1317 - val_loss: 1.7565 - val_mean_squared_error: 1.7565\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.1119 - mean_squared_error: 1.1119 - val_loss: 1.6041 - val_mean_squared_error: 1.6041\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.1684 - mean_squared_error: 1.1684 - val_loss: 1.7989 - val_mean_squared_error: 1.7989\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.0653 - mean_squared_error: 1.0653 - val_loss: 1.7738 - val_mean_squared_error: 1.7738\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.1359 - mean_squared_error: 1.1359 - val_loss: 1.9257 - val_mean_squared_error: 1.9257\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0998 - mean_squared_error: 1.0998 - val_loss: 1.7056 - val_mean_squared_error: 1.7056\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 1s 406us/sample - loss: 1.1296 - mean_squared_error: 1.1296 - val_loss: 2.0779 - val_mean_squared_error: 2.0779\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 1.2529 - mean_squared_error: 1.2529 - val_loss: 1.6448 - val_mean_squared_error: 1.6448\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 1s 402us/sample - loss: 1.0717 - mean_squared_error: 1.0717 - val_loss: 1.7575 - val_mean_squared_error: 1.7575\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.0737 - mean_squared_error: 1.0737 - val_loss: 1.6359 - val_mean_squared_error: 1.6359\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.0690 - mean_squared_error: 1.0690 - val_loss: 1.6204 - val_mean_squared_error: 1.6204\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.0379 - mean_squared_error: 1.0379 - val_loss: 2.5999 - val_mean_squared_error: 2.5999\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 1.0523 - mean_squared_error: 1.0523 - val_loss: 1.7212 - val_mean_squared_error: 1.7212\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.0291 - mean_squared_error: 1.0291 - val_loss: 1.6611 - val_mean_squared_error: 1.6611\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0849 - mean_squared_error: 1.0849 - val_loss: 1.5953 - val_mean_squared_error: 1.5953\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.0213 - mean_squared_error: 1.0213 - val_loss: 1.6260 - val_mean_squared_error: 1.6260\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0905 - mean_squared_error: 1.0905 - val_loss: 1.7584 - val_mean_squared_error: 1.7584\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.0985 - mean_squared_error: 1.0985 - val_loss: 1.6443 - val_mean_squared_error: 1.6443\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 1.0369 - mean_squared_error: 1.0369 - val_loss: 1.7666 - val_mean_squared_error: 1.7666\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.0490 - mean_squared_error: 1.0490 - val_loss: 1.7444 - val_mean_squared_error: 1.7444\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 1.0305 - mean_squared_error: 1.0305 - val_loss: 1.6457 - val_mean_squared_error: 1.6457\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 1.0647 - mean_squared_error: 1.0647 - val_loss: 1.7293 - val_mean_squared_error: 1.7293\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 1.0158 - mean_squared_error: 1.0158 - val_loss: 1.6977 - val_mean_squared_error: 1.6977\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9737 - mean_squared_error: 0.9737 - val_loss: 1.6160 - val_mean_squared_error: 1.6160\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.9981 - mean_squared_error: 0.9981 - val_loss: 1.6912 - val_mean_squared_error: 1.6912\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 1.0255 - mean_squared_error: 1.0255 - val_loss: 1.7398 - val_mean_squared_error: 1.7398\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 1.0149 - mean_squared_error: 1.0149 - val_loss: 1.7451 - val_mean_squared_error: 1.7451\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 1.0289 - mean_squared_error: 1.0289 - val_loss: 1.7014 - val_mean_squared_error: 1.7014\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.9725 - mean_squared_error: 0.9725 - val_loss: 1.7877 - val_mean_squared_error: 1.7877\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.9726 - mean_squared_error: 0.9726 - val_loss: 1.6400 - val_mean_squared_error: 1.6400\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.0001 - mean_squared_error: 1.0001 - val_loss: 1.5485 - val_mean_squared_error: 1.5485\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.9816 - mean_squared_error: 0.9816 - val_loss: 1.7270 - val_mean_squared_error: 1.7270\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.9860 - mean_squared_error: 0.9860 - val_loss: 1.6046 - val_mean_squared_error: 1.6046\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.0190 - mean_squared_error: 1.0190 - val_loss: 1.6005 - val_mean_squared_error: 1.6005\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 1.0212 - mean_squared_error: 1.0212 - val_loss: 1.7311 - val_mean_squared_error: 1.7311\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9782 - mean_squared_error: 0.9782 - val_loss: 1.6406 - val_mean_squared_error: 1.6406\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9501 - mean_squared_error: 0.9501 - val_loss: 1.5966 - val_mean_squared_error: 1.5966\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.9809 - mean_squared_error: 0.9809 - val_loss: 1.7350 - val_mean_squared_error: 1.7350\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.9702 - mean_squared_error: 0.9702 - val_loss: 1.6146 - val_mean_squared_error: 1.6146\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.9420 - mean_squared_error: 0.9420 - val_loss: 1.7158 - val_mean_squared_error: 1.7158\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.9376 - mean_squared_error: 0.9376 - val_loss: 1.7328 - val_mean_squared_error: 1.7328\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.9224 - mean_squared_error: 0.9224 - val_loss: 1.5739 - val_mean_squared_error: 1.5739\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 1s 387us/sample - loss: 0.9279 - mean_squared_error: 0.9279 - val_loss: 1.7619 - val_mean_squared_error: 1.7619\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 1.0180 - mean_squared_error: 1.0180 - val_loss: 1.7576 - val_mean_squared_error: 1.7576\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.9489 - mean_squared_error: 0.9489 - val_loss: 1.5962 - val_mean_squared_error: 1.5962\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9231 - mean_squared_error: 0.9231 - val_loss: 1.6188 - val_mean_squared_error: 1.6189\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9487 - mean_squared_error: 0.9487 - val_loss: 1.6442 - val_mean_squared_error: 1.6442\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9438 - mean_squared_error: 0.9438 - val_loss: 1.6826 - val_mean_squared_error: 1.6826\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.9790 - mean_squared_error: 0.9790 - val_loss: 1.6523 - val_mean_squared_error: 1.6523\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.9392 - mean_squared_error: 0.9392 - val_loss: 1.6583 - val_mean_squared_error: 1.6583\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8835 - mean_squared_error: 0.8835 - val_loss: 1.6112 - val_mean_squared_error: 1.6112\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9062 - mean_squared_error: 0.9062 - val_loss: 1.6775 - val_mean_squared_error: 1.6775\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.8925 - mean_squared_error: 0.8925 - val_loss: 1.6165 - val_mean_squared_error: 1.6165\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8702 - mean_squared_error: 0.8702 - val_loss: 1.7379 - val_mean_squared_error: 1.7379\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9239 - mean_squared_error: 0.9239 - val_loss: 1.7782 - val_mean_squared_error: 1.7782\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9749 - mean_squared_error: 0.9749 - val_loss: 1.6226 - val_mean_squared_error: 1.6226\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9159 - mean_squared_error: 0.9159 - val_loss: 1.6544 - val_mean_squared_error: 1.6544\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8742 - mean_squared_error: 0.8742 - val_loss: 1.6392 - val_mean_squared_error: 1.6392\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.9097 - mean_squared_error: 0.9097 - val_loss: 1.8307 - val_mean_squared_error: 1.8307\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.9088 - mean_squared_error: 0.9088 - val_loss: 1.6260 - val_mean_squared_error: 1.6260\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.9080 - mean_squared_error: 0.9080 - val_loss: 1.6421 - val_mean_squared_error: 1.6421\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 1s 390us/sample - loss: 0.8626 - mean_squared_error: 0.8626 - val_loss: 1.6603 - val_mean_squared_error: 1.6603\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8552 - mean_squared_error: 0.8552 - val_loss: 1.6059 - val_mean_squared_error: 1.6059\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 1s 393us/sample - loss: 0.8609 - mean_squared_error: 0.8609 - val_loss: 1.5937 - val_mean_squared_error: 1.5937\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8991 - mean_squared_error: 0.8991 - val_loss: 1.7119 - val_mean_squared_error: 1.7119\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9274 - mean_squared_error: 0.9274 - val_loss: 1.6606 - val_mean_squared_error: 1.6606\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8632 - mean_squared_error: 0.8632 - val_loss: 1.7058 - val_mean_squared_error: 1.7058\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8629 - mean_squared_error: 0.8629 - val_loss: 1.5902 - val_mean_squared_error: 1.5902\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.9161 - mean_squared_error: 0.9161 - val_loss: 1.8186 - val_mean_squared_error: 1.8186\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8882 - mean_squared_error: 0.8882 - val_loss: 1.6583 - val_mean_squared_error: 1.6583\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 1s 406us/sample - loss: 0.9176 - mean_squared_error: 0.9176 - val_loss: 1.7590 - val_mean_squared_error: 1.7590\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 1s 396us/sample - loss: 0.8912 - mean_squared_error: 0.8912 - val_loss: 1.7101 - val_mean_squared_error: 1.7101\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 1s 398us/sample - loss: 0.8742 - mean_squared_error: 0.8742 - val_loss: 1.8685 - val_mean_squared_error: 1.8685\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 1s 400us/sample - loss: 0.8615 - mean_squared_error: 0.8615 - val_loss: 1.5933 - val_mean_squared_error: 1.5933\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.8193 - mean_squared_error: 0.8193 - val_loss: 1.6387 - val_mean_squared_error: 1.6387\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.8839 - mean_squared_error: 0.8839 - val_loss: 1.6154 - val_mean_squared_error: 1.6154\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 1s 399us/sample - loss: 0.8383 - mean_squared_error: 0.8383 - val_loss: 1.6469 - val_mean_squared_error: 1.6469\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8420 - mean_squared_error: 0.8420 - val_loss: 1.7093 - val_mean_squared_error: 1.7093\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 1s 397us/sample - loss: 0.8973 - mean_squared_error: 0.8973 - val_loss: 1.5820 - val_mean_squared_error: 1.5820\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 1s 392us/sample - loss: 0.8281 - mean_squared_error: 0.8281 - val_loss: 1.5854 - val_mean_squared_error: 1.5854\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8359 - mean_squared_error: 0.8359 - val_loss: 1.7653 - val_mean_squared_error: 1.7653\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 1s 388us/sample - loss: 0.8548 - mean_squared_error: 0.8548 - val_loss: 1.6032 - val_mean_squared_error: 1.6032\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8243 - mean_squared_error: 0.8243 - val_loss: 1.8416 - val_mean_squared_error: 1.8416\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 1s 394us/sample - loss: 0.8880 - mean_squared_error: 0.8880 - val_loss: 1.6708 - val_mean_squared_error: 1.6708\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 1s 391us/sample - loss: 0.8477 - mean_squared_error: 0.8477 - val_loss: 1.5756 - val_mean_squared_error: 1.5756\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 1s 395us/sample - loss: 0.8354 - mean_squared_error: 0.8354 - val_loss: 1.6074 - val_mean_squared_error: 1.6074\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 1s 389us/sample - loss: 0.8282 - mean_squared_error: 0.8282 - val_loss: 1.7118 - val_mean_squared_error: 1.7118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yfMbGZhXTrL",
        "colab_type": "text"
      },
      "source": [
        "## VGG Net Inspired Models\n",
        "\n",
        "As discussed in this [Analytics Vidhya blogpost](https://www.analyticsvidhya.com/blog/2017/08/10-advanced-deep-learning-architectures-data-scientists/), the VGGNet was developed by researchers of the Visual Graphics Group at Oxford University. This model is characteristically different from the AlexNet inspired models the team had been using due to its stacked convolution layers prior to pooling. This gives the VGG Net models additional depth (and additional parameters to train). As the VGG Nets themselves start with different input sizes than we are working with for this task, the team simply used the VGG Net architecture as inspiration for their own deeper network. The network model iteself is coded below. \n",
        "\n",
        "As can be seen through inspection of the code cell the strategy employed looked like the following:\n",
        "\n",
        "* Convolute Input with 3x3 kernel and Same Padding and User Specified Filter Depth (sf) ) - Output Shape (96x96xsf)\n",
        "* Convolute with 3x3 kernel and Same Padding and User Specified Filter Depth (sf) ) - Output Shape (96x96xsf)\n",
        "* Max Pool with 2x2 Kernel  - Output Shape (48x48xsf)\n",
        "* Convolute with 3x3 kernel and Same Padding and double filter depth - Output Shape (48x48x(2x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (48x48x(2x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (48x48x(2x sf))\n",
        "* Max Pool with 2x2 Kernel  - Output Shape (24 x 24 x (2xsf) )\n",
        "* Convolute with 3x3 kernel and Same Padding and double filter depth - Output Shape (24x24x(4x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (24x24x(4x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (24x24x(4x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (24x24x(4x sf))\n",
        "* Max Pool with 2x2 Kernel  - Output Shape (12 x 12 x (4xsf) )\n",
        "* Convolute with 3x3 kernel and Same Padding and double filter depth - Output Shape (12x12x(8x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (12x12x(8x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (12x12x(8x sf))\n",
        "* Convolute with 3x3 kernel and Same Padding and maintain filter depth - Output Shape (12x12x(8x sf))\n",
        "* Max Pool with 2x2 Kernel  - Output Shape (6 x 6 x (8xsf) )\n",
        "* Dense Layer with 500 Hidden Units\n",
        "* Dense Layer with 500 Hidden Units\n",
        "* Output Layer with 30 outputs\n",
        "\n",
        "It was obviously expected that the additional layers would add to model runtime, but the big question was how would it perform? This question is answered by the plot below. The VGG net in the end performed similarly to the AlexNet version, in terms of overall validation set RMSE. However, as one can tell from the plots below, the RMSE and the epoch training time was extremely variable. It was unclear what this performance issue was due to. Even without the spikes in epoch run time the median epoch training time for the VGG Net is approximately twice that of the AlexNet architecture. A few variations on the VGG Net architecture were pursued, but none should significantly better performance than the AlexNet model. For this reason, additional VGG Net studies were foregone.\n",
        "\n",
        "![alt text](https://github.com/tomgoter/w207_finalproject/raw/master/Images/vgg_performance_V2.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PVltysLKpme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_vgg_model(start_filter, d, step, bias):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model \n",
        "    '''\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "    \n",
        "    # Input layer is our grayscale image that is 96 pixels by 96 pixels\n",
        "    cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    \n",
        "    # Add our first convolution layers which is two back-to-back conv with 3x3 kernel and same padding\n",
        "    # Add depth with filters\n",
        "    # Our output from these convolutions will be (96,96,start_filter)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (48,48,32)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add some random dropout\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    \n",
        "    # Add our second convolution layers which is three back-to-back conv with 3x3 kernel and same padding\n",
        "    # Double filter depth - output layer will be (48,48,start_filter*2)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "    \n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (24,24,start_filter*2)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add some random dropout\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    \n",
        "    # Add our third convolution layers which is three back-to-back conv with 3x3 kernel and same padding\n",
        "    # Double filter depth again - output layer will be (24,24,start_filter*4)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "    \n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (12,12,start_filter*4)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Add some random dropout\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+2*step))\n",
        "    \n",
        "    # Add our fourth and final convolution layers which is three back-to-back conv with 3x3 kernel and same padding\n",
        "    # Double filter depth again - output layer will be (12,12,start_filter*8)\n",
        "    if bias:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), padding='same', activation='relu'))\n",
        "    else:\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        cnn_model.add(tf.keras.layers.Conv2D(start_filter*8, (3, 3), use_bias=False, \n",
        "                                             padding='same', activation='relu'))\n",
        "        \n",
        "    # Normalize our features\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Use a max pooling layer to cut our features in half\n",
        "    # Output size will be (6,6,start_filter*8)\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    # Flatten and transition to fully connected layers\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(500))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(500))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(30))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdJY734byccZ",
        "colab_type": "code",
        "outputId": "d1f86772-9ea8-4f0f-dccb-e489d4d3476b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Redefine optimizer list to just focus on adam and sgd\n",
        "opt_list = {'adam':adam}\n",
        "\n",
        "# Use an early stopping callback and our timing callback\n",
        "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                              patience=100, mode='auto')\n",
        "time_callback = TimeHistory()\n",
        "\n",
        "# Initialize a new data frame to hold our output data\n",
        "cnn_vgg_flipped_df = pd.DataFrame()\n",
        "\n",
        "# Start filter list\n",
        "start_filters = [16]\n",
        "\n",
        "# Flag for using or not using bias term\n",
        "biases = [False]\n",
        "\n",
        "# Create a list of initial dropout values and steps to increase\n",
        "dropouts = [(0.0,0.02), (0.0,0.05)]\n",
        "\n",
        "\n",
        "for lr_factor in [10]:\n",
        "  for opt_name, opt in opt_list.items():\n",
        "      for start_filter in start_filters:\n",
        "          for bias in biases:\n",
        "              for d in dropouts:\n",
        "                  adam = optimizers.Adam(lr=0.001*lr_factor, beta_1=0.9, beta_2=0.999)\n",
        "                  model = create_vgg_model(start_filter, d[0], d[1], bias)\n",
        "                  model.compile(\n",
        "                        optimizer=opt,\n",
        "                        loss='mean_squared_error',\n",
        "                        metrics=['mean_squared_error'])\n",
        "                  history = model.fit(\n",
        "                      flipped_X.astype(np.float32), flipped_y.astype(np.float32),\n",
        "                      epochs=200,\n",
        "                      validation_split=0.15, callbacks=[time_callback, early_stop])\n",
        "                  times = time_callback.times\n",
        "\n",
        "                  # Convert to dataframe\n",
        "                  hist = pd.DataFrame(history.history)\n",
        "                  hist['epoch'] = history.epoch\n",
        "                  hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "                  hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "                  hist['times'] = times\n",
        "                  hist['starting_filter'] = start_filter\n",
        "                  hist['layers'] = 4\n",
        "                  hist['pooling'] = 'yes'\n",
        "                  hist['fc_layer'] = 500\n",
        "                  hist['activation'] = 'relu'\n",
        "                  hist['optimizer'] = opt_name\n",
        "                  hist['lrate'] = opt.get_config()['learning_rate']\n",
        "                  hist['dropout_initial'] = d[0]\n",
        "                  hist['dropout_step'] = d[1]\n",
        "                  hist['batch_norm'] = 1\n",
        "                  hist['bias'] = int(bias)\n",
        "                  hist['arch'] = 'vgg'\n",
        "\n",
        "                  # Keep concatenating to dataframe\n",
        "                  cnn_vgg_flipped_df = pd.concat([cnn_vgg_flipped_df,hist])\n",
        "\n",
        "                  # Re-pickle after every model to retain progress\n",
        "                  cnn_vgg_flipped_df.to_pickle(drive_path + \"OutputData/cnn_vgg_flipped4_df.pkl\")\n",
        "\n",
        "                  # Save models.\n",
        "                  filename = \"cnn_vgg_flipped4_model_{}_d{}_s{}_sf{}\".format(opt_name, d[0], d[1], start_filter)\n",
        "                  model.save(drive_path + \"Models/\"+filename+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0725 22:08:00.813345 140301373831040 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 96, 96, 16)        144       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 96, 96, 16)        2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 96, 96, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 48, 48, 32)        4608      \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 24, 24, 64)        18432     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 12, 12, 128)       73728     \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               2304500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,245,598\n",
            "Trainable params: 3,243,118\n",
            "Non-trainable params: 2,480\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 7s 2ms/sample - loss: 1503.0691 - mean_squared_error: 1503.0686 - val_loss: 902.9318 - val_mean_squared_error: 902.9318\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 3s 785us/sample - loss: 231.0996 - mean_squared_error: 231.0995 - val_loss: 77.6797 - val_mean_squared_error: 77.6797\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 3s 791us/sample - loss: 32.0186 - mean_squared_error: 32.0186 - val_loss: 24.6080 - val_mean_squared_error: 24.6080\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 3s 794us/sample - loss: 20.9619 - mean_squared_error: 20.9619 - val_loss: 28.6376 - val_mean_squared_error: 28.6376\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 3s 795us/sample - loss: 15.5489 - mean_squared_error: 15.5489 - val_loss: 14.0135 - val_mean_squared_error: 14.0135\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 3s 792us/sample - loss: 15.2131 - mean_squared_error: 15.2131 - val_loss: 14.5299 - val_mean_squared_error: 14.5299\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 3s 796us/sample - loss: 13.0577 - mean_squared_error: 13.0577 - val_loss: 12.9371 - val_mean_squared_error: 12.9371\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 3s 796us/sample - loss: 12.2642 - mean_squared_error: 12.2642 - val_loss: 12.6177 - val_mean_squared_error: 12.6177\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 3s 800us/sample - loss: 11.5952 - mean_squared_error: 11.5952 - val_loss: 10.3358 - val_mean_squared_error: 10.3358\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 3s 801us/sample - loss: 11.3017 - mean_squared_error: 11.3017 - val_loss: 12.2963 - val_mean_squared_error: 12.2963\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 3s 803us/sample - loss: 10.9783 - mean_squared_error: 10.9783 - val_loss: 10.7083 - val_mean_squared_error: 10.7083\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 10.7583 - mean_squared_error: 10.7583 - val_loss: 10.8048 - val_mean_squared_error: 10.8048\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 10.6175 - mean_squared_error: 10.6175 - val_loss: 11.4092 - val_mean_squared_error: 11.4092\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 10.5943 - mean_squared_error: 10.5943 - val_loss: 10.4830 - val_mean_squared_error: 10.4830\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 10.5227 - mean_squared_error: 10.5227 - val_loss: 13.9042 - val_mean_squared_error: 13.9042\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 10.2316 - mean_squared_error: 10.2316 - val_loss: 9.6822 - val_mean_squared_error: 9.6822\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 10.2786 - mean_squared_error: 10.2786 - val_loss: 10.4266 - val_mean_squared_error: 10.4266\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 10.3456 - mean_squared_error: 10.3456 - val_loss: 9.7416 - val_mean_squared_error: 9.7416\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 10.1203 - mean_squared_error: 10.1202 - val_loss: 11.4136 - val_mean_squared_error: 11.4136\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 9.8893 - mean_squared_error: 9.8893 - val_loss: 9.9292 - val_mean_squared_error: 9.9292\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 9.6789 - mean_squared_error: 9.6789 - val_loss: 9.6395 - val_mean_squared_error: 9.6395\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 3s 822us/sample - loss: 9.5863 - mean_squared_error: 9.5863 - val_loss: 14.9666 - val_mean_squared_error: 14.9666\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 3s 822us/sample - loss: 9.6267 - mean_squared_error: 9.6267 - val_loss: 9.9612 - val_mean_squared_error: 9.9612\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 9.3602 - mean_squared_error: 9.3602 - val_loss: 10.5398 - val_mean_squared_error: 10.5398\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 8.9657 - mean_squared_error: 8.9657 - val_loss: 12.1236 - val_mean_squared_error: 12.1236\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 8.3030 - mean_squared_error: 8.3030 - val_loss: 11.4900 - val_mean_squared_error: 11.4900\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 8.0048 - mean_squared_error: 8.0048 - val_loss: 7.7885 - val_mean_squared_error: 7.7885\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 7.6340 - mean_squared_error: 7.6340 - val_loss: 10.2349 - val_mean_squared_error: 10.2349\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 7.4121 - mean_squared_error: 7.4121 - val_loss: 8.5402 - val_mean_squared_error: 8.5402\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 6.7272 - mean_squared_error: 6.7272 - val_loss: 6.4859 - val_mean_squared_error: 6.4859\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 6.2125 - mean_squared_error: 6.2125 - val_loss: 9.7414 - val_mean_squared_error: 9.7414\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 6.2266 - mean_squared_error: 6.2266 - val_loss: 12.3595 - val_mean_squared_error: 12.3595\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 5.8923 - mean_squared_error: 5.8923 - val_loss: 5.5178 - val_mean_squared_error: 5.5178\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 5.2831 - mean_squared_error: 5.2831 - val_loss: 5.3895 - val_mean_squared_error: 5.3895\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 5.2139 - mean_squared_error: 5.2139 - val_loss: 5.0656 - val_mean_squared_error: 5.0656\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 4.9461 - mean_squared_error: 4.9461 - val_loss: 5.7699 - val_mean_squared_error: 5.7699\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 3s 802us/sample - loss: 4.3066 - mean_squared_error: 4.3066 - val_loss: 4.4356 - val_mean_squared_error: 4.4356\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 3s 802us/sample - loss: 4.1310 - mean_squared_error: 4.1310 - val_loss: 3.9020 - val_mean_squared_error: 3.9020\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 4.0614 - mean_squared_error: 4.0614 - val_loss: 4.0883 - val_mean_squared_error: 4.0883\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 4.1426 - mean_squared_error: 4.1426 - val_loss: 5.2771 - val_mean_squared_error: 5.2771\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 3.8258 - mean_squared_error: 3.8258 - val_loss: 3.9562 - val_mean_squared_error: 3.9562\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 3.7033 - mean_squared_error: 3.7033 - val_loss: 4.7001 - val_mean_squared_error: 4.7001\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 4.1275 - mean_squared_error: 4.1275 - val_loss: 4.0003 - val_mean_squared_error: 4.0003\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 3s 805us/sample - loss: 3.3979 - mean_squared_error: 3.3979 - val_loss: 3.2287 - val_mean_squared_error: 3.2287\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 3.2528 - mean_squared_error: 3.2528 - val_loss: 3.9348 - val_mean_squared_error: 3.9348\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 3.3443 - mean_squared_error: 3.3443 - val_loss: 3.3301 - val_mean_squared_error: 3.3301\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 3.2545 - mean_squared_error: 3.2545 - val_loss: 3.5569 - val_mean_squared_error: 3.5569\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 3.0308 - mean_squared_error: 3.0308 - val_loss: 3.0240 - val_mean_squared_error: 3.0240\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 2.8805 - mean_squared_error: 2.8805 - val_loss: 3.1963 - val_mean_squared_error: 3.1963\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 2.7823 - mean_squared_error: 2.7823 - val_loss: 3.0328 - val_mean_squared_error: 3.0328\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 3s 827us/sample - loss: 2.9146 - mean_squared_error: 2.9146 - val_loss: 3.4589 - val_mean_squared_error: 3.4589\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 2.7606 - mean_squared_error: 2.7606 - val_loss: 3.3263 - val_mean_squared_error: 3.3263\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.6750 - mean_squared_error: 2.6750 - val_loss: 2.7002 - val_mean_squared_error: 2.7002\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 2.6626 - mean_squared_error: 2.6626 - val_loss: 2.4706 - val_mean_squared_error: 2.4706\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 2.6441 - mean_squared_error: 2.6441 - val_loss: 4.1419 - val_mean_squared_error: 4.1419\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 2.7798 - mean_squared_error: 2.7798 - val_loss: 2.8001 - val_mean_squared_error: 2.8001\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.5343 - mean_squared_error: 2.5343 - val_loss: 2.7543 - val_mean_squared_error: 2.7543\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 2.5694 - mean_squared_error: 2.5694 - val_loss: 2.6659 - val_mean_squared_error: 2.6659\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.5152 - mean_squared_error: 2.5152 - val_loss: 2.7019 - val_mean_squared_error: 2.7019\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 2.4735 - mean_squared_error: 2.4735 - val_loss: 2.7386 - val_mean_squared_error: 2.7386\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.3519 - mean_squared_error: 2.3519 - val_loss: 2.4482 - val_mean_squared_error: 2.4482\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 2.4132 - mean_squared_error: 2.4132 - val_loss: 2.7022 - val_mean_squared_error: 2.7022\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 2.2927 - mean_squared_error: 2.2927 - val_loss: 2.4846 - val_mean_squared_error: 2.4846\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 2.4958 - mean_squared_error: 2.4958 - val_loss: 3.6533 - val_mean_squared_error: 3.6533\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 2.4997 - mean_squared_error: 2.4997 - val_loss: 2.4060 - val_mean_squared_error: 2.4060\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 2.3777 - mean_squared_error: 2.3777 - val_loss: 4.7044 - val_mean_squared_error: 4.7044\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.3814 - mean_squared_error: 2.3814 - val_loss: 2.6793 - val_mean_squared_error: 2.6793\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.2522 - mean_squared_error: 2.2522 - val_loss: 2.9455 - val_mean_squared_error: 2.9455\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 2.1121 - mean_squared_error: 2.1121 - val_loss: 2.6455 - val_mean_squared_error: 2.6455\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.1318 - mean_squared_error: 2.1318 - val_loss: 2.2845 - val_mean_squared_error: 2.2845\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 2.1261 - mean_squared_error: 2.1261 - val_loss: 2.4760 - val_mean_squared_error: 2.4760\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 2.1347 - mean_squared_error: 2.1347 - val_loss: 2.5176 - val_mean_squared_error: 2.5176\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.9396 - mean_squared_error: 1.9396 - val_loss: 2.2484 - val_mean_squared_error: 2.2484\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.9848 - mean_squared_error: 1.9848 - val_loss: 2.0955 - val_mean_squared_error: 2.0955\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.8994 - mean_squared_error: 1.8994 - val_loss: 2.2606 - val_mean_squared_error: 2.2606\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.9766 - mean_squared_error: 1.9766 - val_loss: 2.6509 - val_mean_squared_error: 2.6509\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.8706 - mean_squared_error: 1.8706 - val_loss: 2.3129 - val_mean_squared_error: 2.3129\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.9293 - mean_squared_error: 1.9293 - val_loss: 2.5570 - val_mean_squared_error: 2.5570\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 2.0685 - mean_squared_error: 2.0685 - val_loss: 2.5901 - val_mean_squared_error: 2.5901\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.8957 - mean_squared_error: 1.8957 - val_loss: 2.1503 - val_mean_squared_error: 2.1503\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 1.8593 - mean_squared_error: 1.8593 - val_loss: 2.1262 - val_mean_squared_error: 2.1262\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.8238 - mean_squared_error: 1.8238 - val_loss: 2.2783 - val_mean_squared_error: 2.2783\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.8163 - mean_squared_error: 1.8163 - val_loss: 2.3301 - val_mean_squared_error: 2.3301\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.8169 - mean_squared_error: 1.8169 - val_loss: 2.0574 - val_mean_squared_error: 2.0574\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.8313 - mean_squared_error: 1.8313 - val_loss: 2.3836 - val_mean_squared_error: 2.3836\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.7657 - mean_squared_error: 1.7657 - val_loss: 2.4654 - val_mean_squared_error: 2.4654\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.7929 - mean_squared_error: 1.7929 - val_loss: 2.0292 - val_mean_squared_error: 2.0292\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.7475 - mean_squared_error: 1.7475 - val_loss: 2.3269 - val_mean_squared_error: 2.3269\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.7887 - mean_squared_error: 1.7887 - val_loss: 2.2048 - val_mean_squared_error: 2.2048\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.7190 - mean_squared_error: 1.7190 - val_loss: 1.8778 - val_mean_squared_error: 1.8778\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.7100 - mean_squared_error: 1.7100 - val_loss: 2.2725 - val_mean_squared_error: 2.2725\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 2.5097 - mean_squared_error: 2.5097 - val_loss: 2.7079 - val_mean_squared_error: 2.7079\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.7623 - mean_squared_error: 1.7623 - val_loss: 2.1005 - val_mean_squared_error: 2.1005\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.6673 - mean_squared_error: 1.6673 - val_loss: 1.9159 - val_mean_squared_error: 1.9159\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 1.6941 - mean_squared_error: 1.6941 - val_loss: 2.0220 - val_mean_squared_error: 2.0220\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.6115 - mean_squared_error: 1.6115 - val_loss: 1.8630 - val_mean_squared_error: 1.8630\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.5463 - mean_squared_error: 1.5463 - val_loss: 1.9573 - val_mean_squared_error: 1.9573\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.5950 - mean_squared_error: 1.5950 - val_loss: 1.8909 - val_mean_squared_error: 1.8909\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.5851 - mean_squared_error: 1.5851 - val_loss: 2.1106 - val_mean_squared_error: 2.1106\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.5458 - mean_squared_error: 1.5458 - val_loss: 1.8748 - val_mean_squared_error: 1.8748\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.6409 - mean_squared_error: 1.6409 - val_loss: 2.2934 - val_mean_squared_error: 2.2934\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.5652 - mean_squared_error: 1.5652 - val_loss: 1.9642 - val_mean_squared_error: 1.9642\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.6893 - mean_squared_error: 1.6893 - val_loss: 2.2211 - val_mean_squared_error: 2.2211\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.5597 - mean_squared_error: 1.5597 - val_loss: 1.8295 - val_mean_squared_error: 1.8295\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.4955 - mean_squared_error: 1.4955 - val_loss: 1.9565 - val_mean_squared_error: 1.9565\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.5962 - mean_squared_error: 1.5962 - val_loss: 2.1047 - val_mean_squared_error: 2.1047\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.5077 - mean_squared_error: 1.5077 - val_loss: 1.9444 - val_mean_squared_error: 1.9444\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.4053 - mean_squared_error: 1.4053 - val_loss: 1.9084 - val_mean_squared_error: 1.9084\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.5548 - mean_squared_error: 1.5548 - val_loss: 2.1765 - val_mean_squared_error: 2.1765\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.5425 - mean_squared_error: 1.5425 - val_loss: 2.0048 - val_mean_squared_error: 2.0048\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.4078 - mean_squared_error: 1.4078 - val_loss: 1.9682 - val_mean_squared_error: 1.9682\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.4739 - mean_squared_error: 1.4739 - val_loss: 1.8651 - val_mean_squared_error: 1.8651\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.3717 - mean_squared_error: 1.3717 - val_loss: 1.9898 - val_mean_squared_error: 1.9898\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.3974 - mean_squared_error: 1.3974 - val_loss: 1.9178 - val_mean_squared_error: 1.9178\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.4548 - mean_squared_error: 1.4548 - val_loss: 2.1050 - val_mean_squared_error: 2.1050\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.4541 - mean_squared_error: 1.4541 - val_loss: 2.1339 - val_mean_squared_error: 2.1339\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.3923 - mean_squared_error: 1.3923 - val_loss: 2.0165 - val_mean_squared_error: 2.0165\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.4403 - mean_squared_error: 1.4403 - val_loss: 1.8839 - val_mean_squared_error: 1.8839\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.2914 - mean_squared_error: 1.2914 - val_loss: 1.8169 - val_mean_squared_error: 1.8169\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.3645 - mean_squared_error: 1.3645 - val_loss: 2.0411 - val_mean_squared_error: 2.0411\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.3972 - mean_squared_error: 1.3972 - val_loss: 1.8647 - val_mean_squared_error: 1.8647\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.4032 - mean_squared_error: 1.4032 - val_loss: 2.1487 - val_mean_squared_error: 2.1487\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.3643 - mean_squared_error: 1.3643 - val_loss: 1.9899 - val_mean_squared_error: 1.9899\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.3461 - mean_squared_error: 1.3461 - val_loss: 1.8254 - val_mean_squared_error: 1.8254\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.3433 - mean_squared_error: 1.3433 - val_loss: 1.8368 - val_mean_squared_error: 1.8368\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.2782 - mean_squared_error: 1.2782 - val_loss: 1.9814 - val_mean_squared_error: 1.9814\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.2820 - mean_squared_error: 1.2820 - val_loss: 1.8315 - val_mean_squared_error: 1.8315\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.2815 - mean_squared_error: 1.2815 - val_loss: 1.8975 - val_mean_squared_error: 1.8975\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.2823 - mean_squared_error: 1.2823 - val_loss: 1.9023 - val_mean_squared_error: 1.9023\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.3234 - mean_squared_error: 1.3234 - val_loss: 1.9524 - val_mean_squared_error: 1.9524\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.3512 - mean_squared_error: 1.3512 - val_loss: 2.1729 - val_mean_squared_error: 2.1729\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.3333 - mean_squared_error: 1.3333 - val_loss: 1.8951 - val_mean_squared_error: 1.8951\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.3042 - mean_squared_error: 1.3042 - val_loss: 2.4911 - val_mean_squared_error: 2.4911\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.3043 - mean_squared_error: 1.3043 - val_loss: 1.9161 - val_mean_squared_error: 1.9161\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.2298 - mean_squared_error: 1.2298 - val_loss: 1.9100 - val_mean_squared_error: 1.9100\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 1.3306 - mean_squared_error: 1.3306 - val_loss: 1.8345 - val_mean_squared_error: 1.8345\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.3490 - mean_squared_error: 1.3490 - val_loss: 1.9363 - val_mean_squared_error: 1.9363\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 1.2960 - mean_squared_error: 1.2960 - val_loss: 1.7568 - val_mean_squared_error: 1.7568\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.2330 - mean_squared_error: 1.2330 - val_loss: 1.7237 - val_mean_squared_error: 1.7237\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.2602 - mean_squared_error: 1.2602 - val_loss: 1.7288 - val_mean_squared_error: 1.7288\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.2043 - mean_squared_error: 1.2043 - val_loss: 2.1854 - val_mean_squared_error: 2.1854\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.2420 - mean_squared_error: 1.2420 - val_loss: 1.8229 - val_mean_squared_error: 1.8229\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.2355 - mean_squared_error: 1.2355 - val_loss: 1.8637 - val_mean_squared_error: 1.8637\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.1862 - mean_squared_error: 1.1862 - val_loss: 1.6333 - val_mean_squared_error: 1.6333\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.2561 - mean_squared_error: 1.2561 - val_loss: 1.8678 - val_mean_squared_error: 1.8678\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 1.1514 - mean_squared_error: 1.1514 - val_loss: 1.7591 - val_mean_squared_error: 1.7591\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.1946 - mean_squared_error: 1.1946 - val_loss: 1.7072 - val_mean_squared_error: 1.7072\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.1521 - mean_squared_error: 1.1521 - val_loss: 1.9323 - val_mean_squared_error: 1.9323\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.2263 - mean_squared_error: 1.2263 - val_loss: 1.9031 - val_mean_squared_error: 1.9031\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.2521 - mean_squared_error: 1.2521 - val_loss: 2.3134 - val_mean_squared_error: 2.3134\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.1752 - mean_squared_error: 1.1752 - val_loss: 1.7707 - val_mean_squared_error: 1.7707\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.1611 - mean_squared_error: 1.1611 - val_loss: 1.8069 - val_mean_squared_error: 1.8069\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.1111 - mean_squared_error: 1.1111 - val_loss: 1.7490 - val_mean_squared_error: 1.7490\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.1238 - mean_squared_error: 1.1238 - val_loss: 2.1677 - val_mean_squared_error: 2.1677\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.0931 - mean_squared_error: 1.0931 - val_loss: 1.8050 - val_mean_squared_error: 1.8050\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 3s 823us/sample - loss: 1.1622 - mean_squared_error: 1.1622 - val_loss: 1.8085 - val_mean_squared_error: 1.8085\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 3s 824us/sample - loss: 1.1549 - mean_squared_error: 1.1549 - val_loss: 1.8930 - val_mean_squared_error: 1.8930\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 1.1057 - mean_squared_error: 1.1057 - val_loss: 1.6695 - val_mean_squared_error: 1.6695\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.1699 - mean_squared_error: 1.1699 - val_loss: 1.9364 - val_mean_squared_error: 1.9364\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.1191 - mean_squared_error: 1.1191 - val_loss: 1.9345 - val_mean_squared_error: 1.9345\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.0893 - mean_squared_error: 1.0893 - val_loss: 1.7383 - val_mean_squared_error: 1.7383\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.1360 - mean_squared_error: 1.1360 - val_loss: 1.8963 - val_mean_squared_error: 1.8963\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.1666 - mean_squared_error: 1.1666 - val_loss: 2.0031 - val_mean_squared_error: 2.0031\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.1376 - mean_squared_error: 1.1376 - val_loss: 1.6715 - val_mean_squared_error: 1.6715\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.1099 - mean_squared_error: 1.1099 - val_loss: 1.7668 - val_mean_squared_error: 1.7668\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.0868 - mean_squared_error: 1.0868 - val_loss: 1.8729 - val_mean_squared_error: 1.8729\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.0355 - mean_squared_error: 1.0355 - val_loss: 1.7582 - val_mean_squared_error: 1.7582\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.0847 - mean_squared_error: 1.0847 - val_loss: 1.6786 - val_mean_squared_error: 1.6786\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.0650 - mean_squared_error: 1.0650 - val_loss: 1.6590 - val_mean_squared_error: 1.6590\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.0740 - mean_squared_error: 1.0740 - val_loss: 1.7560 - val_mean_squared_error: 1.7560\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.0690 - mean_squared_error: 1.0690 - val_loss: 1.7621 - val_mean_squared_error: 1.7621\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.0904 - mean_squared_error: 1.0904 - val_loss: 1.6967 - val_mean_squared_error: 1.6967\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.1241 - mean_squared_error: 1.1241 - val_loss: 1.7269 - val_mean_squared_error: 1.7269\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.0216 - mean_squared_error: 1.0216 - val_loss: 1.6187 - val_mean_squared_error: 1.6187\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.0451 - mean_squared_error: 1.0451 - val_loss: 1.8779 - val_mean_squared_error: 1.8779\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.0143 - mean_squared_error: 1.0143 - val_loss: 1.8412 - val_mean_squared_error: 1.8412\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.1191 - mean_squared_error: 1.1191 - val_loss: 1.7714 - val_mean_squared_error: 1.7714\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 1.1152 - mean_squared_error: 1.1152 - val_loss: 2.0291 - val_mean_squared_error: 2.0291\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.0178 - mean_squared_error: 1.0178 - val_loss: 1.9098 - val_mean_squared_error: 1.9098\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.0165 - mean_squared_error: 1.0165 - val_loss: 1.7931 - val_mean_squared_error: 1.7931\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.0292 - mean_squared_error: 1.0292 - val_loss: 1.7716 - val_mean_squared_error: 1.7716\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.0232 - mean_squared_error: 1.0232 - val_loss: 1.7122 - val_mean_squared_error: 1.7122\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.0354 - mean_squared_error: 1.0354 - val_loss: 1.8117 - val_mean_squared_error: 1.8117\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.0015 - mean_squared_error: 1.0015 - val_loss: 2.1174 - val_mean_squared_error: 2.1174\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 0.9996 - mean_squared_error: 0.9996 - val_loss: 1.7951 - val_mean_squared_error: 1.7951\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.0205 - mean_squared_error: 1.0205 - val_loss: 1.6512 - val_mean_squared_error: 1.6512\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.0227 - mean_squared_error: 1.0227 - val_loss: 1.6900 - val_mean_squared_error: 1.6900\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.0275 - mean_squared_error: 1.0275 - val_loss: 1.7239 - val_mean_squared_error: 1.7239\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 0.9872 - mean_squared_error: 0.9872 - val_loss: 1.6981 - val_mean_squared_error: 1.6981\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.0155 - mean_squared_error: 1.0155 - val_loss: 1.8362 - val_mean_squared_error: 1.8362\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.0740 - mean_squared_error: 1.0740 - val_loss: 2.1267 - val_mean_squared_error: 2.1267\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 0.9954 - mean_squared_error: 0.9954 - val_loss: 1.8639 - val_mean_squared_error: 1.8639\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 3s 807us/sample - loss: 0.9363 - mean_squared_error: 0.9363 - val_loss: 1.8845 - val_mean_squared_error: 1.8845\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 0.9662 - mean_squared_error: 0.9662 - val_loss: 1.7055 - val_mean_squared_error: 1.7055\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 0.9716 - mean_squared_error: 0.9716 - val_loss: 1.9042 - val_mean_squared_error: 1.9042\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 0.9432 - mean_squared_error: 0.9432 - val_loss: 1.8178 - val_mean_squared_error: 1.8178\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 0.9161 - mean_squared_error: 0.9161 - val_loss: 1.6802 - val_mean_squared_error: 1.6802\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.0386 - mean_squared_error: 1.0386 - val_loss: 1.7276 - val_mean_squared_error: 1.7276\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.0305 - mean_squared_error: 1.0305 - val_loss: 1.6825 - val_mean_squared_error: 1.6825\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 0.9307 - mean_squared_error: 0.9307 - val_loss: 1.8480 - val_mean_squared_error: 1.8480\n",
            "==================================================\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 96, 96, 16)        144       \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 96, 96, 16)        2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 96, 96, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 48, 48, 32)        4608      \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 48, 48, 32)        9216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 24, 24, 64)        18432     \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 24, 24, 64)        36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 12, 12, 128)       73728     \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 12, 12, 128)       147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 500)               2304500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 30)                15030     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 30)                0         \n",
            "=================================================================\n",
            "Total params: 3,245,598\n",
            "Trainable params: 3,243,118\n",
            "Non-trainable params: 2,480\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Train on 3638 samples, validate on 642 samples\n",
            "Epoch 1/200\n",
            "3638/3638 [==============================] - 4s 1ms/sample - loss: 359.6200 - mean_squared_error: 359.6201 - val_loss: 7973.7003 - val_mean_squared_error: 7973.7012\n",
            "Epoch 2/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 23.8004 - mean_squared_error: 23.8004 - val_loss: 25.0413 - val_mean_squared_error: 25.0413\n",
            "Epoch 3/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 20.6984 - mean_squared_error: 20.6984 - val_loss: 76.9191 - val_mean_squared_error: 76.9191\n",
            "Epoch 4/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 17.3981 - mean_squared_error: 17.3981 - val_loss: 156.9874 - val_mean_squared_error: 156.9875\n",
            "Epoch 5/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 17.6791 - mean_squared_error: 17.6791 - val_loss: 13.6750 - val_mean_squared_error: 13.6750\n",
            "Epoch 6/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 16.5321 - mean_squared_error: 16.5321 - val_loss: 11.9649 - val_mean_squared_error: 11.9649\n",
            "Epoch 7/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 15.7400 - mean_squared_error: 15.7400 - val_loss: 10.9494 - val_mean_squared_error: 10.9495\n",
            "Epoch 8/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 15.4877 - mean_squared_error: 15.4877 - val_loss: 10.1268 - val_mean_squared_error: 10.1268\n",
            "Epoch 9/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 15.8821 - mean_squared_error: 15.8821 - val_loss: 12.1346 - val_mean_squared_error: 12.1346\n",
            "Epoch 10/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 14.1005 - mean_squared_error: 14.1005 - val_loss: 10.1803 - val_mean_squared_error: 10.1803\n",
            "Epoch 11/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 13.2821 - mean_squared_error: 13.2821 - val_loss: 11.5947 - val_mean_squared_error: 11.5947\n",
            "Epoch 12/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 13.2061 - mean_squared_error: 13.2061 - val_loss: 13.5007 - val_mean_squared_error: 13.5007\n",
            "Epoch 13/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 13.0883 - mean_squared_error: 13.0883 - val_loss: 9.9821 - val_mean_squared_error: 9.9821\n",
            "Epoch 14/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 12.6410 - mean_squared_error: 12.6410 - val_loss: 9.9776 - val_mean_squared_error: 9.9776\n",
            "Epoch 15/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 12.3707 - mean_squared_error: 12.3707 - val_loss: 12.2840 - val_mean_squared_error: 12.2840\n",
            "Epoch 16/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 11.5549 - mean_squared_error: 11.5549 - val_loss: 9.8940 - val_mean_squared_error: 9.8940\n",
            "Epoch 17/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 11.6520 - mean_squared_error: 11.6520 - val_loss: 10.8655 - val_mean_squared_error: 10.8655\n",
            "Epoch 18/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 11.5297 - mean_squared_error: 11.5297 - val_loss: 10.3046 - val_mean_squared_error: 10.3046\n",
            "Epoch 19/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 11.6861 - mean_squared_error: 11.6861 - val_loss: 9.7664 - val_mean_squared_error: 9.7664\n",
            "Epoch 20/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 11.4645 - mean_squared_error: 11.4645 - val_loss: 10.3496 - val_mean_squared_error: 10.3496\n",
            "Epoch 21/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 11.2142 - mean_squared_error: 11.2142 - val_loss: 10.1159 - val_mean_squared_error: 10.1159\n",
            "Epoch 22/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 11.0739 - mean_squared_error: 11.0739 - val_loss: 9.7431 - val_mean_squared_error: 9.7431\n",
            "Epoch 23/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 10.7695 - mean_squared_error: 10.7695 - val_loss: 10.7072 - val_mean_squared_error: 10.7072\n",
            "Epoch 24/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 10.9254 - mean_squared_error: 10.9254 - val_loss: 10.2044 - val_mean_squared_error: 10.2044\n",
            "Epoch 25/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 10.6208 - mean_squared_error: 10.6208 - val_loss: 9.9746 - val_mean_squared_error: 9.9746\n",
            "Epoch 26/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 10.5558 - mean_squared_error: 10.5558 - val_loss: 9.7736 - val_mean_squared_error: 9.7736\n",
            "Epoch 27/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 10.5311 - mean_squared_error: 10.5311 - val_loss: 11.0120 - val_mean_squared_error: 11.0120\n",
            "Epoch 28/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 10.3229 - mean_squared_error: 10.3229 - val_loss: 9.2332 - val_mean_squared_error: 9.2332\n",
            "Epoch 29/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 10.0644 - mean_squared_error: 10.0644 - val_loss: 9.6001 - val_mean_squared_error: 9.6001\n",
            "Epoch 30/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 10.1466 - mean_squared_error: 10.1466 - val_loss: 9.2063 - val_mean_squared_error: 9.2063\n",
            "Epoch 31/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 10.1588 - mean_squared_error: 10.1588 - val_loss: 9.6684 - val_mean_squared_error: 9.6684\n",
            "Epoch 32/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 9.9150 - mean_squared_error: 9.9150 - val_loss: 9.3432 - val_mean_squared_error: 9.3432\n",
            "Epoch 33/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 9.8422 - mean_squared_error: 9.8422 - val_loss: 8.9940 - val_mean_squared_error: 8.9940\n",
            "Epoch 34/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 9.6811 - mean_squared_error: 9.6811 - val_loss: 9.3499 - val_mean_squared_error: 9.3499\n",
            "Epoch 35/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 9.5079 - mean_squared_error: 9.5079 - val_loss: 9.3007 - val_mean_squared_error: 9.3007\n",
            "Epoch 36/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 9.4958 - mean_squared_error: 9.4958 - val_loss: 8.9605 - val_mean_squared_error: 8.9605\n",
            "Epoch 37/200\n",
            "3638/3638 [==============================] - 3s 806us/sample - loss: 9.1721 - mean_squared_error: 9.1721 - val_loss: 8.9975 - val_mean_squared_error: 8.9975\n",
            "Epoch 38/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 8.9831 - mean_squared_error: 8.9831 - val_loss: 8.9376 - val_mean_squared_error: 8.9376\n",
            "Epoch 39/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 8.8930 - mean_squared_error: 8.8930 - val_loss: 9.0930 - val_mean_squared_error: 9.0930\n",
            "Epoch 40/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 8.8104 - mean_squared_error: 8.8104 - val_loss: 8.4789 - val_mean_squared_error: 8.4789\n",
            "Epoch 41/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 8.5353 - mean_squared_error: 8.5353 - val_loss: 8.4068 - val_mean_squared_error: 8.4068\n",
            "Epoch 42/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 8.4202 - mean_squared_error: 8.4202 - val_loss: 8.4547 - val_mean_squared_error: 8.4547\n",
            "Epoch 43/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 8.1379 - mean_squared_error: 8.1379 - val_loss: 7.8324 - val_mean_squared_error: 7.8324\n",
            "Epoch 44/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 7.9939 - mean_squared_error: 7.9939 - val_loss: 8.1304 - val_mean_squared_error: 8.1304\n",
            "Epoch 45/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 7.9221 - mean_squared_error: 7.9221 - val_loss: 8.5291 - val_mean_squared_error: 8.5291\n",
            "Epoch 46/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 7.8250 - mean_squared_error: 7.8250 - val_loss: 8.1635 - val_mean_squared_error: 8.1635\n",
            "Epoch 47/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 7.6487 - mean_squared_error: 7.6487 - val_loss: 7.5735 - val_mean_squared_error: 7.5735\n",
            "Epoch 48/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 7.4298 - mean_squared_error: 7.4298 - val_loss: 7.6809 - val_mean_squared_error: 7.6809\n",
            "Epoch 49/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 7.3114 - mean_squared_error: 7.3114 - val_loss: 6.9536 - val_mean_squared_error: 6.9536\n",
            "Epoch 50/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 6.9902 - mean_squared_error: 6.9902 - val_loss: 8.0645 - val_mean_squared_error: 8.0645\n",
            "Epoch 51/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 6.7743 - mean_squared_error: 6.7743 - val_loss: 6.8398 - val_mean_squared_error: 6.8398\n",
            "Epoch 52/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 6.3468 - mean_squared_error: 6.3468 - val_loss: 6.0552 - val_mean_squared_error: 6.0552\n",
            "Epoch 53/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 6.0621 - mean_squared_error: 6.0621 - val_loss: 6.4335 - val_mean_squared_error: 6.4335\n",
            "Epoch 54/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 5.7854 - mean_squared_error: 5.7854 - val_loss: 5.4773 - val_mean_squared_error: 5.4773\n",
            "Epoch 55/200\n",
            "3638/3638 [==============================] - 3s 823us/sample - loss: 5.6389 - mean_squared_error: 5.6389 - val_loss: 6.3317 - val_mean_squared_error: 6.3317\n",
            "Epoch 56/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 5.4856 - mean_squared_error: 5.4856 - val_loss: 5.7900 - val_mean_squared_error: 5.7900\n",
            "Epoch 57/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 5.2958 - mean_squared_error: 5.2958 - val_loss: 6.4532 - val_mean_squared_error: 6.4532\n",
            "Epoch 58/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 5.0501 - mean_squared_error: 5.0501 - val_loss: 5.1896 - val_mean_squared_error: 5.1896\n",
            "Epoch 59/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 5.0475 - mean_squared_error: 5.0475 - val_loss: 4.8536 - val_mean_squared_error: 4.8536\n",
            "Epoch 60/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 4.5868 - mean_squared_error: 4.5868 - val_loss: 4.8903 - val_mean_squared_error: 4.8903\n",
            "Epoch 61/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 4.5303 - mean_squared_error: 4.5303 - val_loss: 4.5040 - val_mean_squared_error: 4.5040\n",
            "Epoch 62/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 4.4301 - mean_squared_error: 4.4301 - val_loss: 4.8573 - val_mean_squared_error: 4.8573\n",
            "Epoch 63/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 4.3250 - mean_squared_error: 4.3250 - val_loss: 4.5516 - val_mean_squared_error: 4.5516\n",
            "Epoch 64/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 4.2365 - mean_squared_error: 4.2365 - val_loss: 4.2204 - val_mean_squared_error: 4.2204\n",
            "Epoch 65/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 4.0213 - mean_squared_error: 4.0213 - val_loss: 4.3960 - val_mean_squared_error: 4.3960\n",
            "Epoch 66/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 3.9296 - mean_squared_error: 3.9296 - val_loss: 4.0308 - val_mean_squared_error: 4.0308\n",
            "Epoch 67/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 3.9039 - mean_squared_error: 3.9039 - val_loss: 4.5603 - val_mean_squared_error: 4.5603\n",
            "Epoch 68/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 3.8146 - mean_squared_error: 3.8146 - val_loss: 5.2740 - val_mean_squared_error: 5.2740\n",
            "Epoch 69/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 3.7382 - mean_squared_error: 3.7382 - val_loss: 4.1732 - val_mean_squared_error: 4.1732\n",
            "Epoch 70/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 3.4778 - mean_squared_error: 3.4778 - val_loss: 3.6776 - val_mean_squared_error: 3.6776\n",
            "Epoch 71/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 3.5835 - mean_squared_error: 3.5835 - val_loss: 3.7223 - val_mean_squared_error: 3.7223\n",
            "Epoch 72/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 3.4265 - mean_squared_error: 3.4265 - val_loss: 3.3943 - val_mean_squared_error: 3.3943\n",
            "Epoch 73/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 3.3526 - mean_squared_error: 3.3526 - val_loss: 3.5536 - val_mean_squared_error: 3.5536\n",
            "Epoch 74/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 3.2433 - mean_squared_error: 3.2433 - val_loss: 4.6982 - val_mean_squared_error: 4.6982\n",
            "Epoch 75/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 3.2148 - mean_squared_error: 3.2148 - val_loss: 3.4698 - val_mean_squared_error: 3.4698\n",
            "Epoch 76/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 3.1963 - mean_squared_error: 3.1963 - val_loss: 3.3397 - val_mean_squared_error: 3.3397\n",
            "Epoch 77/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 3.1514 - mean_squared_error: 3.1514 - val_loss: 3.4802 - val_mean_squared_error: 3.4802\n",
            "Epoch 78/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 3.0419 - mean_squared_error: 3.0419 - val_loss: 3.2063 - val_mean_squared_error: 3.2063\n",
            "Epoch 79/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.9725 - mean_squared_error: 2.9725 - val_loss: 3.1267 - val_mean_squared_error: 3.1267\n",
            "Epoch 80/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.9223 - mean_squared_error: 2.9223 - val_loss: 3.1691 - val_mean_squared_error: 3.1691\n",
            "Epoch 81/200\n",
            "3638/3638 [==============================] - 3s 823us/sample - loss: 2.8206 - mean_squared_error: 2.8206 - val_loss: 3.2628 - val_mean_squared_error: 3.2628\n",
            "Epoch 82/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 2.9340 - mean_squared_error: 2.9340 - val_loss: 3.5922 - val_mean_squared_error: 3.5922\n",
            "Epoch 83/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 2.7511 - mean_squared_error: 2.7511 - val_loss: 3.0034 - val_mean_squared_error: 3.0034\n",
            "Epoch 84/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 3.0216 - mean_squared_error: 3.0216 - val_loss: 3.5171 - val_mean_squared_error: 3.5171\n",
            "Epoch 85/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.6508 - mean_squared_error: 2.6508 - val_loss: 3.0134 - val_mean_squared_error: 3.0134\n",
            "Epoch 86/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.6981 - mean_squared_error: 2.6981 - val_loss: 3.0064 - val_mean_squared_error: 3.0064\n",
            "Epoch 87/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 2.6140 - mean_squared_error: 2.6140 - val_loss: 3.1889 - val_mean_squared_error: 3.1889\n",
            "Epoch 88/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.5478 - mean_squared_error: 2.5478 - val_loss: 2.9409 - val_mean_squared_error: 2.9409\n",
            "Epoch 89/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 2.5059 - mean_squared_error: 2.5059 - val_loss: 3.1482 - val_mean_squared_error: 3.1482\n",
            "Epoch 90/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.4985 - mean_squared_error: 2.4985 - val_loss: 3.2798 - val_mean_squared_error: 3.2798\n",
            "Epoch 91/200\n",
            "3638/3638 [==============================] - 3s 822us/sample - loss: 2.5086 - mean_squared_error: 2.5086 - val_loss: 3.1131 - val_mean_squared_error: 3.1131\n",
            "Epoch 92/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.4404 - mean_squared_error: 2.4404 - val_loss: 2.8794 - val_mean_squared_error: 2.8794\n",
            "Epoch 93/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 2.3644 - mean_squared_error: 2.3644 - val_loss: 2.8144 - val_mean_squared_error: 2.8144\n",
            "Epoch 94/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 2.4653 - mean_squared_error: 2.4653 - val_loss: 3.0579 - val_mean_squared_error: 3.0579\n",
            "Epoch 95/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 2.3987 - mean_squared_error: 2.3987 - val_loss: 2.7019 - val_mean_squared_error: 2.7019\n",
            "Epoch 96/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 2.3926 - mean_squared_error: 2.3926 - val_loss: 3.0377 - val_mean_squared_error: 3.0377\n",
            "Epoch 97/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.3482 - mean_squared_error: 2.3482 - val_loss: 2.8627 - val_mean_squared_error: 2.8627\n",
            "Epoch 98/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.2632 - mean_squared_error: 2.2632 - val_loss: 2.9966 - val_mean_squared_error: 2.9966\n",
            "Epoch 99/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.2774 - mean_squared_error: 2.2774 - val_loss: 2.7916 - val_mean_squared_error: 2.7916\n",
            "Epoch 100/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.2089 - mean_squared_error: 2.2089 - val_loss: 2.5826 - val_mean_squared_error: 2.5826\n",
            "Epoch 101/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.2447 - mean_squared_error: 2.2447 - val_loss: 2.5615 - val_mean_squared_error: 2.5615\n",
            "Epoch 102/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 2.2301 - mean_squared_error: 2.2301 - val_loss: 3.0712 - val_mean_squared_error: 3.0712\n",
            "Epoch 103/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.0920 - mean_squared_error: 2.0920 - val_loss: 2.6030 - val_mean_squared_error: 2.6030\n",
            "Epoch 104/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 2.1285 - mean_squared_error: 2.1285 - val_loss: 2.5648 - val_mean_squared_error: 2.5648\n",
            "Epoch 105/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 2.1420 - mean_squared_error: 2.1420 - val_loss: 2.5643 - val_mean_squared_error: 2.5643\n",
            "Epoch 106/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 2.0600 - mean_squared_error: 2.0600 - val_loss: 2.7024 - val_mean_squared_error: 2.7024\n",
            "Epoch 107/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 2.1119 - mean_squared_error: 2.1119 - val_loss: 2.6188 - val_mean_squared_error: 2.6188\n",
            "Epoch 108/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.0171 - mean_squared_error: 2.0171 - val_loss: 2.6572 - val_mean_squared_error: 2.6572\n",
            "Epoch 109/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 2.0441 - mean_squared_error: 2.0441 - val_loss: 2.8237 - val_mean_squared_error: 2.8237\n",
            "Epoch 110/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.9447 - mean_squared_error: 1.9447 - val_loss: 2.5299 - val_mean_squared_error: 2.5299\n",
            "Epoch 111/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.9826 - mean_squared_error: 1.9826 - val_loss: 2.6204 - val_mean_squared_error: 2.6204\n",
            "Epoch 112/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.9921 - mean_squared_error: 1.9921 - val_loss: 2.7583 - val_mean_squared_error: 2.7583\n",
            "Epoch 113/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.9381 - mean_squared_error: 1.9381 - val_loss: 2.5447 - val_mean_squared_error: 2.5447\n",
            "Epoch 114/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.9693 - mean_squared_error: 1.9693 - val_loss: 2.5477 - val_mean_squared_error: 2.5477\n",
            "Epoch 115/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.8951 - mean_squared_error: 1.8951 - val_loss: 2.4096 - val_mean_squared_error: 2.4096\n",
            "Epoch 116/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.8211 - mean_squared_error: 1.8211 - val_loss: 2.3764 - val_mean_squared_error: 2.3764\n",
            "Epoch 117/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.8307 - mean_squared_error: 1.8307 - val_loss: 2.4654 - val_mean_squared_error: 2.4654\n",
            "Epoch 118/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.8703 - mean_squared_error: 1.8703 - val_loss: 2.5337 - val_mean_squared_error: 2.5337\n",
            "Epoch 119/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.8900 - mean_squared_error: 1.8900 - val_loss: 2.6673 - val_mean_squared_error: 2.6673\n",
            "Epoch 120/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.8624 - mean_squared_error: 1.8624 - val_loss: 2.5291 - val_mean_squared_error: 2.5291\n",
            "Epoch 121/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.7986 - mean_squared_error: 1.7986 - val_loss: 2.4719 - val_mean_squared_error: 2.4719\n",
            "Epoch 122/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.7893 - mean_squared_error: 1.7893 - val_loss: 2.6164 - val_mean_squared_error: 2.6164\n",
            "Epoch 123/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.8238 - mean_squared_error: 1.8238 - val_loss: 2.4905 - val_mean_squared_error: 2.4905\n",
            "Epoch 124/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.8246 - mean_squared_error: 1.8246 - val_loss: 2.4323 - val_mean_squared_error: 2.4323\n",
            "Epoch 125/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.7191 - mean_squared_error: 1.7191 - val_loss: 2.5202 - val_mean_squared_error: 2.5202\n",
            "Epoch 126/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.7771 - mean_squared_error: 1.7771 - val_loss: 2.4548 - val_mean_squared_error: 2.4548\n",
            "Epoch 127/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.7094 - mean_squared_error: 1.7094 - val_loss: 2.6318 - val_mean_squared_error: 2.6318\n",
            "Epoch 128/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.7729 - mean_squared_error: 1.7729 - val_loss: 2.4078 - val_mean_squared_error: 2.4078\n",
            "Epoch 129/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.7313 - mean_squared_error: 1.7313 - val_loss: 2.4034 - val_mean_squared_error: 2.4034\n",
            "Epoch 130/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.6588 - mean_squared_error: 1.6588 - val_loss: 2.4682 - val_mean_squared_error: 2.4682\n",
            "Epoch 131/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.6772 - mean_squared_error: 1.6772 - val_loss: 2.5472 - val_mean_squared_error: 2.5472\n",
            "Epoch 132/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.7121 - mean_squared_error: 1.7121 - val_loss: 2.5819 - val_mean_squared_error: 2.5819\n",
            "Epoch 133/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.7150 - mean_squared_error: 1.7150 - val_loss: 2.5585 - val_mean_squared_error: 2.5585\n",
            "Epoch 134/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.6961 - mean_squared_error: 1.6961 - val_loss: 2.3468 - val_mean_squared_error: 2.3468\n",
            "Epoch 135/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.6137 - mean_squared_error: 1.6137 - val_loss: 2.3838 - val_mean_squared_error: 2.3838\n",
            "Epoch 136/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.6084 - mean_squared_error: 1.6084 - val_loss: 2.6342 - val_mean_squared_error: 2.6342\n",
            "Epoch 137/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.6499 - mean_squared_error: 1.6499 - val_loss: 2.2645 - val_mean_squared_error: 2.2645\n",
            "Epoch 138/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.6166 - mean_squared_error: 1.6166 - val_loss: 2.2921 - val_mean_squared_error: 2.2921\n",
            "Epoch 139/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.5730 - mean_squared_error: 1.5730 - val_loss: 2.2975 - val_mean_squared_error: 2.2975\n",
            "Epoch 140/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.5480 - mean_squared_error: 1.5480 - val_loss: 2.2755 - val_mean_squared_error: 2.2755\n",
            "Epoch 141/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.5715 - mean_squared_error: 1.5715 - val_loss: 2.3500 - val_mean_squared_error: 2.3500\n",
            "Epoch 142/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.6063 - mean_squared_error: 1.6063 - val_loss: 2.4623 - val_mean_squared_error: 2.4623\n",
            "Epoch 143/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.5324 - mean_squared_error: 1.5324 - val_loss: 2.5655 - val_mean_squared_error: 2.5655\n",
            "Epoch 144/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.5658 - mean_squared_error: 1.5658 - val_loss: 2.6717 - val_mean_squared_error: 2.6717\n",
            "Epoch 145/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 1.5833 - mean_squared_error: 1.5833 - val_loss: 2.5186 - val_mean_squared_error: 2.5186\n",
            "Epoch 146/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.5359 - mean_squared_error: 1.5359 - val_loss: 2.3668 - val_mean_squared_error: 2.3668\n",
            "Epoch 147/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.4705 - mean_squared_error: 1.4705 - val_loss: 2.3126 - val_mean_squared_error: 2.3126\n",
            "Epoch 148/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.5291 - mean_squared_error: 1.5291 - val_loss: 2.6165 - val_mean_squared_error: 2.6165\n",
            "Epoch 149/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.4419 - mean_squared_error: 1.4419 - val_loss: 2.2157 - val_mean_squared_error: 2.2157\n",
            "Epoch 150/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 1.5068 - mean_squared_error: 1.5068 - val_loss: 2.6012 - val_mean_squared_error: 2.6012\n",
            "Epoch 151/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.4741 - mean_squared_error: 1.4741 - val_loss: 2.2453 - val_mean_squared_error: 2.2453\n",
            "Epoch 152/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.4214 - mean_squared_error: 1.4214 - val_loss: 2.2380 - val_mean_squared_error: 2.2380\n",
            "Epoch 153/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.4495 - mean_squared_error: 1.4495 - val_loss: 2.3959 - val_mean_squared_error: 2.3959\n",
            "Epoch 154/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.5265 - mean_squared_error: 1.5265 - val_loss: 2.2333 - val_mean_squared_error: 2.2333\n",
            "Epoch 155/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.4759 - mean_squared_error: 1.4759 - val_loss: 2.2498 - val_mean_squared_error: 2.2498\n",
            "Epoch 156/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.4739 - mean_squared_error: 1.4739 - val_loss: 2.3853 - val_mean_squared_error: 2.3853\n",
            "Epoch 157/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.4498 - mean_squared_error: 1.4498 - val_loss: 2.2532 - val_mean_squared_error: 2.2532\n",
            "Epoch 158/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.4430 - mean_squared_error: 1.4430 - val_loss: 2.3275 - val_mean_squared_error: 2.3275\n",
            "Epoch 159/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.3723 - mean_squared_error: 1.3723 - val_loss: 2.2530 - val_mean_squared_error: 2.2530\n",
            "Epoch 160/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.4162 - mean_squared_error: 1.4162 - val_loss: 2.3067 - val_mean_squared_error: 2.3067\n",
            "Epoch 161/200\n",
            "3638/3638 [==============================] - 3s 820us/sample - loss: 1.3707 - mean_squared_error: 1.3707 - val_loss: 2.2355 - val_mean_squared_error: 2.2355\n",
            "Epoch 162/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.4046 - mean_squared_error: 1.4046 - val_loss: 2.1292 - val_mean_squared_error: 2.1292\n",
            "Epoch 163/200\n",
            "3638/3638 [==============================] - 3s 824us/sample - loss: 1.4052 - mean_squared_error: 1.4052 - val_loss: 2.2936 - val_mean_squared_error: 2.2936\n",
            "Epoch 164/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 1.3420 - mean_squared_error: 1.3420 - val_loss: 2.2712 - val_mean_squared_error: 2.2712\n",
            "Epoch 165/200\n",
            "3638/3638 [==============================] - 3s 825us/sample - loss: 1.3499 - mean_squared_error: 1.3499 - val_loss: 2.3464 - val_mean_squared_error: 2.3464\n",
            "Epoch 166/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.3985 - mean_squared_error: 1.3985 - val_loss: 2.2478 - val_mean_squared_error: 2.2478\n",
            "Epoch 167/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.3315 - mean_squared_error: 1.3315 - val_loss: 2.4021 - val_mean_squared_error: 2.4021\n",
            "Epoch 168/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.3944 - mean_squared_error: 1.3944 - val_loss: 2.2359 - val_mean_squared_error: 2.2359\n",
            "Epoch 169/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.3462 - mean_squared_error: 1.3462 - val_loss: 2.3668 - val_mean_squared_error: 2.3668\n",
            "Epoch 170/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.4040 - mean_squared_error: 1.4040 - val_loss: 2.3425 - val_mean_squared_error: 2.3425\n",
            "Epoch 171/200\n",
            "3638/3638 [==============================] - 3s 818us/sample - loss: 1.3355 - mean_squared_error: 1.3355 - val_loss: 2.2036 - val_mean_squared_error: 2.2036\n",
            "Epoch 172/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.2965 - mean_squared_error: 1.2965 - val_loss: 2.1066 - val_mean_squared_error: 2.1066\n",
            "Epoch 173/200\n",
            "3638/3638 [==============================] - 3s 814us/sample - loss: 1.2929 - mean_squared_error: 1.2929 - val_loss: 2.2440 - val_mean_squared_error: 2.2440\n",
            "Epoch 174/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.3316 - mean_squared_error: 1.3316 - val_loss: 2.2216 - val_mean_squared_error: 2.2216\n",
            "Epoch 175/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.3195 - mean_squared_error: 1.3195 - val_loss: 2.3684 - val_mean_squared_error: 2.3684\n",
            "Epoch 176/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.2631 - mean_squared_error: 1.2631 - val_loss: 2.2069 - val_mean_squared_error: 2.2069\n",
            "Epoch 177/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.3020 - mean_squared_error: 1.3020 - val_loss: 2.1915 - val_mean_squared_error: 2.1915\n",
            "Epoch 178/200\n",
            "3638/3638 [==============================] - 3s 821us/sample - loss: 1.3379 - mean_squared_error: 1.3379 - val_loss: 2.1487 - val_mean_squared_error: 2.1487\n",
            "Epoch 179/200\n",
            "3638/3638 [==============================] - 3s 817us/sample - loss: 1.3134 - mean_squared_error: 1.3134 - val_loss: 2.2481 - val_mean_squared_error: 2.2481\n",
            "Epoch 180/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.2696 - mean_squared_error: 1.2696 - val_loss: 2.4526 - val_mean_squared_error: 2.4526\n",
            "Epoch 181/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.2778 - mean_squared_error: 1.2778 - val_loss: 2.2099 - val_mean_squared_error: 2.2099\n",
            "Epoch 182/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.2521 - mean_squared_error: 1.2521 - val_loss: 2.2603 - val_mean_squared_error: 2.2603\n",
            "Epoch 183/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.2632 - mean_squared_error: 1.2632 - val_loss: 2.3422 - val_mean_squared_error: 2.3422\n",
            "Epoch 184/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.2624 - mean_squared_error: 1.2624 - val_loss: 2.2580 - val_mean_squared_error: 2.2580\n",
            "Epoch 185/200\n",
            "3638/3638 [==============================] - 3s 810us/sample - loss: 1.2754 - mean_squared_error: 1.2754 - val_loss: 2.1856 - val_mean_squared_error: 2.1856\n",
            "Epoch 186/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.2333 - mean_squared_error: 1.2333 - val_loss: 2.2056 - val_mean_squared_error: 2.2056\n",
            "Epoch 187/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.2111 - mean_squared_error: 1.2111 - val_loss: 2.2010 - val_mean_squared_error: 2.2010\n",
            "Epoch 188/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.2644 - mean_squared_error: 1.2644 - val_loss: 2.4299 - val_mean_squared_error: 2.4299\n",
            "Epoch 189/200\n",
            "3638/3638 [==============================] - 3s 809us/sample - loss: 1.2570 - mean_squared_error: 1.2570 - val_loss: 2.1820 - val_mean_squared_error: 2.1820\n",
            "Epoch 190/200\n",
            "3638/3638 [==============================] - 3s 816us/sample - loss: 1.2024 - mean_squared_error: 1.2024 - val_loss: 2.2777 - val_mean_squared_error: 2.2777\n",
            "Epoch 191/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.1996 - mean_squared_error: 1.1996 - val_loss: 2.3330 - val_mean_squared_error: 2.3330\n",
            "Epoch 192/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.2197 - mean_squared_error: 1.2197 - val_loss: 2.2204 - val_mean_squared_error: 2.2204\n",
            "Epoch 193/200\n",
            "3638/3638 [==============================] - 3s 811us/sample - loss: 1.1592 - mean_squared_error: 1.1592 - val_loss: 2.1616 - val_mean_squared_error: 2.1616\n",
            "Epoch 194/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.1746 - mean_squared_error: 1.1746 - val_loss: 2.2421 - val_mean_squared_error: 2.2421\n",
            "Epoch 195/200\n",
            "3638/3638 [==============================] - 3s 819us/sample - loss: 1.2054 - mean_squared_error: 1.2054 - val_loss: 2.2645 - val_mean_squared_error: 2.2645\n",
            "Epoch 196/200\n",
            "3638/3638 [==============================] - 3s 808us/sample - loss: 1.1760 - mean_squared_error: 1.1760 - val_loss: 2.5849 - val_mean_squared_error: 2.5849\n",
            "Epoch 197/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.2002 - mean_squared_error: 1.2002 - val_loss: 2.2459 - val_mean_squared_error: 2.2459\n",
            "Epoch 198/200\n",
            "3638/3638 [==============================] - 3s 812us/sample - loss: 1.1759 - mean_squared_error: 1.1759 - val_loss: 2.3514 - val_mean_squared_error: 2.3514\n",
            "Epoch 199/200\n",
            "3638/3638 [==============================] - 3s 813us/sample - loss: 1.1586 - mean_squared_error: 1.1586 - val_loss: 2.2307 - val_mean_squared_error: 2.2307\n",
            "Epoch 200/200\n",
            "3638/3638 [==============================] - 3s 815us/sample - loss: 1.1708 - mean_squared_error: 1.1708 - val_loss: 2.2136 - val_mean_squared_error: 2.2136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQlwrYKlvHzd",
        "colab_type": "text"
      },
      "source": [
        "## Specialization Time\n",
        "\n",
        "After getting lackluster test scores using our AlexNet and VGG Net models which made use of only the complete training examples (with data augmentation), the team determined there may be fruit in moving towards highly specialized models. Instead of having one CNN that is training to try to simultaneously identify 15 different keypoints, lets try to develop 15 models that each identify only one keypoint.\n",
        "\n",
        "The benefit of this approach is twofold.\n",
        "\n",
        "1.   Maximize use of available training data - there is no longer any need to throw any examples away. Each model will be trained on all available training examples for that specific keypoint. This is a huge benefit to keypoints such as nose_tip and eye centers as they have more than 7000 training examples; whereas, we were only previously making use of ~2100.\n",
        "2.   The entire model parameter training is focused on identifying a single keypoint. Thus the usefulness of each layer and each hidden unit is maximized.\n",
        "\n",
        "Obviously the downside to this approach is the 15x increase in model training time, and the additional user time it will take to optimize each of these 15 models.\n",
        "\n",
        "Since we now need to train 15 models, we transfer our weights from our combined model to hopefully speed up the learning process. We will train for additional epochs but implement an early stopping criteria.\n",
        "\n",
        "As we are now making use of all of the data, we have updated our DataExploration file in the team's [GitHub repo](https://github.com/tomgoter/w207_finalproject) to generate, pickle and save the complete dataset. The code cell below is used to simply read that in from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfvwtwK2XzlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Load the complete dataframe with xflips from the pickle file\n",
        "df = pd.read_pickle(drive_path + \"df_nostache_w_flip.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KamAqR1D6RSm",
        "colab_type": "text"
      },
      "source": [
        "## Define Specialized CNN model\n",
        "\n",
        "We chose to start with the architecture from our best performing AlexNet inspired CNN. This model had three convolution layers and two fully connected layers leading to the output layer. It is important to note (probably obvious) that the output layer has been modified from 30 outputs (corresponding to the x and y-coordinates of all 15 keypoints) to instead only have two outputs (x,y for single keypoint). We also name the output layer, as this was determined to be an easy way to avoid conflict when bringing the weights from our combined model (assigned by name, so having a new name for this output layer means we won't be trying to assign weights from 30 units to 2 units).  The resultant models each have 3.18M parameters to train for a total of 47.7M parameters! We suspected that overfitting may be an issue, but optimizing each model independently would be very costly in terms of user and GPU time. So for the first iteration the team maintained the original AlexNet model with the two outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsGPjPUp6Raq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_spec_bn_cnn_model(start_filter, d, step, fc1=500, fc2=500):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model \n",
        "    '''\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "#     cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='valid', \n",
        "                                         input_shape=(96, 96, 1),activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (2, 2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (2, 2), padding='valid', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+2*step))\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc1))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc2))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(2, name='Specialist'))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq6_vi6l3juz",
        "colab_type": "text"
      },
      "source": [
        "## Train Keypoint Specific Models\n",
        "\n",
        "The function below looks a lot similar to earlier functions that were used to wrap around model building, training and output storing. There are a few notable differences however. \n",
        "  \n",
        "\n",
        "*   The first is that we have a 'name' parameter which is simply a link to the saved combined model from which we will be loading weights.\n",
        "*   Our early stopping callback is now set to save the best weights (we should have been doing this earlier). We don't expect any of these models to need 300 epochs to train. So this callback will likely be invoked for each model. Using the restore_best_weights option, we ensure our model that we are selecting (and eventually using for predicting the test keypoints) is using the most optimized model weights (as judged by our validation scores).\n",
        "*  We load the weights from our specified model, after first constructing the model using the same architecture\n",
        "*  The training data is subsetted to only contain y-values for a single specified keypoint which is an input parameter to our function (i.e., 'keypoint')\n",
        "* Similar to before, all models will be saved to Google Drive to allow us to make predictions on the test set for each keypoint and then combine and submit\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JngibKC4vbHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_specialists(df, keypoint, name='cnn_flipped6_adam_d0.0_s0.1_sf12_lrfactor10_flipped_100.h5'):\n",
        "  # Use an early stopping callback and our timing callback\n",
        "  early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                                patience=100, mode='auto', restore_best_weights = True)\n",
        "  time_callback = TimeHistory()\n",
        "  \n",
        "  \n",
        "  # Use the adam optimizer with the default learning rate\n",
        "  adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
        "  \n",
        "  \n",
        "  # Path to the model - we will just load the weights from it\n",
        "  model_path = drive_path + 'Models/' + name \n",
        "\n",
        "  \n",
        "  \n",
        "  # Compile the model\n",
        "  \n",
        "  model = create_spec_bn_cnn_model(12, 0.0, 0.10)\n",
        "      \n",
        "  # Load the weights from our no nan model as a starting point\n",
        "  model.load_weights(model_path, by_name=True)\n",
        "  \n",
        "  model.compile(\n",
        "      optimizer=adam,\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['mean_squared_error'])\n",
        "  \n",
        "  df_keypoint = df[[keypoint + '_x',keypoint+'_y', 'Image']]\n",
        "  print(df_keypoint.columns)\n",
        "  df_keypoint = df_keypoint.dropna().reset_index(drop=True)\n",
        "  \n",
        "  # Grab the last column - that is our image data for X matrix\n",
        "  train_X = df_keypoint.iloc[:, -1]\n",
        "\n",
        "  # Convert from a series of arrays to an NDarray\n",
        "  train_X = np.array([x.reshape(96,96,1) for x in train_X])\n",
        " \n",
        "  \n",
        "  # Get the Y data for just the specified keypoint\n",
        "  train_y = np.array(df_keypoint.iloc[:,:-1])\n",
        " \n",
        "  history = model.fit(\n",
        "      train_X.astype(np.float32), train_y.astype(np.float32),\n",
        "      epochs=300,\n",
        "      validation_split=0.20,\n",
        "      callbacks=[time_callback, early_stop])\n",
        "  times = time_callback.times\n",
        "\n",
        "  # Convert to dataframe\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "  hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "  hist['times'] = times\n",
        "  hist['starting_filter'] = 12\n",
        "  hist['layers'] = 3\n",
        "  hist['pooling'] = 'yes'\n",
        "  hist['fc_layer1'] = 500\n",
        "  hist['fc_layer2'] = 500\n",
        "  hist['activation'] = 'relu'\n",
        "  hist['optimizer'] = 'adam'\n",
        "  hist['lrate'] = adam.get_config()['learning_rate']\n",
        "  hist['dropout_initial'] = 0.0\n",
        "  hist['dropout_step'] = 0.1\n",
        "  hist['batch_norm'] = 1\n",
        "  hist['bias'] = 0\n",
        "  hist['stride'] = 1\n",
        "  hist['flipped'] = 1.0\n",
        "  hist['keypoint'] = keypoint\n",
        "\n",
        "  # Re-pickle after every model to retain progress\n",
        "  hist.to_pickle(drive_path+\"OutputData/spec_{}.pkl\".format(keypoint))\n",
        "\n",
        "  # Save models.\n",
        "  filename = \"cnn_spec_{}_d0_s10_sf12_lrfactor1_flipped_100\".format(keypoint)\n",
        "  model.save(drive_path+\"Models/\"+filename+\".h5\")\n",
        "  return hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXZrCYW0CdOV",
        "colab_type": "text"
      },
      "source": [
        "## Train Each Model\n",
        "\n",
        "Run the function below for each keypoint. The first go around no additional optimization of the models was pursued due to the runtime required to train the 15 different models with over 3 million parameters to train for each model. The training was done using the simple line below which was executed 15 times, once for each keypoint. Each execution of the train specialists function results in a dataframe of output and a saved model file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWW-rv1t2Jus",
        "colab_type": "code",
        "outputId": "e1610d71-e32c-49cb-8129-a132616bb50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Initialize a new data frame to hold our output data\n",
        "specialist_df = train_specialists(df, 'mouth_center_bottom_lip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0727 15:48:04.860363 140272171808640 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 94, 94, 12)        108       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 94, 94, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 47, 47, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 46, 46, 24)        1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 46, 46, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 23, 23, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 22, 22, 48)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 22, 22, 48)        192       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 11, 11, 48)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 5808)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               2904500   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 500)               2000      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "Specialist (Dense)           (None, 2)                 1002      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 3,166,206\n",
            "Trainable params: 3,164,038\n",
            "Non-trainable params: 2,168\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Index(['mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y', 'Image'], dtype='object')\n",
            "Train on 11224 samples, validate on 2806 samples\n",
            "Epoch 1/300\n",
            "11224/11224 [==============================] - 10s 931us/sample - loss: 166.3862 - mean_squared_error: 166.3863 - val_loss: 32.1779 - val_mean_squared_error: 32.1779\n",
            "Epoch 2/300\n",
            "11224/11224 [==============================] - 7s 601us/sample - loss: 28.5072 - mean_squared_error: 28.5072 - val_loss: 23.1099 - val_mean_squared_error: 23.1099\n",
            "Epoch 3/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 21.1354 - mean_squared_error: 21.1354 - val_loss: 18.9494 - val_mean_squared_error: 18.9494\n",
            "Epoch 4/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 15.7803 - mean_squared_error: 15.7803 - val_loss: 14.1359 - val_mean_squared_error: 14.1359\n",
            "Epoch 5/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 12.9508 - mean_squared_error: 12.9508 - val_loss: 11.1376 - val_mean_squared_error: 11.1376\n",
            "Epoch 6/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 11.4619 - mean_squared_error: 11.4619 - val_loss: 10.9137 - val_mean_squared_error: 10.9137\n",
            "Epoch 7/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 10.7150 - mean_squared_error: 10.7150 - val_loss: 11.7024 - val_mean_squared_error: 11.7024\n",
            "Epoch 8/300\n",
            "11224/11224 [==============================] - 7s 604us/sample - loss: 9.4589 - mean_squared_error: 9.4589 - val_loss: 9.4017 - val_mean_squared_error: 9.4017\n",
            "Epoch 9/300\n",
            "11224/11224 [==============================] - 7s 605us/sample - loss: 8.9746 - mean_squared_error: 8.9746 - val_loss: 9.3576 - val_mean_squared_error: 9.3576\n",
            "Epoch 10/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 8.4599 - mean_squared_error: 8.4599 - val_loss: 10.0454 - val_mean_squared_error: 10.0454\n",
            "Epoch 11/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 7.6235 - mean_squared_error: 7.6235 - val_loss: 12.1137 - val_mean_squared_error: 12.1137\n",
            "Epoch 12/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 6.9940 - mean_squared_error: 6.9940 - val_loss: 8.3925 - val_mean_squared_error: 8.3925\n",
            "Epoch 13/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 6.5702 - mean_squared_error: 6.5702 - val_loss: 9.1276 - val_mean_squared_error: 9.1276\n",
            "Epoch 14/300\n",
            "11224/11224 [==============================] - 7s 609us/sample - loss: 6.0347 - mean_squared_error: 6.0347 - val_loss: 8.8838 - val_mean_squared_error: 8.8838\n",
            "Epoch 15/300\n",
            "11224/11224 [==============================] - 7s 609us/sample - loss: 5.7161 - mean_squared_error: 5.7161 - val_loss: 8.8581 - val_mean_squared_error: 8.8581\n",
            "Epoch 16/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 5.8700 - mean_squared_error: 5.8700 - val_loss: 10.1397 - val_mean_squared_error: 10.1397\n",
            "Epoch 17/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 5.5906 - mean_squared_error: 5.5906 - val_loss: 9.1881 - val_mean_squared_error: 9.1881\n",
            "Epoch 18/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 5.1712 - mean_squared_error: 5.1712 - val_loss: 11.2490 - val_mean_squared_error: 11.2490\n",
            "Epoch 19/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 4.7811 - mean_squared_error: 4.7811 - val_loss: 8.5590 - val_mean_squared_error: 8.5590\n",
            "Epoch 20/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 4.9751 - mean_squared_error: 4.9751 - val_loss: 10.0889 - val_mean_squared_error: 10.0889\n",
            "Epoch 21/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 4.5916 - mean_squared_error: 4.5916 - val_loss: 9.3839 - val_mean_squared_error: 9.3839\n",
            "Epoch 22/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 4.6254 - mean_squared_error: 4.6254 - val_loss: 9.1777 - val_mean_squared_error: 9.1777\n",
            "Epoch 23/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 4.1844 - mean_squared_error: 4.1844 - val_loss: 9.2528 - val_mean_squared_error: 9.2528\n",
            "Epoch 24/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 4.0265 - mean_squared_error: 4.0265 - val_loss: 11.5981 - val_mean_squared_error: 11.5981\n",
            "Epoch 25/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 4.2185 - mean_squared_error: 4.2185 - val_loss: 9.9581 - val_mean_squared_error: 9.9581\n",
            "Epoch 26/300\n",
            "11224/11224 [==============================] - 7s 601us/sample - loss: 3.8847 - mean_squared_error: 3.8847 - val_loss: 9.5121 - val_mean_squared_error: 9.5121\n",
            "Epoch 27/300\n",
            "11224/11224 [==============================] - 7s 603us/sample - loss: 3.9022 - mean_squared_error: 3.9022 - val_loss: 9.3064 - val_mean_squared_error: 9.3064\n",
            "Epoch 28/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 3.6455 - mean_squared_error: 3.6455 - val_loss: 10.5734 - val_mean_squared_error: 10.5734\n",
            "Epoch 29/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 3.6491 - mean_squared_error: 3.6491 - val_loss: 9.3377 - val_mean_squared_error: 9.3377\n",
            "Epoch 30/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 3.4385 - mean_squared_error: 3.4385 - val_loss: 11.6536 - val_mean_squared_error: 11.6536\n",
            "Epoch 31/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 3.4647 - mean_squared_error: 3.4647 - val_loss: 11.1657 - val_mean_squared_error: 11.1657\n",
            "Epoch 32/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 3.6443 - mean_squared_error: 3.6443 - val_loss: 9.7314 - val_mean_squared_error: 9.7314\n",
            "Epoch 33/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 3.1881 - mean_squared_error: 3.1881 - val_loss: 10.8785 - val_mean_squared_error: 10.8785\n",
            "Epoch 34/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 3.2441 - mean_squared_error: 3.2441 - val_loss: 8.4020 - val_mean_squared_error: 8.4020\n",
            "Epoch 35/300\n",
            "11224/11224 [==============================] - 7s 593us/sample - loss: 3.2652 - mean_squared_error: 3.2652 - val_loss: 11.2950 - val_mean_squared_error: 11.2950\n",
            "Epoch 36/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 3.2429 - mean_squared_error: 3.2429 - val_loss: 11.8995 - val_mean_squared_error: 11.8995\n",
            "Epoch 37/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 3.1539 - mean_squared_error: 3.1539 - val_loss: 13.1377 - val_mean_squared_error: 13.1377\n",
            "Epoch 38/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 3.1294 - mean_squared_error: 3.1294 - val_loss: 10.2872 - val_mean_squared_error: 10.2872\n",
            "Epoch 39/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 2.9429 - mean_squared_error: 2.9429 - val_loss: 11.9942 - val_mean_squared_error: 11.9942\n",
            "Epoch 40/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 3.0296 - mean_squared_error: 3.0296 - val_loss: 10.0049 - val_mean_squared_error: 10.0049\n",
            "Epoch 41/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 2.7941 - mean_squared_error: 2.7941 - val_loss: 10.6158 - val_mean_squared_error: 10.6158\n",
            "Epoch 42/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 2.6234 - mean_squared_error: 2.6234 - val_loss: 12.1483 - val_mean_squared_error: 12.1483\n",
            "Epoch 43/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 2.7361 - mean_squared_error: 2.7361 - val_loss: 10.7333 - val_mean_squared_error: 10.7333\n",
            "Epoch 44/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.6032 - mean_squared_error: 2.6032 - val_loss: 9.8596 - val_mean_squared_error: 9.8596\n",
            "Epoch 45/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 2.5395 - mean_squared_error: 2.5395 - val_loss: 10.7666 - val_mean_squared_error: 10.7666\n",
            "Epoch 46/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.6347 - mean_squared_error: 2.6347 - val_loss: 10.8272 - val_mean_squared_error: 10.8272\n",
            "Epoch 47/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 2.5417 - mean_squared_error: 2.5417 - val_loss: 9.0584 - val_mean_squared_error: 9.0584\n",
            "Epoch 48/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 2.5796 - mean_squared_error: 2.5796 - val_loss: 11.7893 - val_mean_squared_error: 11.7893\n",
            "Epoch 49/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.6230 - mean_squared_error: 2.6230 - val_loss: 9.6174 - val_mean_squared_error: 9.6174\n",
            "Epoch 50/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 2.3272 - mean_squared_error: 2.3272 - val_loss: 10.2643 - val_mean_squared_error: 10.2643\n",
            "Epoch 51/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 2.2871 - mean_squared_error: 2.2871 - val_loss: 10.9192 - val_mean_squared_error: 10.9192\n",
            "Epoch 52/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 2.3804 - mean_squared_error: 2.3804 - val_loss: 13.3764 - val_mean_squared_error: 13.3764\n",
            "Epoch 53/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.4318 - mean_squared_error: 2.4318 - val_loss: 10.0927 - val_mean_squared_error: 10.0927\n",
            "Epoch 54/300\n",
            "11224/11224 [==============================] - 7s 610us/sample - loss: 2.3770 - mean_squared_error: 2.3770 - val_loss: 12.9976 - val_mean_squared_error: 12.9976\n",
            "Epoch 55/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 2.4732 - mean_squared_error: 2.4732 - val_loss: 10.5091 - val_mean_squared_error: 10.5091\n",
            "Epoch 56/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.2497 - mean_squared_error: 2.2497 - val_loss: 10.5901 - val_mean_squared_error: 10.5901\n",
            "Epoch 57/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 2.1670 - mean_squared_error: 2.1670 - val_loss: 11.5544 - val_mean_squared_error: 11.5544\n",
            "Epoch 58/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 2.2429 - mean_squared_error: 2.2429 - val_loss: 9.8691 - val_mean_squared_error: 9.8691\n",
            "Epoch 59/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.3066 - mean_squared_error: 2.3066 - val_loss: 10.4940 - val_mean_squared_error: 10.4940\n",
            "Epoch 60/300\n",
            "11224/11224 [==============================] - 7s 605us/sample - loss: 2.1321 - mean_squared_error: 2.1321 - val_loss: 9.1842 - val_mean_squared_error: 9.1842\n",
            "Epoch 61/300\n",
            "11224/11224 [==============================] - 7s 616us/sample - loss: 2.0641 - mean_squared_error: 2.0641 - val_loss: 9.5009 - val_mean_squared_error: 9.5009\n",
            "Epoch 62/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.0558 - mean_squared_error: 2.0558 - val_loss: 10.6910 - val_mean_squared_error: 10.6910\n",
            "Epoch 63/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 2.0706 - mean_squared_error: 2.0706 - val_loss: 10.9768 - val_mean_squared_error: 10.9768\n",
            "Epoch 64/300\n",
            "11224/11224 [==============================] - 7s 593us/sample - loss: 2.2437 - mean_squared_error: 2.2437 - val_loss: 10.2110 - val_mean_squared_error: 10.2110\n",
            "Epoch 65/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 2.0452 - mean_squared_error: 2.0451 - val_loss: 10.2117 - val_mean_squared_error: 10.2117\n",
            "Epoch 66/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 2.0678 - mean_squared_error: 2.0678 - val_loss: 10.9321 - val_mean_squared_error: 10.9321\n",
            "Epoch 67/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 1.9506 - mean_squared_error: 1.9506 - val_loss: 10.2620 - val_mean_squared_error: 10.2620\n",
            "Epoch 68/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 2.1329 - mean_squared_error: 2.1329 - val_loss: 11.2880 - val_mean_squared_error: 11.2880\n",
            "Epoch 69/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 1.9885 - mean_squared_error: 1.9885 - val_loss: 10.1301 - val_mean_squared_error: 10.1301\n",
            "Epoch 70/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 1.9477 - mean_squared_error: 1.9477 - val_loss: 9.5525 - val_mean_squared_error: 9.5525\n",
            "Epoch 71/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 2.0414 - mean_squared_error: 2.0414 - val_loss: 11.8213 - val_mean_squared_error: 11.8213\n",
            "Epoch 72/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.9268 - mean_squared_error: 1.9268 - val_loss: 8.9484 - val_mean_squared_error: 8.9484\n",
            "Epoch 73/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 1.9989 - mean_squared_error: 1.9989 - val_loss: 9.8604 - val_mean_squared_error: 9.8604\n",
            "Epoch 74/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 1.9260 - mean_squared_error: 1.9260 - val_loss: 10.4302 - val_mean_squared_error: 10.4302\n",
            "Epoch 75/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 1.9129 - mean_squared_error: 1.9129 - val_loss: 10.9411 - val_mean_squared_error: 10.9411\n",
            "Epoch 76/300\n",
            "11224/11224 [==============================] - 7s 602us/sample - loss: 1.8448 - mean_squared_error: 1.8448 - val_loss: 9.9548 - val_mean_squared_error: 9.9548\n",
            "Epoch 77/300\n",
            "11224/11224 [==============================] - 7s 592us/sample - loss: 1.8558 - mean_squared_error: 1.8558 - val_loss: 9.2382 - val_mean_squared_error: 9.2382\n",
            "Epoch 78/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 1.7167 - mean_squared_error: 1.7167 - val_loss: 11.5651 - val_mean_squared_error: 11.5651\n",
            "Epoch 79/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 1.9431 - mean_squared_error: 1.9431 - val_loss: 10.5454 - val_mean_squared_error: 10.5454\n",
            "Epoch 80/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.8703 - mean_squared_error: 1.8703 - val_loss: 11.5071 - val_mean_squared_error: 11.5071\n",
            "Epoch 81/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.7858 - mean_squared_error: 1.7858 - val_loss: 11.0635 - val_mean_squared_error: 11.0635\n",
            "Epoch 82/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 1.8605 - mean_squared_error: 1.8605 - val_loss: 10.3657 - val_mean_squared_error: 10.3657\n",
            "Epoch 83/300\n",
            "11224/11224 [==============================] - 7s 591us/sample - loss: 1.8526 - mean_squared_error: 1.8526 - val_loss: 10.8657 - val_mean_squared_error: 10.8657\n",
            "Epoch 84/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.6802 - mean_squared_error: 1.6802 - val_loss: 10.8227 - val_mean_squared_error: 10.8227\n",
            "Epoch 85/300\n",
            "11224/11224 [==============================] - 7s 602us/sample - loss: 1.8512 - mean_squared_error: 1.8512 - val_loss: 12.1353 - val_mean_squared_error: 12.1353\n",
            "Epoch 86/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 1.8726 - mean_squared_error: 1.8726 - val_loss: 11.4388 - val_mean_squared_error: 11.4388\n",
            "Epoch 87/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.7423 - mean_squared_error: 1.7423 - val_loss: 9.9396 - val_mean_squared_error: 9.9396\n",
            "Epoch 88/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 1.7357 - mean_squared_error: 1.7357 - val_loss: 11.1935 - val_mean_squared_error: 11.1935\n",
            "Epoch 89/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.6608 - mean_squared_error: 1.6608 - val_loss: 11.4153 - val_mean_squared_error: 11.4153\n",
            "Epoch 90/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 1.8831 - mean_squared_error: 1.8831 - val_loss: 10.9537 - val_mean_squared_error: 10.9537\n",
            "Epoch 91/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 1.7491 - mean_squared_error: 1.7491 - val_loss: 9.6476 - val_mean_squared_error: 9.6476\n",
            "Epoch 92/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 1.5639 - mean_squared_error: 1.5639 - val_loss: 9.5865 - val_mean_squared_error: 9.5865\n",
            "Epoch 93/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 1.6905 - mean_squared_error: 1.6905 - val_loss: 11.6873 - val_mean_squared_error: 11.6873\n",
            "Epoch 94/300\n",
            "11224/11224 [==============================] - 7s 598us/sample - loss: 1.7199 - mean_squared_error: 1.7199 - val_loss: 10.9966 - val_mean_squared_error: 10.9966\n",
            "Epoch 95/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 1.7339 - mean_squared_error: 1.7339 - val_loss: 10.5042 - val_mean_squared_error: 10.5042\n",
            "Epoch 96/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 1.7083 - mean_squared_error: 1.7083 - val_loss: 10.5614 - val_mean_squared_error: 10.5614\n",
            "Epoch 97/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 1.6525 - mean_squared_error: 1.6525 - val_loss: 11.9250 - val_mean_squared_error: 11.9250\n",
            "Epoch 98/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 1.6536 - mean_squared_error: 1.6536 - val_loss: 11.0450 - val_mean_squared_error: 11.0450\n",
            "Epoch 99/300\n",
            "11224/11224 [==============================] - 7s 606us/sample - loss: 1.6707 - mean_squared_error: 1.6707 - val_loss: 12.1873 - val_mean_squared_error: 12.1873\n",
            "Epoch 100/300\n",
            "11224/11224 [==============================] - 7s 604us/sample - loss: 1.6210 - mean_squared_error: 1.6210 - val_loss: 9.9100 - val_mean_squared_error: 9.9100\n",
            "Epoch 101/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.6722 - mean_squared_error: 1.6722 - val_loss: 9.7796 - val_mean_squared_error: 9.7796\n",
            "Epoch 102/300\n",
            "11224/11224 [==============================] - 7s 601us/sample - loss: 1.6132 - mean_squared_error: 1.6132 - val_loss: 10.4941 - val_mean_squared_error: 10.4941\n",
            "Epoch 103/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 1.5222 - mean_squared_error: 1.5222 - val_loss: 9.3724 - val_mean_squared_error: 9.3724\n",
            "Epoch 104/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.5836 - mean_squared_error: 1.5836 - val_loss: 11.1845 - val_mean_squared_error: 11.1845\n",
            "Epoch 105/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.6296 - mean_squared_error: 1.6296 - val_loss: 9.7118 - val_mean_squared_error: 9.7118\n",
            "Epoch 106/300\n",
            "11224/11224 [==============================] - 7s 602us/sample - loss: 1.6491 - mean_squared_error: 1.6491 - val_loss: 11.3507 - val_mean_squared_error: 11.3507\n",
            "Epoch 107/300\n",
            "11224/11224 [==============================] - 7s 612us/sample - loss: 1.5178 - mean_squared_error: 1.5178 - val_loss: 10.5453 - val_mean_squared_error: 10.5453\n",
            "Epoch 108/300\n",
            "11224/11224 [==============================] - 7s 601us/sample - loss: 1.6014 - mean_squared_error: 1.6014 - val_loss: 12.0440 - val_mean_squared_error: 12.0440\n",
            "Epoch 109/300\n",
            "11224/11224 [==============================] - 7s 597us/sample - loss: 1.5330 - mean_squared_error: 1.5330 - val_loss: 10.0148 - val_mean_squared_error: 10.0148\n",
            "Epoch 110/300\n",
            "11224/11224 [==============================] - 7s 596us/sample - loss: 1.4666 - mean_squared_error: 1.4666 - val_loss: 11.0279 - val_mean_squared_error: 11.0279\n",
            "Epoch 111/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 1.6252 - mean_squared_error: 1.6252 - val_loss: 10.5352 - val_mean_squared_error: 10.5352\n",
            "Epoch 112/300\n",
            "11224/11224 [==============================] - 7s 611us/sample - loss: 1.4610 - mean_squared_error: 1.4610 - val_loss: 9.5276 - val_mean_squared_error: 9.5276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNnic9ZXmXMZ",
        "colab_type": "text"
      },
      "source": [
        "## Collect the Specialist Epoch Data\n",
        "\n",
        "Once all the specialized training models were trained, the output data from each was combined into a single pickle file. This was useful when comparing relative accuracies and runtimes for models for the different keypoints. This comparison is completed with the neural_net_analysis and Bokeh App in the team's GitHub repository.\n",
        "\n",
        "Once the 15 models were all trained, predictions on the test images were made using all of these models (see submission_notebook). Using no additional optimization over the combined model, the test predictions from our specialized model saw a reduction in RMSE from 2.87 to 2.18 which would put the team in 23rd place on the Kaggle Public Leaderboard and a reduction from 2.76 to 1.78 on the private score which would have been 12th place on the Kaggle Private Leaderboard (acknowledging that the leaderboard has been frozen since the competition closed). This benefit was huge and totally worth the hassle of training 15 models! The next question is, \"Can we continue to improve?\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFQq6Lk4mXRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combine_spec_data():\n",
        "  # Create a path to the Google Drive location of the output\n",
        "  output_path = drive_path+\"OutputData/\"\n",
        "  \n",
        "  # Initialize an empty dataframe\n",
        "  spec_df = pd.DataFrame()\n",
        "  \n",
        "  # Loop over the 15 keypoints to bring in all of the specialist training data\n",
        "  for keypoint in df.columns[:-1:2]:\n",
        "    \n",
        "    # Read the individual keypoint model data from the pkl file\n",
        "    temp_df = pd.read_pickle(output_path+\"spec_{}.pkl\".format(keypoint[:-2]))\n",
        "\n",
        "    # Concatenate the specialist dataframe to the combined dataframe\n",
        "    spec_df = pd.concat([spec_df, temp_df])\n",
        "  \n",
        "  # Return the complete dataframe\n",
        "  return spec_df\n",
        "    \n",
        "  #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpQ7XsyOoHPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute the function above\n",
        "spec_df = combine_spec_data()\n",
        "\n",
        "# Write the combined dataframe to a pickle file\n",
        "spec_df.to_pickle(drive_path+\"OutputData/spec_01.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3cvrhFWpdiD",
        "colab_type": "code",
        "outputId": "0543b54d-4f7e-4de2-d53d-1e5925a3a08a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print(spec_df.columns)\n",
        "print(spec_df.groupby('keypoint').val_RMSE.min().sort_values(ascending=False))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['loss', 'mean_squared_error', 'val_loss', 'val_mean_squared_error',\n",
            "       'epoch', 'RMSE', 'val_RMSE', 'times', 'starting_filter', 'layers',\n",
            "       'pooling', 'fc_layer1', 'fc_layer2', 'activation', 'optimizer', 'lrate',\n",
            "       'dropout_initial', 'dropout_step', 'batch_norm', 'bias', 'stride',\n",
            "       'flipped', 'keypoint'],\n",
            "      dtype='object')\n",
            "keypoint\n",
            "mouth_center_bottom_lip    2.896987\n",
            "nose_tip                   2.649517\n",
            "left_eye_center            2.039986\n",
            "right_eye_center           1.940586\n",
            "left_eyebrow_outer_end     1.862300\n",
            "right_eyebrow_outer_end    1.716092\n",
            "right_eyebrow_inner_end    1.638469\n",
            "mouth_right_corner         1.542316\n",
            "left_eyebrow_inner_end     1.510707\n",
            "mouth_center_top_lip       1.481612\n",
            "mouth_left_corner          1.422979\n",
            "right_eye_outer_corner     1.396080\n",
            "left_eye_outer_corner      1.303956\n",
            "right_eye_inner_corner     1.101775\n",
            "left_eye_inner_corner      1.048386\n",
            "Name: val_RMSE, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtHBHbrlsfh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, group in spec_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlFI5h9n3Fpg",
        "colab_type": "text"
      },
      "source": [
        "## Improved Specialists\n",
        "Our specialist models showed significant improvement over our combined CNN. However, we had higher than desired validation errors for some keypoints. Interestingly enough, the keypoints with the highest errors had the largest set of available training data. Let's see if we can improve the performance of these few keypoints (listed below) through tuning of only these models. The list below is the ranked (worst to best) specialist model RMSE values. We will prioritize model improvements relative to this list.\n",
        "\n",
        "1.   mouth_center_bottom_lip 2.90 \n",
        "2.   nose_tip  2.65\n",
        "3.   left_eye_center 2.04\n",
        "4.   right_eye_center           1.94\n",
        "5.   left_eyebrow_outer_end     1.86\n",
        "6.   right_eyebrow_outer_end    1.72\n",
        "7.   right_eyebrow_inner_end    1.64\n",
        "8.   mouth_right_corner         1.54\n",
        "9. left_eyebrow_inner_end     1.51\n",
        "10. mouth_center_top_lip       1.48\n",
        "11. mouth_left_corner          1.42\n",
        "12. right_eye_outer_corner     1.40\n",
        "13. left_eye_outer_corner      1.30\n",
        "14. right_eye_inner_corner     1.10\n",
        "15. left_eye_inner_corner      1.05\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j39RU4Y33F8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_improved_specialists(df, keypoint, sf=12, doi=0.0, dos=0.1, lrf=10, fc1=500, fc2=500):\n",
        "  # Use an early stopping callback and our timing callback\n",
        "  early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                                patience=100, mode='auto', restore_best_weights = True)\n",
        "  time_callback = TimeHistory()\n",
        "  \n",
        "  \n",
        "  # Use the adam optimizer with the default learning rate\n",
        "  adam = optimizers.Adam(lr=0.001*lrf, beta_1=0.9, beta_2=0.999)\n",
        "    \n",
        "  # Create and Compile the model\n",
        "  model = create_spec_bn_cnn_model(sf, doi, dos, fc1=fc1, fc2=fc2)\n",
        "  model.compile(\n",
        "      optimizer=adam,\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['mean_squared_error'])\n",
        "  \n",
        "  \n",
        "  # Subset the image/keypoint dataframe to just have the training locations for \n",
        "  # the keypoints of interest\n",
        "  df_keypoint = df[[keypoint + '_x',keypoint+'_y', 'Image']]\n",
        "  \n",
        "  # Print the remaining columns as a QA check\n",
        "  print(df_keypoint.columns)\n",
        "  \n",
        "  # Drop the NA values\n",
        "  df_keypoint = df_keypoint.dropna().reset_index(drop=True)\n",
        "  \n",
        "  # Grab the last column - that is our image data for X matrix\n",
        "  train_X = df_keypoint.iloc[:, -1]\n",
        "\n",
        "  # Convert from a series of arrays to an NDarray\n",
        "  train_X = np.array([x.reshape(96,96,1) for x in train_X])\n",
        " \n",
        "  \n",
        "  # Get the Y data for just the specified keypoint\n",
        "  train_y = np.array(df_keypoint.iloc[:,:-1])\n",
        " \n",
        "  history = model.fit(\n",
        "      train_X.astype(np.float32), train_y.astype(np.float32),\n",
        "      epochs=300,\n",
        "      validation_split=0.20,\n",
        "      callbacks=[time_callback, early_stop])\n",
        "  times = time_callback.times\n",
        "\n",
        "  # Convert to dataframe\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "  hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "  hist['times'] = times\n",
        "  hist['starting_filter'] = sf\n",
        "  hist['layers'] = 3\n",
        "  hist['pooling'] = 'yes'\n",
        "  hist['fc_layer1'] = fc1\n",
        "  hist['fc_layer2'] = fc2\n",
        "  hist['activation'] = 'relu'\n",
        "  hist['optimizer'] = 'adam'\n",
        "  hist['lrate'] = adam.get_config()['learning_rate']\n",
        "  hist['dropout_initial'] = doi\n",
        "  hist['dropout_step'] = dos\n",
        "  hist['batch_norm'] = 1\n",
        "  hist['bias'] = 0\n",
        "  hist['stride'] = 1\n",
        "  hist['flipped'] = 1.0\n",
        "  hist['keypoint'] = keypoint\n",
        "\n",
        "  # Re-pickle after every model to retain progress\n",
        "  hist.to_pickle(drive_path+\"OutputData/cnn_3l_spec_{}_d{}_s{}_sf{}_lrf{}_flipped_100.pkl\".format(keypoint, doi, dos, sf, lrf))\n",
        "\n",
        "  # Save models.\n",
        "  filename = \"cnn_spec_{}_d{}_s{}_sf{}_lrfactor{}_flipped_100\".format(keypoint, doi, dos, sf, lrf)\n",
        "  model.save(drive_path+\"Models/\"+filename+\".h5\")\n",
        "  return hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk7vMAwJ6eNr",
        "colab_type": "text"
      },
      "source": [
        "## Train some improved models (hopefully)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crED-6Ts6d48",
        "colab_type": "code",
        "outputId": "80179639-b6ae-40e7-8ac0-7cbb97b3de54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "improved_specialist_df = train_improved_specialists(df, 'mouth_center_bottom_lip', sf=16, doi=0.10, dos=0.10, lrf=10, fc1=100, fc2=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 94, 94, 4)         36        \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 94, 94, 4)         16        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 47, 47, 4)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 47, 47, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 46, 46, 8)         128       \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 46, 46, 8)         32        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 23, 23, 8)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 23, 23, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 22, 22, 16)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1936)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                96850     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "Specialist (Dense)           (None, 2)                 102       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 100,690\n",
            "Trainable params: 100,434\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Index(['mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y', 'Image'], dtype='object')\n",
            "Train on 11224 samples, validate on 2806 samples\n",
            "Epoch 1/300\n",
            "11224/11224 [==============================] - 7s 657us/sample - loss: 438.9785 - mean_squared_error: 438.9786 - val_loss: 27.5374 - val_mean_squared_error: 27.5374\n",
            "Epoch 2/300\n",
            "11224/11224 [==============================] - 6s 562us/sample - loss: 29.1563 - mean_squared_error: 29.1563 - val_loss: 32.0204 - val_mean_squared_error: 32.0204\n",
            "Epoch 3/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 23.7582 - mean_squared_error: 23.7582 - val_loss: 35.1630 - val_mean_squared_error: 35.1630\n",
            "Epoch 4/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 18.8875 - mean_squared_error: 18.8875 - val_loss: 19.1946 - val_mean_squared_error: 19.1946\n",
            "Epoch 5/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 15.7735 - mean_squared_error: 15.7734 - val_loss: 16.2234 - val_mean_squared_error: 16.2234\n",
            "Epoch 6/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 13.8829 - mean_squared_error: 13.8829 - val_loss: 12.1693 - val_mean_squared_error: 12.1693\n",
            "Epoch 7/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 13.1520 - mean_squared_error: 13.1520 - val_loss: 12.4870 - val_mean_squared_error: 12.4870\n",
            "Epoch 8/300\n",
            "11224/11224 [==============================] - 6s 555us/sample - loss: 12.1919 - mean_squared_error: 12.1919 - val_loss: 12.4375 - val_mean_squared_error: 12.4375\n",
            "Epoch 9/300\n",
            "11224/11224 [==============================] - 6s 556us/sample - loss: 11.8714 - mean_squared_error: 11.8714 - val_loss: 11.0968 - val_mean_squared_error: 11.0968\n",
            "Epoch 10/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 11.4733 - mean_squared_error: 11.4733 - val_loss: 12.1071 - val_mean_squared_error: 12.1072\n",
            "Epoch 11/300\n",
            "11224/11224 [==============================] - 6s 572us/sample - loss: 11.3535 - mean_squared_error: 11.3535 - val_loss: 9.5859 - val_mean_squared_error: 9.5859\n",
            "Epoch 12/300\n",
            "11224/11224 [==============================] - 7s 586us/sample - loss: 10.7507 - mean_squared_error: 10.7507 - val_loss: 10.7390 - val_mean_squared_error: 10.7390\n",
            "Epoch 13/300\n",
            "11224/11224 [==============================] - 7s 611us/sample - loss: 10.4079 - mean_squared_error: 10.4079 - val_loss: 9.4820 - val_mean_squared_error: 9.4820\n",
            "Epoch 14/300\n",
            "11224/11224 [==============================] - 7s 595us/sample - loss: 10.1812 - mean_squared_error: 10.1812 - val_loss: 10.2504 - val_mean_squared_error: 10.2504\n",
            "Epoch 15/300\n",
            "11224/11224 [==============================] - 6s 570us/sample - loss: 10.1992 - mean_squared_error: 10.1992 - val_loss: 11.5275 - val_mean_squared_error: 11.5275\n",
            "Epoch 16/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 9.9497 - mean_squared_error: 9.9497 - val_loss: 9.2354 - val_mean_squared_error: 9.2354\n",
            "Epoch 17/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 9.6138 - mean_squared_error: 9.6138 - val_loss: 9.9306 - val_mean_squared_error: 9.9306\n",
            "Epoch 18/300\n",
            "11224/11224 [==============================] - 6s 574us/sample - loss: 9.5203 - mean_squared_error: 9.5203 - val_loss: 10.9124 - val_mean_squared_error: 10.9124\n",
            "Epoch 19/300\n",
            "11224/11224 [==============================] - 6s 579us/sample - loss: 9.4510 - mean_squared_error: 9.4510 - val_loss: 12.1238 - val_mean_squared_error: 12.1238\n",
            "Epoch 20/300\n",
            "11224/11224 [==============================] - 6s 570us/sample - loss: 9.6123 - mean_squared_error: 9.6123 - val_loss: 8.1126 - val_mean_squared_error: 8.1126\n",
            "Epoch 21/300\n",
            "11224/11224 [==============================] - 6s 573us/sample - loss: 9.4743 - mean_squared_error: 9.4743 - val_loss: 10.6267 - val_mean_squared_error: 10.6267\n",
            "Epoch 22/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 9.0694 - mean_squared_error: 9.0694 - val_loss: 10.8385 - val_mean_squared_error: 10.8385\n",
            "Epoch 23/300\n",
            "11224/11224 [==============================] - 6s 551us/sample - loss: 9.1306 - mean_squared_error: 9.1306 - val_loss: 12.1282 - val_mean_squared_error: 12.1282\n",
            "Epoch 24/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 8.7344 - mean_squared_error: 8.7344 - val_loss: 9.6550 - val_mean_squared_error: 9.6550\n",
            "Epoch 25/300\n",
            "11224/11224 [==============================] - 6s 542us/sample - loss: 8.9102 - mean_squared_error: 8.9102 - val_loss: 9.6339 - val_mean_squared_error: 9.6339\n",
            "Epoch 26/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 8.8678 - mean_squared_error: 8.8678 - val_loss: 9.4497 - val_mean_squared_error: 9.4497\n",
            "Epoch 27/300\n",
            "11224/11224 [==============================] - 7s 585us/sample - loss: 8.9774 - mean_squared_error: 8.9774 - val_loss: 9.2100 - val_mean_squared_error: 9.2100\n",
            "Epoch 28/300\n",
            "11224/11224 [==============================] - 6s 566us/sample - loss: 8.6907 - mean_squared_error: 8.6907 - val_loss: 9.5336 - val_mean_squared_error: 9.5336\n",
            "Epoch 29/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 8.6060 - mean_squared_error: 8.6060 - val_loss: 8.5604 - val_mean_squared_error: 8.5604\n",
            "Epoch 30/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 8.6601 - mean_squared_error: 8.6601 - val_loss: 9.3492 - val_mean_squared_error: 9.3492\n",
            "Epoch 31/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 8.4763 - mean_squared_error: 8.4763 - val_loss: 9.9081 - val_mean_squared_error: 9.9081\n",
            "Epoch 32/300\n",
            "11224/11224 [==============================] - 6s 548us/sample - loss: 8.6429 - mean_squared_error: 8.6429 - val_loss: 9.0076 - val_mean_squared_error: 9.0076\n",
            "Epoch 33/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 8.7764 - mean_squared_error: 8.7764 - val_loss: 8.9297 - val_mean_squared_error: 8.9297\n",
            "Epoch 34/300\n",
            "11224/11224 [==============================] - 6s 544us/sample - loss: 8.3981 - mean_squared_error: 8.3981 - val_loss: 10.0273 - val_mean_squared_error: 10.0273\n",
            "Epoch 35/300\n",
            "11224/11224 [==============================] - 6s 572us/sample - loss: 8.0797 - mean_squared_error: 8.0797 - val_loss: 8.4146 - val_mean_squared_error: 8.4146\n",
            "Epoch 36/300\n",
            "11224/11224 [==============================] - 7s 592us/sample - loss: 8.1594 - mean_squared_error: 8.1594 - val_loss: 8.1155 - val_mean_squared_error: 8.1155\n",
            "Epoch 37/300\n",
            "11224/11224 [==============================] - 6s 579us/sample - loss: 8.2703 - mean_squared_error: 8.2703 - val_loss: 8.8129 - val_mean_squared_error: 8.8129\n",
            "Epoch 38/300\n",
            "11224/11224 [==============================] - 6s 576us/sample - loss: 7.9256 - mean_squared_error: 7.9256 - val_loss: 9.4224 - val_mean_squared_error: 9.4224\n",
            "Epoch 39/300\n",
            "11224/11224 [==============================] - 6s 562us/sample - loss: 8.1254 - mean_squared_error: 8.1254 - val_loss: 9.1780 - val_mean_squared_error: 9.1780\n",
            "Epoch 40/300\n",
            "11224/11224 [==============================] - 6s 563us/sample - loss: 7.9655 - mean_squared_error: 7.9655 - val_loss: 8.2614 - val_mean_squared_error: 8.2614\n",
            "Epoch 41/300\n",
            "11224/11224 [==============================] - 6s 572us/sample - loss: 8.0925 - mean_squared_error: 8.0925 - val_loss: 10.4832 - val_mean_squared_error: 10.4832\n",
            "Epoch 42/300\n",
            "11224/11224 [==============================] - 6s 567us/sample - loss: 7.8796 - mean_squared_error: 7.8796 - val_loss: 10.5516 - val_mean_squared_error: 10.5516\n",
            "Epoch 43/300\n",
            "11224/11224 [==============================] - 6s 561us/sample - loss: 7.9978 - mean_squared_error: 7.9978 - val_loss: 9.7201 - val_mean_squared_error: 9.7201\n",
            "Epoch 44/300\n",
            "11224/11224 [==============================] - 6s 563us/sample - loss: 8.1364 - mean_squared_error: 8.1364 - val_loss: 9.3578 - val_mean_squared_error: 9.3578\n",
            "Epoch 45/300\n",
            "11224/11224 [==============================] - 6s 572us/sample - loss: 8.0065 - mean_squared_error: 8.0065 - val_loss: 9.4041 - val_mean_squared_error: 9.4041\n",
            "Epoch 46/300\n",
            "11224/11224 [==============================] - 6s 573us/sample - loss: 7.6971 - mean_squared_error: 7.6971 - val_loss: 9.8468 - val_mean_squared_error: 9.8468\n",
            "Epoch 47/300\n",
            "11224/11224 [==============================] - 6s 571us/sample - loss: 7.8681 - mean_squared_error: 7.8681 - val_loss: 8.8480 - val_mean_squared_error: 8.8480\n",
            "Epoch 48/300\n",
            "11224/11224 [==============================] - 6s 567us/sample - loss: 7.7933 - mean_squared_error: 7.7933 - val_loss: 11.4950 - val_mean_squared_error: 11.4950\n",
            "Epoch 49/300\n",
            "11224/11224 [==============================] - 6s 551us/sample - loss: 7.7576 - mean_squared_error: 7.7576 - val_loss: 10.2658 - val_mean_squared_error: 10.2658\n",
            "Epoch 50/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 7.8973 - mean_squared_error: 7.8973 - val_loss: 8.3836 - val_mean_squared_error: 8.3836\n",
            "Epoch 51/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 7.6256 - mean_squared_error: 7.6256 - val_loss: 8.4850 - val_mean_squared_error: 8.4850\n",
            "Epoch 52/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 7.4258 - mean_squared_error: 7.4258 - val_loss: 8.2751 - val_mean_squared_error: 8.2751\n",
            "Epoch 53/300\n",
            "11224/11224 [==============================] - 6s 555us/sample - loss: 7.5977 - mean_squared_error: 7.5977 - val_loss: 8.8407 - val_mean_squared_error: 8.8407\n",
            "Epoch 54/300\n",
            "11224/11224 [==============================] - 6s 557us/sample - loss: 7.5362 - mean_squared_error: 7.5362 - val_loss: 8.2792 - val_mean_squared_error: 8.2792\n",
            "Epoch 55/300\n",
            "11224/11224 [==============================] - 6s 548us/sample - loss: 7.7151 - mean_squared_error: 7.7151 - val_loss: 8.8584 - val_mean_squared_error: 8.8584\n",
            "Epoch 56/300\n",
            "11224/11224 [==============================] - 6s 555us/sample - loss: 7.4554 - mean_squared_error: 7.4554 - val_loss: 8.3963 - val_mean_squared_error: 8.3963\n",
            "Epoch 57/300\n",
            "11224/11224 [==============================] - 6s 546us/sample - loss: 7.6237 - mean_squared_error: 7.6237 - val_loss: 8.7418 - val_mean_squared_error: 8.7418\n",
            "Epoch 58/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 7.3576 - mean_squared_error: 7.3576 - val_loss: 9.4310 - val_mean_squared_error: 9.4310\n",
            "Epoch 59/300\n",
            "11224/11224 [==============================] - 6s 579us/sample - loss: 7.5236 - mean_squared_error: 7.5236 - val_loss: 9.1627 - val_mean_squared_error: 9.1627\n",
            "Epoch 60/300\n",
            "11224/11224 [==============================] - 6s 566us/sample - loss: 7.4811 - mean_squared_error: 7.4811 - val_loss: 8.1114 - val_mean_squared_error: 8.1114\n",
            "Epoch 61/300\n",
            "11224/11224 [==============================] - 6s 555us/sample - loss: 7.3487 - mean_squared_error: 7.3487 - val_loss: 9.5425 - val_mean_squared_error: 9.5425\n",
            "Epoch 62/300\n",
            "11224/11224 [==============================] - 6s 566us/sample - loss: 7.4883 - mean_squared_error: 7.4883 - val_loss: 8.5798 - val_mean_squared_error: 8.5798\n",
            "Epoch 63/300\n",
            "11224/11224 [==============================] - 6s 561us/sample - loss: 7.3754 - mean_squared_error: 7.3754 - val_loss: 9.1504 - val_mean_squared_error: 9.1504\n",
            "Epoch 64/300\n",
            "11224/11224 [==============================] - 6s 552us/sample - loss: 7.3732 - mean_squared_error: 7.3732 - val_loss: 9.3743 - val_mean_squared_error: 9.3743\n",
            "Epoch 65/300\n",
            "11224/11224 [==============================] - 6s 545us/sample - loss: 7.3420 - mean_squared_error: 7.3420 - val_loss: 10.1818 - val_mean_squared_error: 10.1818\n",
            "Epoch 66/300\n",
            "11224/11224 [==============================] - 6s 548us/sample - loss: 7.2701 - mean_squared_error: 7.2701 - val_loss: 9.0546 - val_mean_squared_error: 9.0546\n",
            "Epoch 67/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 7.5652 - mean_squared_error: 7.5652 - val_loss: 8.9615 - val_mean_squared_error: 8.9615\n",
            "Epoch 68/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 7.1657 - mean_squared_error: 7.1657 - val_loss: 9.1250 - val_mean_squared_error: 9.1250\n",
            "Epoch 69/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 7.3790 - mean_squared_error: 7.3790 - val_loss: 9.0475 - val_mean_squared_error: 9.0475\n",
            "Epoch 70/300\n",
            "11224/11224 [==============================] - 6s 551us/sample - loss: 7.3436 - mean_squared_error: 7.3436 - val_loss: 9.6967 - val_mean_squared_error: 9.6967\n",
            "Epoch 71/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 7.1875 - mean_squared_error: 7.1875 - val_loss: 8.5076 - val_mean_squared_error: 8.5076\n",
            "Epoch 72/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 7.5803 - mean_squared_error: 7.5803 - val_loss: 8.3448 - val_mean_squared_error: 8.3448\n",
            "Epoch 73/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 7.3019 - mean_squared_error: 7.3019 - val_loss: 8.2934 - val_mean_squared_error: 8.2934\n",
            "Epoch 74/300\n",
            "11224/11224 [==============================] - 6s 564us/sample - loss: 7.3627 - mean_squared_error: 7.3627 - val_loss: 9.3077 - val_mean_squared_error: 9.3077\n",
            "Epoch 75/300\n",
            "11224/11224 [==============================] - 6s 569us/sample - loss: 7.2452 - mean_squared_error: 7.2452 - val_loss: 9.4922 - val_mean_squared_error: 9.4922\n",
            "Epoch 76/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 7.3774 - mean_squared_error: 7.3774 - val_loss: 9.2438 - val_mean_squared_error: 9.2438\n",
            "Epoch 77/300\n",
            "11224/11224 [==============================] - 6s 563us/sample - loss: 7.2540 - mean_squared_error: 7.2540 - val_loss: 9.1553 - val_mean_squared_error: 9.1553\n",
            "Epoch 78/300\n",
            "11224/11224 [==============================] - 6s 549us/sample - loss: 7.1523 - mean_squared_error: 7.1523 - val_loss: 8.6123 - val_mean_squared_error: 8.6123\n",
            "Epoch 79/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 7.3256 - mean_squared_error: 7.3256 - val_loss: 8.2878 - val_mean_squared_error: 8.2878\n",
            "Epoch 80/300\n",
            "11224/11224 [==============================] - 6s 564us/sample - loss: 7.1256 - mean_squared_error: 7.1256 - val_loss: 8.7150 - val_mean_squared_error: 8.7150\n",
            "Epoch 81/300\n",
            "11224/11224 [==============================] - 6s 568us/sample - loss: 7.1626 - mean_squared_error: 7.1626 - val_loss: 9.9042 - val_mean_squared_error: 9.9042\n",
            "Epoch 82/300\n",
            "11224/11224 [==============================] - 7s 583us/sample - loss: 6.9730 - mean_squared_error: 6.9730 - val_loss: 10.6249 - val_mean_squared_error: 10.6249\n",
            "Epoch 83/300\n",
            "11224/11224 [==============================] - 7s 600us/sample - loss: 7.0310 - mean_squared_error: 7.0310 - val_loss: 8.5471 - val_mean_squared_error: 8.5471\n",
            "Epoch 84/300\n",
            "11224/11224 [==============================] - 7s 591us/sample - loss: 7.0390 - mean_squared_error: 7.0390 - val_loss: 8.2525 - val_mean_squared_error: 8.2525\n",
            "Epoch 85/300\n",
            "11224/11224 [==============================] - 6s 575us/sample - loss: 6.9613 - mean_squared_error: 6.9613 - val_loss: 8.3097 - val_mean_squared_error: 8.3097\n",
            "Epoch 86/300\n",
            "11224/11224 [==============================] - 6s 570us/sample - loss: 7.0498 - mean_squared_error: 7.0498 - val_loss: 8.6639 - val_mean_squared_error: 8.6639\n",
            "Epoch 87/300\n",
            "11224/11224 [==============================] - 6s 569us/sample - loss: 6.9302 - mean_squared_error: 6.9302 - val_loss: 8.6986 - val_mean_squared_error: 8.6986\n",
            "Epoch 88/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 6.8118 - mean_squared_error: 6.8118 - val_loss: 8.9761 - val_mean_squared_error: 8.9761\n",
            "Epoch 89/300\n",
            "11224/11224 [==============================] - 6s 577us/sample - loss: 7.2025 - mean_squared_error: 7.2025 - val_loss: 7.8163 - val_mean_squared_error: 7.8163\n",
            "Epoch 90/300\n",
            "11224/11224 [==============================] - 6s 557us/sample - loss: 6.8898 - mean_squared_error: 6.8898 - val_loss: 10.0420 - val_mean_squared_error: 10.0420\n",
            "Epoch 91/300\n",
            "11224/11224 [==============================] - 6s 553us/sample - loss: 6.9140 - mean_squared_error: 6.9140 - val_loss: 10.9191 - val_mean_squared_error: 10.9191\n",
            "Epoch 92/300\n",
            "11224/11224 [==============================] - 6s 561us/sample - loss: 7.0483 - mean_squared_error: 7.0483 - val_loss: 8.6121 - val_mean_squared_error: 8.6121\n",
            "Epoch 93/300\n",
            "11224/11224 [==============================] - 6s 555us/sample - loss: 7.0462 - mean_squared_error: 7.0462 - val_loss: 8.5855 - val_mean_squared_error: 8.5855\n",
            "Epoch 94/300\n",
            "11224/11224 [==============================] - 6s 557us/sample - loss: 6.7451 - mean_squared_error: 6.7451 - val_loss: 9.1105 - val_mean_squared_error: 9.1105\n",
            "Epoch 95/300\n",
            "11224/11224 [==============================] - 6s 568us/sample - loss: 6.9382 - mean_squared_error: 6.9382 - val_loss: 9.1169 - val_mean_squared_error: 9.1169\n",
            "Epoch 96/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 6.6502 - mean_squared_error: 6.6502 - val_loss: 8.5890 - val_mean_squared_error: 8.5890\n",
            "Epoch 97/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 6.8331 - mean_squared_error: 6.8331 - val_loss: 9.4256 - val_mean_squared_error: 9.4256\n",
            "Epoch 98/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 6.9988 - mean_squared_error: 6.9988 - val_loss: 10.7724 - val_mean_squared_error: 10.7724\n",
            "Epoch 99/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 6.7185 - mean_squared_error: 6.7185 - val_loss: 8.4142 - val_mean_squared_error: 8.4142\n",
            "Epoch 100/300\n",
            "11224/11224 [==============================] - 6s 546us/sample - loss: 6.8159 - mean_squared_error: 6.8159 - val_loss: 8.8844 - val_mean_squared_error: 8.8844\n",
            "Epoch 101/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 6.7843 - mean_squared_error: 6.7843 - val_loss: 10.4451 - val_mean_squared_error: 10.4451\n",
            "Epoch 102/300\n",
            "11224/11224 [==============================] - 6s 542us/sample - loss: 6.9234 - mean_squared_error: 6.9234 - val_loss: 8.2903 - val_mean_squared_error: 8.2903\n",
            "Epoch 103/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 6.6836 - mean_squared_error: 6.6836 - val_loss: 8.4837 - val_mean_squared_error: 8.4837\n",
            "Epoch 104/300\n",
            "11224/11224 [==============================] - 6s 550us/sample - loss: 6.6273 - mean_squared_error: 6.6273 - val_loss: 8.4869 - val_mean_squared_error: 8.4869\n",
            "Epoch 105/300\n",
            "11224/11224 [==============================] - 6s 547us/sample - loss: 6.8819 - mean_squared_error: 6.8819 - val_loss: 9.2971 - val_mean_squared_error: 9.2971\n",
            "Epoch 106/300\n",
            "11224/11224 [==============================] - 6s 545us/sample - loss: 6.6082 - mean_squared_error: 6.6082 - val_loss: 9.9064 - val_mean_squared_error: 9.9064\n",
            "Epoch 107/300\n",
            "11224/11224 [==============================] - 6s 541us/sample - loss: 6.8380 - mean_squared_error: 6.8380 - val_loss: 10.0317 - val_mean_squared_error: 10.0317\n",
            "Epoch 108/300\n",
            "11224/11224 [==============================] - 6s 536us/sample - loss: 6.7768 - mean_squared_error: 6.7768 - val_loss: 9.0190 - val_mean_squared_error: 9.0190\n",
            "Epoch 109/300\n",
            "11224/11224 [==============================] - 6s 532us/sample - loss: 6.8190 - mean_squared_error: 6.8190 - val_loss: 8.7234 - val_mean_squared_error: 8.7234\n",
            "Epoch 110/300\n",
            "11224/11224 [==============================] - 6s 544us/sample - loss: 6.8114 - mean_squared_error: 6.8114 - val_loss: 8.8734 - val_mean_squared_error: 8.8734\n",
            "Epoch 111/300\n",
            "11224/11224 [==============================] - 6s 549us/sample - loss: 6.8585 - mean_squared_error: 6.8585 - val_loss: 8.8966 - val_mean_squared_error: 8.8966\n",
            "Epoch 112/300\n",
            "11224/11224 [==============================] - 6s 536us/sample - loss: 6.8094 - mean_squared_error: 6.8094 - val_loss: 8.7111 - val_mean_squared_error: 8.7111\n",
            "Epoch 113/300\n",
            "11224/11224 [==============================] - 6s 573us/sample - loss: 6.8509 - mean_squared_error: 6.8509 - val_loss: 9.0076 - val_mean_squared_error: 9.0076\n",
            "Epoch 114/300\n",
            "11224/11224 [==============================] - 7s 608us/sample - loss: 6.6609 - mean_squared_error: 6.6609 - val_loss: 9.0938 - val_mean_squared_error: 9.0938\n",
            "Epoch 115/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 6.8599 - mean_squared_error: 6.8599 - val_loss: 8.8708 - val_mean_squared_error: 8.8708\n",
            "Epoch 116/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 6.7310 - mean_squared_error: 6.7310 - val_loss: 8.3775 - val_mean_squared_error: 8.3775\n",
            "Epoch 117/300\n",
            "11224/11224 [==============================] - 7s 591us/sample - loss: 6.7922 - mean_squared_error: 6.7922 - val_loss: 10.9353 - val_mean_squared_error: 10.9353\n",
            "Epoch 118/300\n",
            "11224/11224 [==============================] - 6s 569us/sample - loss: 6.5850 - mean_squared_error: 6.5850 - val_loss: 8.5429 - val_mean_squared_error: 8.5429\n",
            "Epoch 119/300\n",
            "11224/11224 [==============================] - 6s 556us/sample - loss: 6.5433 - mean_squared_error: 6.5433 - val_loss: 8.3596 - val_mean_squared_error: 8.3596\n",
            "Epoch 120/300\n",
            "11224/11224 [==============================] - 6s 557us/sample - loss: 6.9858 - mean_squared_error: 6.9858 - val_loss: 8.7500 - val_mean_squared_error: 8.7500\n",
            "Epoch 121/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 6.7628 - mean_squared_error: 6.7628 - val_loss: 8.6519 - val_mean_squared_error: 8.6519\n",
            "Epoch 122/300\n",
            "11224/11224 [==============================] - 6s 554us/sample - loss: 6.6708 - mean_squared_error: 6.6708 - val_loss: 9.9031 - val_mean_squared_error: 9.9031\n",
            "Epoch 123/300\n",
            "11224/11224 [==============================] - 6s 575us/sample - loss: 6.5945 - mean_squared_error: 6.5945 - val_loss: 9.4229 - val_mean_squared_error: 9.4229\n",
            "Epoch 124/300\n",
            "11224/11224 [==============================] - 6s 574us/sample - loss: 6.6096 - mean_squared_error: 6.6096 - val_loss: 9.3869 - val_mean_squared_error: 9.3869\n",
            "Epoch 125/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 6.5741 - mean_squared_error: 6.5741 - val_loss: 8.8122 - val_mean_squared_error: 8.8122\n",
            "Epoch 126/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 6.4791 - mean_squared_error: 6.4791 - val_loss: 8.4926 - val_mean_squared_error: 8.4926\n",
            "Epoch 127/300\n",
            "11224/11224 [==============================] - 6s 550us/sample - loss: 6.8485 - mean_squared_error: 6.8485 - val_loss: 11.2326 - val_mean_squared_error: 11.2326\n",
            "Epoch 128/300\n",
            "11224/11224 [==============================] - 6s 548us/sample - loss: 6.7364 - mean_squared_error: 6.7364 - val_loss: 8.5782 - val_mean_squared_error: 8.5782\n",
            "Epoch 129/300\n",
            "11224/11224 [==============================] - 6s 547us/sample - loss: 6.5558 - mean_squared_error: 6.5558 - val_loss: 8.7228 - val_mean_squared_error: 8.7228\n",
            "Epoch 130/300\n",
            "11224/11224 [==============================] - 6s 556us/sample - loss: 6.6859 - mean_squared_error: 6.6859 - val_loss: 8.4272 - val_mean_squared_error: 8.4272\n",
            "Epoch 131/300\n",
            "11224/11224 [==============================] - 6s 556us/sample - loss: 6.5091 - mean_squared_error: 6.5091 - val_loss: 9.0712 - val_mean_squared_error: 9.0712\n",
            "Epoch 132/300\n",
            "11224/11224 [==============================] - 6s 569us/sample - loss: 6.5575 - mean_squared_error: 6.5575 - val_loss: 8.8853 - val_mean_squared_error: 8.8853\n",
            "Epoch 133/300\n",
            "11224/11224 [==============================] - 7s 607us/sample - loss: 6.6937 - mean_squared_error: 6.6937 - val_loss: 8.3492 - val_mean_squared_error: 8.3492\n",
            "Epoch 134/300\n",
            "11224/11224 [==============================] - 7s 616us/sample - loss: 6.6384 - mean_squared_error: 6.6384 - val_loss: 8.5517 - val_mean_squared_error: 8.5517\n",
            "Epoch 135/300\n",
            "11224/11224 [==============================] - 7s 630us/sample - loss: 6.6600 - mean_squared_error: 6.6600 - val_loss: 8.5363 - val_mean_squared_error: 8.5363\n",
            "Epoch 136/300\n",
            "11224/11224 [==============================] - 7s 614us/sample - loss: 6.5836 - mean_squared_error: 6.5836 - val_loss: 8.6610 - val_mean_squared_error: 8.6610\n",
            "Epoch 137/300\n",
            "11224/11224 [==============================] - 7s 586us/sample - loss: 6.5562 - mean_squared_error: 6.5562 - val_loss: 8.4029 - val_mean_squared_error: 8.4029\n",
            "Epoch 138/300\n",
            "11224/11224 [==============================] - 6s 576us/sample - loss: 6.4682 - mean_squared_error: 6.4682 - val_loss: 8.5786 - val_mean_squared_error: 8.5786\n",
            "Epoch 139/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 6.6103 - mean_squared_error: 6.6103 - val_loss: 8.6878 - val_mean_squared_error: 8.6878\n",
            "Epoch 140/300\n",
            "11224/11224 [==============================] - 7s 579us/sample - loss: 6.4204 - mean_squared_error: 6.4204 - val_loss: 9.1446 - val_mean_squared_error: 9.1446\n",
            "Epoch 141/300\n",
            "11224/11224 [==============================] - 6s 562us/sample - loss: 6.6668 - mean_squared_error: 6.6668 - val_loss: 10.5675 - val_mean_squared_error: 10.5675\n",
            "Epoch 142/300\n",
            "11224/11224 [==============================] - 6s 573us/sample - loss: 6.7543 - mean_squared_error: 6.7543 - val_loss: 8.3503 - val_mean_squared_error: 8.3503\n",
            "Epoch 143/300\n",
            "11224/11224 [==============================] - 7s 594us/sample - loss: 6.4097 - mean_squared_error: 6.4097 - val_loss: 8.7332 - val_mean_squared_error: 8.7332\n",
            "Epoch 144/300\n",
            "11224/11224 [==============================] - 7s 585us/sample - loss: 6.3592 - mean_squared_error: 6.3592 - val_loss: 8.0579 - val_mean_squared_error: 8.0579\n",
            "Epoch 145/300\n",
            "11224/11224 [==============================] - 7s 607us/sample - loss: 6.4203 - mean_squared_error: 6.4203 - val_loss: 8.1864 - val_mean_squared_error: 8.1864\n",
            "Epoch 146/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 6.4319 - mean_squared_error: 6.4319 - val_loss: 9.2459 - val_mean_squared_error: 9.2459\n",
            "Epoch 147/300\n",
            "11224/11224 [==============================] - 6s 577us/sample - loss: 6.5365 - mean_squared_error: 6.5365 - val_loss: 8.2355 - val_mean_squared_error: 8.2355\n",
            "Epoch 148/300\n",
            "11224/11224 [==============================] - 6s 577us/sample - loss: 6.5576 - mean_squared_error: 6.5576 - val_loss: 8.6411 - val_mean_squared_error: 8.6411\n",
            "Epoch 149/300\n",
            "11224/11224 [==============================] - 7s 586us/sample - loss: 6.3899 - mean_squared_error: 6.3899 - val_loss: 8.5265 - val_mean_squared_error: 8.5265\n",
            "Epoch 150/300\n",
            "11224/11224 [==============================] - 6s 566us/sample - loss: 6.4235 - mean_squared_error: 6.4235 - val_loss: 13.3596 - val_mean_squared_error: 13.3596\n",
            "Epoch 151/300\n",
            "11224/11224 [==============================] - 6s 573us/sample - loss: 6.4748 - mean_squared_error: 6.4748 - val_loss: 8.4661 - val_mean_squared_error: 8.4661\n",
            "Epoch 152/300\n",
            "11224/11224 [==============================] - 6s 555us/sample - loss: 6.2563 - mean_squared_error: 6.2563 - val_loss: 9.8821 - val_mean_squared_error: 9.8821\n",
            "Epoch 153/300\n",
            "11224/11224 [==============================] - 6s 556us/sample - loss: 6.4367 - mean_squared_error: 6.4367 - val_loss: 9.5723 - val_mean_squared_error: 9.5723\n",
            "Epoch 154/300\n",
            "11224/11224 [==============================] - 6s 562us/sample - loss: 6.3567 - mean_squared_error: 6.3567 - val_loss: 8.7219 - val_mean_squared_error: 8.7219\n",
            "Epoch 155/300\n",
            "11224/11224 [==============================] - 6s 557us/sample - loss: 6.4862 - mean_squared_error: 6.4862 - val_loss: 8.6261 - val_mean_squared_error: 8.6261\n",
            "Epoch 156/300\n",
            "11224/11224 [==============================] - 6s 547us/sample - loss: 6.3709 - mean_squared_error: 6.3709 - val_loss: 9.0639 - val_mean_squared_error: 9.0639\n",
            "Epoch 157/300\n",
            "11224/11224 [==============================] - 6s 549us/sample - loss: 6.4303 - mean_squared_error: 6.4303 - val_loss: 8.7495 - val_mean_squared_error: 8.7495\n",
            "Epoch 158/300\n",
            "11224/11224 [==============================] - 6s 556us/sample - loss: 6.5035 - mean_squared_error: 6.5035 - val_loss: 10.6513 - val_mean_squared_error: 10.6513\n",
            "Epoch 159/300\n",
            "11224/11224 [==============================] - 6s 543us/sample - loss: 6.3415 - mean_squared_error: 6.3415 - val_loss: 9.0323 - val_mean_squared_error: 9.0323\n",
            "Epoch 160/300\n",
            "11224/11224 [==============================] - 6s 544us/sample - loss: 6.3959 - mean_squared_error: 6.3959 - val_loss: 9.2187 - val_mean_squared_error: 9.2187\n",
            "Epoch 161/300\n",
            "11224/11224 [==============================] - 6s 545us/sample - loss: 6.3260 - mean_squared_error: 6.3260 - val_loss: 8.6863 - val_mean_squared_error: 8.6863\n",
            "Epoch 162/300\n",
            "11224/11224 [==============================] - 6s 550us/sample - loss: 6.3582 - mean_squared_error: 6.3582 - val_loss: 9.0109 - val_mean_squared_error: 9.0109\n",
            "Epoch 163/300\n",
            "11224/11224 [==============================] - 6s 546us/sample - loss: 6.4380 - mean_squared_error: 6.4380 - val_loss: 8.8003 - val_mean_squared_error: 8.8003\n",
            "Epoch 164/300\n",
            "11224/11224 [==============================] - 6s 545us/sample - loss: 6.3565 - mean_squared_error: 6.3565 - val_loss: 9.0015 - val_mean_squared_error: 9.0015\n",
            "Epoch 165/300\n",
            "11224/11224 [==============================] - 6s 546us/sample - loss: 6.4185 - mean_squared_error: 6.4185 - val_loss: 9.5119 - val_mean_squared_error: 9.5119\n",
            "Epoch 166/300\n",
            "11224/11224 [==============================] - 6s 534us/sample - loss: 6.3335 - mean_squared_error: 6.3335 - val_loss: 8.9621 - val_mean_squared_error: 8.9621\n",
            "Epoch 167/300\n",
            "11224/11224 [==============================] - 6s 548us/sample - loss: 6.4126 - mean_squared_error: 6.4126 - val_loss: 10.7404 - val_mean_squared_error: 10.7404\n",
            "Epoch 168/300\n",
            "11224/11224 [==============================] - 6s 538us/sample - loss: 6.4032 - mean_squared_error: 6.4032 - val_loss: 8.7105 - val_mean_squared_error: 8.7105\n",
            "Epoch 169/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 6.3268 - mean_squared_error: 6.3268 - val_loss: 9.0662 - val_mean_squared_error: 9.0662\n",
            "Epoch 170/300\n",
            "11224/11224 [==============================] - 6s 542us/sample - loss: 6.4812 - mean_squared_error: 6.4812 - val_loss: 8.6048 - val_mean_squared_error: 8.6048\n",
            "Epoch 171/300\n",
            "11224/11224 [==============================] - 6s 549us/sample - loss: 6.3874 - mean_squared_error: 6.3874 - val_loss: 8.7327 - val_mean_squared_error: 8.7327\n",
            "Epoch 172/300\n",
            "11224/11224 [==============================] - 7s 587us/sample - loss: 6.4164 - mean_squared_error: 6.4164 - val_loss: 8.8576 - val_mean_squared_error: 8.8576\n",
            "Epoch 173/300\n",
            "11224/11224 [==============================] - 7s 582us/sample - loss: 6.4281 - mean_squared_error: 6.4281 - val_loss: 9.1770 - val_mean_squared_error: 9.1770\n",
            "Epoch 174/300\n",
            "11224/11224 [==============================] - 6s 560us/sample - loss: 6.2404 - mean_squared_error: 6.2404 - val_loss: 8.8227 - val_mean_squared_error: 8.8227\n",
            "Epoch 175/300\n",
            "11224/11224 [==============================] - 6s 565us/sample - loss: 6.3494 - mean_squared_error: 6.3494 - val_loss: 9.1181 - val_mean_squared_error: 9.1181\n",
            "Epoch 176/300\n",
            "11224/11224 [==============================] - 6s 566us/sample - loss: 6.2991 - mean_squared_error: 6.2991 - val_loss: 9.2921 - val_mean_squared_error: 9.2921\n",
            "Epoch 177/300\n",
            "11224/11224 [==============================] - 6s 576us/sample - loss: 6.3588 - mean_squared_error: 6.3588 - val_loss: 9.4609 - val_mean_squared_error: 9.4609\n",
            "Epoch 178/300\n",
            "11224/11224 [==============================] - 6s 568us/sample - loss: 6.4024 - mean_squared_error: 6.4024 - val_loss: 10.8043 - val_mean_squared_error: 10.8043\n",
            "Epoch 179/300\n",
            "11224/11224 [==============================] - 6s 564us/sample - loss: 6.3508 - mean_squared_error: 6.3508 - val_loss: 9.5929 - val_mean_squared_error: 9.5929\n",
            "Epoch 180/300\n",
            "11224/11224 [==============================] - 6s 575us/sample - loss: 6.4084 - mean_squared_error: 6.4084 - val_loss: 8.6909 - val_mean_squared_error: 8.6909\n",
            "Epoch 181/300\n",
            "11224/11224 [==============================] - 6s 574us/sample - loss: 6.0515 - mean_squared_error: 6.0515 - val_loss: 8.8053 - val_mean_squared_error: 8.8053\n",
            "Epoch 182/300\n",
            "11224/11224 [==============================] - 6s 561us/sample - loss: 6.2735 - mean_squared_error: 6.2735 - val_loss: 9.0267 - val_mean_squared_error: 9.0267\n",
            "Epoch 183/300\n",
            "11224/11224 [==============================] - 6s 568us/sample - loss: 6.2406 - mean_squared_error: 6.2406 - val_loss: 9.7252 - val_mean_squared_error: 9.7252\n",
            "Epoch 184/300\n",
            "11224/11224 [==============================] - 6s 559us/sample - loss: 6.3768 - mean_squared_error: 6.3768 - val_loss: 9.7331 - val_mean_squared_error: 9.7331\n",
            "Epoch 185/300\n",
            "11224/11224 [==============================] - 6s 568us/sample - loss: 6.3558 - mean_squared_error: 6.3558 - val_loss: 9.2764 - val_mean_squared_error: 9.2764\n",
            "Epoch 186/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 6.2013 - mean_squared_error: 6.2013 - val_loss: 8.8600 - val_mean_squared_error: 8.8600\n",
            "Epoch 187/300\n",
            "11224/11224 [==============================] - 6s 562us/sample - loss: 6.3377 - mean_squared_error: 6.3377 - val_loss: 9.0824 - val_mean_squared_error: 9.0824\n",
            "Epoch 188/300\n",
            "11224/11224 [==============================] - 6s 558us/sample - loss: 6.3259 - mean_squared_error: 6.3259 - val_loss: 8.6587 - val_mean_squared_error: 8.6587\n",
            "Epoch 189/300\n",
            "11224/11224 [==============================] - 7s 599us/sample - loss: 6.0810 - mean_squared_error: 6.0810 - val_loss: 9.2363 - val_mean_squared_error: 9.2363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTya_ZwmQ4Uj",
        "colab_type": "text"
      },
      "source": [
        "## Try a different model architecture\n",
        "\n",
        "We didn't have much success improving our mouth_center_bottom_lip predictions using our existing network structure. We decided to go deeper by adding an additional convolution and pooling layer, but still applying the same increasing filter depth and dropout rate strategy from before. We found that with this deeper network we showed good validation results by increasing starting filter depth. This would tend to increase the number of parameters we are training and lead to additional overfitting. We counteracted this by reducing the hidden units in our fully connected layers from 500 to 200, and increasing our dropout rate. This is shown in the image below. Starting on the left the model is simpler with only a starting filter depth of 4 (increasing each layer) and on the far right we start with as high of a filter depth as 24. Comparing the first three subplots in the first row, one can see how adding filter depth increases our overfitting quite dramatically. But with the use of a hefty dropout rate (up to 65% in some layers) we can reduce the overfitting *and* improve our validation score. Obviously the improvements don't look huge, but we are looking for every little bit! The unfortunate and obvious downside to the increased filter depth and model size is the training time. We have basically doubled our training time between when comparing the model on the left with the fourth model (our final specialized model for the nose_tip and mouth_center_bottom_lip)\n",
        "\n",
        "![alt text](https://github.com/tomgoter/w207_finalproject/raw/master/Images/dropout_spec.png?raw=true \"Dropout Sensitivity\")\n",
        "\n",
        "\n",
        "Using this method we were able to reduce our validation RMSE scores for our worst two keypoints (i.e., mouth_center_bottom_lip and nose_tip).  The resulting validation scores for our improved models were:\n",
        "\n",
        "\n",
        "\n",
        "1.   mouth_center_bottom_lip 2.90  -->  **2.54**\n",
        "2.   nose_tip  2.65 --> **2.41**\n",
        "\n",
        "With improving only our two worst performing specialist models, we scored against our test set and showed public/private Kaggle scores of 2.09/1.71. In fact the private score was good enough to knock us into 9th place on the private leaderboard.  Not bad!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYeLVOt4Q4ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_4layer_spec_model(start_filter, d, step, fc1=500, fc2=500):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model \n",
        "    '''\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "    cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter, (2, 2), padding='same', \n",
        "                                         input_shape=(96, 96, 1),activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (3, 3), padding='same', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*4, (4, 4), padding='same', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+2*step))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*5, (5, 5), padding='same', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc1))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc2))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(2, name='Specialist'))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model\n",
        "  \n",
        "def train_improved_4layer_specialists(df, keypoint, sf=12, doi=0.0, dos=0.1, lrf=10, fc1=500, fc2=500):\n",
        "  # Use an early stopping callback and our timing callback\n",
        "  early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                                patience=100, mode='auto', restore_best_weights = True)\n",
        "  time_callback = TimeHistory()\n",
        "  \n",
        "  \n",
        "  # Use the adam optimizer with the default learning rate\n",
        "  adam = optimizers.Adam(lr=0.001*lrf, beta_1=0.9, beta_2=0.999)\n",
        "    \n",
        "  # Create and Compile the model\n",
        "  model = create_4layer_spec_model(sf, doi, dos, fc1=fc1, fc2=fc2)\n",
        "  model.compile(\n",
        "      optimizer=adam,\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['mean_squared_error'])\n",
        "  \n",
        "  \n",
        "  # Subset the image/keypoint dataframe to just have the training locations for \n",
        "  # the keypoints of interest\n",
        "  df_keypoint = df[[keypoint + '_x',keypoint+'_y', 'Image']]\n",
        "  \n",
        "  # Print the remaining columns as a QA check\n",
        "  print(df_keypoint.columns)\n",
        "  \n",
        "  # Drop the NA values\n",
        "  df_keypoint = df_keypoint.dropna().reset_index(drop=True)\n",
        "  \n",
        "  # Grab the last column - that is our image data for X matrix\n",
        "  train_X = df_keypoint.iloc[:, -1]\n",
        "\n",
        "  # Convert from a series of arrays to an NDarray\n",
        "  train_X = np.array([x.reshape(96,96,1) for x in train_X])\n",
        " \n",
        "  \n",
        "  # Get the Y data for just the specified keypoint\n",
        "  train_y = np.array(df_keypoint.iloc[:,:-1])\n",
        " \n",
        "  history = model.fit(\n",
        "      train_X.astype(np.float32), train_y.astype(np.float32),\n",
        "      epochs=300,\n",
        "      validation_split=0.20,\n",
        "      callbacks=[time_callback, early_stop])\n",
        "  times = time_callback.times\n",
        "\n",
        "  # Convert to dataframe\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "  hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "  hist['times'] = times\n",
        "  hist['starting_filter'] = sf\n",
        "  hist['layers'] = 4\n",
        "  hist['pooling'] = 'yes'\n",
        "  hist['fc_layer1'] = fc1\n",
        "  hist['fc_layer2'] = fc2\n",
        "  hist['activation'] = 'relu'\n",
        "  hist['optimizer'] = 'adam'\n",
        "  hist['lrate'] = adam.get_config()['learning_rate']\n",
        "  hist['dropout_initial'] = doi\n",
        "  hist['dropout_step'] = dos\n",
        "  hist['batch_norm'] = 1\n",
        "  hist['bias'] = 0\n",
        "  hist['stride'] = 1\n",
        "  hist['flipped'] = 1.0\n",
        "  hist['keypoint'] = keypoint\n",
        "\n",
        "  # Re-pickle after every model to retain progress\n",
        "  hist.to_pickle(drive_path+\"OutputData/4l_spec_{}_d{}_s{}_sf{}_lrf{}_fc1{}_fc2{}_kern2345.pkl\".format(keypoint, doi, dos, sf, lrf, fc1, fc2))\n",
        "\n",
        "  # Save models.\n",
        "  filename = \"cnn_4l_spec_{}_d{}_s{}_sf{}_lrfactor{}_flipped_100_fc1{}_fc2{}_kern2345\".format(keypoint, doi, dos, sf, lrf, fc1, fc2)\n",
        "  model.save(drive_path+\"Models/\"+filename+\".h5\")\n",
        "  return hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NKjeEiZR1To",
        "colab_type": "code",
        "outputId": "801f1a6c-5a5b-47b8-f879-c773075b413f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "improved_specialist_df = train_improved_4layer_specialists(df, 'right_eye_center', sf=6, doi=0.00, dos=0.17, lrf=10, fc1=200, fc2=200)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0803 12:51:07.190214 139767557695360 nn_ops.py:4224] Large dropout rate: 0.51 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0803 12:51:07.319262 139767557695360 nn_ops.py:4224] Large dropout rate: 0.51 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 96, 96, 6)         24        \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 96, 96, 6)         24        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 48, 48, 6)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 48, 48, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 12)        648       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 48, 48, 12)        48        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 24, 24, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 24)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 24, 24, 24)        96        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 24)        0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 12, 12, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 30)        18000     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 30)        120       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 30)          0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 6, 6, 30)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1080)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 200)               216200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 200)               40200     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "Specialist (Dense)           (None, 2)                 402       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 281,970\n",
            "Trainable params: 281,026\n",
            "Non-trainable params: 944\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Index(['right_eye_center_x', 'right_eye_center_y', 'Image'], dtype='object')\n",
            "Train on 11255 samples, validate on 2814 samples\n",
            "Epoch 1/300\n",
            "11255/11255 [==============================] - 7s 631us/sample - loss: 57.6686 - mean_squared_error: 57.6686 - val_loss: 12.5177 - val_mean_squared_error: 12.5177\n",
            "Epoch 2/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 12.9025 - mean_squared_error: 12.9025 - val_loss: 10.8997 - val_mean_squared_error: 10.8997\n",
            "Epoch 3/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 11.9535 - mean_squared_error: 11.9535 - val_loss: 10.3473 - val_mean_squared_error: 10.3473\n",
            "Epoch 4/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 10.9291 - mean_squared_error: 10.9291 - val_loss: 9.7213 - val_mean_squared_error: 9.7213\n",
            "Epoch 5/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 9.7473 - mean_squared_error: 9.7473 - val_loss: 8.1200 - val_mean_squared_error: 8.1200\n",
            "Epoch 6/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 8.6950 - mean_squared_error: 8.6950 - val_loss: 7.7472 - val_mean_squared_error: 7.7472\n",
            "Epoch 7/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 7.7512 - mean_squared_error: 7.7512 - val_loss: 7.7653 - val_mean_squared_error: 7.7653\n",
            "Epoch 8/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 7.3672 - mean_squared_error: 7.3672 - val_loss: 6.2554 - val_mean_squared_error: 6.2554\n",
            "Epoch 9/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 6.9197 - mean_squared_error: 6.9197 - val_loss: 6.0314 - val_mean_squared_error: 6.0314\n",
            "Epoch 10/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 6.5670 - mean_squared_error: 6.5670 - val_loss: 5.9496 - val_mean_squared_error: 5.9496\n",
            "Epoch 11/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 6.4719 - mean_squared_error: 6.4719 - val_loss: 6.0739 - val_mean_squared_error: 6.0739\n",
            "Epoch 12/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 6.0658 - mean_squared_error: 6.0658 - val_loss: 5.1404 - val_mean_squared_error: 5.1404\n",
            "Epoch 13/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 5.5122 - mean_squared_error: 5.5122 - val_loss: 5.7279 - val_mean_squared_error: 5.7279\n",
            "Epoch 14/300\n",
            "11255/11255 [==============================] - 6s 532us/sample - loss: 5.8288 - mean_squared_error: 5.8288 - val_loss: 5.1896 - val_mean_squared_error: 5.1896\n",
            "Epoch 15/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 5.4311 - mean_squared_error: 5.4311 - val_loss: 5.0095 - val_mean_squared_error: 5.0095\n",
            "Epoch 16/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 5.1971 - mean_squared_error: 5.1970 - val_loss: 5.2577 - val_mean_squared_error: 5.2577\n",
            "Epoch 17/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 5.2078 - mean_squared_error: 5.2078 - val_loss: 6.2596 - val_mean_squared_error: 6.2596\n",
            "Epoch 18/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 4.9780 - mean_squared_error: 4.9780 - val_loss: 5.1142 - val_mean_squared_error: 5.1142\n",
            "Epoch 19/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 4.9253 - mean_squared_error: 4.9253 - val_loss: 5.0111 - val_mean_squared_error: 5.0111\n",
            "Epoch 20/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 4.7234 - mean_squared_error: 4.7234 - val_loss: 4.7875 - val_mean_squared_error: 4.7875\n",
            "Epoch 21/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 4.7322 - mean_squared_error: 4.7322 - val_loss: 5.0446 - val_mean_squared_error: 5.0446\n",
            "Epoch 22/300\n",
            "11255/11255 [==============================] - 6s 511us/sample - loss: 4.7285 - mean_squared_error: 4.7285 - val_loss: 5.0980 - val_mean_squared_error: 5.0980\n",
            "Epoch 23/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 4.5043 - mean_squared_error: 4.5042 - val_loss: 5.0782 - val_mean_squared_error: 5.0782\n",
            "Epoch 24/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 4.2687 - mean_squared_error: 4.2687 - val_loss: 4.6903 - val_mean_squared_error: 4.6903\n",
            "Epoch 25/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 4.3794 - mean_squared_error: 4.3794 - val_loss: 5.3298 - val_mean_squared_error: 5.3298\n",
            "Epoch 26/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 4.4253 - mean_squared_error: 4.4253 - val_loss: 4.5522 - val_mean_squared_error: 4.5522\n",
            "Epoch 27/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 4.2120 - mean_squared_error: 4.2120 - val_loss: 4.8602 - val_mean_squared_error: 4.8602\n",
            "Epoch 28/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 4.2235 - mean_squared_error: 4.2235 - val_loss: 4.7224 - val_mean_squared_error: 4.7224\n",
            "Epoch 29/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 3.9539 - mean_squared_error: 3.9539 - val_loss: 4.6617 - val_mean_squared_error: 4.6617\n",
            "Epoch 30/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 3.9273 - mean_squared_error: 3.9273 - val_loss: 4.8068 - val_mean_squared_error: 4.8068\n",
            "Epoch 31/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 3.8313 - mean_squared_error: 3.8313 - val_loss: 4.5707 - val_mean_squared_error: 4.5707\n",
            "Epoch 32/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 3.9620 - mean_squared_error: 3.9620 - val_loss: 5.1792 - val_mean_squared_error: 5.1792\n",
            "Epoch 33/300\n",
            "11255/11255 [==============================] - 6s 513us/sample - loss: 3.8257 - mean_squared_error: 3.8257 - val_loss: 4.5185 - val_mean_squared_error: 4.5185\n",
            "Epoch 34/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 3.6065 - mean_squared_error: 3.6065 - val_loss: 5.3235 - val_mean_squared_error: 5.3235\n",
            "Epoch 35/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 3.6556 - mean_squared_error: 3.6556 - val_loss: 4.6279 - val_mean_squared_error: 4.6279\n",
            "Epoch 36/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 3.5734 - mean_squared_error: 3.5734 - val_loss: 5.0400 - val_mean_squared_error: 5.0400\n",
            "Epoch 37/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 3.5005 - mean_squared_error: 3.5005 - val_loss: 4.1442 - val_mean_squared_error: 4.1442\n",
            "Epoch 38/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 3.4587 - mean_squared_error: 3.4587 - val_loss: 5.0875 - val_mean_squared_error: 5.0875\n",
            "Epoch 39/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 3.4134 - mean_squared_error: 3.4134 - val_loss: 4.4585 - val_mean_squared_error: 4.4585\n",
            "Epoch 40/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 3.4388 - mean_squared_error: 3.4388 - val_loss: 4.4652 - val_mean_squared_error: 4.4652\n",
            "Epoch 41/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 3.4401 - mean_squared_error: 3.4401 - val_loss: 4.2628 - val_mean_squared_error: 4.2628\n",
            "Epoch 42/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 3.5936 - mean_squared_error: 3.5936 - val_loss: 4.3991 - val_mean_squared_error: 4.3991\n",
            "Epoch 43/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 3.3428 - mean_squared_error: 3.3428 - val_loss: 4.7982 - val_mean_squared_error: 4.7982\n",
            "Epoch 44/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 3.4644 - mean_squared_error: 3.4644 - val_loss: 4.6041 - val_mean_squared_error: 4.6041\n",
            "Epoch 45/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 3.2534 - mean_squared_error: 3.2534 - val_loss: 4.2920 - val_mean_squared_error: 4.2920\n",
            "Epoch 46/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 3.1482 - mean_squared_error: 3.1482 - val_loss: 4.1371 - val_mean_squared_error: 4.1371\n",
            "Epoch 47/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 3.2500 - mean_squared_error: 3.2500 - val_loss: 4.5050 - val_mean_squared_error: 4.5050\n",
            "Epoch 48/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 3.1958 - mean_squared_error: 3.1958 - val_loss: 4.3486 - val_mean_squared_error: 4.3486\n",
            "Epoch 49/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 3.2327 - mean_squared_error: 3.2327 - val_loss: 4.5436 - val_mean_squared_error: 4.5436\n",
            "Epoch 50/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 3.2156 - mean_squared_error: 3.2156 - val_loss: 5.2236 - val_mean_squared_error: 5.2236\n",
            "Epoch 51/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 3.2764 - mean_squared_error: 3.2764 - val_loss: 4.7821 - val_mean_squared_error: 4.7821\n",
            "Epoch 52/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 3.1770 - mean_squared_error: 3.1770 - val_loss: 5.1590 - val_mean_squared_error: 5.1590\n",
            "Epoch 53/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 3.4550 - mean_squared_error: 3.4550 - val_loss: 4.4995 - val_mean_squared_error: 4.4995\n",
            "Epoch 54/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 3.0990 - mean_squared_error: 3.0990 - val_loss: 4.7466 - val_mean_squared_error: 4.7466\n",
            "Epoch 55/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 3.1991 - mean_squared_error: 3.1991 - val_loss: 4.9006 - val_mean_squared_error: 4.9006\n",
            "Epoch 56/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 3.1293 - mean_squared_error: 3.1293 - val_loss: 4.7691 - val_mean_squared_error: 4.7691\n",
            "Epoch 57/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 3.0320 - mean_squared_error: 3.0320 - val_loss: 4.3656 - val_mean_squared_error: 4.3656\n",
            "Epoch 58/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 3.0754 - mean_squared_error: 3.0754 - val_loss: 4.6651 - val_mean_squared_error: 4.6651\n",
            "Epoch 59/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 3.0745 - mean_squared_error: 3.0745 - val_loss: 4.5016 - val_mean_squared_error: 4.5016\n",
            "Epoch 60/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 3.0274 - mean_squared_error: 3.0274 - val_loss: 5.0127 - val_mean_squared_error: 5.0127\n",
            "Epoch 61/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 3.1107 - mean_squared_error: 3.1107 - val_loss: 4.6132 - val_mean_squared_error: 4.6132\n",
            "Epoch 62/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 3.0377 - mean_squared_error: 3.0377 - val_loss: 4.6049 - val_mean_squared_error: 4.6048\n",
            "Epoch 63/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 3.0467 - mean_squared_error: 3.0467 - val_loss: 4.1159 - val_mean_squared_error: 4.1159\n",
            "Epoch 64/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.8755 - mean_squared_error: 2.8755 - val_loss: 5.0304 - val_mean_squared_error: 5.0304\n",
            "Epoch 65/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.9141 - mean_squared_error: 2.9141 - val_loss: 4.5670 - val_mean_squared_error: 4.5670\n",
            "Epoch 66/300\n",
            "11255/11255 [==============================] - 6s 542us/sample - loss: 3.0051 - mean_squared_error: 3.0051 - val_loss: 4.3163 - val_mean_squared_error: 4.3163\n",
            "Epoch 67/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 2.9879 - mean_squared_error: 2.9879 - val_loss: 3.9154 - val_mean_squared_error: 3.9154\n",
            "Epoch 68/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.9043 - mean_squared_error: 2.9043 - val_loss: 4.4908 - val_mean_squared_error: 4.4908\n",
            "Epoch 69/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 3.0623 - mean_squared_error: 3.0623 - val_loss: 4.2206 - val_mean_squared_error: 4.2206\n",
            "Epoch 70/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.9197 - mean_squared_error: 2.9197 - val_loss: 4.3258 - val_mean_squared_error: 4.3258\n",
            "Epoch 71/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.9201 - mean_squared_error: 2.9201 - val_loss: 4.3942 - val_mean_squared_error: 4.3942\n",
            "Epoch 72/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.8608 - mean_squared_error: 2.8608 - val_loss: 4.6692 - val_mean_squared_error: 4.6692\n",
            "Epoch 73/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.8764 - mean_squared_error: 2.8764 - val_loss: 4.4902 - val_mean_squared_error: 4.4902\n",
            "Epoch 74/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.8068 - mean_squared_error: 2.8068 - val_loss: 4.4177 - val_mean_squared_error: 4.4177\n",
            "Epoch 75/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.9572 - mean_squared_error: 2.9572 - val_loss: 3.9241 - val_mean_squared_error: 3.9241\n",
            "Epoch 76/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.8310 - mean_squared_error: 2.8310 - val_loss: 4.1452 - val_mean_squared_error: 4.1452\n",
            "Epoch 77/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.9051 - mean_squared_error: 2.9051 - val_loss: 4.9660 - val_mean_squared_error: 4.9660\n",
            "Epoch 78/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.9020 - mean_squared_error: 2.9020 - val_loss: 4.1162 - val_mean_squared_error: 4.1162\n",
            "Epoch 79/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.7093 - mean_squared_error: 2.7093 - val_loss: 3.8819 - val_mean_squared_error: 3.8819\n",
            "Epoch 80/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.7719 - mean_squared_error: 2.7719 - val_loss: 4.2845 - val_mean_squared_error: 4.2845\n",
            "Epoch 81/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.8697 - mean_squared_error: 2.8697 - val_loss: 4.5114 - val_mean_squared_error: 4.5114\n",
            "Epoch 82/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.7480 - mean_squared_error: 2.7480 - val_loss: 4.2044 - val_mean_squared_error: 4.2044\n",
            "Epoch 83/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.9379 - mean_squared_error: 2.9379 - val_loss: 4.1151 - val_mean_squared_error: 4.1151\n",
            "Epoch 84/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.8293 - mean_squared_error: 2.8293 - val_loss: 4.5623 - val_mean_squared_error: 4.5623\n",
            "Epoch 85/300\n",
            "11255/11255 [==============================] - 6s 531us/sample - loss: 2.6614 - mean_squared_error: 2.6614 - val_loss: 4.3552 - val_mean_squared_error: 4.3552\n",
            "Epoch 86/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.6947 - mean_squared_error: 2.6947 - val_loss: 4.1107 - val_mean_squared_error: 4.1107\n",
            "Epoch 87/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.7694 - mean_squared_error: 2.7694 - val_loss: 4.4999 - val_mean_squared_error: 4.4999\n",
            "Epoch 88/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.7863 - mean_squared_error: 2.7863 - val_loss: 5.0527 - val_mean_squared_error: 5.0527\n",
            "Epoch 89/300\n",
            "11255/11255 [==============================] - 6s 530us/sample - loss: 2.7089 - mean_squared_error: 2.7089 - val_loss: 4.2896 - val_mean_squared_error: 4.2896\n",
            "Epoch 90/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.8022 - mean_squared_error: 2.8022 - val_loss: 4.5011 - val_mean_squared_error: 4.5011\n",
            "Epoch 91/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.7076 - mean_squared_error: 2.7076 - val_loss: 4.3344 - val_mean_squared_error: 4.3344\n",
            "Epoch 92/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.7094 - mean_squared_error: 2.7094 - val_loss: 4.5561 - val_mean_squared_error: 4.5561\n",
            "Epoch 93/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.6621 - mean_squared_error: 2.6621 - val_loss: 4.2729 - val_mean_squared_error: 4.2729\n",
            "Epoch 94/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 2.5892 - mean_squared_error: 2.5892 - val_loss: 4.1975 - val_mean_squared_error: 4.1975\n",
            "Epoch 95/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.6513 - mean_squared_error: 2.6513 - val_loss: 4.4094 - val_mean_squared_error: 4.4094\n",
            "Epoch 96/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.6726 - mean_squared_error: 2.6726 - val_loss: 4.1190 - val_mean_squared_error: 4.1190\n",
            "Epoch 97/300\n",
            "11255/11255 [==============================] - 6s 538us/sample - loss: 2.7628 - mean_squared_error: 2.7628 - val_loss: 3.9554 - val_mean_squared_error: 3.9554\n",
            "Epoch 98/300\n",
            "11255/11255 [==============================] - 6s 545us/sample - loss: 2.7142 - mean_squared_error: 2.7142 - val_loss: 3.8085 - val_mean_squared_error: 3.8085\n",
            "Epoch 99/300\n",
            "11255/11255 [==============================] - 6s 532us/sample - loss: 2.7162 - mean_squared_error: 2.7162 - val_loss: 4.1205 - val_mean_squared_error: 4.1205\n",
            "Epoch 100/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.7175 - mean_squared_error: 2.7175 - val_loss: 3.8589 - val_mean_squared_error: 3.8589\n",
            "Epoch 101/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.6457 - mean_squared_error: 2.6457 - val_loss: 4.3058 - val_mean_squared_error: 4.3058\n",
            "Epoch 102/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 2.6973 - mean_squared_error: 2.6973 - val_loss: 3.8532 - val_mean_squared_error: 3.8532\n",
            "Epoch 103/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.7695 - mean_squared_error: 2.7695 - val_loss: 3.8407 - val_mean_squared_error: 3.8407\n",
            "Epoch 104/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.6582 - mean_squared_error: 2.6582 - val_loss: 3.9816 - val_mean_squared_error: 3.9816\n",
            "Epoch 105/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.6371 - mean_squared_error: 2.6371 - val_loss: 4.3238 - val_mean_squared_error: 4.3238\n",
            "Epoch 106/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.6806 - mean_squared_error: 2.6806 - val_loss: 4.2072 - val_mean_squared_error: 4.2072\n",
            "Epoch 107/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.5759 - mean_squared_error: 2.5759 - val_loss: 4.1281 - val_mean_squared_error: 4.1281\n",
            "Epoch 108/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.5833 - mean_squared_error: 2.5833 - val_loss: 3.8492 - val_mean_squared_error: 3.8491\n",
            "Epoch 109/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 2.6431 - mean_squared_error: 2.6431 - val_loss: 3.9165 - val_mean_squared_error: 3.9165\n",
            "Epoch 110/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.6185 - mean_squared_error: 2.6185 - val_loss: 4.0472 - val_mean_squared_error: 4.0472\n",
            "Epoch 111/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.5767 - mean_squared_error: 2.5767 - val_loss: 4.9171 - val_mean_squared_error: 4.9171\n",
            "Epoch 112/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.7358 - mean_squared_error: 2.7358 - val_loss: 4.0409 - val_mean_squared_error: 4.0409\n",
            "Epoch 113/300\n",
            "11255/11255 [==============================] - 6s 530us/sample - loss: 2.6173 - mean_squared_error: 2.6173 - val_loss: 3.8717 - val_mean_squared_error: 3.8717\n",
            "Epoch 114/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.5616 - mean_squared_error: 2.5616 - val_loss: 4.0596 - val_mean_squared_error: 4.0596\n",
            "Epoch 115/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.5879 - mean_squared_error: 2.5879 - val_loss: 3.9469 - val_mean_squared_error: 3.9469\n",
            "Epoch 116/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 2.6032 - mean_squared_error: 2.6032 - val_loss: 4.3902 - val_mean_squared_error: 4.3902\n",
            "Epoch 117/300\n",
            "11255/11255 [==============================] - 6s 533us/sample - loss: 2.5439 - mean_squared_error: 2.5439 - val_loss: 4.2786 - val_mean_squared_error: 4.2786\n",
            "Epoch 118/300\n",
            "11255/11255 [==============================] - 6s 537us/sample - loss: 2.6208 - mean_squared_error: 2.6208 - val_loss: 3.9411 - val_mean_squared_error: 3.9411\n",
            "Epoch 119/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.5826 - mean_squared_error: 2.5826 - val_loss: 4.0571 - val_mean_squared_error: 4.0571\n",
            "Epoch 120/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.4739 - mean_squared_error: 2.4739 - val_loss: 4.0434 - val_mean_squared_error: 4.0434\n",
            "Epoch 121/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.5465 - mean_squared_error: 2.5465 - val_loss: 4.2316 - val_mean_squared_error: 4.2316\n",
            "Epoch 122/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.5662 - mean_squared_error: 2.5662 - val_loss: 4.0533 - val_mean_squared_error: 4.0533\n",
            "Epoch 123/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 2.6069 - mean_squared_error: 2.6069 - val_loss: 4.3122 - val_mean_squared_error: 4.3122\n",
            "Epoch 124/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.6267 - mean_squared_error: 2.6267 - val_loss: 4.5184 - val_mean_squared_error: 4.5184\n",
            "Epoch 125/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.5328 - mean_squared_error: 2.5328 - val_loss: 4.2345 - val_mean_squared_error: 4.2345\n",
            "Epoch 126/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.7442 - mean_squared_error: 2.7442 - val_loss: 4.2518 - val_mean_squared_error: 4.2518\n",
            "Epoch 127/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.5688 - mean_squared_error: 2.5688 - val_loss: 3.7697 - val_mean_squared_error: 3.7697\n",
            "Epoch 128/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.4819 - mean_squared_error: 2.4819 - val_loss: 4.0791 - val_mean_squared_error: 4.0791\n",
            "Epoch 129/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.6323 - mean_squared_error: 2.6323 - val_loss: 4.4048 - val_mean_squared_error: 4.4048\n",
            "Epoch 130/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.5711 - mean_squared_error: 2.5711 - val_loss: 4.4588 - val_mean_squared_error: 4.4588\n",
            "Epoch 131/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.6023 - mean_squared_error: 2.6023 - val_loss: 3.8083 - val_mean_squared_error: 3.8083\n",
            "Epoch 132/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.5203 - mean_squared_error: 2.5203 - val_loss: 4.0397 - val_mean_squared_error: 4.0397\n",
            "Epoch 133/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.4926 - mean_squared_error: 2.4926 - val_loss: 4.0015 - val_mean_squared_error: 4.0015\n",
            "Epoch 134/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.4854 - mean_squared_error: 2.4854 - val_loss: 4.3648 - val_mean_squared_error: 4.3648\n",
            "Epoch 135/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.4271 - mean_squared_error: 2.4271 - val_loss: 4.2690 - val_mean_squared_error: 4.2690\n",
            "Epoch 136/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.4578 - mean_squared_error: 2.4578 - val_loss: 4.3065 - val_mean_squared_error: 4.3065\n",
            "Epoch 137/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.4723 - mean_squared_error: 2.4723 - val_loss: 4.4479 - val_mean_squared_error: 4.4479\n",
            "Epoch 138/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.5347 - mean_squared_error: 2.5347 - val_loss: 4.1656 - val_mean_squared_error: 4.1656\n",
            "Epoch 139/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 2.5574 - mean_squared_error: 2.5574 - val_loss: 3.9922 - val_mean_squared_error: 3.9922\n",
            "Epoch 140/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.4162 - mean_squared_error: 2.4162 - val_loss: 4.0755 - val_mean_squared_error: 4.0755\n",
            "Epoch 141/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.4992 - mean_squared_error: 2.4992 - val_loss: 3.7968 - val_mean_squared_error: 3.7968\n",
            "Epoch 142/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.4373 - mean_squared_error: 2.4373 - val_loss: 3.7901 - val_mean_squared_error: 3.7901\n",
            "Epoch 143/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.4273 - mean_squared_error: 2.4273 - val_loss: 3.9541 - val_mean_squared_error: 3.9542\n",
            "Epoch 144/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.4610 - mean_squared_error: 2.4610 - val_loss: 3.9620 - val_mean_squared_error: 3.9620\n",
            "Epoch 145/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.5081 - mean_squared_error: 2.5081 - val_loss: 3.9726 - val_mean_squared_error: 3.9726\n",
            "Epoch 146/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.4247 - mean_squared_error: 2.4247 - val_loss: 4.0245 - val_mean_squared_error: 4.0245\n",
            "Epoch 147/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.5201 - mean_squared_error: 2.5201 - val_loss: 3.7527 - val_mean_squared_error: 3.7527\n",
            "Epoch 148/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 2.5093 - mean_squared_error: 2.5093 - val_loss: 4.0020 - val_mean_squared_error: 4.0020\n",
            "Epoch 149/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.4949 - mean_squared_error: 2.4949 - val_loss: 3.8418 - val_mean_squared_error: 3.8418\n",
            "Epoch 150/300\n",
            "11255/11255 [==============================] - 6s 533us/sample - loss: 2.4302 - mean_squared_error: 2.4302 - val_loss: 4.5640 - val_mean_squared_error: 4.5640\n",
            "Epoch 151/300\n",
            "11255/11255 [==============================] - 6s 532us/sample - loss: 2.4543 - mean_squared_error: 2.4543 - val_loss: 4.3430 - val_mean_squared_error: 4.3430\n",
            "Epoch 152/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.5025 - mean_squared_error: 2.5025 - val_loss: 4.1283 - val_mean_squared_error: 4.1283\n",
            "Epoch 153/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.4637 - mean_squared_error: 2.4637 - val_loss: 4.0968 - val_mean_squared_error: 4.0968\n",
            "Epoch 154/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.3790 - mean_squared_error: 2.3790 - val_loss: 4.4207 - val_mean_squared_error: 4.4207\n",
            "Epoch 155/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.4256 - mean_squared_error: 2.4256 - val_loss: 4.2156 - val_mean_squared_error: 4.2156\n",
            "Epoch 156/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.3785 - mean_squared_error: 2.3785 - val_loss: 4.2734 - val_mean_squared_error: 4.2734\n",
            "Epoch 157/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.4199 - mean_squared_error: 2.4199 - val_loss: 4.0719 - val_mean_squared_error: 4.0719\n",
            "Epoch 158/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.4132 - mean_squared_error: 2.4132 - val_loss: 3.7755 - val_mean_squared_error: 3.7755\n",
            "Epoch 159/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 2.3585 - mean_squared_error: 2.3585 - val_loss: 5.2528 - val_mean_squared_error: 5.2528\n",
            "Epoch 160/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.4490 - mean_squared_error: 2.4490 - val_loss: 4.0305 - val_mean_squared_error: 4.0305\n",
            "Epoch 161/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.3904 - mean_squared_error: 2.3904 - val_loss: 3.9162 - val_mean_squared_error: 3.9162\n",
            "Epoch 162/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.5542 - mean_squared_error: 2.5542 - val_loss: 3.7161 - val_mean_squared_error: 3.7161\n",
            "Epoch 163/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.5112 - mean_squared_error: 2.5112 - val_loss: 3.8276 - val_mean_squared_error: 3.8276\n",
            "Epoch 164/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.4961 - mean_squared_error: 2.4961 - val_loss: 4.0939 - val_mean_squared_error: 4.0939\n",
            "Epoch 165/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.4346 - mean_squared_error: 2.4346 - val_loss: 3.7515 - val_mean_squared_error: 3.7515\n",
            "Epoch 166/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.3453 - mean_squared_error: 2.3453 - val_loss: 4.1106 - val_mean_squared_error: 4.1106\n",
            "Epoch 167/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.4309 - mean_squared_error: 2.4309 - val_loss: 4.0544 - val_mean_squared_error: 4.0544\n",
            "Epoch 168/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.3809 - mean_squared_error: 2.3809 - val_loss: 3.8589 - val_mean_squared_error: 3.8589\n",
            "Epoch 169/300\n",
            "11255/11255 [==============================] - 6s 532us/sample - loss: 2.4479 - mean_squared_error: 2.4479 - val_loss: 4.1394 - val_mean_squared_error: 4.1394\n",
            "Epoch 170/300\n",
            "11255/11255 [==============================] - 6s 532us/sample - loss: 2.3966 - mean_squared_error: 2.3966 - val_loss: 4.3293 - val_mean_squared_error: 4.3293\n",
            "Epoch 171/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.4426 - mean_squared_error: 2.4426 - val_loss: 4.4020 - val_mean_squared_error: 4.4020\n",
            "Epoch 172/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.3104 - mean_squared_error: 2.3104 - val_loss: 3.9965 - val_mean_squared_error: 3.9965\n",
            "Epoch 173/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.3217 - mean_squared_error: 2.3217 - val_loss: 3.8735 - val_mean_squared_error: 3.8735\n",
            "Epoch 174/300\n",
            "11255/11255 [==============================] - 6s 528us/sample - loss: 2.2492 - mean_squared_error: 2.2492 - val_loss: 4.3100 - val_mean_squared_error: 4.3100\n",
            "Epoch 175/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.2538 - mean_squared_error: 2.2538 - val_loss: 4.0912 - val_mean_squared_error: 4.0913\n",
            "Epoch 176/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.3416 - mean_squared_error: 2.3416 - val_loss: 4.1240 - val_mean_squared_error: 4.1240\n",
            "Epoch 177/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.3981 - mean_squared_error: 2.3981 - val_loss: 4.2328 - val_mean_squared_error: 4.2328\n",
            "Epoch 178/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.4179 - mean_squared_error: 2.4179 - val_loss: 4.2249 - val_mean_squared_error: 4.2249\n",
            "Epoch 179/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.3821 - mean_squared_error: 2.3821 - val_loss: 3.8381 - val_mean_squared_error: 3.8381\n",
            "Epoch 180/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.2476 - mean_squared_error: 2.2476 - val_loss: 4.2558 - val_mean_squared_error: 4.2558\n",
            "Epoch 181/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.2408 - mean_squared_error: 2.2408 - val_loss: 3.5797 - val_mean_squared_error: 3.5797\n",
            "Epoch 182/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.3052 - mean_squared_error: 2.3052 - val_loss: 3.6530 - val_mean_squared_error: 3.6530\n",
            "Epoch 183/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.3429 - mean_squared_error: 2.3429 - val_loss: 4.6574 - val_mean_squared_error: 4.6574\n",
            "Epoch 184/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.4109 - mean_squared_error: 2.4109 - val_loss: 3.8511 - val_mean_squared_error: 3.8511\n",
            "Epoch 185/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.2698 - mean_squared_error: 2.2698 - val_loss: 4.3126 - val_mean_squared_error: 4.3126\n",
            "Epoch 186/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.4104 - mean_squared_error: 2.4104 - val_loss: 3.8916 - val_mean_squared_error: 3.8916\n",
            "Epoch 187/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.4161 - mean_squared_error: 2.4161 - val_loss: 4.0222 - val_mean_squared_error: 4.0222\n",
            "Epoch 188/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.3818 - mean_squared_error: 2.3818 - val_loss: 4.1914 - val_mean_squared_error: 4.1914\n",
            "Epoch 189/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.3574 - mean_squared_error: 2.3574 - val_loss: 4.3456 - val_mean_squared_error: 4.3456\n",
            "Epoch 190/300\n",
            "11255/11255 [==============================] - 6s 525us/sample - loss: 2.3316 - mean_squared_error: 2.3316 - val_loss: 4.0272 - val_mean_squared_error: 4.0272\n",
            "Epoch 191/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.3813 - mean_squared_error: 2.3813 - val_loss: 4.0170 - val_mean_squared_error: 4.0170\n",
            "Epoch 192/300\n",
            "11255/11255 [==============================] - 6s 527us/sample - loss: 2.2942 - mean_squared_error: 2.2942 - val_loss: 4.1662 - val_mean_squared_error: 4.1662\n",
            "Epoch 193/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.3660 - mean_squared_error: 2.3660 - val_loss: 4.5209 - val_mean_squared_error: 4.5209\n",
            "Epoch 194/300\n",
            "11255/11255 [==============================] - 6s 529us/sample - loss: 2.4208 - mean_squared_error: 2.4208 - val_loss: 3.6007 - val_mean_squared_error: 3.6007\n",
            "Epoch 195/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.2411 - mean_squared_error: 2.2411 - val_loss: 3.9377 - val_mean_squared_error: 3.9377\n",
            "Epoch 196/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.2336 - mean_squared_error: 2.2336 - val_loss: 4.1889 - val_mean_squared_error: 4.1889\n",
            "Epoch 197/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.2846 - mean_squared_error: 2.2846 - val_loss: 3.8373 - val_mean_squared_error: 3.8373\n",
            "Epoch 198/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.2646 - mean_squared_error: 2.2646 - val_loss: 3.6229 - val_mean_squared_error: 3.6229\n",
            "Epoch 199/300\n",
            "11255/11255 [==============================] - 6s 524us/sample - loss: 2.2997 - mean_squared_error: 2.2997 - val_loss: 4.1516 - val_mean_squared_error: 4.1516\n",
            "Epoch 200/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.2945 - mean_squared_error: 2.2945 - val_loss: 4.0148 - val_mean_squared_error: 4.0148\n",
            "Epoch 201/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.2934 - mean_squared_error: 2.2934 - val_loss: 3.7721 - val_mean_squared_error: 3.7721\n",
            "Epoch 202/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.3973 - mean_squared_error: 2.3973 - val_loss: 4.1211 - val_mean_squared_error: 4.1211\n",
            "Epoch 203/300\n",
            "11255/11255 [==============================] - 6s 533us/sample - loss: 2.3559 - mean_squared_error: 2.3559 - val_loss: 4.5141 - val_mean_squared_error: 4.5141\n",
            "Epoch 204/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.2112 - mean_squared_error: 2.2112 - val_loss: 4.0796 - val_mean_squared_error: 4.0796\n",
            "Epoch 205/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.2607 - mean_squared_error: 2.2607 - val_loss: 3.9760 - val_mean_squared_error: 3.9760\n",
            "Epoch 206/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.2213 - mean_squared_error: 2.2213 - val_loss: 4.0615 - val_mean_squared_error: 4.0615\n",
            "Epoch 207/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.3080 - mean_squared_error: 2.3080 - val_loss: 4.0474 - val_mean_squared_error: 4.0474\n",
            "Epoch 208/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.2931 - mean_squared_error: 2.2931 - val_loss: 4.0708 - val_mean_squared_error: 4.0708\n",
            "Epoch 209/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.3264 - mean_squared_error: 2.3264 - val_loss: 4.6529 - val_mean_squared_error: 4.6529\n",
            "Epoch 210/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.3247 - mean_squared_error: 2.3247 - val_loss: 4.2700 - val_mean_squared_error: 4.2700\n",
            "Epoch 211/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.2599 - mean_squared_error: 2.2599 - val_loss: 3.8273 - val_mean_squared_error: 3.8273\n",
            "Epoch 212/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.2452 - mean_squared_error: 2.2452 - val_loss: 4.3433 - val_mean_squared_error: 4.3433\n",
            "Epoch 213/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.2947 - mean_squared_error: 2.2947 - val_loss: 4.0639 - val_mean_squared_error: 4.0639\n",
            "Epoch 214/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.2777 - mean_squared_error: 2.2777 - val_loss: 4.1097 - val_mean_squared_error: 4.1097\n",
            "Epoch 215/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 2.2483 - mean_squared_error: 2.2483 - val_loss: 3.7089 - val_mean_squared_error: 3.7089\n",
            "Epoch 216/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.2243 - mean_squared_error: 2.2243 - val_loss: 3.9694 - val_mean_squared_error: 3.9694\n",
            "Epoch 217/300\n",
            "11255/11255 [==============================] - 6s 521us/sample - loss: 2.2535 - mean_squared_error: 2.2535 - val_loss: 4.1204 - val_mean_squared_error: 4.1204\n",
            "Epoch 218/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.2761 - mean_squared_error: 2.2761 - val_loss: 3.8052 - val_mean_squared_error: 3.8052\n",
            "Epoch 219/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.3152 - mean_squared_error: 2.3152 - val_loss: 3.8239 - val_mean_squared_error: 3.8239\n",
            "Epoch 220/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.2504 - mean_squared_error: 2.2504 - val_loss: 3.5777 - val_mean_squared_error: 3.5777\n",
            "Epoch 221/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.2732 - mean_squared_error: 2.2732 - val_loss: 4.0029 - val_mean_squared_error: 4.0029\n",
            "Epoch 222/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.2424 - mean_squared_error: 2.2424 - val_loss: 3.9227 - val_mean_squared_error: 3.9227\n",
            "Epoch 223/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.3679 - mean_squared_error: 2.3679 - val_loss: 4.0358 - val_mean_squared_error: 4.0358\n",
            "Epoch 224/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.2240 - mean_squared_error: 2.2240 - val_loss: 3.9755 - val_mean_squared_error: 3.9755\n",
            "Epoch 225/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.2903 - mean_squared_error: 2.2903 - val_loss: 3.9534 - val_mean_squared_error: 3.9534\n",
            "Epoch 226/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.2353 - mean_squared_error: 2.2353 - val_loss: 4.0446 - val_mean_squared_error: 4.0446\n",
            "Epoch 227/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.2288 - mean_squared_error: 2.2288 - val_loss: 4.0238 - val_mean_squared_error: 4.0238\n",
            "Epoch 228/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.2011 - mean_squared_error: 2.2011 - val_loss: 3.8417 - val_mean_squared_error: 3.8417\n",
            "Epoch 229/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.3146 - mean_squared_error: 2.3146 - val_loss: 3.9623 - val_mean_squared_error: 3.9623\n",
            "Epoch 230/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.3222 - mean_squared_error: 2.3222 - val_loss: 4.0696 - val_mean_squared_error: 4.0696\n",
            "Epoch 231/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.2320 - mean_squared_error: 2.2320 - val_loss: 3.8531 - val_mean_squared_error: 3.8531\n",
            "Epoch 232/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.1430 - mean_squared_error: 2.1430 - val_loss: 4.0295 - val_mean_squared_error: 4.0295\n",
            "Epoch 233/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.1775 - mean_squared_error: 2.1775 - val_loss: 3.9926 - val_mean_squared_error: 3.9926\n",
            "Epoch 234/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.2737 - mean_squared_error: 2.2737 - val_loss: 3.8510 - val_mean_squared_error: 3.8510\n",
            "Epoch 235/300\n",
            "11255/11255 [==============================] - 6s 522us/sample - loss: 2.1765 - mean_squared_error: 2.1765 - val_loss: 3.7103 - val_mean_squared_error: 3.7103\n",
            "Epoch 236/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.1867 - mean_squared_error: 2.1867 - val_loss: 3.8198 - val_mean_squared_error: 3.8198\n",
            "Epoch 237/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.2756 - mean_squared_error: 2.2756 - val_loss: 4.1688 - val_mean_squared_error: 4.1688\n",
            "Epoch 238/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.1816 - mean_squared_error: 2.1816 - val_loss: 3.7136 - val_mean_squared_error: 3.7136\n",
            "Epoch 239/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.1418 - mean_squared_error: 2.1418 - val_loss: 3.7531 - val_mean_squared_error: 3.7531\n",
            "Epoch 240/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 2.2566 - mean_squared_error: 2.2566 - val_loss: 4.1413 - val_mean_squared_error: 4.1413\n",
            "Epoch 241/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.3013 - mean_squared_error: 2.3013 - val_loss: 3.7208 - val_mean_squared_error: 3.7208\n",
            "Epoch 242/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.2814 - mean_squared_error: 2.2814 - val_loss: 3.8641 - val_mean_squared_error: 3.8641\n",
            "Epoch 243/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.2913 - mean_squared_error: 2.2913 - val_loss: 3.7470 - val_mean_squared_error: 3.7470\n",
            "Epoch 244/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.1357 - mean_squared_error: 2.1357 - val_loss: 3.7520 - val_mean_squared_error: 3.7520\n",
            "Epoch 245/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.1484 - mean_squared_error: 2.1484 - val_loss: 4.0690 - val_mean_squared_error: 4.0690\n",
            "Epoch 246/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.2483 - mean_squared_error: 2.2483 - val_loss: 3.5662 - val_mean_squared_error: 3.5662\n",
            "Epoch 247/300\n",
            "11255/11255 [==============================] - 6s 517us/sample - loss: 2.1501 - mean_squared_error: 2.1501 - val_loss: 3.7565 - val_mean_squared_error: 3.7565\n",
            "Epoch 248/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.1249 - mean_squared_error: 2.1249 - val_loss: 3.7043 - val_mean_squared_error: 3.7043\n",
            "Epoch 249/300\n",
            "11255/11255 [==============================] - 6s 518us/sample - loss: 2.2725 - mean_squared_error: 2.2725 - val_loss: 3.8496 - val_mean_squared_error: 3.8496\n",
            "Epoch 250/300\n",
            "11255/11255 [==============================] - 6s 520us/sample - loss: 2.2018 - mean_squared_error: 2.2018 - val_loss: 3.7849 - val_mean_squared_error: 3.7849\n",
            "Epoch 251/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.1968 - mean_squared_error: 2.1968 - val_loss: 3.6478 - val_mean_squared_error: 3.6478\n",
            "Epoch 252/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 2.1864 - mean_squared_error: 2.1864 - val_loss: 3.9898 - val_mean_squared_error: 3.9898\n",
            "Epoch 253/300\n",
            "11255/11255 [==============================] - 6s 516us/sample - loss: 2.2830 - mean_squared_error: 2.2830 - val_loss: 3.8394 - val_mean_squared_error: 3.8394\n",
            "Epoch 254/300\n",
            "11255/11255 [==============================] - 6s 519us/sample - loss: 2.1840 - mean_squared_error: 2.1840 - val_loss: 3.7943 - val_mean_squared_error: 3.7943\n",
            "Epoch 255/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.2958 - mean_squared_error: 2.2958 - val_loss: 3.8859 - val_mean_squared_error: 3.8859\n",
            "Epoch 256/300\n",
            "11255/11255 [==============================] - 6s 535us/sample - loss: 2.2748 - mean_squared_error: 2.2748 - val_loss: 3.9087 - val_mean_squared_error: 3.9087\n",
            "Epoch 257/300\n",
            "11255/11255 [==============================] - 6s 526us/sample - loss: 2.1616 - mean_squared_error: 2.1616 - val_loss: 4.0217 - val_mean_squared_error: 4.0217\n",
            "Epoch 258/300\n",
            "11255/11255 [==============================] - 6s 512us/sample - loss: 2.1202 - mean_squared_error: 2.1202 - val_loss: 3.8737 - val_mean_squared_error: 3.8737\n",
            "Epoch 259/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 2.1036 - mean_squared_error: 2.1036 - val_loss: 3.9986 - val_mean_squared_error: 3.9986\n",
            "Epoch 260/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.0837 - mean_squared_error: 2.0837 - val_loss: 3.8155 - val_mean_squared_error: 3.8155\n",
            "Epoch 261/300\n",
            "11255/11255 [==============================] - 6s 513us/sample - loss: 2.1617 - mean_squared_error: 2.1617 - val_loss: 4.0568 - val_mean_squared_error: 4.0568\n",
            "Epoch 262/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.0412 - mean_squared_error: 2.0412 - val_loss: 4.0513 - val_mean_squared_error: 4.0513\n",
            "Epoch 263/300\n",
            "11255/11255 [==============================] - 6s 513us/sample - loss: 2.2575 - mean_squared_error: 2.2575 - val_loss: 3.8069 - val_mean_squared_error: 3.8069\n",
            "Epoch 264/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.1626 - mean_squared_error: 2.1626 - val_loss: 4.0120 - val_mean_squared_error: 4.0120\n",
            "Epoch 265/300\n",
            "11255/11255 [==============================] - 6s 515us/sample - loss: 2.2258 - mean_squared_error: 2.2258 - val_loss: 3.6330 - val_mean_squared_error: 3.6330\n",
            "Epoch 266/300\n",
            "11255/11255 [==============================] - 6s 509us/sample - loss: 2.2272 - mean_squared_error: 2.2272 - val_loss: 4.3292 - val_mean_squared_error: 4.3292\n",
            "Epoch 267/300\n",
            "11255/11255 [==============================] - 6s 507us/sample - loss: 2.1485 - mean_squared_error: 2.1485 - val_loss: 3.5549 - val_mean_squared_error: 3.5549\n",
            "Epoch 268/300\n",
            "11255/11255 [==============================] - 6s 509us/sample - loss: 2.1857 - mean_squared_error: 2.1857 - val_loss: 3.6797 - val_mean_squared_error: 3.6797\n",
            "Epoch 269/300\n",
            "11255/11255 [==============================] - 6s 511us/sample - loss: 2.1618 - mean_squared_error: 2.1618 - val_loss: 3.8243 - val_mean_squared_error: 3.8243\n",
            "Epoch 270/300\n",
            "11255/11255 [==============================] - 6s 508us/sample - loss: 2.1036 - mean_squared_error: 2.1036 - val_loss: 3.9583 - val_mean_squared_error: 3.9583\n",
            "Epoch 271/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.1534 - mean_squared_error: 2.1534 - val_loss: 3.6899 - val_mean_squared_error: 3.6899\n",
            "Epoch 272/300\n",
            "11255/11255 [==============================] - 6s 509us/sample - loss: 2.1297 - mean_squared_error: 2.1297 - val_loss: 4.1665 - val_mean_squared_error: 4.1665\n",
            "Epoch 273/300\n",
            "11255/11255 [==============================] - 6s 506us/sample - loss: 2.1903 - mean_squared_error: 2.1903 - val_loss: 4.1854 - val_mean_squared_error: 4.1854\n",
            "Epoch 274/300\n",
            "11255/11255 [==============================] - 6s 523us/sample - loss: 2.2147 - mean_squared_error: 2.2147 - val_loss: 3.8477 - val_mean_squared_error: 3.8477\n",
            "Epoch 275/300\n",
            "11255/11255 [==============================] - 6s 510us/sample - loss: 2.1409 - mean_squared_error: 2.1409 - val_loss: 4.4374 - val_mean_squared_error: 4.4374\n",
            "Epoch 276/300\n",
            "11255/11255 [==============================] - 6s 509us/sample - loss: 2.0631 - mean_squared_error: 2.0631 - val_loss: 3.9420 - val_mean_squared_error: 3.9420\n",
            "Epoch 277/300\n",
            "11255/11255 [==============================] - 6s 510us/sample - loss: 2.1096 - mean_squared_error: 2.1096 - val_loss: 3.9827 - val_mean_squared_error: 3.9827\n",
            "Epoch 278/300\n",
            "11255/11255 [==============================] - 6s 511us/sample - loss: 2.2585 - mean_squared_error: 2.2585 - val_loss: 3.9206 - val_mean_squared_error: 3.9206\n",
            "Epoch 279/300\n",
            "11255/11255 [==============================] - 6s 512us/sample - loss: 2.0648 - mean_squared_error: 2.0648 - val_loss: 3.6780 - val_mean_squared_error: 3.6780\n",
            "Epoch 280/300\n",
            "11255/11255 [==============================] - 6s 514us/sample - loss: 2.0886 - mean_squared_error: 2.0886 - val_loss: 3.7733 - val_mean_squared_error: 3.7733\n",
            "Epoch 281/300\n",
            "11255/11255 [==============================] - 6s 536us/sample - loss: 2.1805 - mean_squared_error: 2.1805 - val_loss: 3.9237 - val_mean_squared_error: 3.9237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xIdB8ovZASW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_2layer_spec_model(start_filter, d, step, fc1=500, fc2=500):\n",
        "    '''\n",
        "    Simple function that retruns a keras cnn model \n",
        "    '''\n",
        "    cnn_model = tf.keras.models.Sequential()\n",
        "#     cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter, (3, 3), padding='same', \n",
        "                                         input_shape=(96, 96, 1),activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d))\n",
        "    cnn_model.add(tf.keras.layers.Conv2D(start_filter*2, (5, 5), padding='same', activation='relu', use_bias=False))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+step))\n",
        "    cnn_model.add(tf.keras.layers.Flatten())\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc1))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Dropout(d+3*step))\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(fc2))\n",
        "    cnn_model.add(layers.BatchNormalization())\n",
        "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "    cnn_model.add(tf.keras.layers.Dense(2, name='Specialist'))\n",
        "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "    print(50*\"=\")\n",
        "    print(cnn_model.summary())\n",
        "    print(50*\"=\")\n",
        "    \n",
        "    return cnn_model\n",
        "  \n",
        "def train_improved_2layer_specialists(df, keypoint, sf=16, doi=0.0, dos=0.0, lrf=10, fc1=200, fc2=200):\n",
        "  # Use an early stopping callback and our timing callback\n",
        "  early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1,\n",
        "                                patience=100, mode='auto', restore_best_weights = True)\n",
        "  time_callback = TimeHistory()\n",
        "  \n",
        "  \n",
        "  # Use the adam optimizer with the default learning rate\n",
        "  adam = optimizers.Adam(lr=0.001*lrf, beta_1=0.9, beta_2=0.999)\n",
        "    \n",
        "  # Create and Compile the model\n",
        "  model = create_2layer_spec_model(sf, doi, dos, fc1=fc1, fc2=fc2)\n",
        "  model.compile(\n",
        "      optimizer=adam,\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['mean_squared_error'])\n",
        "  \n",
        "  \n",
        "  # Subset the image/keypoint dataframe to just have the training locations for \n",
        "  # the keypoints of interest\n",
        "  df_keypoint = df[[keypoint + '_x',keypoint+'_y', 'Image']]\n",
        "  \n",
        "  # Print the remaining columns as a QA check\n",
        "  print(df_keypoint.columns)\n",
        "  \n",
        "  # Drop the NA values\n",
        "  df_keypoint = df_keypoint.dropna().reset_index(drop=True)\n",
        "  \n",
        "  # Grab the last column - that is our image data for X matrix\n",
        "  train_X = df_keypoint.iloc[:, -1]\n",
        "\n",
        "  # Convert from a series of arrays to an NDarray\n",
        "  train_X = np.array([x.reshape(96,96,1) for x in train_X])\n",
        " \n",
        "  \n",
        "  # Get the Y data for just the specified keypoint\n",
        "  train_y = np.array(df_keypoint.iloc[:,:-1])\n",
        " \n",
        "  history = model.fit(\n",
        "      train_X.astype(np.float32), train_y.astype(np.float32),\n",
        "      epochs=300,\n",
        "      validation_split=0.20,\n",
        "      callbacks=[time_callback, early_stop])\n",
        "  times = time_callback.times\n",
        "\n",
        "  # Convert to dataframe\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
        "  hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
        "  hist['times'] = times\n",
        "  hist['starting_filter'] = sf\n",
        "  hist['layers'] = 4\n",
        "  hist['pooling'] = 'yes'\n",
        "  hist['fc_layer1'] = fc1\n",
        "  hist['fc_layer2'] = fc2\n",
        "  hist['activation'] = 'relu'\n",
        "  hist['optimizer'] = 'adam'\n",
        "  hist['lrate'] = adam.get_config()['learning_rate']\n",
        "  hist['dropout_initial'] = doi\n",
        "  hist['dropout_step'] = dos\n",
        "  hist['batch_norm'] = 1\n",
        "  hist['bias'] = 0\n",
        "  hist['stride'] = 1\n",
        "  hist['flipped'] = 1.0\n",
        "  hist['keypoint'] = keypoint\n",
        "\n",
        "  # Re-pickle after every model to retain progress\n",
        "  hist.to_pickle(drive_path+\"OutputData/4l_spec_{}_d{}_s{}_sf{}_lrf{}_fc1{}_fc2{}_kern2345.pkl\".format(keypoint, doi, dos, sf, lrf, fc1, fc2))\n",
        "\n",
        "  # Save models.\n",
        "  filename = \"cnn_4l_spec_{}_d{}_s{}_sf{}_lrfactor{}_flipped_100_fc1{}_fc2{}_kern2334\".format(keypoint, doi, dos, sf, lrf, fc1, fc2)\n",
        "  model.save(drive_path+\"Models/\"+filename+\".h5\")\n",
        "  return hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR6dz4FC4MPT",
        "colab_type": "code",
        "outputId": "3c12d146-0531-41f5-9283-ca55b43c2435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "improved_specialist_df = train_improved_2layer_specialists(df, 'mouth_center_bottom_lip', sf=16, doi=0.0, dos=0.0, lrf=10, fc1=200, fc2=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0729 22:39:23.985434 140073071294336 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 96, 96, 16)        144       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 96, 96, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 48, 48, 32)        12800     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               3686600   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 200)               40200     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "Specialist (Dense)           (None, 2)                 402       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 3,741,938\n",
            "Trainable params: 3,741,042\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "None\n",
            "==================================================\n",
            "Index(['mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y', 'Image'], dtype='object')\n",
            "Train on 11224 samples, validate on 2806 samples\n",
            "Epoch 1/300\n",
            "11224/11224 [==============================] - 11s 977us/sample - loss: 272.2932 - mean_squared_error: 272.2932 - val_loss: 45.1824 - val_mean_squared_error: 45.1824\n",
            "Epoch 2/300\n",
            "11224/11224 [==============================] - 8s 678us/sample - loss: 24.5768 - mean_squared_error: 24.5768 - val_loss: 26.6834 - val_mean_squared_error: 26.6834\n",
            "Epoch 3/300\n",
            "11224/11224 [==============================] - 8s 675us/sample - loss: 18.2130 - mean_squared_error: 18.2130 - val_loss: 19.8072 - val_mean_squared_error: 19.8072\n",
            "Epoch 4/300\n",
            "11224/11224 [==============================] - 8s 674us/sample - loss: 14.3425 - mean_squared_error: 14.3425 - val_loss: 14.0490 - val_mean_squared_error: 14.0490\n",
            "Epoch 5/300\n",
            "11224/11224 [==============================] - 8s 680us/sample - loss: 11.3517 - mean_squared_error: 11.3517 - val_loss: 12.8728 - val_mean_squared_error: 12.8728\n",
            "Epoch 6/300\n",
            "11224/11224 [==============================] - 8s 690us/sample - loss: 9.5094 - mean_squared_error: 9.5094 - val_loss: 12.4209 - val_mean_squared_error: 12.4209\n",
            "Epoch 7/300\n",
            "11224/11224 [==============================] - 8s 681us/sample - loss: 8.4553 - mean_squared_error: 8.4553 - val_loss: 10.3116 - val_mean_squared_error: 10.3116\n",
            "Epoch 8/300\n",
            "11224/11224 [==============================] - 8s 683us/sample - loss: 7.3966 - mean_squared_error: 7.3966 - val_loss: 12.2237 - val_mean_squared_error: 12.2237\n",
            "Epoch 9/300\n",
            "11224/11224 [==============================] - 8s 674us/sample - loss: 6.4089 - mean_squared_error: 6.4089 - val_loss: 12.0273 - val_mean_squared_error: 12.0273\n",
            "Epoch 10/300\n",
            "11224/11224 [==============================] - 8s 684us/sample - loss: 5.9675 - mean_squared_error: 5.9675 - val_loss: 11.9305 - val_mean_squared_error: 11.9305\n",
            "Epoch 11/300\n",
            "11224/11224 [==============================] - 8s 686us/sample - loss: 5.3327 - mean_squared_error: 5.3327 - val_loss: 11.3076 - val_mean_squared_error: 11.3076\n",
            "Epoch 12/300\n",
            "11224/11224 [==============================] - 8s 681us/sample - loss: 5.0894 - mean_squared_error: 5.0894 - val_loss: 12.5283 - val_mean_squared_error: 12.5283\n",
            "Epoch 13/300\n",
            "11224/11224 [==============================] - 8s 686us/sample - loss: 4.7108 - mean_squared_error: 4.7108 - val_loss: 11.0120 - val_mean_squared_error: 11.0120\n",
            "Epoch 14/300\n",
            "11224/11224 [==============================] - 8s 677us/sample - loss: 4.4098 - mean_squared_error: 4.4098 - val_loss: 10.9980 - val_mean_squared_error: 10.9980\n",
            "Epoch 15/300\n",
            "11224/11224 [==============================] - 8s 681us/sample - loss: 4.0184 - mean_squared_error: 4.0184 - val_loss: 10.3199 - val_mean_squared_error: 10.3199\n",
            "Epoch 16/300\n",
            "11224/11224 [==============================] - 8s 678us/sample - loss: 4.0222 - mean_squared_error: 4.0222 - val_loss: 14.1247 - val_mean_squared_error: 14.1247\n",
            "Epoch 17/300\n",
            "11224/11224 [==============================] - 8s 684us/sample - loss: 3.7834 - mean_squared_error: 3.7834 - val_loss: 12.1841 - val_mean_squared_error: 12.1841\n",
            "Epoch 18/300\n",
            "11224/11224 [==============================] - 8s 679us/sample - loss: 3.5349 - mean_squared_error: 3.5349 - val_loss: 12.5287 - val_mean_squared_error: 12.5287\n",
            "Epoch 19/300\n",
            "11224/11224 [==============================] - 8s 678us/sample - loss: 3.6932 - mean_squared_error: 3.6932 - val_loss: 14.7205 - val_mean_squared_error: 14.7205\n",
            "Epoch 20/300\n",
            "11224/11224 [==============================] - 8s 673us/sample - loss: 3.2639 - mean_squared_error: 3.2639 - val_loss: 11.3323 - val_mean_squared_error: 11.3323\n",
            "Epoch 21/300\n",
            "11224/11224 [==============================] - 8s 675us/sample - loss: 3.1404 - mean_squared_error: 3.1404 - val_loss: 9.8775 - val_mean_squared_error: 9.8775\n",
            "Epoch 22/300\n",
            "11224/11224 [==============================] - 8s 676us/sample - loss: 3.0518 - mean_squared_error: 3.0518 - val_loss: 10.3312 - val_mean_squared_error: 10.3312\n",
            "Epoch 23/300\n",
            "11224/11224 [==============================] - 8s 676us/sample - loss: 3.1271 - mean_squared_error: 3.1271 - val_loss: 10.2236 - val_mean_squared_error: 10.2236\n",
            "Epoch 24/300\n",
            "11224/11224 [==============================] - 8s 679us/sample - loss: 2.9528 - mean_squared_error: 2.9528 - val_loss: 12.4143 - val_mean_squared_error: 12.4143\n",
            "Epoch 25/300\n",
            "11224/11224 [==============================] - 8s 680us/sample - loss: 2.9739 - mean_squared_error: 2.9739 - val_loss: 10.3923 - val_mean_squared_error: 10.3923\n",
            "Epoch 26/300\n",
            "11224/11224 [==============================] - 8s 679us/sample - loss: 2.9920 - mean_squared_error: 2.9920 - val_loss: 12.0433 - val_mean_squared_error: 12.0433\n",
            "Epoch 27/300\n",
            "11224/11224 [==============================] - 8s 678us/sample - loss: 2.7276 - mean_squared_error: 2.7276 - val_loss: 11.1281 - val_mean_squared_error: 11.1281\n",
            "Epoch 28/300\n",
            "11224/11224 [==============================] - 8s 676us/sample - loss: 2.6763 - mean_squared_error: 2.6763 - val_loss: 11.9581 - val_mean_squared_error: 11.9581\n",
            "Epoch 29/300\n",
            "11224/11224 [==============================] - 8s 685us/sample - loss: 2.6961 - mean_squared_error: 2.6961 - val_loss: 11.2653 - val_mean_squared_error: 11.2653\n",
            "Epoch 30/300\n",
            "11224/11224 [==============================] - 8s 676us/sample - loss: 2.5668 - mean_squared_error: 2.5668 - val_loss: 10.8351 - val_mean_squared_error: 10.8351\n",
            "Epoch 31/300\n",
            "11224/11224 [==============================] - 8s 692us/sample - loss: 2.6609 - mean_squared_error: 2.6609 - val_loss: 10.9835 - val_mean_squared_error: 10.9835\n",
            "Epoch 32/300\n",
            "11224/11224 [==============================] - 8s 678us/sample - loss: 2.4246 - mean_squared_error: 2.4246 - val_loss: 10.8630 - val_mean_squared_error: 10.8630\n",
            "Epoch 33/300\n",
            "11224/11224 [==============================] - 8s 690us/sample - loss: 2.3939 - mean_squared_error: 2.3939 - val_loss: 11.4180 - val_mean_squared_error: 11.4180\n",
            "Epoch 34/300\n",
            " 6560/11224 [================>.............] - ETA: 2s - loss: 2.5616 - mean_squared_error: 2.5616"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f15356c92850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimproved_specialist_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_improved_2layer_specialists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mouth_center_bottom_lip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-8616189aa334>\u001b[0m in \u001b[0;36mtrain_improved_2layer_specialists\u001b[0;34m(df, keypoint, sf, doi, dos, lrf, fc1, fc2)\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m       callbacks=[time_callback, early_stop])\n\u001b[0m\u001b[1;32m     76\u001b[0m   \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n13Km-GIYadP",
        "colab_type": "text"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "Our final project consisted of learning how to build, train and improve a convolutional neural network in order to learn to predict the location of facial keypoints such as the tips of noses and the centers of eyes. We compared CNN performance against two baseline models (mean model and KNN) and showed vastly superior performance. This superior performance did not come for free, and did require significant CPU/GPU training time and user time to determine how to optimize hyperparameters such as filter depth, dropout rate, and dense layer size. This project was very effective and providing our team with a empirical framework from which to build on our fundamental understanding of simple neural networks and extend that into the deep learning architecture space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4WPlohzzTMU",
        "colab_type": "text"
      },
      "source": [
        "## Plot Creation for Report\n",
        "\n",
        "The section below is used soley for creating plots for the final presentation. This is not part of the main content of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jwr8n_TZ0_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77416f8d-db02-429e-8e2d-bc26858e21ac"
      },
      "source": [
        "! ls /content/drive/My\\ Drive/FacialKeypointDetection/OutputData\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4l_spec_left_eye_center_d0.2_s0.15_sf20_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_left_eye_center_d0.2_s0.2_sf20_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.05_s0.1_sf8_lrf10_fc1200_fc250_kern2334.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.05_s0.1_sf8_lrf10_fc150_fc250_kern2334.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.0_sf4_lrf1_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.0_sf4_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.1_sf16_lrf10_fc1100_fc250_kern2334.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.1_sf16_lrf10_fc1100_fc250.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.1_sf16_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.0_s0.1_sf4_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.15_s0.15_sf12_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.15_s0.15_sf16_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.12_sf12_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.15_sf12_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf12_lrf10_fc1100_fc250.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf12_lrf10_fc1200_fc250.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf16_lrf10_fc1200_fc250.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf2_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf10_fc1200_fc2200_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1100_fc2100_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1100_fc2100_kern5432.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1150_fc2150_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1200_fc2200_kern2222.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1200_fc2200_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1200_fc2200_kern5432.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf20_fc1300_fc2300_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf30_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf6_lrf10_fc1100_fc2100_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf8_lrf20_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.25_s0.15_sf24_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.05_sf16_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.15_sf20_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.17_sf24_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf6_lrf10_fc1100_fc2100_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf6_lrf10_fc1100_fc2100_kern2345.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf6_lrf10_fc1100_fc250_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf6_lrf10_fc150_fc250_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf8_lrf10_fc1100_fc250_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip_d0.2_s0.1_sf8_lrf10_fc150_fc250_kern2345_nf.pkl\n",
            "4l_spec_mouth_center_bottom_lip.pkl\n",
            "4l_spec_nose_tip_d0.0_s0.1_sf16_lrf10_fc1100_fc250_kern2334.pkl\n",
            "4l_spec_nose_tip_d0.2_s0.15_sf20_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.05_s0.15_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.0_s0.15_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.0_s0.17_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.15_s0.1_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.1_s0.15_sf12_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "4l_spec_right_eye_center_d0.1_s0.1_sf6_lrf10_fc1200_fc2200_kern2345.pkl\n",
            "cnn_flipped_df2.pkl\n",
            "cnn_flipped_df3.pkl\n",
            "cnn_flipped_df4.pkl\n",
            "cnn_flipped_df5.pkl\n",
            "cnn_flipped_df6.pkl\n",
            "cnn_flipped_df.pkl\n",
            "cnn_lr_df.pkl\n",
            "cnn_stride_df.pkl\n",
            "cnn_vgg_df.pkl\n",
            "cnn_vgg_flipped2_df.pkl\n",
            "cnn_vgg_flipped3_df.pkl\n",
            "cnn_vgg_flipped4_df.pkl\n",
            "cnn_vgg_flipped_df.pkl\n",
            "cnn_vgg_lr_df.pkl\n",
            "single_layer_df.pkl\n",
            "spec_01.pkl\n",
            "spec_left_eyebrow_inner_end.pkl\n",
            "spec_left_eyebrow_outer_end.pkl\n",
            "spec_left_eye_center.pkl\n",
            "spec_left_eye_inner_corner.pkl\n",
            "spec_left_eye_outer_corner.pkl\n",
            "spec_mouth_center_bottom_lip.pkl\n",
            "spec_mouth_center_top_lip.pkl\n",
            "spec_mouth_left_corner.pkl\n",
            "spec_mouth_right_corner.pkl\n",
            "spec_nose_tip.pkl\n",
            "spec_right_eyebrow_inner_end.pkl\n",
            "spec_right_eyebrow_outer_end.pkl\n",
            "spec_right_eye_center.pkl\n",
            "spec_right_eye_inner_corner.pkl\n",
            "spec_right_eye_outer_corner.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3I5EYHDWmUP",
        "colab_type": "text"
      },
      "source": [
        "### Create a plot of dropout vs starting filter depth\n",
        "Make a list of four of the model outputs we ran to show the sensitivity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uq1Fa2ttxJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List of outputs from four sensitivities\n",
        "files = ['4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf4_lrf10_fc1200_fc2200_kern2345.pkl',\n",
        "         '4l_spec_mouth_center_bottom_lip_d0.1_s0.1_sf8_lrf20_fc1200_fc2200_kern2345.pkl',\n",
        "         '4l_spec_mouth_center_bottom_lip_d0.2_s0.05_sf16_lrf10_fc1200_fc2200_kern2345.pkl',\n",
        "         '4l_spec_mouth_center_bottom_lip_d0.2_s0.15_sf20_lrf10_fc1200_fc2200_kern2345.pkl',\n",
        "         '4l_spec_mouth_center_bottom_lip_d0.2_s0.17_sf24_lrf10_fc1200_fc2200_kern2345.pkl']\n",
        "\n",
        "# Read in and combin the above files into one data frame\n",
        "do_plot_df = pd.DataFrame()\n",
        "for file in files:\n",
        "  df = pd.read_pickle(drive_path+\"OutputData/\"+file)\n",
        "  do_plot_df = pd.concat([do_plot_df, df])\n",
        "  \n",
        "# Group the data\n",
        "groups = do_plot_df.groupby(['starting_filter', 'dropout_initial', 'dropout_step'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmjChyKI0OIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "262db4a2-219e-4d07-b2fc-e9774219ac94"
      },
      "source": [
        "# Create some labels that we will use for the subplots\n",
        "labels = ['Dropout 40% - Filter Depth - 4\\n196,946 Parameters',\n",
        "          'Dropout 40% - Filter Depth - 8\\n372,162 Parameters',\n",
        "          'Dropout 35% - Filter Depth - 16\\n784,610 Parameters',\n",
        "          'Dropout 65% - Filter Depth - 20\\n1,021,842 Parameters',\n",
        "          'Dropout 71% - Filter Depth - 24\\n1,279,746 Parameters']\n",
        "\n",
        "# Create the subplots\n",
        "fig, axes = plt.subplots(2, len(groups), figsize=(20,5), sharex=True)\n",
        "axes = axes.flatten()\n",
        "# Loop over the groups and plot each on its own subplot\n",
        "for g, (name, group) in enumerate(groups):\n",
        "  # Plot Accuracies\n",
        "  axes[g].plot(group.epoch, group.val_RMSE, c='tab:red',linewidth=1)\n",
        "  axes[g].plot(group.epoch, group.RMSE, c='tab:cyan',linewidth=3)\n",
        "  axes[g].grid(True)\n",
        "  axes[g].set_ylim((0,4))\n",
        "  axes[g].set_xlim((0,300))\n",
        "  axes[g].set_title(labels[g])\n",
        "  # Plot epoch training times\n",
        "  axes[g+len(groups)].plot(group.epoch, group.times, c='tab:orange',linewidth=1)\n",
        "  axes[g+len(groups)].grid(True)\n",
        "  axes[g+len(groups)].set_ylim((0,15))\n",
        "  axes[g+len(groups)].set_xlim((0,300))\n",
        "  axes[g+len(groups)].set_xlabel(\"Epoch Number\")\n",
        "  if (g==0):\n",
        "    axes[g].set_ylabel(\"Root Mean Squared Error\")\n",
        "    axes[g+len(groups)].set_ylabel(\"Epoch Training Time\")\n",
        "    \n",
        "fig.subplots_adjust(wspace=0.20,hspace=0.15)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ8AAAFcCAYAAABmyR/MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1440x360 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMyj9r7gXMov",
        "colab_type": "text"
      },
      "source": [
        "### Barchart of Specialist Model Errors\n",
        "\n",
        "Create a bar chart that shows error for each specialist model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvmB3677cV86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "b21f571f-9403-44da-ffef-7126d49d54ce"
      },
      "source": [
        "# Read in the original specialist model output\n",
        "spec_df = pd.read_pickle(drive_path+\"OutputData/spec_01.pkl\")\n",
        "\n",
        "# Grab the accuracies for each keypoint\n",
        "keypoint_mins = spec_df.groupby(['keypoint']).val_RMSE.min().sort_values(ascending=True)\n",
        "\n",
        "# Set some plot features\n",
        "bar_width = 0.50\n",
        "index = np.arange(len(keypoint_mins))\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,10))\n",
        "fig.patch.set_facecolor('gainsboro')\n",
        "ax.barh(index, keypoint_mins, bar_width, \n",
        "       color='dodgerblue')\n",
        "ax.set_yticks(index)\n",
        "ax.set_yticklabels(keypoint_mins.index, fontweight='bold', fontsize=12)\n",
        "ax.set_xlabel(\"Keypoint RMSE\", fontsize=14, fontweight='bold')\n",
        "ax.grid(True, axis='x')\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoIAAAJUCAYAAACBl0cnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 576x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNwDKK9mKbQX",
        "colab_type": "text"
      },
      "source": [
        "### Plot a random assortment of nosetips\n",
        "This shows the variability in what has been labelled a nose tip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMQ4jQ3YIncZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "0817ee5c-2445-448f-c214-3b554279f54e"
      },
      "source": [
        "# Get 10 random images\n",
        "random_images = np.random.randint(0, len(df), 10)\n",
        "\n",
        "# Create a figure with 10 subplots\n",
        "fig, axes = plt.subplots(2,5,figsize=(20,8), subplot_kw={'xticks':[],'yticks':[]})\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Loop over each and plot the image and the nose_tip keypoint\n",
        "for a, ax in enumerate(axes):\n",
        "  ax.imshow(df.iloc[random_images[a],-1].reshape(96,96),cmap='gray')\n",
        "  ax.scatter(x=df.iloc[random_images[a],-11],\n",
        "             y=df.iloc[random_images[a],-10])\n",
        "  \n",
        "fig.subplots_adjust(hspace=0.0,wspace=0.05)\n",
        "fig.suptitle(\"Random Nose Tips from Training Data\", fontsize=18, y=0.91)\n",
        "print()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAAHUCAYAAAC9AbS7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAPoQp9FoNBqNRqPRaDQajUbjAOD/AiDTmLsRaAVjAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x576 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89uq3UlwXqfJ",
        "colab_type": "text"
      },
      "source": [
        "### Convolution Layers\n",
        "\n",
        "Plots for the outputs of each convolution layer for our nose tip specialist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQCtJFQaJB65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4ef6589d-1e8a-4a37-d60d-1cb874e82f27"
      },
      "source": [
        "nt_model_path = drive_path+\"Models/cnn_4l_spec_nose_tip_d0.2_s0.15_sf20_lrfactor10_flipped_100_fc1200_fc2200_kern2345.h5\"\n",
        "nt_model = models.load_model(nt_model_path)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0804 16:07:26.568227 140259710695296 nn_ops.py:4224] Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0804 16:07:26.715865 140259710695296 nn_ops.py:4224] Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF8yPT_VYvaE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ebf0853-1232-4f54-ba69-f00b060c5da2"
      },
      "source": [
        "nt_model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 96, 96, 20)        80        \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 96, 96, 20)        80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 48, 48, 20)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 48, 48, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 48, 48, 40)        7200      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 40)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 40)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 40)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 80)        51200     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 80)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 80)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 80)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 100)       200000    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 100)       400       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 100)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6, 6, 100)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               720200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 200)               40200     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "Specialist (Dense)           (None, 2)                 402       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 1,021,842\n",
            "Trainable params: 1,020,562\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU7OvZ9-Yz2V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "0b245f7f-d0f8-42d0-87c6-a673b72c31ea"
      },
      "source": [
        "layer_names = ['batch_normalization',\n",
        "               'batch_normalization_1',\n",
        "               'batch_normalization_2',\n",
        "               'batch_normalization_3']\n",
        "\n",
        "labels = ['Normalized Output\\nFirst Convolution Layer\\n96x96\\n20 of These',\n",
        "          'Normalized Output\\nFirst Convolution Layer\\n48x48\\n40 of These',\n",
        "          'Normalized Output\\nFirst Convolution Layer\\n24x24\\n80 of These',\n",
        "          'Normalized Output\\nFirst Convolution Layer\\n12x12\\n100 of These']\n",
        "\n",
        "fig, axes = plt.subplots(1, len(layer_names)+1, figsize=(20,8),\n",
        "                         subplot_kw={'xticks':[],'yticks':[]})\n",
        "\n",
        "axes = axes.flatten()\n",
        "axes[0].imshow(df.iloc[0, -1].reshape(96,96),cmap='gray')\n",
        "\n",
        "for l, layer in enumerate(layer_names):\n",
        "  intermediate_layer_model = models.Model(inputs=nt_model.input,\n",
        "                                   outputs=nt_model.get_layer(layer).output)\n",
        "  intermediate_output = intermediate_layer_model.predict([df.iloc[0, -1].reshape(1,96,96,1)])\n",
        "  pixels = len(intermediate_output[0,:,:,1])\n",
        "  plot_data = intermediate_output[0,:,:,1].reshape(pixels,pixels)\n",
        "  axes[l+1].imshow(plot_data,cmap='gray_r')\n",
        "  axes[l+1].set_title(labels[l])\n",
        "axes[0].set_title(\"Original Image\")\n",
        "  \n",
        "print()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAAEPCAYAAADF+FhXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCFNA/ogTQgghhBBCCCGEMAX8P1ODWPkVgW8zAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x576 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f2REez4dhhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da16b0d7-1cd5-48ea-f614-5124c811c86d"
      },
      "source": [
        "nt_model.inputs"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'conv2d_input:0' shape=(?, 96, 96, 1) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    }
  ]
}