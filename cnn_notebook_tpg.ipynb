{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Flow Notebook\n",
    "## W207 Final Project\n",
    "### T. P. Goter\n",
    "### July 15, 2019\n",
    "\n",
    "This workbook is used to create convolutional neural nets for facial keypoint detection on CPUs (not TPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras import models, layers, callbacks\n",
    "from tensorflow.keras import optimizers, metrics\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "- Load in the pickle file that was created as part of the EDA in DataExploration.ipynb. \n",
    "- This dataset has the NaNs removed and a few mislabeled images removed as well. \n",
    "- As such there is only limited training and development data to use. \n",
    "- The image data has already been normalized to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Seed for reproducibility\n",
    "np.random.seed(13)\n",
    "\n",
    "# Load the dataframe from the pickle file\n",
    "df_nostache_nonan = pd.read_pickle(\"df_nostache_nonan.pkl\")\n",
    "\n",
    "# Grab the last column - that is our image data for X matrix\n",
    "X = df_nostache_nonan.iloc[:, -1]\n",
    "\n",
    "# Convert from a series of arrays to an NDarray\n",
    "X = np.array([x.reshape(96,96,1) for x in X])\n",
    "\n",
    "# Grab the keypoints and stick into our y-variable\n",
    "y = np.array(df_nostache_nonan.iloc[:,:-1])\n",
    "# y = (y - 48) / 48  # scale target coordinates to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2140, 96, 96, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Neural Net\n",
    "CNNs are combinations of convolution layers, pooling layers and dropout layers followed by one or two fully connected layers once the number of dimensions has been sufficiently reduced.\n",
    "\n",
    "1. Let's include just one hidden layer and one output layer\n",
    "2. The input layer will reduce our flattened 96x96 matrix (i.e., 9216 in length) to a predetermined number of hidden units\n",
    "3. We will then run sensitivities to # of hidden units, activation, optimizer, and learning rate\n",
    "4. We will judge our model based on RMSE error and run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    '''\n",
    "    Simple function that retruns a keras cnn model \n",
    "    '''\n",
    "    cnn_model = tf.keras.models.Sequential()\n",
    "    cnn_model.add(tf.keras.layers.InputLayer(input_shape=(96, 96, 1)))\n",
    "    cnn_model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='valid', activation='relu'))\n",
    "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn_model.add(tf.keras.layers.Conv2D(64, (2, 2), padding='valid', activation='relu'))\n",
    "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn_model.add(tf.keras.layers.Conv2D(128, (2, 2), padding='valid', activation='relu'))\n",
    "    cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    cnn_model.add(tf.keras.layers.Flatten())\n",
    "    cnn_model.add(tf.keras.layers.Dense(500))\n",
    "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
    "    cnn_model.add(tf.keras.layers.Dense(500))\n",
    "    cnn_model.add(tf.keras.layers.Activation('relu'))\n",
    "    cnn_model.add(tf.keras.layers.Dense(30))\n",
    "    cnn_model.add(tf.keras.layers.Activation('linear'))\n",
    "\n",
    "    print(50*\"=\")\n",
    "    print(cnn_model.summary())\n",
    "    print(50*\"=\")\n",
    "  \n",
    "    return cnn_model\n",
    "   \n",
    "class TimeHistory(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define our optimizers for our initial study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "sgd = optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=True)\n",
    "nadam = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "adagrad = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "\n",
    "opt_list = {'adam':adam, 'sgd':sgd, 'nadam':nadam, 'adagrad':adagrad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 94, 94, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 47, 47, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 46, 46, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 22, 22, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 15488)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 500)               7744500   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 30)                15030     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 30)                0         \n",
      "=================================================================\n",
      "Total params: 8,051,502\n",
      "Trainable params: 8,051,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up our first DOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 94, 94, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 47, 47, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 46, 46, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 22, 22, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 15488)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 500)               7744500   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 30)                15030     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 30)                0         \n",
      "=================================================================\n",
      "Total params: 8,051,502\n",
      "Trainable params: 8,051,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "==================================================\n",
      "Train on 1819 samples, validate on 321 samples\n",
      "Epoch 1/200\n",
      "1819/1819 [==============================] - 10s 6ms/sample - loss: 522.2943 - mean_squared_error: 522.2944 - val_loss: 12.8314 - val_mean_squared_error: 12.8314\n",
      "Epoch 2/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 12.2483 - mean_squared_error: 12.2483 - val_loss: 14.7813 - val_mean_squared_error: 14.7813\n",
      "Epoch 3/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 11.4575 - mean_squared_error: 11.4575 - val_loss: 11.1600 - val_mean_squared_error: 11.1600\n",
      "Epoch 4/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 10.7529 - mean_squared_error: 10.7529 - val_loss: 10.4139 - val_mean_squared_error: 10.4139\n",
      "Epoch 5/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 10.5129 - mean_squared_error: 10.5129 - val_loss: 10.1504 - val_mean_squared_error: 10.1504\n",
      "Epoch 6/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 10.2183 - mean_squared_error: 10.2183 - val_loss: 10.1813 - val_mean_squared_error: 10.1813\n",
      "Epoch 7/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 10.2979 - mean_squared_error: 10.2979 - val_loss: 9.6575 - val_mean_squared_error: 9.6575\n",
      "Epoch 8/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 10.0909 - mean_squared_error: 10.0909 - val_loss: 9.6776 - val_mean_squared_error: 9.6776\n",
      "Epoch 9/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 9.9090 - mean_squared_error: 9.9090 - val_loss: 9.7846 - val_mean_squared_error: 9.7846\n",
      "Epoch 10/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 10.0349 - mean_squared_error: 10.0349 - val_loss: 9.3836 - val_mean_squared_error: 9.3836\n",
      "Epoch 11/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 9.6630 - mean_squared_error: 9.6630 - val_loss: 9.6773 - val_mean_squared_error: 9.6773\n",
      "Epoch 12/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 10.1223 - mean_squared_error: 10.1223 - val_loss: 9.2760 - val_mean_squared_error: 9.2760\n",
      "Epoch 13/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 9.5880 - mean_squared_error: 9.5880 - val_loss: 9.2896 - val_mean_squared_error: 9.2896\n",
      "Epoch 14/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 9.3844 - mean_squared_error: 9.3844 - val_loss: 8.9254 - val_mean_squared_error: 8.9254\n",
      "Epoch 15/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 9.2763 - mean_squared_error: 9.2763 - val_loss: 8.8448 - val_mean_squared_error: 8.8448\n",
      "Epoch 16/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 9.0134 - mean_squared_error: 9.0134 - val_loss: 8.6266 - val_mean_squared_error: 8.6266\n",
      "Epoch 17/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 8.7205 - mean_squared_error: 8.7205 - val_loss: 8.7148 - val_mean_squared_error: 8.7148\n",
      "Epoch 18/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 8.5170 - mean_squared_error: 8.5170 - val_loss: 8.2926 - val_mean_squared_error: 8.2926\n",
      "Epoch 19/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 8.2090 - mean_squared_error: 8.2090 - val_loss: 7.9502 - val_mean_squared_error: 7.9502\n",
      "Epoch 20/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 7.9642 - mean_squared_error: 7.9642 - val_loss: 8.5713 - val_mean_squared_error: 8.5713\n",
      "Epoch 21/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 7.5044 - mean_squared_error: 7.5044 - val_loss: 7.3605 - val_mean_squared_error: 7.3605\n",
      "Epoch 22/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 7.0444 - mean_squared_error: 7.0444 - val_loss: 6.9347 - val_mean_squared_error: 6.9347\n",
      "Epoch 23/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 6.9151 - mean_squared_error: 6.9151 - val_loss: 6.6453 - val_mean_squared_error: 6.6453\n",
      "Epoch 24/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 7.0650 - mean_squared_error: 7.0650 - val_loss: 6.5795 - val_mean_squared_error: 6.5795\n",
      "Epoch 25/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 6.2459 - mean_squared_error: 6.2459 - val_loss: 6.3944 - val_mean_squared_error: 6.3944\n",
      "Epoch 26/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 5.9498 - mean_squared_error: 5.9498 - val_loss: 5.9235 - val_mean_squared_error: 5.9235\n",
      "Epoch 27/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 5.8498 - mean_squared_error: 5.8498 - val_loss: 6.5677 - val_mean_squared_error: 6.5677\n",
      "Epoch 28/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 5.6168 - mean_squared_error: 5.6168 - val_loss: 5.9864 - val_mean_squared_error: 5.9864\n",
      "Epoch 29/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 5.3582 - mean_squared_error: 5.3582 - val_loss: 5.3049 - val_mean_squared_error: 5.3049\n",
      "Epoch 30/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 5.0865 - mean_squared_error: 5.0865 - val_loss: 5.0687 - val_mean_squared_error: 5.0687\n",
      "Epoch 31/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 5.0028 - mean_squared_error: 5.0028 - val_loss: 4.9365 - val_mean_squared_error: 4.9365\n",
      "Epoch 32/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 4.7872 - mean_squared_error: 4.7872 - val_loss: 4.7577 - val_mean_squared_error: 4.7577\n",
      "Epoch 33/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 4.6041 - mean_squared_error: 4.6041 - val_loss: 4.5973 - val_mean_squared_error: 4.5973\n",
      "Epoch 34/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 4.3970 - mean_squared_error: 4.3970 - val_loss: 4.5907 - val_mean_squared_error: 4.5907\n",
      "Epoch 35/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 4.3943 - mean_squared_error: 4.3943 - val_loss: 4.5755 - val_mean_squared_error: 4.5755\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 4.2366 - mean_squared_error: 4.2366 - val_loss: 4.6964 - val_mean_squared_error: 4.6964\n",
      "Epoch 37/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 4.1095 - mean_squared_error: 4.1095 - val_loss: 4.3056 - val_mean_squared_error: 4.3056\n",
      "Epoch 38/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 4.1825 - mean_squared_error: 4.1825 - val_loss: 4.2648 - val_mean_squared_error: 4.2648\n",
      "Epoch 39/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.8935 - mean_squared_error: 3.8935 - val_loss: 4.2980 - val_mean_squared_error: 4.2980\n",
      "Epoch 40/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 4.0959 - mean_squared_error: 4.0959 - val_loss: 4.1418 - val_mean_squared_error: 4.1418\n",
      "Epoch 41/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.7454 - mean_squared_error: 3.7454 - val_loss: 4.1137 - val_mean_squared_error: 4.1137\n",
      "Epoch 42/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.6906 - mean_squared_error: 3.6906 - val_loss: 3.9263 - val_mean_squared_error: 3.9263\n",
      "Epoch 43/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.6724 - mean_squared_error: 3.6724 - val_loss: 4.1104 - val_mean_squared_error: 4.1104\n",
      "Epoch 44/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.5073 - mean_squared_error: 3.5073 - val_loss: 4.5073 - val_mean_squared_error: 4.5073\n",
      "Epoch 45/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.4862 - mean_squared_error: 3.4862 - val_loss: 3.9627 - val_mean_squared_error: 3.9627\n",
      "Epoch 46/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.9108 - mean_squared_error: 3.9108 - val_loss: 4.4858 - val_mean_squared_error: 4.4858\n",
      "Epoch 47/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.6200 - mean_squared_error: 3.6200 - val_loss: 3.8401 - val_mean_squared_error: 3.8401\n",
      "Epoch 48/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.3464 - mean_squared_error: 3.3464 - val_loss: 3.7165 - val_mean_squared_error: 3.7165\n",
      "Epoch 49/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.3208 - mean_squared_error: 3.3208 - val_loss: 4.5839 - val_mean_squared_error: 4.5839\n",
      "Epoch 50/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.5923 - mean_squared_error: 3.5923 - val_loss: 3.7849 - val_mean_squared_error: 3.7849\n",
      "Epoch 51/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.1198 - mean_squared_error: 3.1198 - val_loss: 3.6505 - val_mean_squared_error: 3.6505\n",
      "Epoch 52/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.4231 - mean_squared_error: 3.4231 - val_loss: 3.6687 - val_mean_squared_error: 3.6687\n",
      "Epoch 53/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 3.0098 - mean_squared_error: 3.0098 - val_loss: 3.3861 - val_mean_squared_error: 3.3861\n",
      "Epoch 54/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.9493 - mean_squared_error: 2.9493 - val_loss: 3.4386 - val_mean_squared_error: 3.4386\n",
      "Epoch 55/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.8740 - mean_squared_error: 2.8740 - val_loss: 3.3560 - val_mean_squared_error: 3.3560\n",
      "Epoch 56/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.8185 - mean_squared_error: 2.8185 - val_loss: 3.4962 - val_mean_squared_error: 3.4962\n",
      "Epoch 57/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.8149 - mean_squared_error: 2.8149 - val_loss: 3.4551 - val_mean_squared_error: 3.4551\n",
      "Epoch 58/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.6908 - mean_squared_error: 2.6908 - val_loss: 3.3917 - val_mean_squared_error: 3.3917\n",
      "Epoch 59/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.6030 - mean_squared_error: 2.6030 - val_loss: 3.2512 - val_mean_squared_error: 3.2512\n",
      "Epoch 60/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.5890 - mean_squared_error: 2.5890 - val_loss: 3.6396 - val_mean_squared_error: 3.6396\n",
      "Epoch 61/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.6714 - mean_squared_error: 2.6714 - val_loss: 3.5038 - val_mean_squared_error: 3.5038\n",
      "Epoch 62/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.5426 - mean_squared_error: 2.5426 - val_loss: 3.1785 - val_mean_squared_error: 3.1785\n",
      "Epoch 63/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.5650 - mean_squared_error: 2.5650 - val_loss: 3.2076 - val_mean_squared_error: 3.2076\n",
      "Epoch 64/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.4931 - mean_squared_error: 2.4931 - val_loss: 3.4452 - val_mean_squared_error: 3.4452\n",
      "Epoch 65/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.4524 - mean_squared_error: 2.4524 - val_loss: 3.1653 - val_mean_squared_error: 3.1653\n",
      "Epoch 66/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.3461 - mean_squared_error: 2.3461 - val_loss: 3.1022 - val_mean_squared_error: 3.1022\n",
      "Epoch 67/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.4554 - mean_squared_error: 2.4554 - val_loss: 3.8417 - val_mean_squared_error: 3.8417\n",
      "Epoch 68/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.2943 - mean_squared_error: 2.2943 - val_loss: 3.0375 - val_mean_squared_error: 3.0375\n",
      "Epoch 69/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.2332 - mean_squared_error: 2.2332 - val_loss: 3.2204 - val_mean_squared_error: 3.2204\n",
      "Epoch 70/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.2501 - mean_squared_error: 2.2501 - val_loss: 3.0882 - val_mean_squared_error: 3.0882\n",
      "Epoch 71/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.2067 - mean_squared_error: 2.2067 - val_loss: 2.9002 - val_mean_squared_error: 2.9002\n",
      "Epoch 72/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.1937 - mean_squared_error: 2.1937 - val_loss: 3.1264 - val_mean_squared_error: 3.1264\n",
      "Epoch 73/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.2517 - mean_squared_error: 2.2517 - val_loss: 3.1031 - val_mean_squared_error: 3.1031\n",
      "Epoch 74/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.1503 - mean_squared_error: 2.1503 - val_loss: 2.9200 - val_mean_squared_error: 2.9200\n",
      "Epoch 75/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.0835 - mean_squared_error: 2.0835 - val_loss: 3.1168 - val_mean_squared_error: 3.1168\n",
      "Epoch 76/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.9938 - mean_squared_error: 1.9938 - val_loss: 3.0687 - val_mean_squared_error: 3.0687\n",
      "Epoch 77/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.9274 - mean_squared_error: 1.9274 - val_loss: 3.0656 - val_mean_squared_error: 3.0656\n",
      "Epoch 78/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.0278 - mean_squared_error: 2.0278 - val_loss: 4.1932 - val_mean_squared_error: 4.1932\n",
      "Epoch 79/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.3402 - mean_squared_error: 2.3402 - val_loss: 2.9382 - val_mean_squared_error: 2.9382\n",
      "Epoch 80/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 2.0329 - mean_squared_error: 2.0329 - val_loss: 2.9346 - val_mean_squared_error: 2.9346\n",
      "Epoch 81/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.8349 - mean_squared_error: 1.8349 - val_loss: 2.7686 - val_mean_squared_error: 2.7686\n",
      "Epoch 82/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.8241 - mean_squared_error: 1.8241 - val_loss: 3.0277 - val_mean_squared_error: 3.0277\n",
      "Epoch 83/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.8736 - mean_squared_error: 1.8736 - val_loss: 2.9927 - val_mean_squared_error: 2.9927\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.7629 - mean_squared_error: 1.7629 - val_loss: 2.8327 - val_mean_squared_error: 2.8327\n",
      "Epoch 85/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.7365 - mean_squared_error: 1.7365 - val_loss: 2.8916 - val_mean_squared_error: 2.8916\n",
      "Epoch 86/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.6457 - mean_squared_error: 1.6457 - val_loss: 2.8114 - val_mean_squared_error: 2.8114\n",
      "Epoch 87/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.6751 - mean_squared_error: 1.6751 - val_loss: 2.7579 - val_mean_squared_error: 2.7579\n",
      "Epoch 88/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.6433 - mean_squared_error: 1.6433 - val_loss: 2.8155 - val_mean_squared_error: 2.8155\n",
      "Epoch 89/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.6258 - mean_squared_error: 1.6258 - val_loss: 2.9246 - val_mean_squared_error: 2.9246\n",
      "Epoch 90/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.5905 - mean_squared_error: 1.5905 - val_loss: 2.7129 - val_mean_squared_error: 2.7129\n",
      "Epoch 91/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.5384 - mean_squared_error: 1.5384 - val_loss: 2.7533 - val_mean_squared_error: 2.7533\n",
      "Epoch 92/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.5124 - mean_squared_error: 1.5124 - val_loss: 2.7838 - val_mean_squared_error: 2.7838\n",
      "Epoch 93/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.5544 - mean_squared_error: 1.5544 - val_loss: 2.7370 - val_mean_squared_error: 2.7370\n",
      "Epoch 94/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.7489 - mean_squared_error: 1.7489 - val_loss: 3.0112 - val_mean_squared_error: 3.0112\n",
      "Epoch 95/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.5916 - mean_squared_error: 1.5916 - val_loss: 3.2693 - val_mean_squared_error: 3.2693\n",
      "Epoch 96/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.4745 - mean_squared_error: 1.4745 - val_loss: 2.6484 - val_mean_squared_error: 2.6484\n",
      "Epoch 97/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.4756 - mean_squared_error: 1.4756 - val_loss: 2.7209 - val_mean_squared_error: 2.7209\n",
      "Epoch 98/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.3670 - mean_squared_error: 1.3670 - val_loss: 2.7063 - val_mean_squared_error: 2.7063\n",
      "Epoch 99/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.2845 - mean_squared_error: 1.2845 - val_loss: 2.6491 - val_mean_squared_error: 2.6491\n",
      "Epoch 100/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.3498 - mean_squared_error: 1.3498 - val_loss: 2.8090 - val_mean_squared_error: 2.8090\n",
      "Epoch 101/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.3683 - mean_squared_error: 1.3683 - val_loss: 2.7887 - val_mean_squared_error: 2.7887\n",
      "Epoch 102/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.7578 - mean_squared_error: 1.7578 - val_loss: 2.9453 - val_mean_squared_error: 2.9453\n",
      "Epoch 103/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.3604 - mean_squared_error: 1.3604 - val_loss: 2.7103 - val_mean_squared_error: 2.7103\n",
      "Epoch 104/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.2782 - mean_squared_error: 1.2782 - val_loss: 2.6300 - val_mean_squared_error: 2.6300\n",
      "Epoch 105/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.1962 - mean_squared_error: 1.1962 - val_loss: 2.6592 - val_mean_squared_error: 2.6592\n",
      "Epoch 106/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.1764 - mean_squared_error: 1.1764 - val_loss: 2.7391 - val_mean_squared_error: 2.7391\n",
      "Epoch 107/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.1210 - mean_squared_error: 1.1210 - val_loss: 2.6863 - val_mean_squared_error: 2.6863\n",
      "Epoch 108/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.2791 - mean_squared_error: 1.2791 - val_loss: 2.9369 - val_mean_squared_error: 2.9369\n",
      "Epoch 109/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.4495 - mean_squared_error: 1.4495 - val_loss: 2.8608 - val_mean_squared_error: 2.8608\n",
      "Epoch 110/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.2796 - mean_squared_error: 1.2796 - val_loss: 2.7610 - val_mean_squared_error: 2.7610\n",
      "Epoch 111/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.1948 - mean_squared_error: 1.1948 - val_loss: 2.8391 - val_mean_squared_error: 2.8391\n",
      "Epoch 112/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.0433 - mean_squared_error: 1.0433 - val_loss: 2.6867 - val_mean_squared_error: 2.6867\n",
      "Epoch 113/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.0507 - mean_squared_error: 1.0507 - val_loss: 2.6771 - val_mean_squared_error: 2.6771\n",
      "Epoch 114/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.0259 - mean_squared_error: 1.0259 - val_loss: 2.6849 - val_mean_squared_error: 2.6849\n",
      "Epoch 115/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.2090 - mean_squared_error: 1.2090 - val_loss: 2.7222 - val_mean_squared_error: 2.7222\n",
      "Epoch 116/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.3457 - mean_squared_error: 1.3457 - val_loss: 2.7049 - val_mean_squared_error: 2.7049\n",
      "Epoch 117/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.0552 - mean_squared_error: 1.0552 - val_loss: 2.7371 - val_mean_squared_error: 2.7371\n",
      "Epoch 118/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.9714 - mean_squared_error: 0.9714 - val_loss: 2.9147 - val_mean_squared_error: 2.9147\n",
      "Epoch 119/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.9670 - mean_squared_error: 0.9670 - val_loss: 2.7300 - val_mean_squared_error: 2.7300\n",
      "Epoch 120/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.9121 - mean_squared_error: 0.9121 - val_loss: 2.6774 - val_mean_squared_error: 2.6774\n",
      "Epoch 121/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.0464 - mean_squared_error: 1.0464 - val_loss: 2.6820 - val_mean_squared_error: 2.6820\n",
      "Epoch 122/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.1550 - mean_squared_error: 1.1550 - val_loss: 2.7666 - val_mean_squared_error: 2.7666\n",
      "Epoch 123/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 1.0641 - mean_squared_error: 1.0641 - val_loss: 3.6069 - val_mean_squared_error: 3.6069\n",
      "Epoch 124/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.9583 - mean_squared_error: 0.9583 - val_loss: 2.7787 - val_mean_squared_error: 2.7787\n",
      "Epoch 125/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.8763 - mean_squared_error: 0.8763 - val_loss: 2.7604 - val_mean_squared_error: 2.7604\n",
      "Epoch 126/200\n",
      "1819/1819 [==============================] - 10s 6ms/sample - loss: 0.9251 - mean_squared_error: 0.9251 - val_loss: 2.7505 - val_mean_squared_error: 2.7505\n",
      "Epoch 127/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 1.3063 - mean_squared_error: 1.3063 - val_loss: 3.3741 - val_mean_squared_error: 3.3741\n",
      "Epoch 128/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 1.0643 - mean_squared_error: 1.0643 - val_loss: 2.7144 - val_mean_squared_error: 2.7144\n",
      "Epoch 129/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.8106 - mean_squared_error: 0.8106 - val_loss: 2.6175 - val_mean_squared_error: 2.6175\n",
      "Epoch 130/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.9052 - mean_squared_error: 0.9052 - val_loss: 2.9391 - val_mean_squared_error: 2.9391\n",
      "Epoch 131/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.9430 - mean_squared_error: 0.9430 - val_loss: 2.6771 - val_mean_squared_error: 2.6771\n",
      "Epoch 132/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.9025 - mean_squared_error: 0.9025 - val_loss: 2.8500 - val_mean_squared_error: 2.8500\n",
      "Epoch 133/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.7754 - mean_squared_error: 0.7754 - val_loss: 2.7243 - val_mean_squared_error: 2.7243\n",
      "Epoch 134/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.8464 - mean_squared_error: 0.8464 - val_loss: 2.6734 - val_mean_squared_error: 2.6734\n",
      "Epoch 135/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.8815 - mean_squared_error: 0.8815 - val_loss: 2.6399 - val_mean_squared_error: 2.6399\n",
      "Epoch 136/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.7124 - mean_squared_error: 0.7124 - val_loss: 2.6558 - val_mean_squared_error: 2.6558\n",
      "Epoch 137/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.7539 - mean_squared_error: 0.7539 - val_loss: 2.7712 - val_mean_squared_error: 2.7712\n",
      "Epoch 138/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.8458 - mean_squared_error: 0.8458 - val_loss: 3.5915 - val_mean_squared_error: 3.5915\n",
      "Epoch 139/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 1.1997 - mean_squared_error: 1.1997 - val_loss: 3.2585 - val_mean_squared_error: 3.2585\n",
      "Epoch 140/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.9787 - mean_squared_error: 0.9787 - val_loss: 2.7053 - val_mean_squared_error: 2.7053\n",
      "Epoch 141/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.7424 - mean_squared_error: 0.7424 - val_loss: 2.6967 - val_mean_squared_error: 2.6967\n",
      "Epoch 142/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.6989 - mean_squared_error: 0.6989 - val_loss: 2.6182 - val_mean_squared_error: 2.6182\n",
      "Epoch 143/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.6701 - mean_squared_error: 0.6701 - val_loss: 2.6868 - val_mean_squared_error: 2.6868\n",
      "Epoch 144/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.6401 - mean_squared_error: 0.6401 - val_loss: 2.5855 - val_mean_squared_error: 2.5855\n",
      "Epoch 145/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.6412 - mean_squared_error: 0.6412 - val_loss: 2.8333 - val_mean_squared_error: 2.8333\n",
      "Epoch 146/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.6217 - mean_squared_error: 0.6217 - val_loss: 2.6166 - val_mean_squared_error: 2.6166\n",
      "Epoch 147/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.6196 - mean_squared_error: 0.6196 - val_loss: 2.6919 - val_mean_squared_error: 2.6919\n",
      "Epoch 148/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.6560 - mean_squared_error: 0.6560 - val_loss: 2.7170 - val_mean_squared_error: 2.7170\n",
      "Epoch 149/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.6642 - mean_squared_error: 0.6642 - val_loss: 2.7428 - val_mean_squared_error: 2.7428\n",
      "Epoch 150/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.6794 - mean_squared_error: 0.6794 - val_loss: 2.8245 - val_mean_squared_error: 2.8245\n",
      "Epoch 151/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.6530 - mean_squared_error: 0.6530 - val_loss: 2.6674 - val_mean_squared_error: 2.6674\n",
      "Epoch 152/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.6040 - mean_squared_error: 0.6040 - val_loss: 2.7112 - val_mean_squared_error: 2.7112\n",
      "Epoch 153/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.7928 - mean_squared_error: 0.7928 - val_loss: 2.8944 - val_mean_squared_error: 2.8944\n",
      "Epoch 154/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 1.5398 - mean_squared_error: 1.5398 - val_loss: 2.9494 - val_mean_squared_error: 2.9494\n",
      "Epoch 155/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.6372 - mean_squared_error: 0.6372 - val_loss: 2.6841 - val_mean_squared_error: 2.6841\n",
      "Epoch 156/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.6055 - mean_squared_error: 0.6055 - val_loss: 2.7889 - val_mean_squared_error: 2.7889\n",
      "Epoch 157/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.6387 - mean_squared_error: 0.6387 - val_loss: 2.7379 - val_mean_squared_error: 2.7379\n",
      "Epoch 158/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.5425 - mean_squared_error: 0.5425 - val_loss: 2.6302 - val_mean_squared_error: 2.6302\n",
      "Epoch 159/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.5377 - mean_squared_error: 0.5377 - val_loss: 2.6778 - val_mean_squared_error: 2.6778\n",
      "Epoch 160/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.5483 - mean_squared_error: 0.5483 - val_loss: 2.6831 - val_mean_squared_error: 2.6831\n",
      "Epoch 161/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.5509 - mean_squared_error: 0.5509 - val_loss: 2.7422 - val_mean_squared_error: 2.7422\n",
      "Epoch 162/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.5318 - mean_squared_error: 0.5318 - val_loss: 2.6515 - val_mean_squared_error: 2.6515\n",
      "Epoch 163/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.7068 - mean_squared_error: 0.7068 - val_loss: 2.6425 - val_mean_squared_error: 2.6425\n",
      "Epoch 164/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.5022 - mean_squared_error: 0.5022 - val_loss: 2.7244 - val_mean_squared_error: 2.7244\n",
      "Epoch 165/200\n",
      "1819/1819 [==============================] - 10s 6ms/sample - loss: 0.5039 - mean_squared_error: 0.5039 - val_loss: 2.6685 - val_mean_squared_error: 2.6685\n",
      "Epoch 166/200\n",
      "1819/1819 [==============================] - 10s 6ms/sample - loss: 0.5049 - mean_squared_error: 0.5049 - val_loss: 2.7841 - val_mean_squared_error: 2.7841\n",
      "Epoch 167/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.5073 - mean_squared_error: 0.5073 - val_loss: 2.8741 - val_mean_squared_error: 2.8741\n",
      "Epoch 168/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.5280 - mean_squared_error: 0.5280 - val_loss: 2.6615 - val_mean_squared_error: 2.6615\n",
      "Epoch 169/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.4977 - mean_squared_error: 0.4977 - val_loss: 2.6509 - val_mean_squared_error: 2.6509\n",
      "Epoch 170/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.5341 - mean_squared_error: 0.5341 - val_loss: 2.6818 - val_mean_squared_error: 2.6818\n",
      "Epoch 171/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.5692 - mean_squared_error: 0.5692 - val_loss: 2.8031 - val_mean_squared_error: 2.8031\n",
      "Epoch 172/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.5542 - mean_squared_error: 0.5542 - val_loss: 2.8525 - val_mean_squared_error: 2.8525\n",
      "Epoch 173/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.5196 - mean_squared_error: 0.5196 - val_loss: 2.8269 - val_mean_squared_error: 2.8269\n",
      "Epoch 174/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.5065 - mean_squared_error: 0.5065 - val_loss: 2.6715 - val_mean_squared_error: 2.6715\n",
      "Epoch 175/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.4820 - mean_squared_error: 0.4820 - val_loss: 2.6954 - val_mean_squared_error: 2.6954\n",
      "Epoch 176/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.4313 - mean_squared_error: 0.4313 - val_loss: 2.6789 - val_mean_squared_error: 2.6789\n",
      "Epoch 177/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.4419 - mean_squared_error: 0.4419 - val_loss: 2.6943 - val_mean_squared_error: 2.6943\n",
      "Epoch 178/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.4244 - mean_squared_error: 0.4244 - val_loss: 2.7531 - val_mean_squared_error: 2.7531\n",
      "Epoch 179/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.4826 - mean_squared_error: 0.4826 - val_loss: 2.6671 - val_mean_squared_error: 2.6671\n",
      "Epoch 180/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.4652 - mean_squared_error: 0.4652 - val_loss: 2.8808 - val_mean_squared_error: 2.8808\n",
      "Epoch 181/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.4906 - mean_squared_error: 0.4906 - val_loss: 3.0376 - val_mean_squared_error: 3.0376\n",
      "Epoch 182/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.8421 - mean_squared_error: 0.8421 - val_loss: 3.1382 - val_mean_squared_error: 3.1382\n",
      "Epoch 183/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.6145 - mean_squared_error: 0.6145 - val_loss: 2.7591 - val_mean_squared_error: 2.7591\n",
      "Epoch 184/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.4124 - mean_squared_error: 0.4124 - val_loss: 2.7161 - val_mean_squared_error: 2.7161\n",
      "Epoch 185/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.3859 - mean_squared_error: 0.3859 - val_loss: 2.7095 - val_mean_squared_error: 2.7095\n",
      "Epoch 186/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.4203 - mean_squared_error: 0.4203 - val_loss: 2.6794 - val_mean_squared_error: 2.6794\n",
      "Epoch 187/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.3789 - mean_squared_error: 0.3789 - val_loss: 2.6514 - val_mean_squared_error: 2.6514\n",
      "Epoch 188/200\n",
      "1819/1819 [==============================] - 9s 5ms/sample - loss: 0.3748 - mean_squared_error: 0.3748 - val_loss: 2.6902 - val_mean_squared_error: 2.6902\n",
      "Epoch 189/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.4102 - mean_squared_error: 0.4102 - val_loss: 2.8163 - val_mean_squared_error: 2.8163\n",
      "Epoch 190/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.3795 - mean_squared_error: 0.3795 - val_loss: 2.6979 - val_mean_squared_error: 2.6979\n",
      "Epoch 191/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.3579 - mean_squared_error: 0.3579 - val_loss: 2.7373 - val_mean_squared_error: 2.7373\n",
      "Epoch 192/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.4150 - mean_squared_error: 0.4150 - val_loss: 2.7707 - val_mean_squared_error: 2.7707\n",
      "Epoch 193/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.3726 - mean_squared_error: 0.3726 - val_loss: 2.7936 - val_mean_squared_error: 2.7936\n",
      "Epoch 194/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.4265 - mean_squared_error: 0.4265 - val_loss: 2.6948 - val_mean_squared_error: 2.6948\n",
      "Epoch 195/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.3829 - mean_squared_error: 0.3829 - val_loss: 2.7540 - val_mean_squared_error: 2.7540\n",
      "Epoch 196/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.5057 - mean_squared_error: 0.5057 - val_loss: 2.7735 - val_mean_squared_error: 2.7735\n",
      "Epoch 197/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.3907 - mean_squared_error: 0.3907 - val_loss: 2.7465 - val_mean_squared_error: 2.7465\n",
      "Epoch 198/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.3614 - mean_squared_error: 0.3614 - val_loss: 2.7352 - val_mean_squared_error: 2.7352\n",
      "Epoch 199/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.3334 - mean_squared_error: 0.3334 - val_loss: 2.7378 - val_mean_squared_error: 2.7378\n",
      "Epoch 200/200\n",
      "1819/1819 [==============================] - 10s 5ms/sample - loss: 0.3541 - mean_squared_error: 0.3541 - val_loss: 2.7390 - val_mean_squared_error: 2.7390\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hidden_unit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-9ecb56edd653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_RMSE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_mean_squared_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'times'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hunits'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_unit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'activation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden_unit' is not defined"
     ]
    }
   ],
   "source": [
    "time_callback = TimeHistory()\n",
    "cnn_base_df = pd.DataFrame()\n",
    "for opt_name, opt in opt_list.items():\n",
    "    model = create_cnn_model()\n",
    "    model.compile(\n",
    "          optimizer=opt_list['adam'],\n",
    "          loss='mean_squared_error',\n",
    "          metrics=['mean_squared_error'])\n",
    "    history = model.fit(\n",
    "        X.astype(np.float32), y.astype(np.float32),\n",
    "        epochs=200,\n",
    "        validation_split=0.15, callbacks=[time_callback])\n",
    "    times = time_callback.times\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    hist['RMSE'] = np.sqrt(hist.mean_squared_error)\n",
    "    hist['val_RMSE'] = np.sqrt(hist.val_mean_squared_error)\n",
    "    hist['times'] = times\n",
    "    hist['layers'] = 3\n",
    "    hist['pooling'] = 'yes'\n",
    "    hist['fc_layer'] = 500\n",
    "    hist['activation'] = 'relu'\n",
    "    hist['optimizer'] = opt_name\n",
    "    hist['lrate'] = opt.get_config()['learning_rate']\n",
    "    \n",
    "    # Keep concatenating to dataframe\n",
    "    cnn_base_df = pd.concat([cnn_base_df,hist])\n",
    "\n",
    "    # Re-pickle after every model to retain progress\n",
    "    cnn_base_df.to_pickle(\"OutputData/cnn_base_df.pkl\")\n",
    "\n",
    "    # Save models.\n",
    "    filename = \"cnn_model_{}\".format( opt_name)\n",
    "    model.save(\"Models/\"+filename+\".h5\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [301.0397809465355,\n",
       "  300.9012669609955,\n",
       "  300.8729474202429,\n",
       "  300.8986479353944,\n",
       "  300.8937921172777,\n",
       "  300.9504336650192,\n",
       "  300.88579441466914,\n",
       "  300.8508411174688,\n",
       "  300.84935885907004,\n",
       "  300.89306451043564,\n",
       "  300.8886131215318,\n",
       "  301.07204356306266,\n",
       "  301.0099840463027,\n",
       "  301.21548827655243,\n",
       "  300.99505954132167,\n",
       "  300.94686564172343,\n",
       "  300.8758025670065,\n",
       "  300.88346840143856,\n",
       "  300.90661314072486,\n",
       "  300.9949002090295,\n",
       "  301.05165486810233,\n",
       "  300.97625205620363,\n",
       "  300.954881513951,\n",
       "  300.9295923401852,\n",
       "  300.8546888153261,\n",
       "  300.8519757707542,\n",
       "  300.86015919606194,\n",
       "  300.8660421764674,\n",
       "  300.8818669080603,\n",
       "  300.92487491651707,\n",
       "  301.22669745513934,\n",
       "  301.12876500344134,\n",
       "  300.89463105707654,\n",
       "  300.86598989896686,\n",
       "  300.86747727438666,\n",
       "  300.8321356707841,\n",
       "  300.86906863438554,\n",
       "  300.8664869546497,\n",
       "  300.8651480399494,\n",
       "  300.86164417235386,\n",
       "  300.83562084866986,\n",
       "  300.9438621013488,\n",
       "  301.00247746027713,\n",
       "  300.8665602538794,\n",
       "  300.8337125185756,\n",
       "  300.8587711747365,\n",
       "  300.83736686014487,\n",
       "  300.8594276130429,\n",
       "  300.9145133179449,\n",
       "  300.90941450004726,\n",
       "  301.48240206122335,\n",
       "  301.11621803422105,\n",
       "  300.8678918202019,\n",
       "  300.82068127783657,\n",
       "  300.804878440383,\n",
       "  300.7968293494612,\n",
       "  300.79425575629637,\n",
       "  300.80801254029194,\n",
       "  300.84150798980056,\n",
       "  300.81937932312786,\n",
       "  300.95679997742474,\n",
       "  300.8966707352297,\n",
       "  300.8752597768992,\n",
       "  300.8705156272817,\n",
       "  300.8223804308203,\n",
       "  300.83600293076125,\n",
       "  300.97036020070266,\n",
       "  301.0104191105336,\n",
       "  301.49351774093014,\n",
       "  301.0341685978248,\n",
       "  300.89354584595867,\n",
       "  300.81791468070327,\n",
       "  300.79014229210867,\n",
       "  300.78635725678816,\n",
       "  300.78687820308744,\n",
       "  300.78236732252225,\n",
       "  300.7703160668153,\n",
       "  300.7885104657006,\n",
       "  300.7904858874908,\n",
       "  300.78210682420695,\n",
       "  300.83091505154465,\n",
       "  300.9261373443352,\n",
       "  300.905135697359,\n",
       "  300.9578976874957,\n",
       "  300.8758770909658,\n",
       "  300.85477478128007,\n",
       "  300.91488617262127,\n",
       "  300.8606599091566,\n",
       "  300.81943389909355,\n",
       "  300.79421795744895,\n",
       "  300.83283502495897,\n",
       "  300.7967776423827,\n",
       "  300.79412808242637,\n",
       "  300.8008553713085,\n",
       "  300.8276838288981,\n",
       "  300.90927321993433,\n",
       "  301.122776629179,\n",
       "  301.0198331710203,\n",
       "  301.0501747403976,\n",
       "  300.91898022568313,\n",
       "  300.80910605608335,\n",
       "  300.8122572183216,\n",
       "  300.79916866694657,\n",
       "  300.78594326461786,\n",
       "  300.80468929515166,\n",
       "  300.857433786754,\n",
       "  300.7906459244219,\n",
       "  300.85708071230005,\n",
       "  300.81539539507025,\n",
       "  300.8435740751379,\n",
       "  300.8068688105855,\n",
       "  300.95009240165655,\n",
       "  300.95814897517306,\n",
       "  300.89420443173617,\n",
       "  300.9780174955816,\n",
       "  300.8430024954957,\n",
       "  300.7945288374504,\n",
       "  300.7737129635048,\n",
       "  300.77675089697186,\n",
       "  300.773177605661,\n",
       "  300.7790416782541,\n",
       "  300.7778458052641,\n",
       "  300.84342425547175,\n",
       "  300.8210190851129,\n",
       "  300.8589302553719,\n",
       "  300.81721621571563,\n",
       "  300.99389294440306,\n",
       "  301.0093510959619,\n",
       "  300.84932461697167,\n",
       "  300.8071765532659,\n",
       "  300.7811757948059,\n",
       "  300.7906701673578,\n",
       "  300.8146494172837,\n",
       "  300.7776784367323,\n",
       "  300.8198593164814,\n",
       "  300.89202365801845,\n",
       "  300.8447281059949,\n",
       "  300.9580976707454,\n",
       "  300.88659463287905,\n",
       "  300.83720878613656,\n",
       "  300.8399254881202,\n",
       "  300.7906758044695,\n",
       "  300.7877958611252,\n",
       "  300.81216947399304,\n",
       "  300.8123151329338,\n",
       "  300.80707760182236,\n",
       "  300.81944015695865,\n",
       "  300.84117831942666,\n",
       "  300.8675001248218,\n",
       "  300.87724122167486,\n",
       "  300.93488456470226,\n",
       "  300.91426189605056,\n",
       "  300.86706925487044,\n",
       "  300.8362129131734,\n",
       "  300.80774710950385,\n",
       "  300.84196033446324,\n",
       "  300.9005567016937,\n",
       "  301.17772097008253,\n",
       "  301.009945509262,\n",
       "  300.8263461557045,\n",
       "  300.7886688249199,\n",
       "  300.78742005367604,\n",
       "  300.77820543957125,\n",
       "  300.75826281463134,\n",
       "  300.76095287455115,\n",
       "  300.76013858033906,\n",
       "  300.7601575552598,\n",
       "  300.7569942967193,\n",
       "  300.7765536148382,\n",
       "  300.8164348046553,\n",
       "  300.79343882807666,\n",
       "  300.77734692087535,\n",
       "  300.7889444897501,\n",
       "  300.8400354453533,\n",
       "  300.8194422037671,\n",
       "  300.9477530841712,\n",
       "  300.92423515466623,\n",
       "  300.88335785700633,\n",
       "  300.84501695764004,\n",
       "  300.9082873796879,\n",
       "  300.8539876659995,\n",
       "  300.7903540193412,\n",
       "  300.79303990175856,\n",
       "  300.78639389801447,\n",
       "  300.78085609004023,\n",
       "  300.7670414082096,\n",
       "  300.77276039228394,\n",
       "  300.8489329887,\n",
       "  300.80984176627356,\n",
       "  300.7866216977263,\n",
       "  300.78462812309414,\n",
       "  300.8070424034282,\n",
       "  300.832099348323,\n",
       "  300.93170652552055,\n",
       "  300.9535943566606,\n",
       "  300.93238059657824,\n",
       "  300.8953393702326,\n",
       "  300.79389045132325,\n",
       "  300.7672975780284,\n",
       "  300.7648020152406],\n",
       " 'mean_squared_error': [301.03976,\n",
       "  300.90125,\n",
       "  300.87302,\n",
       "  300.8986,\n",
       "  300.8938,\n",
       "  300.9504,\n",
       "  300.88583,\n",
       "  300.8508,\n",
       "  300.84937,\n",
       "  300.8931,\n",
       "  300.88858,\n",
       "  301.07205,\n",
       "  301.01,\n",
       "  301.21542,\n",
       "  300.99506,\n",
       "  300.94684,\n",
       "  300.87582,\n",
       "  300.8835,\n",
       "  300.90656,\n",
       "  300.9949,\n",
       "  301.05164,\n",
       "  300.97626,\n",
       "  300.95493,\n",
       "  300.9296,\n",
       "  300.8547,\n",
       "  300.85202,\n",
       "  300.8602,\n",
       "  300.866,\n",
       "  300.88187,\n",
       "  300.9249,\n",
       "  301.2267,\n",
       "  301.12875,\n",
       "  300.89462,\n",
       "  300.86594,\n",
       "  300.86752,\n",
       "  300.83212,\n",
       "  300.86905,\n",
       "  300.86652,\n",
       "  300.86514,\n",
       "  300.86166,\n",
       "  300.83566,\n",
       "  300.94385,\n",
       "  301.0025,\n",
       "  300.86655,\n",
       "  300.83374,\n",
       "  300.85873,\n",
       "  300.8374,\n",
       "  300.85944,\n",
       "  300.91455,\n",
       "  300.9095,\n",
       "  301.48242,\n",
       "  301.11624,\n",
       "  300.86792,\n",
       "  300.82065,\n",
       "  300.80487,\n",
       "  300.79678,\n",
       "  300.79425,\n",
       "  300.808,\n",
       "  300.84146,\n",
       "  300.8194,\n",
       "  300.9568,\n",
       "  300.89664,\n",
       "  300.87518,\n",
       "  300.87057,\n",
       "  300.8224,\n",
       "  300.83597,\n",
       "  300.97034,\n",
       "  301.0104,\n",
       "  301.4935,\n",
       "  301.0342,\n",
       "  300.89355,\n",
       "  300.8179,\n",
       "  300.79016,\n",
       "  300.78635,\n",
       "  300.78687,\n",
       "  300.7824,\n",
       "  300.77026,\n",
       "  300.7885,\n",
       "  300.79047,\n",
       "  300.78214,\n",
       "  300.8309,\n",
       "  300.92612,\n",
       "  300.9051,\n",
       "  300.95795,\n",
       "  300.87585,\n",
       "  300.8548,\n",
       "  300.9149,\n",
       "  300.8607,\n",
       "  300.8194,\n",
       "  300.7942,\n",
       "  300.8328,\n",
       "  300.79672,\n",
       "  300.79413,\n",
       "  300.80093,\n",
       "  300.8277,\n",
       "  300.90927,\n",
       "  301.12286,\n",
       "  301.01993,\n",
       "  301.05017,\n",
       "  300.91898,\n",
       "  300.8091,\n",
       "  300.8123,\n",
       "  300.7992,\n",
       "  300.78595,\n",
       "  300.80463,\n",
       "  300.85748,\n",
       "  300.79065,\n",
       "  300.8571,\n",
       "  300.81546,\n",
       "  300.84354,\n",
       "  300.80685,\n",
       "  300.95007,\n",
       "  300.95816,\n",
       "  300.89426,\n",
       "  300.978,\n",
       "  300.84302,\n",
       "  300.7945,\n",
       "  300.77365,\n",
       "  300.7767,\n",
       "  300.7732,\n",
       "  300.77908,\n",
       "  300.7779,\n",
       "  300.84344,\n",
       "  300.82098,\n",
       "  300.85895,\n",
       "  300.81723,\n",
       "  300.9939,\n",
       "  301.00934,\n",
       "  300.84933,\n",
       "  300.80713,\n",
       "  300.78113,\n",
       "  300.7907,\n",
       "  300.81467,\n",
       "  300.77774,\n",
       "  300.81982,\n",
       "  300.89203,\n",
       "  300.8447,\n",
       "  300.95816,\n",
       "  300.88657,\n",
       "  300.83728,\n",
       "  300.8399,\n",
       "  300.7907,\n",
       "  300.78775,\n",
       "  300.81216,\n",
       "  300.8123,\n",
       "  300.8071,\n",
       "  300.81943,\n",
       "  300.84122,\n",
       "  300.86755,\n",
       "  300.87723,\n",
       "  300.93488,\n",
       "  300.91428,\n",
       "  300.86707,\n",
       "  300.83624,\n",
       "  300.80774,\n",
       "  300.84195,\n",
       "  300.90048,\n",
       "  301.17773,\n",
       "  301.0099,\n",
       "  300.82635,\n",
       "  300.78867,\n",
       "  300.7874,\n",
       "  300.77823,\n",
       "  300.75824,\n",
       "  300.76093,\n",
       "  300.7601,\n",
       "  300.76016,\n",
       "  300.75702,\n",
       "  300.77658,\n",
       "  300.81644,\n",
       "  300.79343,\n",
       "  300.7773,\n",
       "  300.78894,\n",
       "  300.8401,\n",
       "  300.8194,\n",
       "  300.94778,\n",
       "  300.9242,\n",
       "  300.88336,\n",
       "  300.84503,\n",
       "  300.9083,\n",
       "  300.85403,\n",
       "  300.79034,\n",
       "  300.79306,\n",
       "  300.78635,\n",
       "  300.78085,\n",
       "  300.76703,\n",
       "  300.77277,\n",
       "  300.8489,\n",
       "  300.80988,\n",
       "  300.78662,\n",
       "  300.78464,\n",
       "  300.80704,\n",
       "  300.8321,\n",
       "  300.93173,\n",
       "  300.9536,\n",
       "  300.93234,\n",
       "  300.89536,\n",
       "  300.7939,\n",
       "  300.76727,\n",
       "  300.7648],\n",
       " 'val_loss': [301.9693118656907,\n",
       "  301.9461314358815,\n",
       "  302.01106751819265,\n",
       "  302.0161555875499,\n",
       "  302.0368166534329,\n",
       "  301.9616059395383,\n",
       "  301.965314253097,\n",
       "  301.9849971402843,\n",
       "  301.95643696532443,\n",
       "  301.96962189005916,\n",
       "  302.00495316454925,\n",
       "  302.00465065073746,\n",
       "  301.9908799976575,\n",
       "  301.98501206632716,\n",
       "  302.2301766939252,\n",
       "  302.05338494131496,\n",
       "  302.12828429985643,\n",
       "  301.94598692896955,\n",
       "  301.98597075560383,\n",
       "  302.1933201109509,\n",
       "  302.03459067656615,\n",
       "  301.98291880765066,\n",
       "  302.0877760652441,\n",
       "  302.03636364327787,\n",
       "  301.9707788009881,\n",
       "  302.10498721874393,\n",
       "  301.94068694783147,\n",
       "  301.9802726198951,\n",
       "  302.0876211005951,\n",
       "  302.12208637938693,\n",
       "  303.1085129972559,\n",
       "  301.9606012362186,\n",
       "  301.95317462671585,\n",
       "  301.92323003751096,\n",
       "  301.96034150405836,\n",
       "  301.96697180441976,\n",
       "  302.1422294070045,\n",
       "  302.0593472774898,\n",
       "  301.95439380871545,\n",
       "  301.984059746763,\n",
       "  301.9398687725127,\n",
       "  302.4778944354191,\n",
       "  302.0647906246958,\n",
       "  302.0031634654583,\n",
       "  301.9886799749927,\n",
       "  301.98384137019934,\n",
       "  301.93694469416255,\n",
       "  302.0555914287627,\n",
       "  301.9782277520201,\n",
       "  302.0620825461509,\n",
       "  302.09048595012536,\n",
       "  302.0379362017195,\n",
       "  301.992892256407,\n",
       "  301.9532776354258,\n",
       "  301.9644527257046,\n",
       "  301.9302060136171,\n",
       "  301.9353966638678,\n",
       "  301.94960625669296,\n",
       "  302.0080770807474,\n",
       "  302.0753638722072,\n",
       "  301.948820215148,\n",
       "  302.0076506902867,\n",
       "  301.971779036002,\n",
       "  301.9919958382009,\n",
       "  301.96157608745256,\n",
       "  302.09729859539283,\n",
       "  302.1586796175282,\n",
       "  302.55213286497883,\n",
       "  301.9968252211716,\n",
       "  301.95165074428667,\n",
       "  301.92931472920924,\n",
       "  301.94995221764873,\n",
       "  301.9915647892937,\n",
       "  301.9536745065469,\n",
       "  301.9340205207421,\n",
       "  301.95519819883543,\n",
       "  301.94348467770396,\n",
       "  302.0779275389104,\n",
       "  301.94580334814907,\n",
       "  302.04662658120986,\n",
       "  302.1558433841693,\n",
       "  302.03970764730576,\n",
       "  302.00867830555757,\n",
       "  302.0646387973678,\n",
       "  302.2850054684458,\n",
       "  302.1377787842557,\n",
       "  301.9647560000791,\n",
       "  301.93659559588565,\n",
       "  301.97951110649706,\n",
       "  301.9774448477962,\n",
       "  301.9410841516988,\n",
       "  301.958061146959,\n",
       "  301.952192455066,\n",
       "  302.0028363284292,\n",
       "  302.1456107736748,\n",
       "  302.4285921471141,\n",
       "  302.12164401710965,\n",
       "  302.09038099247346,\n",
       "  302.08494487061307,\n",
       "  301.9510760441005,\n",
       "  302.0206958616263,\n",
       "  301.93696456386294,\n",
       "  301.93544191734816,\n",
       "  301.9204330682012,\n",
       "  301.92861710307756,\n",
       "  301.95543235707504,\n",
       "  301.9393316251095,\n",
       "  301.9491958380488,\n",
       "  301.9803769120546,\n",
       "  301.9201235191844,\n",
       "  302.01843214183583,\n",
       "  301.9784103821371,\n",
       "  301.98339625088227,\n",
       "  301.91815004913235,\n",
       "  301.94427756431315,\n",
       "  302.0808596031688,\n",
       "  301.929941242729,\n",
       "  301.9469768488519,\n",
       "  301.9750289203965,\n",
       "  301.9454819153402,\n",
       "  301.9420916120583,\n",
       "  301.9886941404729,\n",
       "  302.03212093935576,\n",
       "  302.05186343564424,\n",
       "  301.95230710989216,\n",
       "  301.9870171472662,\n",
       "  301.99581661996814,\n",
       "  302.01041200822016,\n",
       "  301.92857004326083,\n",
       "  301.94109479957655,\n",
       "  301.9146297846999,\n",
       "  301.95506329402747,\n",
       "  302.00996346637095,\n",
       "  302.08351529795806,\n",
       "  301.9576723092813,\n",
       "  301.97039766400775,\n",
       "  302.1198978602329,\n",
       "  301.97584110628407,\n",
       "  302.0641510865399,\n",
       "  302.03016962069216,\n",
       "  301.93694431388116,\n",
       "  301.9435139593677,\n",
       "  301.9642901554286,\n",
       "  301.9928695345965,\n",
       "  301.9438817865009,\n",
       "  302.1433477193767,\n",
       "  301.92775053695726,\n",
       "  301.94984431281637,\n",
       "  301.9650920737198,\n",
       "  302.0701135653202,\n",
       "  301.9387134777794,\n",
       "  301.96368683907104,\n",
       "  302.27848411869036,\n",
       "  301.93933904059577,\n",
       "  301.9965376334027,\n",
       "  301.93188105788187,\n",
       "  302.288374380902,\n",
       "  301.93156485394155,\n",
       "  301.93850090050626,\n",
       "  301.89954597259236,\n",
       "  301.91810612663676,\n",
       "  301.98081889405057,\n",
       "  301.90328665760075,\n",
       "  301.973686146959,\n",
       "  301.8866078029169,\n",
       "  301.8796904852086,\n",
       "  301.95974170530326,\n",
       "  301.93553356515287,\n",
       "  302.00822890807535,\n",
       "  301.91683132849005,\n",
       "  301.90818487149534,\n",
       "  301.9015281415803,\n",
       "  301.8906205316942,\n",
       "  301.9294627537237,\n",
       "  301.9347501855773,\n",
       "  301.980748732142,\n",
       "  301.9122740368234,\n",
       "  301.88524420907567,\n",
       "  302.3240997219383,\n",
       "  301.95546040282443,\n",
       "  301.97227359189424,\n",
       "  301.8890093746958,\n",
       "  301.9564386765905,\n",
       "  301.9005843783465,\n",
       "  301.9183892460998,\n",
       "  301.87906406675916,\n",
       "  301.9154784775969,\n",
       "  302.0319697775202,\n",
       "  301.9099495670877,\n",
       "  301.88066305475445,\n",
       "  301.916224209319,\n",
       "  302.0359007458078,\n",
       "  302.09382520063645,\n",
       "  302.39663962486003,\n",
       "  301.92868427027054,\n",
       "  302.26371180454146,\n",
       "  301.92053574416497,\n",
       "  301.87247825931536,\n",
       "  301.89233645620374,\n",
       "  301.86726716894225],\n",
       " 'val_mean_squared_error': [301.9693,\n",
       "  301.9461,\n",
       "  302.0111,\n",
       "  302.01617,\n",
       "  302.0368,\n",
       "  301.9616,\n",
       "  301.9653,\n",
       "  301.985,\n",
       "  301.95648,\n",
       "  301.9696,\n",
       "  302.00497,\n",
       "  302.00464,\n",
       "  301.9909,\n",
       "  301.98502,\n",
       "  302.23016,\n",
       "  302.05338,\n",
       "  302.1283,\n",
       "  301.94595,\n",
       "  301.986,\n",
       "  302.19333,\n",
       "  302.0346,\n",
       "  301.9829,\n",
       "  302.08777,\n",
       "  302.03635,\n",
       "  301.9708,\n",
       "  302.10498,\n",
       "  301.94067,\n",
       "  301.9803,\n",
       "  302.0876,\n",
       "  302.12207,\n",
       "  303.10852,\n",
       "  301.96063,\n",
       "  301.95316,\n",
       "  301.92325,\n",
       "  301.96033,\n",
       "  301.96698,\n",
       "  302.14224,\n",
       "  302.05933,\n",
       "  301.95438,\n",
       "  301.98407,\n",
       "  301.93988,\n",
       "  302.47787,\n",
       "  302.06482,\n",
       "  302.00317,\n",
       "  301.98868,\n",
       "  301.98383,\n",
       "  301.93695,\n",
       "  302.0556,\n",
       "  301.97824,\n",
       "  302.06207,\n",
       "  302.09048,\n",
       "  302.03793,\n",
       "  301.99286,\n",
       "  301.9533,\n",
       "  301.96448,\n",
       "  301.93024,\n",
       "  301.9354,\n",
       "  301.94962,\n",
       "  302.0081,\n",
       "  302.07538,\n",
       "  301.94882,\n",
       "  302.00763,\n",
       "  301.97177,\n",
       "  301.992,\n",
       "  301.9616,\n",
       "  302.0973,\n",
       "  302.1587,\n",
       "  302.55215,\n",
       "  301.99683,\n",
       "  301.95166,\n",
       "  301.92935,\n",
       "  301.94992,\n",
       "  301.99155,\n",
       "  301.9537,\n",
       "  301.93402,\n",
       "  301.9552,\n",
       "  301.94348,\n",
       "  302.07794,\n",
       "  301.94577,\n",
       "  302.04663,\n",
       "  302.15585,\n",
       "  302.03973,\n",
       "  302.0087,\n",
       "  302.0647,\n",
       "  302.285,\n",
       "  302.1378,\n",
       "  301.96472,\n",
       "  301.93658,\n",
       "  301.9795,\n",
       "  301.97745,\n",
       "  301.94107,\n",
       "  301.95804,\n",
       "  301.95218,\n",
       "  302.00284,\n",
       "  302.1456,\n",
       "  302.4286,\n",
       "  302.12167,\n",
       "  302.09036,\n",
       "  302.08493,\n",
       "  301.95108,\n",
       "  302.02072,\n",
       "  301.93695,\n",
       "  301.93542,\n",
       "  301.9204,\n",
       "  301.92865,\n",
       "  301.95544,\n",
       "  301.93936,\n",
       "  301.9492,\n",
       "  301.98035,\n",
       "  301.92014,\n",
       "  302.01846,\n",
       "  301.97842,\n",
       "  301.9834,\n",
       "  301.91815,\n",
       "  301.94424,\n",
       "  302.08084,\n",
       "  301.92993,\n",
       "  301.94696,\n",
       "  301.97504,\n",
       "  301.9455,\n",
       "  301.94208,\n",
       "  301.9887,\n",
       "  302.0321,\n",
       "  302.05188,\n",
       "  301.95227,\n",
       "  301.987,\n",
       "  301.9958,\n",
       "  302.0104,\n",
       "  301.92862,\n",
       "  301.94107,\n",
       "  301.91464,\n",
       "  301.95502,\n",
       "  302.00995,\n",
       "  302.0835,\n",
       "  301.95767,\n",
       "  301.97037,\n",
       "  302.1199,\n",
       "  301.97586,\n",
       "  302.06412,\n",
       "  302.03015,\n",
       "  301.93695,\n",
       "  301.9435,\n",
       "  301.96432,\n",
       "  301.99286,\n",
       "  301.94385,\n",
       "  302.14337,\n",
       "  301.92773,\n",
       "  301.94986,\n",
       "  301.9651,\n",
       "  302.07013,\n",
       "  301.93875,\n",
       "  301.96365,\n",
       "  302.27847,\n",
       "  301.93933,\n",
       "  301.99652,\n",
       "  301.9319,\n",
       "  302.2884,\n",
       "  301.93155,\n",
       "  301.9385,\n",
       "  301.89957,\n",
       "  301.9181,\n",
       "  301.98087,\n",
       "  301.90332,\n",
       "  301.9737,\n",
       "  301.8866,\n",
       "  301.87967,\n",
       "  301.95975,\n",
       "  301.93555,\n",
       "  302.00824,\n",
       "  301.91687,\n",
       "  301.90817,\n",
       "  301.90152,\n",
       "  301.89062,\n",
       "  301.9295,\n",
       "  301.93475,\n",
       "  301.98074,\n",
       "  301.91232,\n",
       "  301.88528,\n",
       "  302.32407,\n",
       "  301.95544,\n",
       "  301.9723,\n",
       "  301.88898,\n",
       "  301.95642,\n",
       "  301.90057,\n",
       "  301.9184,\n",
       "  301.87906,\n",
       "  301.91547,\n",
       "  302.03198,\n",
       "  301.90997,\n",
       "  301.88068,\n",
       "  301.9162,\n",
       "  302.0359,\n",
       "  302.0938,\n",
       "  302.39664,\n",
       "  301.92868,\n",
       "  302.26373,\n",
       "  301.92053,\n",
       "  301.8725,\n",
       "  301.89233,\n",
       "  301.86725]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
