{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Analysis Notebook\n",
    "## W207 Final Project\n",
    "### T. P. Goter\n",
    "### July 6, 2019\n",
    "\n",
    "This workbook is used to assess various models created as part of the Facial Keypoint Detection project for W207."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the packages we need\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>77.049095</td>\n",
       "      <td>77.049088</td>\n",
       "      <td>111.200488</td>\n",
       "      <td>111.200485</td>\n",
       "      <td>396</td>\n",
       "      <td>8.777761</td>\n",
       "      <td>10.545164</td>\n",
       "      <td>0.474865</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2525.411537</td>\n",
       "      <td>2525.411377</td>\n",
       "      <td>2520.734831</td>\n",
       "      <td>2520.734863</td>\n",
       "      <td>76</td>\n",
       "      <td>50.253471</td>\n",
       "      <td>50.206920</td>\n",
       "      <td>0.320276</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>2525.412289</td>\n",
       "      <td>2525.412354</td>\n",
       "      <td>2520.735620</td>\n",
       "      <td>2520.735596</td>\n",
       "      <td>305</td>\n",
       "      <td>50.253481</td>\n",
       "      <td>50.206928</td>\n",
       "      <td>0.527155</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>2525.411770</td>\n",
       "      <td>2525.411377</td>\n",
       "      <td>2520.735104</td>\n",
       "      <td>2520.735107</td>\n",
       "      <td>244</td>\n",
       "      <td>50.253471</td>\n",
       "      <td>50.206923</td>\n",
       "      <td>0.319467</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>2427.317860</td>\n",
       "      <td>2427.318359</td>\n",
       "      <td>2424.645512</td>\n",
       "      <td>2424.645752</td>\n",
       "      <td>306</td>\n",
       "      <td>49.267823</td>\n",
       "      <td>49.240692</td>\n",
       "      <td>0.448901</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>174.564472</td>\n",
       "      <td>174.564453</td>\n",
       "      <td>198.644428</td>\n",
       "      <td>198.644440</td>\n",
       "      <td>261</td>\n",
       "      <td>13.212284</td>\n",
       "      <td>14.094128</td>\n",
       "      <td>0.673629</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2525.412145</td>\n",
       "      <td>2525.411865</td>\n",
       "      <td>2520.735503</td>\n",
       "      <td>2520.735596</td>\n",
       "      <td>100</td>\n",
       "      <td>50.253476</td>\n",
       "      <td>50.206928</td>\n",
       "      <td>0.316765</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2619.589233</td>\n",
       "      <td>2619.589111</td>\n",
       "      <td>2614.870316</td>\n",
       "      <td>2614.870605</td>\n",
       "      <td>238</td>\n",
       "      <td>51.181922</td>\n",
       "      <td>51.135806</td>\n",
       "      <td>0.294078</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2525.411534</td>\n",
       "      <td>2525.411865</td>\n",
       "      <td>2520.734893</td>\n",
       "      <td>2520.735107</td>\n",
       "      <td>186</td>\n",
       "      <td>50.253476</td>\n",
       "      <td>50.206923</td>\n",
       "      <td>0.355427</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>89.210516</td>\n",
       "      <td>89.210503</td>\n",
       "      <td>117.918552</td>\n",
       "      <td>117.918556</td>\n",
       "      <td>101</td>\n",
       "      <td>9.445131</td>\n",
       "      <td>10.859031</td>\n",
       "      <td>0.446759</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "396    77.049095           77.049088   111.200488              111.200485   \n",
       "76   2525.411537         2525.411377  2520.734831             2520.734863   \n",
       "305  2525.412289         2525.412354  2520.735620             2520.735596   \n",
       "244  2525.411770         2525.411377  2520.735104             2520.735107   \n",
       "306  2427.317860         2427.318359  2424.645512             2424.645752   \n",
       "261   174.564472          174.564453   198.644428              198.644440   \n",
       "100  2525.412145         2525.411865  2520.735503             2520.735596   \n",
       "238  2619.589233         2619.589111  2614.870316             2614.870605   \n",
       "186  2525.411534         2525.411865  2520.734893             2520.735107   \n",
       "101    89.210516           89.210503   117.918552              117.918556   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation optimizer  lrate  \n",
       "396    396   8.777761  10.545164  0.474865     150       relu      adam  0.001  \n",
       "76      76  50.253471  50.206920  0.320276     100       tanh       sgd  0.010  \n",
       "305    305  50.253481  50.206928  0.527155     100       tanh   adagrad  0.010  \n",
       "244    244  50.253471  50.206923  0.319467     100    sigmoid       sgd  0.010  \n",
       "306    306  49.267823  49.240692  0.448901     200       relu       sgd  0.010  \n",
       "261    261  13.212284  14.094128  0.673629     150       relu   adagrad  0.010  \n",
       "100    100  50.253476  50.206928  0.316765     100    sigmoid       sgd  0.010  \n",
       "238    238  51.181922  51.135806  0.294078     100       relu       sgd  0.010  \n",
       "186    186  50.253476  50.206923  0.355427     100       tanh       sgd  0.010  \n",
       "101    101   9.445131  10.859031  0.446759     150       relu      adam  0.001  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "bl_sl_df = pd.read_pickle(\"OutputData/single_layer_df.pkl\")\n",
    "bl_sl_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9985a0a17964ab9b5d838524c1dcdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd', 'nadam', 'adagrad'), value='adâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a plotting function to pass to the interact widget function\n",
    "def plot_validation_loss(optimizer = bl_sl_df.optimizer.unique(), \n",
    "                    activation = bl_sl_df.activation.unique()):\n",
    "    \n",
    "    # Subset the baseline df by the specified optimizer and activation\n",
    "    sub_df = bl_sl_df[bl_sl_df.optimizer.str.match(optimizer)]\n",
    "    sub_df = sub_df[sub_df.activation.str.match(activation)]\n",
    "     \n",
    "    # Group the neural net data by optimizer and activation\n",
    "    groups = sub_df.groupby(['hunits'])\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over the grouped data and plot out epoch timing and validation loss data\n",
    "    for name, group in groups:\n",
    "        axes[0].scatter(group.epoch, group.val_RMSE, label=str(name)+' Validation Loss')\n",
    "    #     axes[0].scatter(group.epoch, group.RMSE, label=' '.join(name)+' Training Loss')\n",
    "        axes[1].scatter(group.epoch, group.times*1000, label=str(name)+' Fit Time')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Root Mean Square Error')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Fit Time (milliseconds)')\n",
    "        axes[0].set_ylim([0,sub_df.val_RMSE.max()])\n",
    "        axes[1].set_ylim([0,1000])\n",
    "        axes[0].legend()\n",
    "        axes[1].legend()\n",
    "        axes[0].set_title(\"{} Optimizer and {} Activation\".format(group.optimizer.unique(), group.activation.unique()))\n",
    "    \n",
    "    # Adjust the spacing of the subplots\n",
    "    fig.subplots_adjust(left=0.03, right=0.97, hspace=0.1, wspace=0.15)\n",
    "\n",
    "    # Add an overarching title for these plots\n",
    "    fig.suptitle(\"Performance Comparison for Single Layer, Fully Connected Neural Nets\",\n",
    "                 fontsize=18, y=0.93)\n",
    "\n",
    "#     # Print out the table of data for viewing\n",
    "#     print(sub_df)\n",
    "interact_manual(plot_validation_loss)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment of Baseline Results\n",
    "1. Adam and Adagrad Optimizers are working well. \n",
    "2. Adam is faster and works well with 200 hidden units\n",
    "3. Adagrad is slower buts works best with 100 hidden units.\n",
    "\n",
    "In the evaluation above, both the hidden layer and the output layer used the activation function specified by the user. For the study below, the activation function of the output layer was set to softmax which is a multinomial classifier version of the sigmoid function. The plots below help to assess if the choise of activation function for the output layer significant alters are perception of which activation function and optimizers work well for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
