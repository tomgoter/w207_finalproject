{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Analysis Notebook\n",
    "## W207 Final Project\n",
    "### T. P. Goter\n",
    "### July 6, 2019\n",
    "\n",
    "This workbook is used to assess various models created as part of the Facial Keypoint Detection project for W207."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we need\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, fixed\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>2525.411546</td>\n",
       "      <td>2525.411377</td>\n",
       "      <td>2520.734829</td>\n",
       "      <td>2520.735107</td>\n",
       "      <td>393</td>\n",
       "      <td>50.253471</td>\n",
       "      <td>50.206923</td>\n",
       "      <td>1.332076</td>\n",
       "      <td>200</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>2427.317821</td>\n",
       "      <td>2427.318115</td>\n",
       "      <td>2424.645473</td>\n",
       "      <td>2424.645752</td>\n",
       "      <td>212</td>\n",
       "      <td>49.267820</td>\n",
       "      <td>49.240692</td>\n",
       "      <td>0.443440</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2427.317869</td>\n",
       "      <td>2427.318359</td>\n",
       "      <td>2424.645338</td>\n",
       "      <td>2424.645020</td>\n",
       "      <td>41</td>\n",
       "      <td>49.267823</td>\n",
       "      <td>49.240685</td>\n",
       "      <td>0.431231</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2525.412949</td>\n",
       "      <td>2525.412598</td>\n",
       "      <td>2520.736259</td>\n",
       "      <td>2520.736328</td>\n",
       "      <td>59</td>\n",
       "      <td>50.253483</td>\n",
       "      <td>50.206935</td>\n",
       "      <td>0.763112</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1025.098842</td>\n",
       "      <td>1025.098755</td>\n",
       "      <td>1037.942122</td>\n",
       "      <td>1037.942139</td>\n",
       "      <td>68</td>\n",
       "      <td>32.017163</td>\n",
       "      <td>32.217109</td>\n",
       "      <td>1.363042</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>272.703951</td>\n",
       "      <td>272.703979</td>\n",
       "      <td>299.593716</td>\n",
       "      <td>299.593689</td>\n",
       "      <td>340</td>\n",
       "      <td>16.513751</td>\n",
       "      <td>17.308775</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>611.926440</td>\n",
       "      <td>611.926331</td>\n",
       "      <td>633.599868</td>\n",
       "      <td>633.599854</td>\n",
       "      <td>246</td>\n",
       "      <td>24.737145</td>\n",
       "      <td>25.171409</td>\n",
       "      <td>1.064145</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2525.411581</td>\n",
       "      <td>2525.411865</td>\n",
       "      <td>2520.734858</td>\n",
       "      <td>2520.735107</td>\n",
       "      <td>164</td>\n",
       "      <td>50.253476</td>\n",
       "      <td>50.206923</td>\n",
       "      <td>0.345940</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2525.411545</td>\n",
       "      <td>2525.411865</td>\n",
       "      <td>2520.734847</td>\n",
       "      <td>2520.734863</td>\n",
       "      <td>369</td>\n",
       "      <td>50.253476</td>\n",
       "      <td>50.206920</td>\n",
       "      <td>0.318350</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>2525.421268</td>\n",
       "      <td>2525.421631</td>\n",
       "      <td>2520.744610</td>\n",
       "      <td>2520.744629</td>\n",
       "      <td>278</td>\n",
       "      <td>50.253573</td>\n",
       "      <td>50.207018</td>\n",
       "      <td>0.305015</td>\n",
       "      <td>50</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "393  2525.411546         2525.411377  2520.734829             2520.735107   \n",
       "212  2427.317821         2427.318115  2424.645473             2424.645752   \n",
       "41   2427.317869         2427.318359  2424.645338             2424.645020   \n",
       "59   2525.412949         2525.412598  2520.736259             2520.736328   \n",
       "68   1025.098842         1025.098755  1037.942122             1037.942139   \n",
       "340   272.703951          272.703979   299.593716              299.593689   \n",
       "246   611.926440          611.926331   633.599868              633.599854   \n",
       "164  2525.411581         2525.411865  2520.734858             2520.735107   \n",
       "369  2525.411545         2525.411865  2520.734847             2520.734863   \n",
       "278  2525.421268         2525.421631  2520.744610             2520.744629   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation optimizer  lrate  \n",
       "393    393  50.253471  50.206923  1.332076     200    sigmoid     nadam  0.002  \n",
       "212    212  49.267820  49.240692  0.443440     200       relu       sgd  0.010  \n",
       "41      41  49.267823  49.240685  0.431231     200       relu       sgd  0.010  \n",
       "59      59  50.253483  50.206935  0.763112     200       tanh   adagrad  0.010  \n",
       "68      68  32.017163  32.217109  1.363042     200       relu     nadam  0.002  \n",
       "340    340  16.513751  17.308775  0.305263     100       relu      adam  0.001  \n",
       "246    246  24.737145  25.171409  1.064145     150       relu     nadam  0.002  \n",
       "164    164  50.253476  50.206923  0.345940     100    sigmoid      adam  0.001  \n",
       "369    369  50.253476  50.206920  0.318350     100       tanh       sgd  0.010  \n",
       "278    278  50.253573  50.207018  0.305015      50    sigmoid   adagrad  0.010  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "bl_sl_df = pd.read_pickle(\"OutputData/single_layer_df.pkl\")\n",
    "bl_sl_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plotting function to pass to the interact widget function\n",
    "def plot_validation_loss(df=bl_sl_df, optimizer = bl_sl_df.optimizer.unique(), \n",
    "                    activation = bl_sl_df.activation.unique()):\n",
    "    \n",
    "    # Subset the baseline df by the specified optimizer and activation\n",
    "    sub_df = df[df.optimizer.str.match(optimizer)]\n",
    "    sub_df = sub_df[sub_df.activation.str.match(activation)]\n",
    "     \n",
    "    # Group the neural net data by optimizer and activation\n",
    "    groups = sub_df.groupby(['hunits'])\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over the grouped data and plot out epoch timing and validation loss data\n",
    "    for name, group in groups:\n",
    "        axes[0].plot(group.epoch, group.val_RMSE, label=str(name)+' Validation Loss')\n",
    "    #     axes[0].scatter(group.epoch, group.RMSE, label=' '.join(name)+' Training Loss')\n",
    "        axes[1].scatter(group.epoch, group.times*1000, label=str(name)+' Fit Time')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Root Mean Square Error')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Fit Time (milliseconds)')\n",
    "        axes[0].set_ylim([0,sub_df.val_RMSE.max()])\n",
    "        axes[1].set_ylim([0,1000])\n",
    "        axes[0].legend()\n",
    "        axes[1].legend()\n",
    "        axes[0].set_title(\"{} Optimizer and {} Activation\".format(group.optimizer.unique(), group.activation.unique()))\n",
    "    \n",
    "    # Adjust the spacing of the subplots\n",
    "    fig.subplots_adjust(left=0.03, right=0.97, hspace=0.1, wspace=0.15)\n",
    "\n",
    "    # Add an overarching title for these plots\n",
    "    fig.suptitle(\"Performance Comparison for Single Layer, Fully Connected Neural Nets\",\n",
    "                 fontsize=18, y=0.93)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377713d3600046cd924a70d8cee3ecd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd', 'nadam', 'adagrad'), value='ad…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interact_manual(plot_validation_loss, df=fixed(bl_sl_df), \n",
    "                optimizer = bl_sl_df.optimizer.unique(), \n",
    "                    activation = bl_sl_df.activation.unique())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment of Baseline Results\n",
    "1. Adam and Adagrad Optimizers are working well. \n",
    "2. Adam is faster and works well with 200 hidden units\n",
    "3. Adagrad is slower buts works best with 100 hidden units.\n",
    "\n",
    "In the evaluation above, both the hidden layer and the output layer used the activation function specified by the user. For the study below, the activation function of the output layer was set to softmax which is a multinomial classifier version of the sigmoid function. The plots below help to assess if the choice of activation function for the output layer significant alters are perception of which activation function and optimizers work well for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2614.113111</td>\n",
       "      <td>2614.113037</td>\n",
       "      <td>2609.399139</td>\n",
       "      <td>2609.399170</td>\n",
       "      <td>258</td>\n",
       "      <td>51.128398</td>\n",
       "      <td>51.082278</td>\n",
       "      <td>0.367262</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>2614.506091</td>\n",
       "      <td>2614.506104</td>\n",
       "      <td>2609.812108</td>\n",
       "      <td>2609.812012</td>\n",
       "      <td>338</td>\n",
       "      <td>51.132241</td>\n",
       "      <td>51.086319</td>\n",
       "      <td>0.559323</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2614.113154</td>\n",
       "      <td>2614.112793</td>\n",
       "      <td>2609.399169</td>\n",
       "      <td>2609.399170</td>\n",
       "      <td>87</td>\n",
       "      <td>51.128395</td>\n",
       "      <td>51.082278</td>\n",
       "      <td>0.329807</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2614.113120</td>\n",
       "      <td>2614.113281</td>\n",
       "      <td>2609.399132</td>\n",
       "      <td>2609.399170</td>\n",
       "      <td>33</td>\n",
       "      <td>51.128400</td>\n",
       "      <td>51.082278</td>\n",
       "      <td>0.377566</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2614.113173</td>\n",
       "      <td>2614.113037</td>\n",
       "      <td>2609.399151</td>\n",
       "      <td>2609.399414</td>\n",
       "      <td>52</td>\n",
       "      <td>51.128398</td>\n",
       "      <td>51.082281</td>\n",
       "      <td>0.811855</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2614.315106</td>\n",
       "      <td>2614.314941</td>\n",
       "      <td>2609.594195</td>\n",
       "      <td>2609.594238</td>\n",
       "      <td>81</td>\n",
       "      <td>51.130372</td>\n",
       "      <td>51.084188</td>\n",
       "      <td>0.284950</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2614.113223</td>\n",
       "      <td>2614.113525</td>\n",
       "      <td>2609.399267</td>\n",
       "      <td>2609.399414</td>\n",
       "      <td>41</td>\n",
       "      <td>51.128402</td>\n",
       "      <td>51.082281</td>\n",
       "      <td>0.776023</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>2614.314997</td>\n",
       "      <td>2614.314453</td>\n",
       "      <td>2609.594201</td>\n",
       "      <td>2609.594238</td>\n",
       "      <td>208</td>\n",
       "      <td>51.130367</td>\n",
       "      <td>51.084188</td>\n",
       "      <td>0.429409</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>2614.544200</td>\n",
       "      <td>2614.544434</td>\n",
       "      <td>2609.843605</td>\n",
       "      <td>2609.843750</td>\n",
       "      <td>244</td>\n",
       "      <td>51.132616</td>\n",
       "      <td>51.086630</td>\n",
       "      <td>0.390927</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>2614.113134</td>\n",
       "      <td>2614.112305</td>\n",
       "      <td>2609.399160</td>\n",
       "      <td>2609.399414</td>\n",
       "      <td>337</td>\n",
       "      <td>51.128390</td>\n",
       "      <td>51.082281</td>\n",
       "      <td>0.379356</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "258  2614.113111         2614.113037  2609.399139             2609.399170   \n",
       "338  2614.506091         2614.506104  2609.812108             2609.812012   \n",
       "87   2614.113154         2614.112793  2609.399169             2609.399170   \n",
       "33   2614.113120         2614.113281  2609.399132             2609.399170   \n",
       "52   2614.113173         2614.113037  2609.399151             2609.399414   \n",
       "81   2614.315106         2614.314941  2609.594195             2609.594238   \n",
       "41   2614.113223         2614.113525  2609.399267             2609.399414   \n",
       "208  2614.314997         2614.314453  2609.594201             2609.594238   \n",
       "244  2614.544200         2614.544434  2609.843605             2609.843750   \n",
       "337  2614.113134         2614.112305  2609.399160             2609.399414   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation optimizer  lrate  \n",
       "258    258  51.128398  51.082278  0.367262     100    sigmoid      adam  0.001  \n",
       "338    338  51.132241  51.086319  0.559323     100       relu   adagrad  0.010  \n",
       "87      87  51.128395  51.082278  0.329807     100       tanh      adam  0.001  \n",
       "33      33  51.128400  51.082278  0.377566      50       tanh     nadam  0.002  \n",
       "52      52  51.128398  51.082281  0.811855     100    sigmoid     nadam  0.002  \n",
       "81      81  51.130372  51.084188  0.284950     100    sigmoid       sgd  0.010  \n",
       "41      41  51.128402  51.082281  0.776023     200       tanh   adagrad  0.010  \n",
       "208    208  51.130367  51.084188  0.429409     150       relu      adam  0.001  \n",
       "244    244  51.132616  51.086630  0.390927     200       relu       sgd  0.010  \n",
       "337    337  51.128390  51.082281  0.379356      50       tanh     nadam  0.002  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "sm_sl_df = pd.read_pickle(\"OutputData/single_layer_softmax_df.pkl\")\n",
    "sm_sl_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86ce03129b44bf683578343f0f9612c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd', 'nadam', 'adagrad'), value='ad…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the softmax data\n",
    "interact_manual(plot_validation_loss, df=fixed(sm_sl_df), \n",
    "                optimizer = sm_sl_df.optimizer.unique(), \n",
    "                    activation = sm_sl_df.activation.unique())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>502.155229</td>\n",
       "      <td>502.155182</td>\n",
       "      <td>500.816102</td>\n",
       "      <td>500.816101</td>\n",
       "      <td>115</td>\n",
       "      <td>22.408819</td>\n",
       "      <td>22.378921</td>\n",
       "      <td>0.699443</td>\n",
       "      <td>150</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>180.949600</td>\n",
       "      <td>180.949585</td>\n",
       "      <td>180.257479</td>\n",
       "      <td>180.257477</td>\n",
       "      <td>157</td>\n",
       "      <td>13.451750</td>\n",
       "      <td>13.426000</td>\n",
       "      <td>1.237069</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>623.835086</td>\n",
       "      <td>623.834961</td>\n",
       "      <td>622.083832</td>\n",
       "      <td>622.083801</td>\n",
       "      <td>178</td>\n",
       "      <td>24.976688</td>\n",
       "      <td>24.941608</td>\n",
       "      <td>0.206128</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>509.132992</td>\n",
       "      <td>509.132904</td>\n",
       "      <td>504.411733</td>\n",
       "      <td>504.411713</td>\n",
       "      <td>9</td>\n",
       "      <td>22.563974</td>\n",
       "      <td>22.459112</td>\n",
       "      <td>0.780349</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>225.888963</td>\n",
       "      <td>225.888962</td>\n",
       "      <td>225.608633</td>\n",
       "      <td>225.608627</td>\n",
       "      <td>175</td>\n",
       "      <td>15.029603</td>\n",
       "      <td>15.020274</td>\n",
       "      <td>0.961487</td>\n",
       "      <td>150</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>879.276197</td>\n",
       "      <td>879.276184</td>\n",
       "      <td>876.833828</td>\n",
       "      <td>876.833801</td>\n",
       "      <td>37</td>\n",
       "      <td>29.652592</td>\n",
       "      <td>29.611380</td>\n",
       "      <td>0.294891</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>1196.653272</td>\n",
       "      <td>1196.653076</td>\n",
       "      <td>1192.762382</td>\n",
       "      <td>1192.762451</td>\n",
       "      <td>309</td>\n",
       "      <td>34.592674</td>\n",
       "      <td>34.536393</td>\n",
       "      <td>0.761255</td>\n",
       "      <td>200</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1108.908832</td>\n",
       "      <td>1108.908813</td>\n",
       "      <td>1104.147715</td>\n",
       "      <td>1104.147705</td>\n",
       "      <td>31</td>\n",
       "      <td>33.300282</td>\n",
       "      <td>33.228718</td>\n",
       "      <td>0.414289</td>\n",
       "      <td>50</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>1108.918343</td>\n",
       "      <td>1108.918213</td>\n",
       "      <td>1104.151216</td>\n",
       "      <td>1104.151245</td>\n",
       "      <td>225</td>\n",
       "      <td>33.300424</td>\n",
       "      <td>33.228771</td>\n",
       "      <td>0.418424</td>\n",
       "      <td>50</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>779.769239</td>\n",
       "      <td>779.769165</td>\n",
       "      <td>774.285601</td>\n",
       "      <td>774.285583</td>\n",
       "      <td>23</td>\n",
       "      <td>27.924347</td>\n",
       "      <td>27.825988</td>\n",
       "      <td>0.592959</td>\n",
       "      <td>150</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "115   502.155229          502.155182   500.816102              500.816101   \n",
       "157   180.949600          180.949585   180.257479              180.257477   \n",
       "178   623.835086          623.834961   622.083832              622.083801   \n",
       "9     509.132992          509.132904   504.411733              504.411713   \n",
       "175   225.888963          225.888962   225.608633              225.608627   \n",
       "37    879.276197          879.276184   876.833828              876.833801   \n",
       "309  1196.653272         1196.653076  1192.762382             1192.762451   \n",
       "31   1108.908832         1108.908813  1104.147715             1104.147705   \n",
       "225  1108.918343         1108.918213  1104.151216             1104.151245   \n",
       "23    779.769239          779.769165   774.285601              774.285583   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation optimizer  lrate  \n",
       "115    115  22.408819  22.378921  0.699443     150       tanh   adagrad  0.010  \n",
       "157    157  13.451750  13.426000  1.237069     200       tanh     nadam  0.002  \n",
       "178    178  24.976688  24.941608  0.206128      50       tanh       sgd  0.010  \n",
       "9        9  22.563974  22.459112  0.780349     200       tanh   adagrad  0.010  \n",
       "175    175  15.029603  15.020274  0.961487     150    sigmoid     nadam  0.002  \n",
       "37      37  29.652592  29.611380  0.294891     100    sigmoid       sgd  0.010  \n",
       "309    309  34.592674  34.536393  0.761255     200    sigmoid   adagrad  0.010  \n",
       "31      31  33.300282  33.228718  0.414289      50    sigmoid     nadam  0.002  \n",
       "225    225  33.300424  33.228771  0.418424      50    sigmoid     nadam  0.002  \n",
       "23      23  27.924347  27.825988  0.592959     150    sigmoid   adagrad  0.010  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "relu_sl_df = pd.read_pickle(\"OutputData/single_layer_relu_df.pkl\")\n",
    "relu_sl_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2ffd3188c749d7a9bbdf8d4b1df474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd', 'nadam', 'adagrad'), value='ad…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the relu data\n",
    "interact_manual(plot_validation_loss, df=fixed(relu_sl_df), \n",
    "                optimizer = relu_sl_df.optimizer.unique(), \n",
    "                    activation = relu_sl_df.activation.unique())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimizer\n",
       "adagrad    22.046067\n",
       "adam        3.142767\n",
       "nadam      13.423687\n",
       "sgd         7.542013\n",
       "Name: val_RMSE, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_sl_df.groupby('optimizer').val_RMSE.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Assessment\n",
    "Based on the three different experiments run, we will use a RELU final layer activation function. We will continue to assess adagrad and adam optimizers. We likely do not need to train for more than 200 epochs or so to get reasonably converged nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>585.102651</td>\n",
       "      <td>585.102600</td>\n",
       "      <td>602.217882</td>\n",
       "      <td>602.217896</td>\n",
       "      <td>166</td>\n",
       "      <td>24.188894</td>\n",
       "      <td>24.540128</td>\n",
       "      <td>0.777923</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad_02</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>101.179164</td>\n",
       "      <td>101.179169</td>\n",
       "      <td>121.990278</td>\n",
       "      <td>121.990288</td>\n",
       "      <td>197</td>\n",
       "      <td>10.058786</td>\n",
       "      <td>11.044921</td>\n",
       "      <td>0.195769</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam_005</td>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1382.132048</td>\n",
       "      <td>1382.132080</td>\n",
       "      <td>1389.458887</td>\n",
       "      <td>1389.458862</td>\n",
       "      <td>190</td>\n",
       "      <td>37.177037</td>\n",
       "      <td>37.275446</td>\n",
       "      <td>0.561554</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad_005</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>84.662256</td>\n",
       "      <td>84.662247</td>\n",
       "      <td>116.007755</td>\n",
       "      <td>116.007736</td>\n",
       "      <td>184</td>\n",
       "      <td>9.201209</td>\n",
       "      <td>10.770689</td>\n",
       "      <td>0.490967</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam_0005</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>154.702597</td>\n",
       "      <td>154.702591</td>\n",
       "      <td>174.748838</td>\n",
       "      <td>174.748825</td>\n",
       "      <td>154</td>\n",
       "      <td>12.437950</td>\n",
       "      <td>13.219260</td>\n",
       "      <td>0.468345</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam_005</td>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>293.815909</td>\n",
       "      <td>293.815887</td>\n",
       "      <td>292.408084</td>\n",
       "      <td>292.408112</td>\n",
       "      <td>121</td>\n",
       "      <td>17.141059</td>\n",
       "      <td>17.099945</td>\n",
       "      <td>0.259666</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad_02</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>360.313932</td>\n",
       "      <td>360.313965</td>\n",
       "      <td>370.613009</td>\n",
       "      <td>370.613007</td>\n",
       "      <td>62</td>\n",
       "      <td>18.981938</td>\n",
       "      <td>19.251312</td>\n",
       "      <td>0.271495</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad_005</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>244.381297</td>\n",
       "      <td>244.381256</td>\n",
       "      <td>244.194078</td>\n",
       "      <td>244.194077</td>\n",
       "      <td>60</td>\n",
       "      <td>15.632698</td>\n",
       "      <td>15.626710</td>\n",
       "      <td>0.247615</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad_005</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>87.490673</td>\n",
       "      <td>87.490669</td>\n",
       "      <td>87.137560</td>\n",
       "      <td>87.137558</td>\n",
       "      <td>115</td>\n",
       "      <td>9.353645</td>\n",
       "      <td>9.334750</td>\n",
       "      <td>0.756261</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad_005</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1393.413407</td>\n",
       "      <td>1393.413452</td>\n",
       "      <td>1396.197367</td>\n",
       "      <td>1396.197388</td>\n",
       "      <td>60</td>\n",
       "      <td>37.328454</td>\n",
       "      <td>37.365725</td>\n",
       "      <td>0.543673</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad_005</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "166   585.102651          585.102600   602.217882              602.217896   \n",
       "197   101.179164          101.179169   121.990278              121.990288   \n",
       "190  1382.132048         1382.132080  1389.458887             1389.458862   \n",
       "184    84.662256           84.662247   116.007755              116.007736   \n",
       "154   154.702597          154.702591   174.748838              174.748825   \n",
       "121   293.815909          293.815887   292.408084              292.408112   \n",
       "62    360.313932          360.313965   370.613009              370.613007   \n",
       "60    244.381297          244.381256   244.194078              244.194077   \n",
       "115    87.490673           87.490669    87.137560               87.137558   \n",
       "60   1393.413407         1393.413452  1396.197367             1396.197388   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation    optimizer  \\\n",
       "166    166  24.188894  24.540128  0.777923     200       relu   adagrad_02   \n",
       "197    197  10.058786  11.044921  0.195769      50       relu     adam_005   \n",
       "190    190  37.177037  37.275446  0.561554     150       relu  adagrad_005   \n",
       "184    184   9.201209  10.770689  0.490967     200       relu    adam_0005   \n",
       "154    154  12.437950  13.219260  0.468345     200       relu     adam_005   \n",
       "121    121  17.141059  17.099945  0.259666      50       tanh   adagrad_02   \n",
       "62      62  18.981938  19.251312  0.271495      50       relu  adagrad_005   \n",
       "60      60  15.632698  15.626710  0.247615      50       tanh  adagrad_005   \n",
       "115    115   9.353645   9.334750  0.756261     200       tanh  adagrad_005   \n",
       "60      60  37.328454  37.365725  0.543673     150       relu  adagrad_005   \n",
       "\n",
       "      lrate  \n",
       "166  0.0200  \n",
       "197  0.0050  \n",
       "190  0.0500  \n",
       "184  0.0005  \n",
       "154  0.0050  \n",
       "121  0.0200  \n",
       "62   0.0500  \n",
       "60   0.0500  \n",
       "115  0.0500  \n",
       "60   0.0500  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "relu_sl_lr_df = pd.read_pickle(\"OutputData/single_layer_relu_lr_df.pkl\")\n",
    "relu_sl_lr_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2c1428d5454edd80856b531a9ca261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam_005', 'adam_0005', 'adagrad_02', 'adagr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the relu learning rate data\n",
    "interact_manual(plot_validation_loss, df=fixed(relu_sl_lr_df), \n",
    "                optimizer = relu_sl_lr_df.optimizer.unique(), \n",
    "                    activation = relu_sl_lr_df.activation.unique())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the dataframes to get a comprehensive look at the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_bl_sl_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>123.751275</td>\n",
       "      <td>123.751266</td>\n",
       "      <td>123.281732</td>\n",
       "      <td>123.281738</td>\n",
       "      <td>229</td>\n",
       "      <td>11.124355</td>\n",
       "      <td>11.103231</td>\n",
       "      <td>0.439289</td>\n",
       "      <td>150</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>569.614534</td>\n",
       "      <td>569.614502</td>\n",
       "      <td>593.652979</td>\n",
       "      <td>593.652954</td>\n",
       "      <td>209</td>\n",
       "      <td>23.866598</td>\n",
       "      <td>24.364994</td>\n",
       "      <td>0.699659</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>423.108962</td>\n",
       "      <td>423.108948</td>\n",
       "      <td>451.090325</td>\n",
       "      <td>451.090332</td>\n",
       "      <td>245</td>\n",
       "      <td>20.569612</td>\n",
       "      <td>21.238887</td>\n",
       "      <td>0.692946</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>444.135960</td>\n",
       "      <td>444.135986</td>\n",
       "      <td>462.668193</td>\n",
       "      <td>462.668213</td>\n",
       "      <td>59</td>\n",
       "      <td>21.074534</td>\n",
       "      <td>21.509724</td>\n",
       "      <td>0.691606</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>413.230153</td>\n",
       "      <td>413.230103</td>\n",
       "      <td>412.724919</td>\n",
       "      <td>412.724884</td>\n",
       "      <td>79</td>\n",
       "      <td>20.328062</td>\n",
       "      <td>20.315632</td>\n",
       "      <td>0.709746</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>514.039798</td>\n",
       "      <td>514.039734</td>\n",
       "      <td>537.720856</td>\n",
       "      <td>537.720764</td>\n",
       "      <td>289</td>\n",
       "      <td>22.672444</td>\n",
       "      <td>23.188807</td>\n",
       "      <td>0.689042</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>412.497806</td>\n",
       "      <td>412.497833</td>\n",
       "      <td>412.097011</td>\n",
       "      <td>412.097015</td>\n",
       "      <td>233</td>\n",
       "      <td>20.310043</td>\n",
       "      <td>20.300173</td>\n",
       "      <td>0.688420</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>410.932827</td>\n",
       "      <td>410.932770</td>\n",
       "      <td>428.343918</td>\n",
       "      <td>428.343933</td>\n",
       "      <td>32</td>\n",
       "      <td>20.271477</td>\n",
       "      <td>20.696472</td>\n",
       "      <td>0.458140</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1315.643724</td>\n",
       "      <td>1315.643921</td>\n",
       "      <td>1294.415446</td>\n",
       "      <td>1294.415405</td>\n",
       "      <td>11</td>\n",
       "      <td>36.271806</td>\n",
       "      <td>35.977985</td>\n",
       "      <td>0.789763</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>123.745365</td>\n",
       "      <td>123.745346</td>\n",
       "      <td>123.306409</td>\n",
       "      <td>123.306412</td>\n",
       "      <td>165</td>\n",
       "      <td>11.124089</td>\n",
       "      <td>11.104342</td>\n",
       "      <td>0.439983</td>\n",
       "      <td>150</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "229   123.751275          123.751266   123.281732              123.281738   \n",
       "209   569.614534          569.614502   593.652979              593.652954   \n",
       "245   423.108962          423.108948   451.090325              451.090332   \n",
       "59    444.135960          444.135986   462.668193              462.668213   \n",
       "79    413.230153          413.230103   412.724919              412.724884   \n",
       "289   514.039798          514.039734   537.720856              537.720764   \n",
       "233   412.497806          412.497833   412.097011              412.097015   \n",
       "32    410.932827          410.932770   428.343918              428.343933   \n",
       "11   1315.643724         1315.643921  1294.415446             1294.415405   \n",
       "165   123.745365          123.745346   123.306409              123.306412   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation optimizer  lrate  \n",
       "229    229  11.124355  11.103231  0.439289     150       tanh      adam  0.001  \n",
       "209    209  23.866598  24.364994  0.699659     100       relu   adagrad  0.010  \n",
       "245    245  20.569612  21.238887  0.692946      50       relu   adagrad  0.010  \n",
       "59      59  21.074534  21.509724  0.691606      50       relu   adagrad  0.010  \n",
       "79      79  20.328062  20.315632  0.709746     100       tanh   adagrad  0.010  \n",
       "289    289  22.672444  23.188807  0.689042     150       relu   adagrad  0.010  \n",
       "233    233  20.310043  20.300173  0.688420     100       tanh   adagrad  0.010  \n",
       "32      32  20.271477  20.696472  0.458140     100       relu      adam  0.001  \n",
       "11      11  36.271806  35.977985  0.789763      50       tanh   adagrad  0.010  \n",
       "165    165  11.124089  11.104342  0.439983     150       tanh      adam  0.001  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "tl_df = pd.read_pickle(\"OutputData/two_layer_relu_df.pkl\")\n",
    "tl_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c305ae2aa4f3490aac69a13a3723e328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'adagrad'), value='adam'), Dropdown(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the relu learning rate data\n",
    "interact_manual(plot_validation_loss, df=fixed(tl_df), \n",
    "                optimizer = tl_df.optimizer.unique(), \n",
    "                    activation = tl_df.activation.unique())\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
