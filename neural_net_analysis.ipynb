{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Analysis Notebook\n",
    "## W207 Final Project\n",
    "### T. P. Goter\n",
    "### July 6, 2019\n",
    "\n",
    "This workbook is used to assess various models created as part of the Facial Keypoint Detection project for W207."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we need\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>77.049095</td>\n",
       "      <td>77.049088</td>\n",
       "      <td>111.200488</td>\n",
       "      <td>111.200485</td>\n",
       "      <td>396</td>\n",
       "      <td>8.777761</td>\n",
       "      <td>10.545164</td>\n",
       "      <td>0.474865</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2525.411537</td>\n",
       "      <td>2525.411377</td>\n",
       "      <td>2520.734831</td>\n",
       "      <td>2520.734863</td>\n",
       "      <td>76</td>\n",
       "      <td>50.253471</td>\n",
       "      <td>50.206920</td>\n",
       "      <td>0.320276</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>2525.412289</td>\n",
       "      <td>2525.412354</td>\n",
       "      <td>2520.735620</td>\n",
       "      <td>2520.735596</td>\n",
       "      <td>305</td>\n",
       "      <td>50.253481</td>\n",
       "      <td>50.206928</td>\n",
       "      <td>0.527155</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>2525.411770</td>\n",
       "      <td>2525.411377</td>\n",
       "      <td>2520.735104</td>\n",
       "      <td>2520.735107</td>\n",
       "      <td>244</td>\n",
       "      <td>50.253471</td>\n",
       "      <td>50.206923</td>\n",
       "      <td>0.319467</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>2427.317860</td>\n",
       "      <td>2427.318359</td>\n",
       "      <td>2424.645512</td>\n",
       "      <td>2424.645752</td>\n",
       "      <td>306</td>\n",
       "      <td>49.267823</td>\n",
       "      <td>49.240692</td>\n",
       "      <td>0.448901</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>174.564472</td>\n",
       "      <td>174.564453</td>\n",
       "      <td>198.644428</td>\n",
       "      <td>198.644440</td>\n",
       "      <td>261</td>\n",
       "      <td>13.212284</td>\n",
       "      <td>14.094128</td>\n",
       "      <td>0.673629</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2525.412145</td>\n",
       "      <td>2525.411865</td>\n",
       "      <td>2520.735503</td>\n",
       "      <td>2520.735596</td>\n",
       "      <td>100</td>\n",
       "      <td>50.253476</td>\n",
       "      <td>50.206928</td>\n",
       "      <td>0.316765</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2619.589233</td>\n",
       "      <td>2619.589111</td>\n",
       "      <td>2614.870316</td>\n",
       "      <td>2614.870605</td>\n",
       "      <td>238</td>\n",
       "      <td>51.181922</td>\n",
       "      <td>51.135806</td>\n",
       "      <td>0.294078</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2525.411534</td>\n",
       "      <td>2525.411865</td>\n",
       "      <td>2520.734893</td>\n",
       "      <td>2520.735107</td>\n",
       "      <td>186</td>\n",
       "      <td>50.253476</td>\n",
       "      <td>50.206923</td>\n",
       "      <td>0.355427</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>89.210516</td>\n",
       "      <td>89.210503</td>\n",
       "      <td>117.918552</td>\n",
       "      <td>117.918556</td>\n",
       "      <td>101</td>\n",
       "      <td>9.445131</td>\n",
       "      <td>10.859031</td>\n",
       "      <td>0.446759</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "396    77.049095           77.049088   111.200488              111.200485   \n",
       "76   2525.411537         2525.411377  2520.734831             2520.734863   \n",
       "305  2525.412289         2525.412354  2520.735620             2520.735596   \n",
       "244  2525.411770         2525.411377  2520.735104             2520.735107   \n",
       "306  2427.317860         2427.318359  2424.645512             2424.645752   \n",
       "261   174.564472          174.564453   198.644428              198.644440   \n",
       "100  2525.412145         2525.411865  2520.735503             2520.735596   \n",
       "238  2619.589233         2619.589111  2614.870316             2614.870605   \n",
       "186  2525.411534         2525.411865  2520.734893             2520.735107   \n",
       "101    89.210516           89.210503   117.918552              117.918556   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation optimizer  lrate  \n",
       "396    396   8.777761  10.545164  0.474865     150       relu      adam  0.001  \n",
       "76      76  50.253471  50.206920  0.320276     100       tanh       sgd  0.010  \n",
       "305    305  50.253481  50.206928  0.527155     100       tanh   adagrad  0.010  \n",
       "244    244  50.253471  50.206923  0.319467     100    sigmoid       sgd  0.010  \n",
       "306    306  49.267823  49.240692  0.448901     200       relu       sgd  0.010  \n",
       "261    261  13.212284  14.094128  0.673629     150       relu   adagrad  0.010  \n",
       "100    100  50.253476  50.206928  0.316765     100    sigmoid       sgd  0.010  \n",
       "238    238  51.181922  51.135806  0.294078     100       relu       sgd  0.010  \n",
       "186    186  50.253476  50.206923  0.355427     100       tanh       sgd  0.010  \n",
       "101    101   9.445131  10.859031  0.446759     150       relu      adam  0.001  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "bl_sl_df = pd.read_pickle(\"OutputData/single_layer_df.pkl\")\n",
    "bl_sl_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957f1785c9874693b2492d7138d33943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd', 'nadam', 'adagrad'), value='ad…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a plotting function to pass to the interact widget function\n",
    "def plot_validation_loss(optimizer = bl_sl_df.optimizer.unique(), \n",
    "                    activation = bl_sl_df.activation.unique()):\n",
    "    \n",
    "    # Subset the baseline df by the specified optimizer and activation\n",
    "    sub_df = bl_sl_df[bl_sl_df.optimizer.str.match(optimizer)]\n",
    "    sub_df = sub_df[sub_df.activation.str.match(activation)]\n",
    "     \n",
    "    # Group the neural net data by optimizer and activation\n",
    "    groups = sub_df.groupby(['hunits'])\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over the grouped data and plot out epoch timing and validation loss data\n",
    "    for name, group in groups:\n",
    "        axes[0].plot(group.epoch, group.val_RMSE, label=str(name)+' Validation Loss')\n",
    "    #     axes[0].scatter(group.epoch, group.RMSE, label=' '.join(name)+' Training Loss')\n",
    "        axes[1].scatter(group.epoch, group.times*1000, label=str(name)+' Fit Time')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Root Mean Square Error')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Fit Time (milliseconds)')\n",
    "        axes[0].set_ylim([0,sub_df.val_RMSE.max()])\n",
    "        axes[1].set_ylim([0,1000])\n",
    "        axes[0].legend()\n",
    "        axes[1].legend()\n",
    "        axes[0].set_title(\"{} Optimizer and {} Activation\".format(group.optimizer.unique(), group.activation.unique()))\n",
    "    \n",
    "    # Adjust the spacing of the subplots\n",
    "    fig.subplots_adjust(left=0.03, right=0.97, hspace=0.1, wspace=0.15)\n",
    "\n",
    "    # Add an overarching title for these plots\n",
    "    fig.suptitle(\"Performance Comparison for Single Layer, Fully Connected Neural Nets\",\n",
    "                 fontsize=18, y=0.93)\n",
    "\n",
    "#     # Print out the table of data for viewing\n",
    "#     print(sub_df)\n",
    "interact_manual(plot_validation_loss)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment of Baseline Results\n",
    "1. Adam and Adagrad Optimizers are working well. \n",
    "2. Adam is faster and works well with 200 hidden units\n",
    "3. Adagrad is slower buts works best with 100 hidden units.\n",
    "\n",
    "In the evaluation above, both the hidden layer and the output layer used the activation function specified by the user. For the study below, the activation function of the output layer was set to softmax which is a multinomial classifier version of the sigmoid function. The plots below help to assess if the choice of activation function for the output layer significant alters are perception of which activation function and optimizers work well for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2615.206895</td>\n",
       "      <td>2615.207275</td>\n",
       "      <td>2610.494079</td>\n",
       "      <td>2610.494141</td>\n",
       "      <td>168</td>\n",
       "      <td>51.139097</td>\n",
       "      <td>51.092995</td>\n",
       "      <td>0.207070</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2614.113165</td>\n",
       "      <td>2614.113525</td>\n",
       "      <td>2609.399187</td>\n",
       "      <td>2609.399414</td>\n",
       "      <td>187</td>\n",
       "      <td>51.128402</td>\n",
       "      <td>51.082281</td>\n",
       "      <td>0.785358</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>2614.315019</td>\n",
       "      <td>2614.315186</td>\n",
       "      <td>2609.594234</td>\n",
       "      <td>2609.594238</td>\n",
       "      <td>391</td>\n",
       "      <td>51.130374</td>\n",
       "      <td>51.084188</td>\n",
       "      <td>0.418621</td>\n",
       "      <td>200</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>2614.113151</td>\n",
       "      <td>2614.112793</td>\n",
       "      <td>2609.399146</td>\n",
       "      <td>2609.398926</td>\n",
       "      <td>350</td>\n",
       "      <td>51.128395</td>\n",
       "      <td>51.082276</td>\n",
       "      <td>0.376620</td>\n",
       "      <td>50</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2614.544201</td>\n",
       "      <td>2614.544434</td>\n",
       "      <td>2609.843739</td>\n",
       "      <td>2609.843750</td>\n",
       "      <td>204</td>\n",
       "      <td>51.132616</td>\n",
       "      <td>51.086630</td>\n",
       "      <td>0.280232</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2615.206869</td>\n",
       "      <td>2615.206543</td>\n",
       "      <td>2610.493995</td>\n",
       "      <td>2610.493896</td>\n",
       "      <td>41</td>\n",
       "      <td>51.139090</td>\n",
       "      <td>51.092993</td>\n",
       "      <td>0.685046</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>2614.272288</td>\n",
       "      <td>2614.272705</td>\n",
       "      <td>2609.577266</td>\n",
       "      <td>2609.577148</td>\n",
       "      <td>309</td>\n",
       "      <td>51.129959</td>\n",
       "      <td>51.084020</td>\n",
       "      <td>0.298626</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>2615.878739</td>\n",
       "      <td>2615.878174</td>\n",
       "      <td>2611.163881</td>\n",
       "      <td>2611.163818</td>\n",
       "      <td>222</td>\n",
       "      <td>51.145656</td>\n",
       "      <td>51.099548</td>\n",
       "      <td>1.040879</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>2616.416941</td>\n",
       "      <td>2616.416748</td>\n",
       "      <td>2611.699417</td>\n",
       "      <td>2611.699463</td>\n",
       "      <td>284</td>\n",
       "      <td>51.150921</td>\n",
       "      <td>51.104789</td>\n",
       "      <td>0.380148</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>2614.113117</td>\n",
       "      <td>2614.113281</td>\n",
       "      <td>2609.399095</td>\n",
       "      <td>2609.399170</td>\n",
       "      <td>252</td>\n",
       "      <td>51.128400</td>\n",
       "      <td>51.082278</td>\n",
       "      <td>0.382913</td>\n",
       "      <td>150</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "168  2615.206895         2615.207275  2610.494079             2610.494141   \n",
       "187  2614.113165         2614.113525  2609.399187             2609.399414   \n",
       "391  2614.315019         2614.315186  2609.594234             2609.594238   \n",
       "350  2614.113151         2614.112793  2609.399146             2609.398926   \n",
       "204  2614.544201         2614.544434  2609.843739             2609.843750   \n",
       "41   2615.206869         2615.206543  2610.493995             2610.493896   \n",
       "309  2614.272288         2614.272705  2609.577266             2609.577148   \n",
       "222  2615.878739         2615.878174  2611.163881             2611.163818   \n",
       "284  2616.416941         2616.416748  2611.699417             2611.699463   \n",
       "252  2614.113117         2614.113281  2609.399095             2609.399170   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation optimizer  lrate  \n",
       "168    168  51.139097  51.092995  0.207070      50       relu      adam  0.001  \n",
       "187    187  51.128402  51.082281  0.785358     200       tanh   adagrad  0.010  \n",
       "391    391  51.130374  51.084188  0.418621     200    sigmoid       sgd  0.010  \n",
       "350    350  51.128395  51.082276  0.376620      50    sigmoid     nadam  0.002  \n",
       "204    204  51.132616  51.086630  0.280232      50       relu   adagrad  0.010  \n",
       "41      41  51.139090  51.092993  0.685046     100       relu     nadam  0.002  \n",
       "309    309  51.129959  51.084020  0.298626     100       tanh       sgd  0.010  \n",
       "222    222  51.145656  51.099548  1.040879     150       relu     nadam  0.002  \n",
       "284    284  51.150921  51.104789  0.380148      50       relu     nadam  0.002  \n",
       "252    252  51.128400  51.082278  0.382913     150    sigmoid      adam  0.001  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "sm_sl_df = pd.read_pickle(\"OutputData/single_layer_softmax_df.pkl\")\n",
    "sm_sl_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcfb381d3cb413aa455f35b04caa1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd', 'nadam', 'adagrad'), value='ad…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a plotting function to pass to the interact widget function\n",
    "def plot_validation_loss(optimizer = sm_sl_df.optimizer.unique(), \n",
    "                    activation = sm_sl_df.activation.unique()):\n",
    "    \n",
    "    # Subset the baseline df by the specified optimizer and activation\n",
    "    sub_df = sm_sl_df[sm_sl_df.optimizer.str.match(optimizer)]\n",
    "    sub_df = sub_df[sub_df.activation.str.match(activation)]\n",
    "     \n",
    "    # Group the neural net data by optimizer and activation\n",
    "    groups = sub_df.groupby(['hunits'])\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over the grouped data and plot out epoch timing and validation loss data\n",
    "    for name, group in groups:\n",
    "        axes[0].scatter(group.epoch, group.val_RMSE, label=str(name)+' Validation Loss')\n",
    "    #     axes[0].scatter(group.epoch, group.RMSE, label=' '.join(name)+' Training Loss')\n",
    "        axes[1].scatter(group.epoch, group.times*1000, label=str(name)+' Fit Time')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Root Mean Square Error')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Fit Time (milliseconds)')\n",
    "        axes[0].set_ylim([0,sub_df.val_RMSE.max()])\n",
    "        axes[1].set_ylim([0,1000])\n",
    "        axes[0].legend()\n",
    "        axes[1].legend()\n",
    "        axes[0].set_title(\"{} Optimizer and {} Activation\".format(group.optimizer.unique(), group.activation.unique()))\n",
    "    \n",
    "    # Adjust the spacing of the subplots\n",
    "    fig.subplots_adjust(left=0.03, right=0.97, hspace=0.1, wspace=0.15)\n",
    "\n",
    "    # Add an overarching title for these plots\n",
    "    fig.suptitle(\"Performance Comparison for Single Layer, Fully Connected Neural Nets\",\n",
    "                 fontsize=18, y=0.93)\n",
    "\n",
    "#     # Print out the table of data for viewing\n",
    "#     print(sub_df)\n",
    "interact_manual(plot_validation_loss)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>693.430185</td>\n",
       "      <td>693.430115</td>\n",
       "      <td>691.275398</td>\n",
       "      <td>691.275391</td>\n",
       "      <td>209</td>\n",
       "      <td>26.333061</td>\n",
       "      <td>26.292117</td>\n",
       "      <td>0.312216</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>693.777985</td>\n",
       "      <td>693.777954</td>\n",
       "      <td>692.507681</td>\n",
       "      <td>692.507629</td>\n",
       "      <td>294</td>\n",
       "      <td>26.339665</td>\n",
       "      <td>26.315540</td>\n",
       "      <td>0.329013</td>\n",
       "      <td>150</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>921.213388</td>\n",
       "      <td>921.213501</td>\n",
       "      <td>919.227193</td>\n",
       "      <td>919.227112</td>\n",
       "      <td>239</td>\n",
       "      <td>30.351499</td>\n",
       "      <td>30.318758</td>\n",
       "      <td>0.261787</td>\n",
       "      <td>50</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>285.910883</td>\n",
       "      <td>285.910858</td>\n",
       "      <td>286.802552</td>\n",
       "      <td>286.802521</td>\n",
       "      <td>62</td>\n",
       "      <td>16.908899</td>\n",
       "      <td>16.935245</td>\n",
       "      <td>0.383565</td>\n",
       "      <td>150</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1108.915862</td>\n",
       "      <td>1108.916016</td>\n",
       "      <td>1104.156994</td>\n",
       "      <td>1104.156982</td>\n",
       "      <td>170</td>\n",
       "      <td>33.300391</td>\n",
       "      <td>33.228858</td>\n",
       "      <td>0.422945</td>\n",
       "      <td>50</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>640.055802</td>\n",
       "      <td>640.055786</td>\n",
       "      <td>639.210816</td>\n",
       "      <td>639.210754</td>\n",
       "      <td>75</td>\n",
       "      <td>25.299324</td>\n",
       "      <td>25.282618</td>\n",
       "      <td>0.484436</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>693.432417</td>\n",
       "      <td>693.432373</td>\n",
       "      <td>691.271096</td>\n",
       "      <td>691.271057</td>\n",
       "      <td>256</td>\n",
       "      <td>26.333104</td>\n",
       "      <td>26.292034</td>\n",
       "      <td>0.310465</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>487.449163</td>\n",
       "      <td>487.449127</td>\n",
       "      <td>486.786071</td>\n",
       "      <td>486.786011</td>\n",
       "      <td>261</td>\n",
       "      <td>22.078250</td>\n",
       "      <td>22.063228</td>\n",
       "      <td>0.760776</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1107.811088</td>\n",
       "      <td>1107.811279</td>\n",
       "      <td>1103.192954</td>\n",
       "      <td>1103.192993</td>\n",
       "      <td>120</td>\n",
       "      <td>33.283799</td>\n",
       "      <td>33.214349</td>\n",
       "      <td>0.376640</td>\n",
       "      <td>150</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>466.308174</td>\n",
       "      <td>466.308167</td>\n",
       "      <td>464.690888</td>\n",
       "      <td>464.690857</td>\n",
       "      <td>223</td>\n",
       "      <td>21.594170</td>\n",
       "      <td>21.556689</td>\n",
       "      <td>0.360670</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "209   693.430185          693.430115   691.275398              691.275391   \n",
       "294   693.777985          693.777954   692.507681              692.507629   \n",
       "239   921.213388          921.213501   919.227193              919.227112   \n",
       "62    285.910883          285.910858   286.802552              286.802521   \n",
       "170  1108.915862         1108.916016  1104.156994             1104.156982   \n",
       "75    640.055802          640.055786   639.210816              639.210754   \n",
       "256   693.432417          693.432373   691.271096              691.271057   \n",
       "261   487.449163          487.449127   486.786071              486.786011   \n",
       "120  1107.811088         1107.811279  1103.192954             1103.192993   \n",
       "223   466.308174          466.308167   464.690888              464.690857   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation optimizer  lrate  \n",
       "209    209  26.333061  26.292117  0.312216     100       tanh      adam  0.001  \n",
       "294    294  26.339665  26.315540  0.329013     150    sigmoid       sgd  0.010  \n",
       "239    239  30.351499  30.318758  0.261787      50    sigmoid   adagrad  0.010  \n",
       "62      62  16.908899  16.935245  0.383565     150    sigmoid      adam  0.001  \n",
       "170    170  33.300391  33.228858  0.422945      50    sigmoid     nadam  0.002  \n",
       "75      75  25.299324  25.282618  0.484436     100    sigmoid   adagrad  0.010  \n",
       "256    256  26.333104  26.292034  0.310465     100       tanh      adam  0.001  \n",
       "261    261  22.078250  22.063228  0.760776     200       tanh   adagrad  0.010  \n",
       "120    120  33.283799  33.214349  0.376640     150       tanh      adam  0.001  \n",
       "223    223  21.594170  21.556689  0.360670      50       tanh     nadam  0.002  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "relu_sl_df = pd.read_pickle(\"OutputData/single_layer_relu_df.pkl\")\n",
    "relu_sl_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0708d8239142d5b2d4b12f854b4d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd', 'nadam', 'adagrad'), value='ad…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a plotting function to pass to the interact widget function\n",
    "def plot_validation_loss(optimizer = relu_sl_df.optimizer.unique(), \n",
    "                    activation = relu_sl_df.activation.unique()):\n",
    "    \n",
    "    # Subset the baseline df by the specified optimizer and activation\n",
    "    sub_df = relu_sl_df[relu_sl_df.optimizer.str.match(optimizer)]\n",
    "    sub_df = sub_df[sub_df.activation.str.match(activation)]\n",
    "     \n",
    "    # Group the neural net data by optimizer and activation\n",
    "    groups = sub_df.groupby(['hunits'])\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over the grouped data and plot out epoch timing and validation loss data\n",
    "    for name, group in groups:\n",
    "        axes[0].scatter(group.epoch, group.val_RMSE, label=str(name)+' Validation Loss')\n",
    "    #     axes[0].scatter(group.epoch, group.RMSE, label=' '.join(name)+' Training Loss')\n",
    "        axes[1].scatter(group.epoch, group.times*1000, label=str(name)+' Fit Time')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Root Mean Square Error')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Fit Time (milliseconds)')\n",
    "        axes[0].set_ylim([0,sub_df.val_RMSE.max()])\n",
    "        axes[1].set_ylim([0,1000])\n",
    "        axes[0].legend()\n",
    "        axes[1].legend()\n",
    "        axes[0].set_title(\"{} Optimizer and {} Activation\".format(group.optimizer.unique(), group.activation.unique()))\n",
    "    \n",
    "    # Adjust the spacing of the subplots\n",
    "    fig.subplots_adjust(left=0.03, right=0.97, hspace=0.1, wspace=0.15)\n",
    "\n",
    "    # Add an overarching title for these plots\n",
    "    fig.suptitle(\"Performance Comparison for Single Layer, Fully Connected Neural Nets\",\n",
    "                 fontsize=18, y=0.93)\n",
    "\n",
    "#     # Print out the table of data for viewing\n",
    "#     print(sub_df)\n",
    "interact_manual(plot_validation_loss)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimizer\n",
       "adagrad    22.046067\n",
       "adam        3.142767\n",
       "nadam      13.423687\n",
       "sgd         7.542013\n",
       "Name: val_RMSE, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_sl_df.groupby('optimizer').val_RMSE.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Assessment\n",
    "Based on the three different experiments run, we will use a RELU final layer activation function. We will continue to assess adagrad and adam optimizers. We likely do not need to train for more than 200 epochs or so to get reasonably converged nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>580.891349</td>\n",
       "      <td>580.891296</td>\n",
       "      <td>580.287992</td>\n",
       "      <td>580.287964</td>\n",
       "      <td>20</td>\n",
       "      <td>24.101687</td>\n",
       "      <td>24.089167</td>\n",
       "      <td>0.297721</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad_005</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>187.435284</td>\n",
       "      <td>187.435287</td>\n",
       "      <td>185.993254</td>\n",
       "      <td>185.993240</td>\n",
       "      <td>3</td>\n",
       "      <td>13.690701</td>\n",
       "      <td>13.637934</td>\n",
       "      <td>0.526855</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad_005</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>814.482202</td>\n",
       "      <td>814.482361</td>\n",
       "      <td>813.402330</td>\n",
       "      <td>813.402344</td>\n",
       "      <td>161</td>\n",
       "      <td>28.539137</td>\n",
       "      <td>28.520209</td>\n",
       "      <td>0.256438</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad_02</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>135.214011</td>\n",
       "      <td>135.214005</td>\n",
       "      <td>133.959131</td>\n",
       "      <td>133.959122</td>\n",
       "      <td>160</td>\n",
       "      <td>11.628156</td>\n",
       "      <td>11.574071</td>\n",
       "      <td>0.818475</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad_005</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>240.972260</td>\n",
       "      <td>240.972244</td>\n",
       "      <td>240.499103</td>\n",
       "      <td>240.499100</td>\n",
       "      <td>31</td>\n",
       "      <td>15.523281</td>\n",
       "      <td>15.508033</td>\n",
       "      <td>0.493412</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad_02</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>35.179296</td>\n",
       "      <td>35.179291</td>\n",
       "      <td>35.044818</td>\n",
       "      <td>35.044819</td>\n",
       "      <td>22</td>\n",
       "      <td>5.931213</td>\n",
       "      <td>5.919866</td>\n",
       "      <td>0.453727</td>\n",
       "      <td>150</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam_005</td>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>815.030874</td>\n",
       "      <td>815.030945</td>\n",
       "      <td>813.811062</td>\n",
       "      <td>813.811035</td>\n",
       "      <td>72</td>\n",
       "      <td>28.548747</td>\n",
       "      <td>28.527373</td>\n",
       "      <td>0.261414</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad_02</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>35.293242</td>\n",
       "      <td>35.293247</td>\n",
       "      <td>35.034047</td>\n",
       "      <td>35.034046</td>\n",
       "      <td>181</td>\n",
       "      <td>5.940812</td>\n",
       "      <td>5.918957</td>\n",
       "      <td>0.393565</td>\n",
       "      <td>150</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam_005</td>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>152.251990</td>\n",
       "      <td>152.251999</td>\n",
       "      <td>151.503053</td>\n",
       "      <td>151.503036</td>\n",
       "      <td>9</td>\n",
       "      <td>12.339044</td>\n",
       "      <td>12.308657</td>\n",
       "      <td>0.600839</td>\n",
       "      <td>150</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad_02</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>680.404938</td>\n",
       "      <td>680.404968</td>\n",
       "      <td>675.654405</td>\n",
       "      <td>675.654419</td>\n",
       "      <td>56</td>\n",
       "      <td>26.084573</td>\n",
       "      <td>25.993353</td>\n",
       "      <td>0.200003</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam_0005</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           loss  mean_squared_error    val_loss  val_mean_squared_error  \\\n",
       "20   580.891349          580.891296  580.287992              580.287964   \n",
       "3    187.435284          187.435287  185.993254              185.993240   \n",
       "161  814.482202          814.482361  813.402330              813.402344   \n",
       "160  135.214011          135.214005  133.959131              133.959122   \n",
       "31   240.972260          240.972244  240.499103              240.499100   \n",
       "22    35.179296           35.179291   35.044818               35.044819   \n",
       "72   815.030874          815.030945  813.811062              813.811035   \n",
       "181   35.293242           35.293247   35.034047               35.034046   \n",
       "9    152.251990          152.251999  151.503053              151.503036   \n",
       "56   680.404938          680.404968  675.654405              675.654419   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation    optimizer  \\\n",
       "20      20  24.101687  24.089167  0.297721      50       tanh  adagrad_005   \n",
       "3        3  13.690701  13.637934  0.526855     100       tanh  adagrad_005   \n",
       "161    161  28.539137  28.520209  0.256438      50       tanh   adagrad_02   \n",
       "160    160  11.628156  11.574071  0.818475     200       tanh  adagrad_005   \n",
       "31      31  15.523281  15.508033  0.493412     100       tanh   adagrad_02   \n",
       "22      22   5.931213   5.919866  0.453727     150       tanh     adam_005   \n",
       "72      72  28.548747  28.527373  0.261414      50       tanh   adagrad_02   \n",
       "181    181   5.940812   5.918957  0.393565     150       tanh     adam_005   \n",
       "9        9  12.339044  12.308657  0.600839     150       tanh   adagrad_02   \n",
       "56      56  26.084573  25.993353  0.200003      50       tanh    adam_0005   \n",
       "\n",
       "      lrate  \n",
       "20   0.0500  \n",
       "3    0.0500  \n",
       "161  0.0200  \n",
       "160  0.0500  \n",
       "31   0.0200  \n",
       "22   0.0050  \n",
       "72   0.0200  \n",
       "181  0.0050  \n",
       "9    0.0200  \n",
       "56   0.0005  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "relu_sl_lr_df = pd.read_pickle(\"OutputData/single_layer_relu_lr_df.pkl\")\n",
    "relu_sl_lr_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b59fef0a15548869ba451630a929863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam_005', 'adam_0005', 'adagrad_02', 'adagr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a plotting function to pass to the interact widget function\n",
    "def plot_validation_loss(optimizer = relu_sl_lr_df.optimizer.unique(), \n",
    "                    activation = relu_sl_lr_df.activation.unique()):\n",
    "    \n",
    "    # Subset the baseline df by the specified optimizer and activation\n",
    "    sub_df = relu_sl_lr_df[relu_sl_lr_df.optimizer.str.match(optimizer)]\n",
    "    sub_df = sub_df[sub_df.activation.str.match(activation)]\n",
    "     \n",
    "    # Group the neural net data by optimizer and activation\n",
    "    groups = sub_df.groupby(['hunits'])\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over the grouped data and plot out epoch timing and validation loss data\n",
    "    for name, group in groups:\n",
    "        axes[0].scatter(group.epoch, group.val_RMSE, label=str(name)+' Validation Loss')\n",
    "    #     axes[0].scatter(group.epoch, group.RMSE, label=' '.join(name)+' Training Loss')\n",
    "        axes[1].scatter(group.epoch, group.times*1000, label=str(name)+' Fit Time')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Root Mean Square Error')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Fit Time (milliseconds)')\n",
    "        axes[0].set_ylim([0,sub_df.val_RMSE.max()])\n",
    "        axes[1].set_ylim([0,1000])\n",
    "        axes[0].legend()\n",
    "        axes[1].legend()\n",
    "        axes[0].set_title(\"{} Optimizer and {} Activation\".format(group.optimizer.unique(), group.activation.unique()))\n",
    "    \n",
    "    # Adjust the spacing of the subplots\n",
    "    fig.subplots_adjust(left=0.03, right=0.97, hspace=0.1, wspace=0.15)\n",
    "\n",
    "    # Add an overarching title for these plots\n",
    "    fig.suptitle(\"Performance Comparison for Single Layer, Fully Connected Neural Nets\",\n",
    "                 fontsize=18, y=0.93)\n",
    "\n",
    "#     # Print out the table of data for viewing\n",
    "#     print(sub_df)\n",
    "interact_manual(plot_validation_loss)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimizer\n",
       "adagrad_005    11.573482\n",
       "adagrad_02     12.259225\n",
       "adam_0005      16.874466\n",
       "adam_005        5.915614\n",
       "Name: val_RMSE, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_sl_lr_df.groupby('optimizer').val_RMSE.min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
