{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Analysis Notebook\n",
    "## W207 Final Project\n",
    "### T. P. Goter\n",
    "### July 6, 2019\n",
    "\n",
    "This workbook is used to assess various models created as part of the Facial Keypoint Detection project for W207."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we need\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, fixed\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2525.411497</td>\n",
       "      <td>2525.411865</td>\n",
       "      <td>2520.734895</td>\n",
       "      <td>2520.734863</td>\n",
       "      <td>236</td>\n",
       "      <td>50.253476</td>\n",
       "      <td>50.206920</td>\n",
       "      <td>0.990022</td>\n",
       "      <td>150</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>2525.411764</td>\n",
       "      <td>2525.411865</td>\n",
       "      <td>2520.735151</td>\n",
       "      <td>2520.735107</td>\n",
       "      <td>320</td>\n",
       "      <td>50.253476</td>\n",
       "      <td>50.206923</td>\n",
       "      <td>0.760812</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>2525.411546</td>\n",
       "      <td>2525.411377</td>\n",
       "      <td>2520.734875</td>\n",
       "      <td>2520.734619</td>\n",
       "      <td>181</td>\n",
       "      <td>50.253471</td>\n",
       "      <td>50.206918</td>\n",
       "      <td>0.341651</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>67.675726</td>\n",
       "      <td>67.675735</td>\n",
       "      <td>99.739942</td>\n",
       "      <td>99.739944</td>\n",
       "      <td>346</td>\n",
       "      <td>8.226526</td>\n",
       "      <td>9.986989</td>\n",
       "      <td>0.488164</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>2525.411545</td>\n",
       "      <td>2525.411133</td>\n",
       "      <td>2520.734860</td>\n",
       "      <td>2520.734619</td>\n",
       "      <td>292</td>\n",
       "      <td>50.253469</td>\n",
       "      <td>50.206918</td>\n",
       "      <td>0.431173</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>246.198381</td>\n",
       "      <td>246.198410</td>\n",
       "      <td>263.961196</td>\n",
       "      <td>263.961182</td>\n",
       "      <td>221</td>\n",
       "      <td>15.690711</td>\n",
       "      <td>16.246882</td>\n",
       "      <td>0.337510</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2619.589251</td>\n",
       "      <td>2619.588867</td>\n",
       "      <td>2614.870264</td>\n",
       "      <td>2614.870117</td>\n",
       "      <td>246</td>\n",
       "      <td>51.181919</td>\n",
       "      <td>51.135801</td>\n",
       "      <td>0.292686</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>2525.411532</td>\n",
       "      <td>2525.411377</td>\n",
       "      <td>2520.734826</td>\n",
       "      <td>2520.735107</td>\n",
       "      <td>347</td>\n",
       "      <td>50.253471</td>\n",
       "      <td>50.206923</td>\n",
       "      <td>0.800628</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2525.427266</td>\n",
       "      <td>2525.427246</td>\n",
       "      <td>2520.750329</td>\n",
       "      <td>2520.750488</td>\n",
       "      <td>25</td>\n",
       "      <td>50.253629</td>\n",
       "      <td>50.207076</td>\n",
       "      <td>0.613186</td>\n",
       "      <td>150</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>2525.411532</td>\n",
       "      <td>2525.410889</td>\n",
       "      <td>2520.734777</td>\n",
       "      <td>2520.734863</td>\n",
       "      <td>271</td>\n",
       "      <td>50.253466</td>\n",
       "      <td>50.206920</td>\n",
       "      <td>0.439407</td>\n",
       "      <td>50</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "236  2525.411497         2525.411865  2520.734895             2520.734863   \n",
       "320  2525.411764         2525.411865  2520.735151             2520.735107   \n",
       "181  2525.411546         2525.411377  2520.734875             2520.734619   \n",
       "346    67.675726           67.675735    99.739942               99.739944   \n",
       "292  2525.411545         2525.411133  2520.734860             2520.734619   \n",
       "221   246.198381          246.198410   263.961196              263.961182   \n",
       "246  2619.589251         2619.588867  2614.870264             2614.870117   \n",
       "347  2525.411532         2525.411377  2520.734826             2520.735107   \n",
       "25   2525.427266         2525.427246  2520.750329             2520.750488   \n",
       "271  2525.411532         2525.410889  2520.734777             2520.734863   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation optimizer  lrate  \n",
       "236    236  50.253476  50.206920  0.990022     150    sigmoid     nadam  0.002  \n",
       "320    320  50.253476  50.206923  0.760812     200       tanh   adagrad  0.010  \n",
       "181    181  50.253471  50.206918  0.341651     100    sigmoid      adam  0.001  \n",
       "346    346   8.226526   9.986989  0.488164     200       relu      adam  0.001  \n",
       "292    292  50.253469  50.206918  0.431173     200       tanh       sgd  0.010  \n",
       "221    221  15.690711  16.246882  0.337510      50       relu   adagrad  0.010  \n",
       "246    246  51.181919  51.135801  0.292686     100       relu       sgd  0.010  \n",
       "347    347  50.253471  50.206923  0.800628     100       tanh     nadam  0.002  \n",
       "25      25  50.253629  50.207076  0.613186     150    sigmoid   adagrad  0.010  \n",
       "271    271  50.253466  50.206920  0.439407      50    sigmoid     nadam  0.002  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "bl_sl_df = pd.read_pickle(\"OutputData/single_layer_df.pkl\")\n",
    "bl_sl_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plotting function to pass to the interact widget function\n",
    "def plot_validation_loss(df=bl_sl_df, optimizer = bl_sl_df.optimizer.unique(), \n",
    "                    activation = bl_sl_df.activation.unique()):\n",
    "    \n",
    "    # Subset the baseline df by the specified optimizer and activation\n",
    "    sub_df = df[df.optimizer.str.match(optimizer)]\n",
    "    sub_df = sub_df[sub_df.activation.str.match(activation)]\n",
    "     \n",
    "    # Group the neural net data by optimizer and activation\n",
    "    groups = sub_df.groupby(['hunits'])\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over the grouped data and plot out epoch timing and validation loss data\n",
    "    for name, group in groups:\n",
    "        axes[0].plot(group.epoch, group.val_RMSE, label=str(name)+' Validation Loss')\n",
    "    #     axes[0].scatter(group.epoch, group.RMSE, label=' '.join(name)+' Training Loss')\n",
    "        axes[1].scatter(group.epoch, group.times*1000, label=str(name)+' Fit Time')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Root Mean Square Error')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Fit Time (milliseconds)')\n",
    "        axes[0].set_ylim([0,sub_df.val_RMSE.max()])\n",
    "        axes[1].set_ylim([0,1000])\n",
    "        axes[0].legend()\n",
    "        axes[1].legend()\n",
    "        axes[0].set_title(\"{} Optimizer and {} Activation\".format(group.optimizer.unique(), group.activation.unique()))\n",
    "    \n",
    "    # Adjust the spacing of the subplots\n",
    "    fig.subplots_adjust(left=0.03, right=0.97, hspace=0.1, wspace=0.15)\n",
    "\n",
    "    # Add an overarching title for these plots\n",
    "    fig.suptitle(\"Performance Comparison for Single Layer, Fully Connected Neural Nets\",\n",
    "                 fontsize=18, y=0.93)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff450fb946f4aa6a843d06963bc4d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd', 'nadam', 'adagrad'), value='ad…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interact_manual(plot_validation_loss, df=fixed(bl_sl_df), \n",
    "                optimizer = bl_sl_df.optimizer.unique(), \n",
    "                    activation = bl_sl_df.activation.unique())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment of Baseline Results\n",
    "1. Adam and Adagrad Optimizers are working well. \n",
    "2. Adam is faster and works well with 200 hidden units\n",
    "3. Adagrad is slower buts works best with 100 hidden units.\n",
    "\n",
    "In the evaluation above, both the hidden layer and the output layer used the activation function specified by the user. For the study below, the activation function of the output layer was set to softmax which is a multinomial classifier version of the sigmoid function. The plots below help to assess if the choice of activation function for the output layer significant alters are perception of which activation function and optimizers work well for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2614.113149</td>\n",
       "      <td>2614.113037</td>\n",
       "      <td>2609.399151</td>\n",
       "      <td>2609.399170</td>\n",
       "      <td>224</td>\n",
       "      <td>51.128398</td>\n",
       "      <td>51.082278</td>\n",
       "      <td>0.375341</td>\n",
       "      <td>150</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2614.113185</td>\n",
       "      <td>2614.113281</td>\n",
       "      <td>2609.399150</td>\n",
       "      <td>2609.398926</td>\n",
       "      <td>193</td>\n",
       "      <td>51.128400</td>\n",
       "      <td>51.082276</td>\n",
       "      <td>0.196371</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2614.113111</td>\n",
       "      <td>2614.113281</td>\n",
       "      <td>2609.399093</td>\n",
       "      <td>2609.398926</td>\n",
       "      <td>253</td>\n",
       "      <td>51.128400</td>\n",
       "      <td>51.082276</td>\n",
       "      <td>0.997336</td>\n",
       "      <td>150</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>2614.113245</td>\n",
       "      <td>2614.113281</td>\n",
       "      <td>2609.399322</td>\n",
       "      <td>2609.399414</td>\n",
       "      <td>284</td>\n",
       "      <td>51.128400</td>\n",
       "      <td>51.082281</td>\n",
       "      <td>0.219080</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2614.559659</td>\n",
       "      <td>2614.559326</td>\n",
       "      <td>2609.867712</td>\n",
       "      <td>2609.867676</td>\n",
       "      <td>30</td>\n",
       "      <td>51.132762</td>\n",
       "      <td>51.086864</td>\n",
       "      <td>0.503690</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>2616.416937</td>\n",
       "      <td>2616.416748</td>\n",
       "      <td>2611.699415</td>\n",
       "      <td>2611.699463</td>\n",
       "      <td>370</td>\n",
       "      <td>51.150921</td>\n",
       "      <td>51.104789</td>\n",
       "      <td>0.388754</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2614.544216</td>\n",
       "      <td>2614.544189</td>\n",
       "      <td>2609.843707</td>\n",
       "      <td>2609.843750</td>\n",
       "      <td>132</td>\n",
       "      <td>51.132614</td>\n",
       "      <td>51.086630</td>\n",
       "      <td>0.391998</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>2614.113138</td>\n",
       "      <td>2614.113281</td>\n",
       "      <td>2609.399080</td>\n",
       "      <td>2609.398926</td>\n",
       "      <td>336</td>\n",
       "      <td>51.128400</td>\n",
       "      <td>51.082276</td>\n",
       "      <td>0.223160</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>2614.315085</td>\n",
       "      <td>2614.314941</td>\n",
       "      <td>2609.594211</td>\n",
       "      <td>2609.594238</td>\n",
       "      <td>371</td>\n",
       "      <td>51.130372</td>\n",
       "      <td>51.084188</td>\n",
       "      <td>0.203373</td>\n",
       "      <td>50</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2614.113122</td>\n",
       "      <td>2614.113281</td>\n",
       "      <td>2609.399158</td>\n",
       "      <td>2609.399170</td>\n",
       "      <td>187</td>\n",
       "      <td>51.128400</td>\n",
       "      <td>51.082278</td>\n",
       "      <td>0.636158</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "224  2614.113149         2614.113037  2609.399151             2609.399170   \n",
       "193  2614.113185         2614.113281  2609.399150             2609.398926   \n",
       "253  2614.113111         2614.113281  2609.399093             2609.398926   \n",
       "284  2614.113245         2614.113281  2609.399322             2609.399414   \n",
       "30   2614.559659         2614.559326  2609.867712             2609.867676   \n",
       "370  2616.416937         2616.416748  2611.699415             2611.699463   \n",
       "132  2614.544216         2614.544189  2609.843707             2609.843750   \n",
       "336  2614.113138         2614.113281  2609.399080             2609.398926   \n",
       "371  2614.315085         2614.314941  2609.594211             2609.594238   \n",
       "187  2614.113122         2614.113281  2609.399158             2609.399170   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation optimizer  lrate  \n",
       "224    224  51.128398  51.082278  0.375341     150       tanh      adam  0.001  \n",
       "193    193  51.128400  51.082276  0.196371      50       tanh       sgd  0.010  \n",
       "253    253  51.128400  51.082276  0.997336     150    sigmoid     nadam  0.002  \n",
       "284    284  51.128400  51.082281  0.219080      50       tanh   adagrad  0.010  \n",
       "30      30  51.132762  51.086864  0.503690     100    sigmoid   adagrad  0.010  \n",
       "370    370  51.150921  51.104789  0.388754      50       relu     nadam  0.002  \n",
       "132    132  51.132614  51.086630  0.391998     200       relu       sgd  0.010  \n",
       "336    336  51.128400  51.082276  0.223160      50       tanh      adam  0.001  \n",
       "371    371  51.130372  51.084188  0.203373      50    sigmoid       sgd  0.010  \n",
       "187    187  51.128400  51.082278  0.636158     150       relu   adagrad  0.010  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "sm_sl_df = pd.read_pickle(\"OutputData/single_layer_softmax_df.pkl\")\n",
    "sm_sl_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0d7a3dd3624eb4806e75e5f5ef7d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd', 'nadam', 'adagrad'), value='ad…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the softmax data\n",
    "interact_manual(plot_validation_loss, df=fixed(sm_sl_df), \n",
    "                optimizer = sm_sl_df.optimizer.unique(), \n",
    "                    activation = sm_sl_df.activation.unique())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>57.552650</td>\n",
       "      <td>57.552654</td>\n",
       "      <td>56.939717</td>\n",
       "      <td>56.939720</td>\n",
       "      <td>141</td>\n",
       "      <td>7.586347</td>\n",
       "      <td>7.545841</td>\n",
       "      <td>0.356258</td>\n",
       "      <td>150</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>1127.115141</td>\n",
       "      <td>1127.115234</td>\n",
       "      <td>1125.635432</td>\n",
       "      <td>1125.635376</td>\n",
       "      <td>333</td>\n",
       "      <td>33.572537</td>\n",
       "      <td>33.550490</td>\n",
       "      <td>0.267730</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>1155.966775</td>\n",
       "      <td>1155.967041</td>\n",
       "      <td>1150.548828</td>\n",
       "      <td>1150.548828</td>\n",
       "      <td>289</td>\n",
       "      <td>33.999515</td>\n",
       "      <td>33.919741</td>\n",
       "      <td>0.210961</td>\n",
       "      <td>50</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>693.239986</td>\n",
       "      <td>693.240112</td>\n",
       "      <td>691.441554</td>\n",
       "      <td>691.441528</td>\n",
       "      <td>212</td>\n",
       "      <td>26.329453</td>\n",
       "      <td>26.295276</td>\n",
       "      <td>0.331476</td>\n",
       "      <td>150</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>454.136303</td>\n",
       "      <td>454.136292</td>\n",
       "      <td>453.087433</td>\n",
       "      <td>453.087433</td>\n",
       "      <td>325</td>\n",
       "      <td>21.310474</td>\n",
       "      <td>21.285851</td>\n",
       "      <td>0.479637</td>\n",
       "      <td>200</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>1620.196116</td>\n",
       "      <td>1620.196045</td>\n",
       "      <td>1615.747777</td>\n",
       "      <td>1615.747803</td>\n",
       "      <td>338</td>\n",
       "      <td>40.251659</td>\n",
       "      <td>40.196366</td>\n",
       "      <td>0.416274</td>\n",
       "      <td>200</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>879.267893</td>\n",
       "      <td>879.267822</td>\n",
       "      <td>876.853924</td>\n",
       "      <td>876.853943</td>\n",
       "      <td>117</td>\n",
       "      <td>29.652451</td>\n",
       "      <td>29.611720</td>\n",
       "      <td>0.334154</td>\n",
       "      <td>100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>285.918818</td>\n",
       "      <td>285.918884</td>\n",
       "      <td>286.812901</td>\n",
       "      <td>286.812927</td>\n",
       "      <td>399</td>\n",
       "      <td>16.909136</td>\n",
       "      <td>16.935552</td>\n",
       "      <td>0.390663</td>\n",
       "      <td>150</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>759.176771</td>\n",
       "      <td>759.176758</td>\n",
       "      <td>755.215111</td>\n",
       "      <td>755.215088</td>\n",
       "      <td>75</td>\n",
       "      <td>27.553162</td>\n",
       "      <td>27.481177</td>\n",
       "      <td>0.599911</td>\n",
       "      <td>150</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>502.154205</td>\n",
       "      <td>502.154205</td>\n",
       "      <td>500.816491</td>\n",
       "      <td>500.816437</td>\n",
       "      <td>273</td>\n",
       "      <td>22.408797</td>\n",
       "      <td>22.378928</td>\n",
       "      <td>0.604426</td>\n",
       "      <td>150</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "141    57.552650           57.552654    56.939717               56.939720   \n",
       "333  1127.115141         1127.115234  1125.635432             1125.635376   \n",
       "289  1155.966775         1155.967041  1150.548828             1150.548828   \n",
       "212   693.239986          693.240112   691.441554              691.441528   \n",
       "325   454.136303          454.136292   453.087433              453.087433   \n",
       "338  1620.196116         1620.196045  1615.747777             1615.747803   \n",
       "117   879.267893          879.267822   876.853924              876.853943   \n",
       "399   285.918818          285.918884   286.812901              286.812927   \n",
       "75    759.176771          759.176758   755.215111              755.215088   \n",
       "273   502.154205          502.154205   500.816491              500.816437   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation optimizer  lrate  \n",
       "141    141   7.586347   7.545841  0.356258     150       tanh       sgd  0.010  \n",
       "333    333  33.572537  33.550490  0.267730      50       tanh   adagrad  0.010  \n",
       "289    289  33.999515  33.919741  0.210961      50    sigmoid      adam  0.001  \n",
       "212    212  26.329453  26.295276  0.331476     150    sigmoid       sgd  0.010  \n",
       "325    325  21.310474  21.285851  0.479637     200    sigmoid      adam  0.001  \n",
       "338    338  40.251659  40.196366  0.416274     200    sigmoid       sgd  0.010  \n",
       "117    117  29.652451  29.611720  0.334154     100    sigmoid       sgd  0.010  \n",
       "399    399  16.909136  16.935552  0.390663     150    sigmoid      adam  0.001  \n",
       "75      75  27.553162  27.481177  0.599911     150    sigmoid   adagrad  0.010  \n",
       "273    273  22.408797  22.378928  0.604426     150       tanh   adagrad  0.010  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "relu_sl_df = pd.read_pickle(\"OutputData/single_layer_relu_df.pkl\")\n",
    "relu_sl_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b44523b9a3c42b19fd2c80c96f1c085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd', 'nadam', 'adagrad'), value='ad…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the relu data\n",
    "interact_manual(plot_validation_loss, df=fixed(relu_sl_df), \n",
    "                optimizer = relu_sl_df.optimizer.unique(), \n",
    "                    activation = relu_sl_df.activation.unique())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relu_sl_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f9bf308271ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrelu_sl_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_RMSE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'relu_sl_df' is not defined"
     ]
    }
   ],
   "source": [
    "relu_sl_df.groupby('optimizer').val_RMSE.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Assessment\n",
    "Based on the three different experiments run, we will use a RELU final layer activation function. We will continue to assess adagrad and adam optimizers. We likely do not need to train for more than 200 epochs or so to get reasonably converged nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>585.102651</td>\n",
       "      <td>585.102600</td>\n",
       "      <td>602.217882</td>\n",
       "      <td>602.217896</td>\n",
       "      <td>166</td>\n",
       "      <td>24.188894</td>\n",
       "      <td>24.540128</td>\n",
       "      <td>0.777923</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad_02</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>101.179164</td>\n",
       "      <td>101.179169</td>\n",
       "      <td>121.990278</td>\n",
       "      <td>121.990288</td>\n",
       "      <td>197</td>\n",
       "      <td>10.058786</td>\n",
       "      <td>11.044921</td>\n",
       "      <td>0.195769</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam_005</td>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1382.132048</td>\n",
       "      <td>1382.132080</td>\n",
       "      <td>1389.458887</td>\n",
       "      <td>1389.458862</td>\n",
       "      <td>190</td>\n",
       "      <td>37.177037</td>\n",
       "      <td>37.275446</td>\n",
       "      <td>0.561554</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad_005</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>84.662256</td>\n",
       "      <td>84.662247</td>\n",
       "      <td>116.007755</td>\n",
       "      <td>116.007736</td>\n",
       "      <td>184</td>\n",
       "      <td>9.201209</td>\n",
       "      <td>10.770689</td>\n",
       "      <td>0.490967</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam_0005</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>154.702597</td>\n",
       "      <td>154.702591</td>\n",
       "      <td>174.748838</td>\n",
       "      <td>174.748825</td>\n",
       "      <td>154</td>\n",
       "      <td>12.437950</td>\n",
       "      <td>13.219260</td>\n",
       "      <td>0.468345</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam_005</td>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>293.815909</td>\n",
       "      <td>293.815887</td>\n",
       "      <td>292.408084</td>\n",
       "      <td>292.408112</td>\n",
       "      <td>121</td>\n",
       "      <td>17.141059</td>\n",
       "      <td>17.099945</td>\n",
       "      <td>0.259666</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad_02</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>360.313932</td>\n",
       "      <td>360.313965</td>\n",
       "      <td>370.613009</td>\n",
       "      <td>370.613007</td>\n",
       "      <td>62</td>\n",
       "      <td>18.981938</td>\n",
       "      <td>19.251312</td>\n",
       "      <td>0.271495</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad_005</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>244.381297</td>\n",
       "      <td>244.381256</td>\n",
       "      <td>244.194078</td>\n",
       "      <td>244.194077</td>\n",
       "      <td>60</td>\n",
       "      <td>15.632698</td>\n",
       "      <td>15.626710</td>\n",
       "      <td>0.247615</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad_005</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>87.490673</td>\n",
       "      <td>87.490669</td>\n",
       "      <td>87.137560</td>\n",
       "      <td>87.137558</td>\n",
       "      <td>115</td>\n",
       "      <td>9.353645</td>\n",
       "      <td>9.334750</td>\n",
       "      <td>0.756261</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adagrad_005</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1393.413407</td>\n",
       "      <td>1393.413452</td>\n",
       "      <td>1396.197367</td>\n",
       "      <td>1396.197388</td>\n",
       "      <td>60</td>\n",
       "      <td>37.328454</td>\n",
       "      <td>37.365725</td>\n",
       "      <td>0.543673</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad_005</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "166   585.102651          585.102600   602.217882              602.217896   \n",
       "197   101.179164          101.179169   121.990278              121.990288   \n",
       "190  1382.132048         1382.132080  1389.458887             1389.458862   \n",
       "184    84.662256           84.662247   116.007755              116.007736   \n",
       "154   154.702597          154.702591   174.748838              174.748825   \n",
       "121   293.815909          293.815887   292.408084              292.408112   \n",
       "62    360.313932          360.313965   370.613009              370.613007   \n",
       "60    244.381297          244.381256   244.194078              244.194077   \n",
       "115    87.490673           87.490669    87.137560               87.137558   \n",
       "60   1393.413407         1393.413452  1396.197367             1396.197388   \n",
       "\n",
       "     epoch       RMSE   val_RMSE     times  hunits activation    optimizer  \\\n",
       "166    166  24.188894  24.540128  0.777923     200       relu   adagrad_02   \n",
       "197    197  10.058786  11.044921  0.195769      50       relu     adam_005   \n",
       "190    190  37.177037  37.275446  0.561554     150       relu  adagrad_005   \n",
       "184    184   9.201209  10.770689  0.490967     200       relu    adam_0005   \n",
       "154    154  12.437950  13.219260  0.468345     200       relu     adam_005   \n",
       "121    121  17.141059  17.099945  0.259666      50       tanh   adagrad_02   \n",
       "62      62  18.981938  19.251312  0.271495      50       relu  adagrad_005   \n",
       "60      60  15.632698  15.626710  0.247615      50       tanh  adagrad_005   \n",
       "115    115   9.353645   9.334750  0.756261     200       tanh  adagrad_005   \n",
       "60      60  37.328454  37.365725  0.543673     150       relu  adagrad_005   \n",
       "\n",
       "      lrate  \n",
       "166  0.0200  \n",
       "197  0.0050  \n",
       "190  0.0500  \n",
       "184  0.0005  \n",
       "154  0.0050  \n",
       "121  0.0200  \n",
       "62   0.0500  \n",
       "60   0.0500  \n",
       "115  0.0500  \n",
       "60   0.0500  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "relu_sl_lr_df = pd.read_pickle(\"OutputData/single_layer_relu_lr_df.pkl\")\n",
    "relu_sl_lr_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2c1428d5454edd80856b531a9ca261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam_005', 'adam_0005', 'adagrad_02', 'adagr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the relu learning rate data\n",
    "interact_manual(plot_validation_loss, df=fixed(relu_sl_lr_df), \n",
    "                optimizer = relu_sl_lr_df.optimizer.unique(), \n",
    "                    activation = relu_sl_lr_df.activation.unique())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the dataframes to get a comprehensive look at the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_bl_sl_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "tl_df = pd.read_pickle(\"OutputData/two_layer_relu_df.pkll\")\n",
    "relu_sl_lr_df.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
