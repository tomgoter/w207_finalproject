{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Analysis Notebook\n",
    "## W207 Final Project\n",
    "### T. P. Goter\n",
    "### July 6, 2019\n",
    "\n",
    "This workbook is used to assess various models created as part of the Facial Keypoint Detection project for W207."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we need\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, fixed\n",
    "import os\n",
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook, push_notebook\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.models import HoverTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "bl_sl_df = pd.read_pickle(\"OutputData/single_layer_df.pkl\")\n",
    "bl_sl_df['cum_times'] = bl_sl_df.groupby(['hunits', 'activation', 'optimizer', 'lrate']).times.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "      <th>cum_times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28010.575914</td>\n",
       "      <td>28010.572266</td>\n",
       "      <td>2291.178069</td>\n",
       "      <td>2291.178223</td>\n",
       "      <td>0</td>\n",
       "      <td>167.363593</td>\n",
       "      <td>47.866253</td>\n",
       "      <td>0.265849</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.265849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1619.893235</td>\n",
       "      <td>1619.893311</td>\n",
       "      <td>1050.184416</td>\n",
       "      <td>1050.184448</td>\n",
       "      <td>1</td>\n",
       "      <td>40.247898</td>\n",
       "      <td>32.406549</td>\n",
       "      <td>0.166498</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.432347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.095874</td>\n",
       "      <td>731.095886</td>\n",
       "      <td>472.246410</td>\n",
       "      <td>472.246399</td>\n",
       "      <td>2</td>\n",
       "      <td>27.038785</td>\n",
       "      <td>21.731231</td>\n",
       "      <td>0.166131</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.598478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>330.953334</td>\n",
       "      <td>330.953400</td>\n",
       "      <td>215.345170</td>\n",
       "      <td>215.345169</td>\n",
       "      <td>3</td>\n",
       "      <td>18.192125</td>\n",
       "      <td>14.674644</td>\n",
       "      <td>0.165214</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.763692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152.905530</td>\n",
       "      <td>152.905533</td>\n",
       "      <td>101.042233</td>\n",
       "      <td>101.042221</td>\n",
       "      <td>4</td>\n",
       "      <td>12.365498</td>\n",
       "      <td>10.051976</td>\n",
       "      <td>0.192475</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.956167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>73.695376</td>\n",
       "      <td>73.695374</td>\n",
       "      <td>50.227416</td>\n",
       "      <td>50.227413</td>\n",
       "      <td>5</td>\n",
       "      <td>8.584601</td>\n",
       "      <td>7.087130</td>\n",
       "      <td>0.173594</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.129761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38.447532</td>\n",
       "      <td>38.447533</td>\n",
       "      <td>27.734740</td>\n",
       "      <td>27.734743</td>\n",
       "      <td>6</td>\n",
       "      <td>6.200607</td>\n",
       "      <td>5.266379</td>\n",
       "      <td>0.179937</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.309698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22.766007</td>\n",
       "      <td>22.766003</td>\n",
       "      <td>17.744032</td>\n",
       "      <td>17.744032</td>\n",
       "      <td>7</td>\n",
       "      <td>4.771373</td>\n",
       "      <td>4.212367</td>\n",
       "      <td>0.197338</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.507036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15.785918</td>\n",
       "      <td>15.785920</td>\n",
       "      <td>13.324637</td>\n",
       "      <td>13.324637</td>\n",
       "      <td>8</td>\n",
       "      <td>3.973150</td>\n",
       "      <td>3.650293</td>\n",
       "      <td>0.196654</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.703690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.676719</td>\n",
       "      <td>12.676719</td>\n",
       "      <td>11.381038</td>\n",
       "      <td>11.381038</td>\n",
       "      <td>9</td>\n",
       "      <td>3.560438</td>\n",
       "      <td>3.373579</td>\n",
       "      <td>0.197424</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.901114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.295556</td>\n",
       "      <td>11.295556</td>\n",
       "      <td>10.524236</td>\n",
       "      <td>10.524236</td>\n",
       "      <td>10</td>\n",
       "      <td>3.360886</td>\n",
       "      <td>3.244108</td>\n",
       "      <td>0.195943</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.097057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.680425</td>\n",
       "      <td>10.680424</td>\n",
       "      <td>10.151030</td>\n",
       "      <td>10.151030</td>\n",
       "      <td>11</td>\n",
       "      <td>3.268092</td>\n",
       "      <td>3.186068</td>\n",
       "      <td>0.194626</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.291683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.407656</td>\n",
       "      <td>10.407654</td>\n",
       "      <td>9.994275</td>\n",
       "      <td>9.994275</td>\n",
       "      <td>12</td>\n",
       "      <td>3.226090</td>\n",
       "      <td>3.161372</td>\n",
       "      <td>0.191835</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.483518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.285179</td>\n",
       "      <td>10.285176</td>\n",
       "      <td>9.926514</td>\n",
       "      <td>9.926513</td>\n",
       "      <td>13</td>\n",
       "      <td>3.207051</td>\n",
       "      <td>3.150637</td>\n",
       "      <td>0.190176</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.673694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.232328</td>\n",
       "      <td>10.232327</td>\n",
       "      <td>9.897226</td>\n",
       "      <td>9.897226</td>\n",
       "      <td>14</td>\n",
       "      <td>3.198801</td>\n",
       "      <td>3.145986</td>\n",
       "      <td>0.197212</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.870906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.208601</td>\n",
       "      <td>10.208599</td>\n",
       "      <td>9.886523</td>\n",
       "      <td>9.886524</td>\n",
       "      <td>15</td>\n",
       "      <td>3.195090</td>\n",
       "      <td>3.144284</td>\n",
       "      <td>0.195675</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.066581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.197788</td>\n",
       "      <td>10.197786</td>\n",
       "      <td>9.883666</td>\n",
       "      <td>9.883667</td>\n",
       "      <td>16</td>\n",
       "      <td>3.193397</td>\n",
       "      <td>3.143830</td>\n",
       "      <td>0.198677</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.265258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.193322</td>\n",
       "      <td>10.193323</td>\n",
       "      <td>9.881729</td>\n",
       "      <td>9.881729</td>\n",
       "      <td>17</td>\n",
       "      <td>3.192698</td>\n",
       "      <td>3.143522</td>\n",
       "      <td>0.190144</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.455402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.190890</td>\n",
       "      <td>10.190888</td>\n",
       "      <td>9.882380</td>\n",
       "      <td>9.882380</td>\n",
       "      <td>18</td>\n",
       "      <td>3.192317</td>\n",
       "      <td>3.143625</td>\n",
       "      <td>0.192386</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.647788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.189954</td>\n",
       "      <td>10.189955</td>\n",
       "      <td>9.882910</td>\n",
       "      <td>9.882910</td>\n",
       "      <td>19</td>\n",
       "      <td>3.192171</td>\n",
       "      <td>3.143710</td>\n",
       "      <td>0.189246</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.837034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.189060</td>\n",
       "      <td>10.189058</td>\n",
       "      <td>9.883073</td>\n",
       "      <td>9.883073</td>\n",
       "      <td>20</td>\n",
       "      <td>3.192030</td>\n",
       "      <td>3.143735</td>\n",
       "      <td>0.197904</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.034938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.189136</td>\n",
       "      <td>10.189137</td>\n",
       "      <td>9.883344</td>\n",
       "      <td>9.883344</td>\n",
       "      <td>21</td>\n",
       "      <td>3.192043</td>\n",
       "      <td>3.143779</td>\n",
       "      <td>0.193161</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.228099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.188972</td>\n",
       "      <td>10.188971</td>\n",
       "      <td>9.882991</td>\n",
       "      <td>9.882991</td>\n",
       "      <td>22</td>\n",
       "      <td>3.192017</td>\n",
       "      <td>3.143722</td>\n",
       "      <td>0.190955</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.419054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.190036</td>\n",
       "      <td>10.190036</td>\n",
       "      <td>9.883156</td>\n",
       "      <td>9.883156</td>\n",
       "      <td>23</td>\n",
       "      <td>3.192184</td>\n",
       "      <td>3.143749</td>\n",
       "      <td>0.196169</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.615223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.189199</td>\n",
       "      <td>10.189199</td>\n",
       "      <td>9.883128</td>\n",
       "      <td>9.883126</td>\n",
       "      <td>24</td>\n",
       "      <td>3.192053</td>\n",
       "      <td>3.143744</td>\n",
       "      <td>0.192374</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.807597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.189037</td>\n",
       "      <td>10.189037</td>\n",
       "      <td>9.883755</td>\n",
       "      <td>9.883755</td>\n",
       "      <td>25</td>\n",
       "      <td>3.192027</td>\n",
       "      <td>3.143844</td>\n",
       "      <td>0.192280</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.999877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.189171</td>\n",
       "      <td>10.189171</td>\n",
       "      <td>9.883424</td>\n",
       "      <td>9.883424</td>\n",
       "      <td>26</td>\n",
       "      <td>3.192048</td>\n",
       "      <td>3.143791</td>\n",
       "      <td>0.190844</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.190721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10.189639</td>\n",
       "      <td>10.189639</td>\n",
       "      <td>9.883240</td>\n",
       "      <td>9.883240</td>\n",
       "      <td>27</td>\n",
       "      <td>3.192121</td>\n",
       "      <td>3.143762</td>\n",
       "      <td>0.190339</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.381060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.189099</td>\n",
       "      <td>10.189096</td>\n",
       "      <td>9.883447</td>\n",
       "      <td>9.883448</td>\n",
       "      <td>28</td>\n",
       "      <td>3.192036</td>\n",
       "      <td>3.143795</td>\n",
       "      <td>0.191548</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.572608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.188727</td>\n",
       "      <td>10.188728</td>\n",
       "      <td>9.883504</td>\n",
       "      <td>9.883505</td>\n",
       "      <td>29</td>\n",
       "      <td>3.191979</td>\n",
       "      <td>3.143804</td>\n",
       "      <td>0.193201</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5.765809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>10.346942</td>\n",
       "      <td>10.346941</td>\n",
       "      <td>9.920106</td>\n",
       "      <td>9.920107</td>\n",
       "      <td>370</td>\n",
       "      <td>3.216666</td>\n",
       "      <td>3.149620</td>\n",
       "      <td>0.395988</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>151.741212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>10.313827</td>\n",
       "      <td>10.313828</td>\n",
       "      <td>10.002073</td>\n",
       "      <td>10.002071</td>\n",
       "      <td>371</td>\n",
       "      <td>3.211515</td>\n",
       "      <td>3.162605</td>\n",
       "      <td>0.397263</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>152.138475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>10.306898</td>\n",
       "      <td>10.306899</td>\n",
       "      <td>9.967006</td>\n",
       "      <td>9.967007</td>\n",
       "      <td>372</td>\n",
       "      <td>3.210436</td>\n",
       "      <td>3.157057</td>\n",
       "      <td>0.397621</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>152.536096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>10.310567</td>\n",
       "      <td>10.310567</td>\n",
       "      <td>10.082919</td>\n",
       "      <td>10.082919</td>\n",
       "      <td>373</td>\n",
       "      <td>3.211007</td>\n",
       "      <td>3.175361</td>\n",
       "      <td>0.396995</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>152.933091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>10.309304</td>\n",
       "      <td>10.309303</td>\n",
       "      <td>10.013269</td>\n",
       "      <td>10.013270</td>\n",
       "      <td>374</td>\n",
       "      <td>3.210810</td>\n",
       "      <td>3.164375</td>\n",
       "      <td>0.395976</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>153.329067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>10.308249</td>\n",
       "      <td>10.308249</td>\n",
       "      <td>9.979765</td>\n",
       "      <td>9.979766</td>\n",
       "      <td>375</td>\n",
       "      <td>3.210646</td>\n",
       "      <td>3.159077</td>\n",
       "      <td>0.395793</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>153.724860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>10.316122</td>\n",
       "      <td>10.316121</td>\n",
       "      <td>9.984764</td>\n",
       "      <td>9.984764</td>\n",
       "      <td>376</td>\n",
       "      <td>3.211872</td>\n",
       "      <td>3.159868</td>\n",
       "      <td>0.397340</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>154.122200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>10.305968</td>\n",
       "      <td>10.305968</td>\n",
       "      <td>10.145667</td>\n",
       "      <td>10.145666</td>\n",
       "      <td>377</td>\n",
       "      <td>3.210291</td>\n",
       "      <td>3.185226</td>\n",
       "      <td>0.395974</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>154.518174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>10.319811</td>\n",
       "      <td>10.319810</td>\n",
       "      <td>10.059600</td>\n",
       "      <td>10.059599</td>\n",
       "      <td>378</td>\n",
       "      <td>3.212446</td>\n",
       "      <td>3.171687</td>\n",
       "      <td>0.398185</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>154.916359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>10.324991</td>\n",
       "      <td>10.324991</td>\n",
       "      <td>10.008092</td>\n",
       "      <td>10.008091</td>\n",
       "      <td>379</td>\n",
       "      <td>3.213252</td>\n",
       "      <td>3.163557</td>\n",
       "      <td>0.396807</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>155.313166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>10.316302</td>\n",
       "      <td>10.316301</td>\n",
       "      <td>10.274332</td>\n",
       "      <td>10.274332</td>\n",
       "      <td>380</td>\n",
       "      <td>3.211900</td>\n",
       "      <td>3.205360</td>\n",
       "      <td>0.395872</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>155.709038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>10.355930</td>\n",
       "      <td>10.355928</td>\n",
       "      <td>10.035330</td>\n",
       "      <td>10.035331</td>\n",
       "      <td>381</td>\n",
       "      <td>3.218063</td>\n",
       "      <td>3.167859</td>\n",
       "      <td>0.395187</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>156.104225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>10.322531</td>\n",
       "      <td>10.322530</td>\n",
       "      <td>9.982851</td>\n",
       "      <td>9.982849</td>\n",
       "      <td>382</td>\n",
       "      <td>3.212869</td>\n",
       "      <td>3.159565</td>\n",
       "      <td>0.397494</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>156.501719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>10.300704</td>\n",
       "      <td>10.300705</td>\n",
       "      <td>9.972132</td>\n",
       "      <td>9.972132</td>\n",
       "      <td>383</td>\n",
       "      <td>3.209471</td>\n",
       "      <td>3.157868</td>\n",
       "      <td>0.396135</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>156.897854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>10.356612</td>\n",
       "      <td>10.356612</td>\n",
       "      <td>9.964415</td>\n",
       "      <td>9.964416</td>\n",
       "      <td>384</td>\n",
       "      <td>3.218169</td>\n",
       "      <td>3.156646</td>\n",
       "      <td>0.397479</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>157.295333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>10.315353</td>\n",
       "      <td>10.315351</td>\n",
       "      <td>9.969181</td>\n",
       "      <td>9.969180</td>\n",
       "      <td>385</td>\n",
       "      <td>3.211752</td>\n",
       "      <td>3.157401</td>\n",
       "      <td>0.394701</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>157.690034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>10.334829</td>\n",
       "      <td>10.334830</td>\n",
       "      <td>9.952611</td>\n",
       "      <td>9.952612</td>\n",
       "      <td>386</td>\n",
       "      <td>3.214783</td>\n",
       "      <td>3.154776</td>\n",
       "      <td>0.396619</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>158.086653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>10.323140</td>\n",
       "      <td>10.323138</td>\n",
       "      <td>9.920694</td>\n",
       "      <td>9.920694</td>\n",
       "      <td>387</td>\n",
       "      <td>3.212964</td>\n",
       "      <td>3.149713</td>\n",
       "      <td>0.395752</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>158.482405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>10.299123</td>\n",
       "      <td>10.299122</td>\n",
       "      <td>9.903626</td>\n",
       "      <td>9.903626</td>\n",
       "      <td>388</td>\n",
       "      <td>3.209224</td>\n",
       "      <td>3.147003</td>\n",
       "      <td>0.400494</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>158.882899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>10.339846</td>\n",
       "      <td>10.339847</td>\n",
       "      <td>9.966111</td>\n",
       "      <td>9.966110</td>\n",
       "      <td>389</td>\n",
       "      <td>3.215563</td>\n",
       "      <td>3.156915</td>\n",
       "      <td>0.395095</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>159.277994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>10.329108</td>\n",
       "      <td>10.329108</td>\n",
       "      <td>10.059111</td>\n",
       "      <td>10.059111</td>\n",
       "      <td>390</td>\n",
       "      <td>3.213893</td>\n",
       "      <td>3.171610</td>\n",
       "      <td>0.396951</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>159.674945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>10.297079</td>\n",
       "      <td>10.297081</td>\n",
       "      <td>9.971048</td>\n",
       "      <td>9.971048</td>\n",
       "      <td>391</td>\n",
       "      <td>3.208907</td>\n",
       "      <td>3.157697</td>\n",
       "      <td>0.396823</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>160.071768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>10.305806</td>\n",
       "      <td>10.305807</td>\n",
       "      <td>10.114078</td>\n",
       "      <td>10.114079</td>\n",
       "      <td>392</td>\n",
       "      <td>3.210266</td>\n",
       "      <td>3.180264</td>\n",
       "      <td>0.397766</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>160.469534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>10.326233</td>\n",
       "      <td>10.326232</td>\n",
       "      <td>10.078709</td>\n",
       "      <td>10.078709</td>\n",
       "      <td>393</td>\n",
       "      <td>3.213445</td>\n",
       "      <td>3.174698</td>\n",
       "      <td>0.397049</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>160.866583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>10.308622</td>\n",
       "      <td>10.308621</td>\n",
       "      <td>10.017620</td>\n",
       "      <td>10.017621</td>\n",
       "      <td>394</td>\n",
       "      <td>3.210704</td>\n",
       "      <td>3.165063</td>\n",
       "      <td>0.398645</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>161.265228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>10.311956</td>\n",
       "      <td>10.311956</td>\n",
       "      <td>10.031062</td>\n",
       "      <td>10.031063</td>\n",
       "      <td>395</td>\n",
       "      <td>3.211224</td>\n",
       "      <td>3.167185</td>\n",
       "      <td>0.396075</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>161.661303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>10.323963</td>\n",
       "      <td>10.323963</td>\n",
       "      <td>10.076065</td>\n",
       "      <td>10.076065</td>\n",
       "      <td>396</td>\n",
       "      <td>3.213092</td>\n",
       "      <td>3.174282</td>\n",
       "      <td>0.396886</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>162.058189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>10.344393</td>\n",
       "      <td>10.344391</td>\n",
       "      <td>9.960818</td>\n",
       "      <td>9.960818</td>\n",
       "      <td>397</td>\n",
       "      <td>3.216270</td>\n",
       "      <td>3.156076</td>\n",
       "      <td>0.395299</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>162.453488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>10.293114</td>\n",
       "      <td>10.293115</td>\n",
       "      <td>9.939336</td>\n",
       "      <td>9.939336</td>\n",
       "      <td>398</td>\n",
       "      <td>3.208288</td>\n",
       "      <td>3.152671</td>\n",
       "      <td>0.396431</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>162.849919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>10.302657</td>\n",
       "      <td>10.302656</td>\n",
       "      <td>9.948581</td>\n",
       "      <td>9.948582</td>\n",
       "      <td>399</td>\n",
       "      <td>3.209775</td>\n",
       "      <td>3.154137</td>\n",
       "      <td>0.396593</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>163.246512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "0    28010.575914        28010.572266  2291.178069             2291.178223   \n",
       "1     1619.893235         1619.893311  1050.184416             1050.184448   \n",
       "2      731.095874          731.095886   472.246410              472.246399   \n",
       "3      330.953334          330.953400   215.345170              215.345169   \n",
       "4      152.905530          152.905533   101.042233              101.042221   \n",
       "5       73.695376           73.695374    50.227416               50.227413   \n",
       "6       38.447532           38.447533    27.734740               27.734743   \n",
       "7       22.766007           22.766003    17.744032               17.744032   \n",
       "8       15.785918           15.785920    13.324637               13.324637   \n",
       "9       12.676719           12.676719    11.381038               11.381038   \n",
       "10      11.295556           11.295556    10.524236               10.524236   \n",
       "11      10.680425           10.680424    10.151030               10.151030   \n",
       "12      10.407656           10.407654     9.994275                9.994275   \n",
       "13      10.285179           10.285176     9.926514                9.926513   \n",
       "14      10.232328           10.232327     9.897226                9.897226   \n",
       "15      10.208601           10.208599     9.886523                9.886524   \n",
       "16      10.197788           10.197786     9.883666                9.883667   \n",
       "17      10.193322           10.193323     9.881729                9.881729   \n",
       "18      10.190890           10.190888     9.882380                9.882380   \n",
       "19      10.189954           10.189955     9.882910                9.882910   \n",
       "20      10.189060           10.189058     9.883073                9.883073   \n",
       "21      10.189136           10.189137     9.883344                9.883344   \n",
       "22      10.188972           10.188971     9.882991                9.882991   \n",
       "23      10.190036           10.190036     9.883156                9.883156   \n",
       "24      10.189199           10.189199     9.883128                9.883126   \n",
       "25      10.189037           10.189037     9.883755                9.883755   \n",
       "26      10.189171           10.189171     9.883424                9.883424   \n",
       "27      10.189639           10.189639     9.883240                9.883240   \n",
       "28      10.189099           10.189096     9.883447                9.883448   \n",
       "29      10.188727           10.188728     9.883504                9.883505   \n",
       "..            ...                 ...          ...                     ...   \n",
       "370     10.346942           10.346941     9.920106                9.920107   \n",
       "371     10.313827           10.313828    10.002073               10.002071   \n",
       "372     10.306898           10.306899     9.967006                9.967007   \n",
       "373     10.310567           10.310567    10.082919               10.082919   \n",
       "374     10.309304           10.309303    10.013269               10.013270   \n",
       "375     10.308249           10.308249     9.979765                9.979766   \n",
       "376     10.316122           10.316121     9.984764                9.984764   \n",
       "377     10.305968           10.305968    10.145667               10.145666   \n",
       "378     10.319811           10.319810    10.059600               10.059599   \n",
       "379     10.324991           10.324991    10.008092               10.008091   \n",
       "380     10.316302           10.316301    10.274332               10.274332   \n",
       "381     10.355930           10.355928    10.035330               10.035331   \n",
       "382     10.322531           10.322530     9.982851                9.982849   \n",
       "383     10.300704           10.300705     9.972132                9.972132   \n",
       "384     10.356612           10.356612     9.964415                9.964416   \n",
       "385     10.315353           10.315351     9.969181                9.969180   \n",
       "386     10.334829           10.334830     9.952611                9.952612   \n",
       "387     10.323140           10.323138     9.920694                9.920694   \n",
       "388     10.299123           10.299122     9.903626                9.903626   \n",
       "389     10.339846           10.339847     9.966111                9.966110   \n",
       "390     10.329108           10.329108    10.059111               10.059111   \n",
       "391     10.297079           10.297081     9.971048                9.971048   \n",
       "392     10.305806           10.305807    10.114078               10.114079   \n",
       "393     10.326233           10.326232    10.078709               10.078709   \n",
       "394     10.308622           10.308621    10.017620               10.017621   \n",
       "395     10.311956           10.311956    10.031062               10.031063   \n",
       "396     10.323963           10.323963    10.076065               10.076065   \n",
       "397     10.344393           10.344391     9.960818                9.960818   \n",
       "398     10.293114           10.293115     9.939336                9.939336   \n",
       "399     10.302657           10.302656     9.948581                9.948582   \n",
       "\n",
       "     epoch        RMSE   val_RMSE     times  hunits activation optimizer  \\\n",
       "0        0  167.363593  47.866253  0.265849      50       relu       sgd   \n",
       "1        1   40.247898  32.406549  0.166498      50       relu       sgd   \n",
       "2        2   27.038785  21.731231  0.166131      50       relu       sgd   \n",
       "3        3   18.192125  14.674644  0.165214      50       relu       sgd   \n",
       "4        4   12.365498  10.051976  0.192475      50       relu       sgd   \n",
       "5        5    8.584601   7.087130  0.173594      50       relu       sgd   \n",
       "6        6    6.200607   5.266379  0.179937      50       relu       sgd   \n",
       "7        7    4.771373   4.212367  0.197338      50       relu       sgd   \n",
       "8        8    3.973150   3.650293  0.196654      50       relu       sgd   \n",
       "9        9    3.560438   3.373579  0.197424      50       relu       sgd   \n",
       "10      10    3.360886   3.244108  0.195943      50       relu       sgd   \n",
       "11      11    3.268092   3.186068  0.194626      50       relu       sgd   \n",
       "12      12    3.226090   3.161372  0.191835      50       relu       sgd   \n",
       "13      13    3.207051   3.150637  0.190176      50       relu       sgd   \n",
       "14      14    3.198801   3.145986  0.197212      50       relu       sgd   \n",
       "15      15    3.195090   3.144284  0.195675      50       relu       sgd   \n",
       "16      16    3.193397   3.143830  0.198677      50       relu       sgd   \n",
       "17      17    3.192698   3.143522  0.190144      50       relu       sgd   \n",
       "18      18    3.192317   3.143625  0.192386      50       relu       sgd   \n",
       "19      19    3.192171   3.143710  0.189246      50       relu       sgd   \n",
       "20      20    3.192030   3.143735  0.197904      50       relu       sgd   \n",
       "21      21    3.192043   3.143779  0.193161      50       relu       sgd   \n",
       "22      22    3.192017   3.143722  0.190955      50       relu       sgd   \n",
       "23      23    3.192184   3.143749  0.196169      50       relu       sgd   \n",
       "24      24    3.192053   3.143744  0.192374      50       relu       sgd   \n",
       "25      25    3.192027   3.143844  0.192280      50       relu       sgd   \n",
       "26      26    3.192048   3.143791  0.190844      50       relu       sgd   \n",
       "27      27    3.192121   3.143762  0.190339      50       relu       sgd   \n",
       "28      28    3.192036   3.143795  0.191548      50       relu       sgd   \n",
       "29      29    3.191979   3.143804  0.193201      50       relu       sgd   \n",
       "..     ...         ...        ...       ...     ...        ...       ...   \n",
       "370    370    3.216666   3.149620  0.395988     200       tanh       sgd   \n",
       "371    371    3.211515   3.162605  0.397263     200       tanh       sgd   \n",
       "372    372    3.210436   3.157057  0.397621     200       tanh       sgd   \n",
       "373    373    3.211007   3.175361  0.396995     200       tanh       sgd   \n",
       "374    374    3.210810   3.164375  0.395976     200       tanh       sgd   \n",
       "375    375    3.210646   3.159077  0.395793     200       tanh       sgd   \n",
       "376    376    3.211872   3.159868  0.397340     200       tanh       sgd   \n",
       "377    377    3.210291   3.185226  0.395974     200       tanh       sgd   \n",
       "378    378    3.212446   3.171687  0.398185     200       tanh       sgd   \n",
       "379    379    3.213252   3.163557  0.396807     200       tanh       sgd   \n",
       "380    380    3.211900   3.205360  0.395872     200       tanh       sgd   \n",
       "381    381    3.218063   3.167859  0.395187     200       tanh       sgd   \n",
       "382    382    3.212869   3.159565  0.397494     200       tanh       sgd   \n",
       "383    383    3.209471   3.157868  0.396135     200       tanh       sgd   \n",
       "384    384    3.218169   3.156646  0.397479     200       tanh       sgd   \n",
       "385    385    3.211752   3.157401  0.394701     200       tanh       sgd   \n",
       "386    386    3.214783   3.154776  0.396619     200       tanh       sgd   \n",
       "387    387    3.212964   3.149713  0.395752     200       tanh       sgd   \n",
       "388    388    3.209224   3.147003  0.400494     200       tanh       sgd   \n",
       "389    389    3.215563   3.156915  0.395095     200       tanh       sgd   \n",
       "390    390    3.213893   3.171610  0.396951     200       tanh       sgd   \n",
       "391    391    3.208907   3.157697  0.396823     200       tanh       sgd   \n",
       "392    392    3.210266   3.180264  0.397766     200       tanh       sgd   \n",
       "393    393    3.213445   3.174698  0.397049     200       tanh       sgd   \n",
       "394    394    3.210704   3.165063  0.398645     200       tanh       sgd   \n",
       "395    395    3.211224   3.167185  0.396075     200       tanh       sgd   \n",
       "396    396    3.213092   3.174282  0.396886     200       tanh       sgd   \n",
       "397    397    3.216270   3.156076  0.395299     200       tanh       sgd   \n",
       "398    398    3.208288   3.152671  0.396431     200       tanh       sgd   \n",
       "399    399    3.209775   3.154137  0.396593     200       tanh       sgd   \n",
       "\n",
       "     lrate   cum_times  \n",
       "0     0.01    0.265849  \n",
       "1     0.01    0.432347  \n",
       "2     0.01    0.598478  \n",
       "3     0.01    0.763692  \n",
       "4     0.01    0.956167  \n",
       "5     0.01    1.129761  \n",
       "6     0.01    1.309698  \n",
       "7     0.01    1.507036  \n",
       "8     0.01    1.703690  \n",
       "9     0.01    1.901114  \n",
       "10    0.01    2.097057  \n",
       "11    0.01    2.291683  \n",
       "12    0.01    2.483518  \n",
       "13    0.01    2.673694  \n",
       "14    0.01    2.870906  \n",
       "15    0.01    3.066581  \n",
       "16    0.01    3.265258  \n",
       "17    0.01    3.455402  \n",
       "18    0.01    3.647788  \n",
       "19    0.01    3.837034  \n",
       "20    0.01    4.034938  \n",
       "21    0.01    4.228099  \n",
       "22    0.01    4.419054  \n",
       "23    0.01    4.615223  \n",
       "24    0.01    4.807597  \n",
       "25    0.01    4.999877  \n",
       "26    0.01    5.190721  \n",
       "27    0.01    5.381060  \n",
       "28    0.01    5.572608  \n",
       "29    0.01    5.765809  \n",
       "..     ...         ...  \n",
       "370   0.01  151.741212  \n",
       "371   0.01  152.138475  \n",
       "372   0.01  152.536096  \n",
       "373   0.01  152.933091  \n",
       "374   0.01  153.329067  \n",
       "375   0.01  153.724860  \n",
       "376   0.01  154.122200  \n",
       "377   0.01  154.518174  \n",
       "378   0.01  154.916359  \n",
       "379   0.01  155.313166  \n",
       "380   0.01  155.709038  \n",
       "381   0.01  156.104225  \n",
       "382   0.01  156.501719  \n",
       "383   0.01  156.897854  \n",
       "384   0.01  157.295333  \n",
       "385   0.01  157.690034  \n",
       "386   0.01  158.086653  \n",
       "387   0.01  158.482405  \n",
       "388   0.01  158.882899  \n",
       "389   0.01  159.277994  \n",
       "390   0.01  159.674945  \n",
       "391   0.01  160.071768  \n",
       "392   0.01  160.469534  \n",
       "393   0.01  160.866583  \n",
       "394   0.01  161.265228  \n",
       "395   0.01  161.661303  \n",
       "396   0.01  162.058189  \n",
       "397   0.01  162.453488  \n",
       "398   0.01  162.849919  \n",
       "399   0.01  163.246512  \n",
       "\n",
       "[4800 rows x 13 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_sl_df[bl_sl_df.optimizer == 'sgd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plotting function to pass to the interact widget function\n",
    "def plot_validation_loss(df=bl_sl_df, optimizer = bl_sl_df.optimizer.unique(), \n",
    "                    activation = bl_sl_df.activation.unique(), \n",
    "                         ymax=[y for y in range(0,100)],\n",
    "                        timemax=250):\n",
    "    \n",
    "    # Subset the baseline df by the specified optimizer and activation\n",
    "    sub_df = df[df.optimizer.str.match(optimizer)]\n",
    "    sub_df = sub_df[sub_df.activation.str.match(activation)]\n",
    "     \n",
    "    # Group the neural net data by optimizer and activation\n",
    "    groups = sub_df.groupby(['hunits'])\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over the grouped data and plot out epoch timing and validation loss data\n",
    "    for name, group in groups:\n",
    "        \n",
    "        # Plot training and validation losses by epoch\n",
    "        axes[0].plot(group.epoch, group.val_RMSE, label=str(name)+' Validation Loss')\n",
    "        axes[0].scatter(group.epoch, group.RMSE, label=str(name)+' Training Loss')\n",
    "        axes[0].set_ylim([0,ymax])\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Root Mean Square Error')\n",
    "        axes[0].set_title(\"{} Optimizer and {} Activation\".format(group.optimizer.unique(), group.activation.unique()))\n",
    "        \n",
    "        # Plot train time by epoch\n",
    "        axes[1].scatter(group.epoch, group.times*1000, label=str(name)+' Fit Time')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Fit Time (milliseconds)')\n",
    "        axes[1].set_ylim([0,1000])\n",
    "        axes[1].legend()\n",
    "        \n",
    "        # Plot cumulative training time\n",
    "        axes[2].plot(group.epoch, group.cum_times, label=str(name)+' Fit Time (s)', lw=4)\n",
    "        axes[2].set_xlabel('Epoch')\n",
    "        axes[2].set_ylabel('Cumulative Fit Time (seconds)')\n",
    "        axes[2].grid(b=True)\n",
    "        axes[2].set_ylim([0,timemax])\n",
    "        axes[2].legend()\n",
    "        \n",
    "        # Plot cumulative validation loss by cumulative time\n",
    "        axes[3].plot(group.cum_times, group.val_RMSE, label=str(name)+' Validation Loss', lw=3)\n",
    "        axes[3].set_xlabel('Cumulative Fit Time (seconds)')\n",
    "        axes[3].set_ylabel('Validation RMSE')\n",
    "        axes[3].set_xlim([0,timemax])\n",
    "        axes[3].set_ylim([0,ymax])\n",
    "        axes[3].legend()\n",
    "    \n",
    "    # Add line for knr score\n",
    "    axes[0].axhline(2.49, label='kNR Score', lw=5, c='k')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Adjust the spacing of the subplots\n",
    "    fig.subplots_adjust(left=0.03, right=0.97, hspace=0.1, wspace=0.15)\n",
    "\n",
    "    # Add an overarching title for these plots\n",
    "    fig.suptitle(\"Performance Comparison for Single Layer, Fully Connected Neural Nets\",\n",
    "                 fontsize=18, y=0.93)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a385304b9e452ab261b67ad1867659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd', 'nadam', 'adagrad'), value='ad…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interact_manual(plot_validation_loss, df=fixed(bl_sl_df), \n",
    "                optimizer = bl_sl_df.optimizer.unique(), \n",
    "                    activation = bl_sl_df.activation.unique(), ymax=10, timemax=250)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment of Baseline Results\n",
    "\n",
    "Now that we have some real results, we can make some real assessments of what is working and what is not.\n",
    "\n",
    "1. Overall the sgd optimizer seems to be working the best. There is not a large difference between training and validation accuracies. In other words we aren't overfitting to the data.\n",
    "2. SGD is also faster than the other optimizers. It is relatively close to Adam. NAdam on the other hand is twice as slow.\n",
    "3. No neural net does better than our kNR model\n",
    "\n",
    "In the evaluation above, both the output layer used a linear activation function while the hidden layer used a user specified optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer Assessment\n",
    "Based on the three different experiments run, we will use a linear final layer activation function. We will continue to assess sgd and adam optimizers. We likely do not need to train for more than 200 epochs or so to get reasonably converged nets.\n",
    "\n",
    "The study below was conducted to determine the effects of learning rate on convergence and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "      <th>cum_times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>4.217018</td>\n",
       "      <td>4.217018</td>\n",
       "      <td>33.002135</td>\n",
       "      <td>33.002140</td>\n",
       "      <td>391</td>\n",
       "      <td>2.053538</td>\n",
       "      <td>5.744749</td>\n",
       "      <td>0.954722</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>367.614386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>34.569408</td>\n",
       "      <td>34.569401</td>\n",
       "      <td>54.616973</td>\n",
       "      <td>54.616970</td>\n",
       "      <td>79</td>\n",
       "      <td>5.879575</td>\n",
       "      <td>7.390329</td>\n",
       "      <td>0.226374</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>17.775728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>86.400589</td>\n",
       "      <td>86.400597</td>\n",
       "      <td>99.930309</td>\n",
       "      <td>99.930313</td>\n",
       "      <td>6</td>\n",
       "      <td>9.295192</td>\n",
       "      <td>9.996515</td>\n",
       "      <td>0.530176</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.962211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>33.866867</td>\n",
       "      <td>33.866867</td>\n",
       "      <td>52.360566</td>\n",
       "      <td>52.360569</td>\n",
       "      <td>44</td>\n",
       "      <td>5.819525</td>\n",
       "      <td>7.236060</td>\n",
       "      <td>0.384534</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>17.735224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>55.886526</td>\n",
       "      <td>55.886520</td>\n",
       "      <td>72.869890</td>\n",
       "      <td>72.869888</td>\n",
       "      <td>13</td>\n",
       "      <td>7.475729</td>\n",
       "      <td>8.536386</td>\n",
       "      <td>0.707683</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>10.232548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>10.189049</td>\n",
       "      <td>10.189051</td>\n",
       "      <td>9.883392</td>\n",
       "      <td>9.883392</td>\n",
       "      <td>282</td>\n",
       "      <td>3.192029</td>\n",
       "      <td>3.143786</td>\n",
       "      <td>0.322250</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>98.417383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>2.924278</td>\n",
       "      <td>2.924278</td>\n",
       "      <td>31.046858</td>\n",
       "      <td>31.046856</td>\n",
       "      <td>353</td>\n",
       "      <td>1.710052</td>\n",
       "      <td>5.571971</td>\n",
       "      <td>0.398027</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>135.740402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>34.191320</td>\n",
       "      <td>34.191319</td>\n",
       "      <td>55.489284</td>\n",
       "      <td>55.489285</td>\n",
       "      <td>329</td>\n",
       "      <td>5.847334</td>\n",
       "      <td>7.449113</td>\n",
       "      <td>0.285192</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>adagrad</td>\n",
       "      <td>0.010</td>\n",
       "      <td>89.823591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>4.435796</td>\n",
       "      <td>4.435796</td>\n",
       "      <td>36.281730</td>\n",
       "      <td>36.281727</td>\n",
       "      <td>186</td>\n",
       "      <td>2.106133</td>\n",
       "      <td>6.023431</td>\n",
       "      <td>0.434601</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>89.114765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>4.846084</td>\n",
       "      <td>4.846084</td>\n",
       "      <td>39.477733</td>\n",
       "      <td>39.477730</td>\n",
       "      <td>328</td>\n",
       "      <td>2.201382</td>\n",
       "      <td>6.283131</td>\n",
       "      <td>0.408713</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>nadam</td>\n",
       "      <td>0.002</td>\n",
       "      <td>130.862197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  mean_squared_error   val_loss  val_mean_squared_error  epoch  \\\n",
       "391   4.217018            4.217018  33.002135               33.002140    391   \n",
       "79   34.569408           34.569401  54.616973               54.616970     79   \n",
       "6    86.400589           86.400597  99.930309               99.930313      6   \n",
       "44   33.866867           33.866867  52.360566               52.360569     44   \n",
       "13   55.886526           55.886520  72.869890               72.869888     13   \n",
       "282  10.189049           10.189051   9.883392                9.883392    282   \n",
       "353   2.924278            2.924278  31.046858               31.046856    353   \n",
       "329  34.191320           34.191319  55.489284               55.489285    329   \n",
       "186   4.435796            4.435796  36.281730               36.281727    186   \n",
       "328   4.846084            4.846084  39.477733               39.477730    328   \n",
       "\n",
       "         RMSE  val_RMSE     times  hunits activation optimizer  lrate  \\\n",
       "391  2.053538  5.744749  0.954722     150       relu     nadam  0.002   \n",
       "79   5.879575  7.390329  0.226374      50       relu      adam  0.001   \n",
       "6    9.295192  9.996515  0.530176     200       relu      adam  0.001   \n",
       "44   5.819525  7.236060  0.384534     150       relu      adam  0.001   \n",
       "13   7.475729  8.536386  0.707683     100       relu     nadam  0.002   \n",
       "282  3.192029  3.143786  0.322250     150       relu       sgd  0.010   \n",
       "353  1.710052  5.571971  0.398027     150       relu      adam  0.001   \n",
       "329  5.847334  7.449113  0.285192      50       relu   adagrad  0.010   \n",
       "186  2.106133  6.023431  0.434601     200       relu      adam  0.001   \n",
       "328  2.201382  6.283131  0.408713      50       relu     nadam  0.002   \n",
       "\n",
       "      cum_times  \n",
       "391  367.614386  \n",
       "79    17.775728  \n",
       "6      3.962211  \n",
       "44    17.735224  \n",
       "13    10.232548  \n",
       "282   98.417383  \n",
       "353  135.740402  \n",
       "329   89.823591  \n",
       "186   89.114765  \n",
       "328  130.862197  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "relu_sl_lr_df = pd.read_pickle(\"OutputData/single_layer_lr_df.pkl\")\n",
    "relu_sl_lr_df['cum_times'] = relu_sl_lr_df.groupby(['hunits', 'activation', 'optimizer', 'lrate']).times.cumsum()\n",
    "relu_sl_lr_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3defe9882932419798c8af707e678c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd', 'nadam', 'adagrad'), value='ad…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the relu learning rate data\n",
    "interact_manual(plot_validation_loss, df=fixed(relu_sl_lr_df), \n",
    "                optimizer = relu_sl_lr_df.optimizer.unique(), \n",
    "                    activation = relu_sl_lr_df.activation.unique())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Layer Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>hunits</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "      <th>cum_times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.410201</td>\n",
       "      <td>50</td>\n",
       "      <td>elu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>26.104618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.417943</td>\n",
       "      <td>100</td>\n",
       "      <td>elu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>25.272020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.410311</td>\n",
       "      <td>50</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>85.895397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.401661</td>\n",
       "      <td>50</td>\n",
       "      <td>elu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>34.426956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1.210054</td>\n",
       "      <td>1.210054</td>\n",
       "      <td>4.361899</td>\n",
       "      <td>4.361898</td>\n",
       "      <td>280</td>\n",
       "      <td>1.100025</td>\n",
       "      <td>2.088516</td>\n",
       "      <td>0.453382</td>\n",
       "      <td>50</td>\n",
       "      <td>elu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>141.840752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>2.045783</td>\n",
       "      <td>2.045783</td>\n",
       "      <td>27.729059</td>\n",
       "      <td>27.729061</td>\n",
       "      <td>291</td>\n",
       "      <td>1.430309</td>\n",
       "      <td>5.265839</td>\n",
       "      <td>0.483608</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>144.433593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.485357</td>\n",
       "      <td>100</td>\n",
       "      <td>elu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>126.208189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.410233</td>\n",
       "      <td>150</td>\n",
       "      <td>elu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>26.592160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>3.388154</td>\n",
       "      <td>3.388154</td>\n",
       "      <td>29.360584</td>\n",
       "      <td>29.360586</td>\n",
       "      <td>270</td>\n",
       "      <td>1.840694</td>\n",
       "      <td>5.418541</td>\n",
       "      <td>0.485379</td>\n",
       "      <td>100</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>134.093936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.427797</td>\n",
       "      <td>150</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.299155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  mean_squared_error   val_loss  val_mean_squared_error  epoch  \\\n",
       "59        NaN                 NaN        NaN                     NaN     59   \n",
       "56        NaN                 NaN        NaN                     NaN     56   \n",
       "204       NaN                 NaN        NaN                     NaN    204   \n",
       "79        NaN                 NaN        NaN                     NaN     79   \n",
       "280  1.210054            1.210054   4.361899                4.361898    280   \n",
       "291  2.045783            2.045783  27.729059               27.729061    291   \n",
       "289       NaN                 NaN        NaN                     NaN    289   \n",
       "64        NaN                 NaN        NaN                     NaN     64   \n",
       "270  3.388154            3.388154  29.360584               29.360586    270   \n",
       "20        NaN                 NaN        NaN                     NaN     20   \n",
       "\n",
       "         RMSE  val_RMSE     times  hunits activation optimizer  lrate  \\\n",
       "59        NaN       NaN  0.410201      50        elu       sgd  0.010   \n",
       "56        NaN       NaN  0.417943     100        elu       sgd  0.010   \n",
       "204       NaN       NaN  0.410311      50       relu       sgd  0.010   \n",
       "79        NaN       NaN  0.401661      50        elu       sgd  0.010   \n",
       "280  1.100025  2.088516  0.453382      50        elu      adam  0.001   \n",
       "291  1.430309  5.265839  0.483608     100       relu      adam  0.001   \n",
       "289       NaN       NaN  0.485357     100        elu       sgd  0.010   \n",
       "64        NaN       NaN  0.410233     150        elu       sgd  0.010   \n",
       "270  1.840694  5.418541  0.485379     100       relu      adam  0.001   \n",
       "20        NaN       NaN  0.427797     150       relu       sgd  0.010   \n",
       "\n",
       "      cum_times  \n",
       "59    26.104618  \n",
       "56    25.272020  \n",
       "204   85.895397  \n",
       "79    34.426956  \n",
       "280  141.840752  \n",
       "291  144.433593  \n",
       "289  126.208189  \n",
       "64    26.592160  \n",
       "270  134.093936  \n",
       "20     9.299155  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pkled dataframe for the baseline single layer neural net\n",
    "tl_df = pd.read_pickle(\"OutputData/two_layer_df.pkl\")\n",
    "tl_df['cum_times'] = tl_df.groupby(['hunits', 'activation', 'optimizer', 'lrate']).times.cumsum()\n",
    "tl_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f01aeeb1c134eff91e9c5ea9a485872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd'), value='adam'), Dropdown(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the relu learning rate data\n",
    "interact_manual(plot_validation_loss, df=fixed(tl_df), \n",
    "                optimizer = tl_df.optimizer.unique(), \n",
    "                    activation = tl_df.activation.unique(),\n",
    "               ymax=10,\n",
    "               timemax=250)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Data below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline CNN\n",
    "\n",
    "The baseline CNN was structured following Nuori blog on this Facial Keypoint Detection topic. It uses a CNN architecture of the following:\n",
    "\n",
    "1. Input Layer\n",
    "2. Convolution Layer with valid padding and add N filters\n",
    "3. Max Pooling with 2,2 kernel\n",
    "4. Repeat Steps 2 and 3, doubling the number of filters each time you convolve\n",
    "5. After three full layers - flatten and condense to 500 units\n",
    "6. 500 --> 500 fully connected layer\n",
    "7. Linear activation to 30 output nodes\n",
    "\n",
    "For our Baseline CNN we have chosen to use the relu activation and not use dropout layers. We train only on the non-nan data. And we have identified many follow on studies including:\n",
    "\n",
    "\n",
    "1. *Model for each keypoint* - Some keypoints have a lot more training data than others. We can maximize the use of our training data by building separate specialized models for each keypoint.\n",
    "2. *Transform images* - We can also expand our training data by applying transformations to our training images. We can do things like flipping on the x and y axes or rotating our images. We now will have a different feature matrix. The trick will be to also transform our keypoint locations appropriately.\n",
    "3. *Filters* - Investigate different filter strategies - the 32, 64, 128 strategy employed is expensive. Can we reduce the number of filters and still get good results\n",
    "4. *Drop Out* - Drop out layers can act as a means of regularization - If we notice overfitting we can try this as a correction mechanism\n",
    "5. *Stride v Pool* - Evaluate time and accuracy differences between pooling and using a stride\n",
    "6. *TPU Usage* - Identify when and how to best utilize TPU resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle('OutputData/cnn_vgg_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>epoch</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>times</th>\n",
       "      <th>starting_filter</th>\n",
       "      <th>layers</th>\n",
       "      <th>pooling</th>\n",
       "      <th>fc_layer</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lrate</th>\n",
       "      <th>dropout_initial</th>\n",
       "      <th>dropout_step</th>\n",
       "      <th>batch_norm</th>\n",
       "      <th>bias</th>\n",
       "      <th>arch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014.237548</td>\n",
       "      <td>2014.237671</td>\n",
       "      <td>1966.873270</td>\n",
       "      <td>1966.873413</td>\n",
       "      <td>0</td>\n",
       "      <td>44.880259</td>\n",
       "      <td>44.349447</td>\n",
       "      <td>4.343745</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>500</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>vgg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>944.676497</td>\n",
       "      <td>944.676331</td>\n",
       "      <td>1121.239891</td>\n",
       "      <td>1121.239868</td>\n",
       "      <td>1</td>\n",
       "      <td>30.735587</td>\n",
       "      <td>33.484920</td>\n",
       "      <td>1.400704</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>500</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>vgg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>330.445365</td>\n",
       "      <td>330.445312</td>\n",
       "      <td>331.134516</td>\n",
       "      <td>331.134521</td>\n",
       "      <td>2</td>\n",
       "      <td>18.178155</td>\n",
       "      <td>18.197102</td>\n",
       "      <td>1.425586</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>500</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>vgg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105.024883</td>\n",
       "      <td>105.024895</td>\n",
       "      <td>288.332089</td>\n",
       "      <td>288.332062</td>\n",
       "      <td>3</td>\n",
       "      <td>10.248165</td>\n",
       "      <td>16.980343</td>\n",
       "      <td>1.410569</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>500</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>vgg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.974663</td>\n",
       "      <td>43.974663</td>\n",
       "      <td>37.349535</td>\n",
       "      <td>37.349537</td>\n",
       "      <td>4</td>\n",
       "      <td>6.631339</td>\n",
       "      <td>6.111427</td>\n",
       "      <td>1.403307</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>500</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>vgg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  mean_squared_error     val_loss  val_mean_squared_error  \\\n",
       "0  2014.237548         2014.237671  1966.873270             1966.873413   \n",
       "1   944.676497          944.676331  1121.239891             1121.239868   \n",
       "2   330.445365          330.445312   331.134516              331.134521   \n",
       "3   105.024883          105.024895   288.332089              288.332062   \n",
       "4    43.974663           43.974663    37.349535               37.349537   \n",
       "\n",
       "   epoch       RMSE   val_RMSE     times  starting_filter  layers pooling  \\\n",
       "0      0  44.880259  44.349447  4.343745               16       4     yes   \n",
       "1      1  30.735587  33.484920  1.400704               16       4     yes   \n",
       "2      2  18.178155  18.197102  1.425586               16       4     yes   \n",
       "3      3  10.248165  16.980343  1.410569               16       4     yes   \n",
       "4      4   6.631339   6.111427  1.403307               16       4     yes   \n",
       "\n",
       "   fc_layer activation optimizer  lrate  dropout_initial  dropout_step  \\\n",
       "0       500       relu      adam  0.001              0.0           0.0   \n",
       "1       500       relu      adam  0.001              0.0           0.0   \n",
       "2       500       relu      adam  0.001              0.0           0.0   \n",
       "3       500       relu      adam  0.001              0.0           0.0   \n",
       "4       500       relu      adam  0.001              0.0           0.0   \n",
       "\n",
       "   batch_norm  bias arch  \n",
       "0           1     0  vgg  \n",
       "1           1     0  vgg  \n",
       "2           1     0  vgg  \n",
       "3           1     0  vgg  \n",
       "4           1     0  vgg  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     arch optimizer  dropout_initial  dropout_step  starting_filter  bias  \\\n",
      "0    alex   adagrad             0.00          0.00               32     1   \n",
      "1    alex      adam             0.00          0.00                3     1   \n",
      "2    alex      adam             0.00          0.00                5     1   \n",
      "3    alex      adam             0.00          0.00               12     0   \n",
      "4    alex      adam             0.00          0.00               12     0   \n",
      "5    alex      adam             0.00          0.00               12     0   \n",
      "6    alex      adam             0.00          0.00               12     0   \n",
      "7    alex      adam             0.00          0.00               12     0   \n",
      "8    alex      adam             0.00          0.00               12     0   \n",
      "9    alex      adam             0.00          0.00               12     0   \n",
      "10   alex      adam             0.00          0.00               12     0   \n",
      "11   alex      adam             0.00          0.00               12     0   \n",
      "12   alex      adam             0.00          0.00               12     0   \n",
      "13   alex      adam             0.00          0.00               12     0   \n",
      "14   alex      adam             0.00          0.00               12     1   \n",
      "15   alex      adam             0.00          0.00               12     1   \n",
      "16   alex      adam             0.00          0.00               16     0   \n",
      "17   alex      adam             0.00          0.00               16     0   \n",
      "18   alex      adam             0.00          0.00               16     0   \n",
      "19   alex      adam             0.00          0.00               16     0   \n",
      "20   alex      adam             0.00          0.00               16     0   \n",
      "21   alex      adam             0.00          0.00               16     0   \n",
      "22   alex      adam             0.00          0.00               16     0   \n",
      "23   alex      adam             0.00          0.00               16     0   \n",
      "24   alex      adam             0.00          0.00               16     0   \n",
      "25   alex      adam             0.00          0.00               16     0   \n",
      "26   alex      adam             0.00          0.00               16     0   \n",
      "27   alex      adam             0.00          0.00               16     1   \n",
      "28   alex      adam             0.00          0.00               16     1   \n",
      "29   alex      adam             0.00          0.00               32     1   \n",
      "..    ...       ...              ...           ...              ...   ...   \n",
      "90   alex      adam             0.00          0.10               12     0   \n",
      "91   alex      adam             0.00          0.10               12     0   \n",
      "92   alex      adam             0.00          0.10               12     0   \n",
      "93   alex      adam             0.00          0.10               12     0   \n",
      "94   alex      adam             0.00          0.10               12     0   \n",
      "95   alex      adam             0.00          0.10               16     0   \n",
      "96   alex      adam             0.00          0.10               16     1   \n",
      "97   alex      adam             0.05          0.05               16     1   \n",
      "98   alex      adam             0.05          0.10               16     1   \n",
      "99   alex      adam             0.10          0.00               32     1   \n",
      "100  alex      adam             0.10          0.05               16     1   \n",
      "101  alex      adam             0.10          0.10               16     1   \n",
      "102  alex      adam             0.15          0.00               32     1   \n",
      "103  alex     nadam             0.00          0.00               32     1   \n",
      "104  alex       sgd             0.00          0.00                3     1   \n",
      "105  alex       sgd             0.00          0.00                5     1   \n",
      "106  alex       sgd             0.00          0.00               12     1   \n",
      "107  alex       sgd             0.00          0.00               16     0   \n",
      "108  alex       sgd             0.00          0.00               16     1   \n",
      "109  alex       sgd             0.00          0.00               32     1   \n",
      "110  alex       sgd             0.00          0.01               16     0   \n",
      "111  alex       sgd             0.00          0.02               16     0   \n",
      "112  alex       sgd             0.15          0.00               32     1   \n",
      "113   vgg      adam             0.00          0.00                8     0   \n",
      "114   vgg      adam             0.00          0.00               12     0   \n",
      "115   vgg      adam             0.00          0.00               16     0   \n",
      "116   vgg      adam             0.00          0.00               16     0   \n",
      "117   vgg      adam             0.00          0.00               24     0   \n",
      "118   vgg      adam             0.00          0.02               16     0   \n",
      "119   vgg      adam             0.00          0.05               16     0   \n",
      "\n",
      "     batch_norm  stride  lrate  flipped  fc_layer1  fc_layer2  \\\n",
      "0             0       1  0.010      0.0        500        500   \n",
      "1             0       1  0.001      0.0        500        500   \n",
      "2             0       1  0.001      0.0        500        500   \n",
      "3             1       1  0.001      0.0        500        500   \n",
      "4             1       1  0.002      0.0        500        500   \n",
      "5             1       1  0.005      0.0        500        500   \n",
      "6             1       1  0.010      0.0        500        500   \n",
      "7             1       1  0.010      1.0        500        500   \n",
      "8             1       1  0.010      1.0        750        750   \n",
      "9             1       1  0.010      1.0       1000       1000   \n",
      "10            1       1  0.015      1.0        500        500   \n",
      "11            1       1  0.015      1.0        750        750   \n",
      "12            1       1  0.015      1.0       1000       1000   \n",
      "13            1       1  0.020      1.0        750        750   \n",
      "14            0       1  0.001      0.0        500        500   \n",
      "15            1       1  0.001      0.0        500        500   \n",
      "16            1       1  0.002      0.0        500        500   \n",
      "17            1       1  0.005      0.0        500        500   \n",
      "18            1       1  0.010      0.0        500        500   \n",
      "19            1       1  0.010      1.0        500        500   \n",
      "20            1       1  0.010      1.0        750        750   \n",
      "21            1       1  0.010      1.0       1000       1000   \n",
      "22            1       1  0.015      1.0        500        500   \n",
      "23            1       1  0.015      1.0        750        750   \n",
      "24            1       1  0.015      1.0       1000       1000   \n",
      "25            1       1  0.020      1.0        750        750   \n",
      "26            1       2  0.001      0.0        500        500   \n",
      "27            0       1  0.001      0.0        500        500   \n",
      "28            1       1  0.001      0.0        500        500   \n",
      "29            0       1  0.001      0.0        500        500   \n",
      "..          ...     ...    ...      ...        ...        ...   \n",
      "90            1       1  0.010      1.0        500        500   \n",
      "91            1       1  0.010      1.0        500        500   \n",
      "92            1       1  0.010      1.0        500        500   \n",
      "93            1       1  0.010      1.0        500        500   \n",
      "94            1       1  0.010      1.0        500        500   \n",
      "95            1       1  0.010      1.0        500        500   \n",
      "96            0       1  0.001      0.0        500        500   \n",
      "97            0       1  0.001      0.0        500        500   \n",
      "98            0       1  0.001      0.0        500        500   \n",
      "99            0       1  0.001      0.0        500        500   \n",
      "100           0       1  0.001      0.0        500        500   \n",
      "101           0       1  0.001      0.0        500        500   \n",
      "102           0       1  0.001      0.0        500        500   \n",
      "103           0       1  0.002      0.0        500        500   \n",
      "104           0       1  0.010      0.0        500        500   \n",
      "105           0       1  0.010      0.0        500        500   \n",
      "106           0       1  0.010      0.0        500        500   \n",
      "107           1       2  0.010      0.0        500        500   \n",
      "108           0       1  0.010      0.0        500        500   \n",
      "109           0       1  0.010      0.0        500        500   \n",
      "110           1       2  0.010      0.0        500        500   \n",
      "111           1       2  0.010      0.0        500        500   \n",
      "112           0       1  0.010      0.0        500        500   \n",
      "113           1       1  0.010      1.0        500        500   \n",
      "114           1       1  0.010      1.0        500        500   \n",
      "115           1       1  0.001      0.0        500        500   \n",
      "116           1       1  0.005      1.0        500        500   \n",
      "117           1       1  0.005      1.0        500        500   \n",
      "118           1       1  0.001      1.0        500        500   \n",
      "119           1       1  0.001      1.0        500        500   \n",
      "\n",
      "                    keypoint   val_RMSE  median_epoch_time  \n",
      "0                        All   1.644353           9.506325  \n",
      "1                        All   1.719329           2.163795  \n",
      "2                        All   1.682283           3.311566  \n",
      "3                        All   1.594904          10.987443  \n",
      "4                        All   1.545263           0.609046  \n",
      "5                        All   1.488751           0.647482  \n",
      "6                        All   1.452622           0.665524  \n",
      "7                        All   1.344899           1.280828  \n",
      "8                        All   1.344505           1.244262  \n",
      "9                        All   1.317546           1.260780  \n",
      "10                       All   1.342092           1.241603  \n",
      "11                       All   1.323635           1.278458  \n",
      "12                       All   1.313110           1.298687  \n",
      "13                       All   1.322215           1.294999  \n",
      "14                       All   1.607822           4.692118  \n",
      "15                       All   1.575011          10.954134  \n",
      "16                       All   1.498317           0.701309  \n",
      "17                       All   1.463689           0.727590  \n",
      "18                       All   1.408915           0.736484  \n",
      "19                       All   1.337524           1.380338  \n",
      "20                       All   1.309395           1.439435  \n",
      "21                       All   1.318624           1.455712  \n",
      "22                       All   1.299829           1.410991  \n",
      "23                       All   1.307998           1.429017  \n",
      "24                       All   1.307468           1.466461  \n",
      "25                       All   1.317572           1.480212  \n",
      "26                       All   1.688898           0.486250  \n",
      "27                       All   1.554097           5.160864  \n",
      "28                       All   1.515688          12.280156  \n",
      "29                       All   1.732766           9.480670  \n",
      "..                       ...        ...                ...  \n",
      "90          right_eye_center   1.940586          10.187203  \n",
      "91    right_eye_inner_corner   1.101775           2.608855  \n",
      "92    right_eye_outer_corner   1.396080           2.809549  \n",
      "93   right_eyebrow_inner_end   1.638469           2.791154  \n",
      "94   right_eyebrow_outer_end   1.716092           2.643472  \n",
      "95                       All   1.242204           1.424104  \n",
      "96                       All   2.882988           5.271692  \n",
      "97                       All   4.365535           5.606160  \n",
      "98                       All   4.592773           5.812181  \n",
      "99                       All   1.677179          10.476731  \n",
      "100                      All  10.596334           5.886947  \n",
      "101                      All   3.070011           5.780764  \n",
      "102                      All   2.442981          10.331106  \n",
      "103                      All   1.863655           9.330733  \n",
      "104                      All   3.143482           2.300698  \n",
      "105                      All        NaN           3.110994  \n",
      "106                      All        NaN           4.461308  \n",
      "107                      All   1.855710           0.433058  \n",
      "108                      All        NaN           4.573561  \n",
      "109                      All   1.710158           9.410767  \n",
      "110                      All   1.703979           0.447956  \n",
      "111                      All   1.660872           0.451321  \n",
      "112                      All        NaN          10.021858  \n",
      "113                      All   1.689379           1.902064  \n",
      "114                      All   1.454214           2.461033  \n",
      "115                      All   1.437625           1.445546  \n",
      "116                      All   1.277219           3.014773  \n",
      "117                      All   1.361362           4.476290  \n",
      "118                      All   1.253744           6.898639  \n",
      "119                      All   1.342900           6.856158  \n",
      "\n",
      "[120 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "def agglomerate_data():\n",
    "    '''\n",
    "    Function used to concatentate results from various sensitivities together\n",
    "    '''\n",
    "    # Read in base data frames\n",
    "    cnn_base_df = pd.read_pickle('OutputData/cnn_base_df.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_base_df['cum_times'] = cnn_base_df.groupby(['optimizer']).times.cumsum()\n",
    "    # Adjust data for consistency with other dataframes\n",
    "    cnn_base_df['starting_filter'] = 32\n",
    "    cnn_base_df['dropout_initial'] = 0\n",
    "    cnn_base_df['dropout_step'] = 0\n",
    "    cnn_base_df['bias'] = 1\n",
    "    cnn_base_df['batch_norm'] = 0\n",
    "    cnn_base_df['stride'] = 1\n",
    "    # Add flipped flag\n",
    "    cnn_base_df['flipped'] = 0\n",
    "    cnn_base_df['fc_layer1'], cnn_base_df['fc_layer2'] = cnn_base_df.fc_layer, cnn_base_df.fc_layer\n",
    "    cnn_base_df['arch'] = 'alex'\n",
    "    cnn_base_df['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in filter data frames\n",
    "    cnn_filter_df = pd.read_pickle('OutputData/cnn_filter_df.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_filter_df['cum_times'] = cnn_filter_df.groupby(['starting_filter', 'optimizer']).times.cumsum()\n",
    "    # Adjust data for consistency with other dataframes\n",
    "    cnn_filter_df['dropout_initial'] = 0\n",
    "    cnn_filter_df['dropout_step'] = 0\n",
    "    cnn_filter_df['bias'] = 1\n",
    "    cnn_filter_df['batch_norm'] = 0\n",
    "    cnn_filter_df['stride'] = 1\n",
    "    # Add flipped flag\n",
    "    cnn_filter_df['flipped'] = 0\n",
    "    cnn_filter_df['fc_layer1'], cnn_filter_df['fc_layer2'] = cnn_filter_df.fc_layer, cnn_filter_df.fc_layer\n",
    "    cnn_filter_df['arch'] = 'alex'\n",
    "    cnn_filter_df['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in dropout data frames\n",
    "    cnn_do_df = pd.read_pickle('OutputData/cnn_dropout_df.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_do_df['cum_times'] = cnn_do_df.groupby(['dropout', 'optimizer']).times.cumsum()\n",
    "    # Adjust data for consistency with other dataframes\n",
    "    cnn_do_df['dropout_initial'] = cnn_do_df.dropout\n",
    "    cnn_do_df['dropout_step'] = 0\n",
    "    cnn_do_df = cnn_do_df.drop(['dropout'], axis=1)\n",
    "    cnn_do_df['bias'] = 1\n",
    "    cnn_do_df['batch_norm'] = 0\n",
    "    cnn_do_df['stride'] = 1\n",
    "    # Add flipped flag\n",
    "    cnn_do_df['flipped'] = 0\n",
    "    cnn_do_df['fc_layer1'], cnn_do_df['fc_layer2'] = cnn_do_df.fc_layer, cnn_do_df.fc_layer\n",
    "    cnn_do_df['arch'] = 'alex'\n",
    "    cnn_do_df['keypoint'] = \"All\"\n",
    "   \n",
    "    # Second dropout dataframe\n",
    "    cnn_do2_df = pd.read_pickle('OutputData/cnn_dropout_df2.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_do2_df['cum_times'] = cnn_do2_df.groupby(['dropout', 'optimizer']).times.cumsum()\n",
    "    # Adjust data for consistency with other dataframes\n",
    "    cnn_do2_df['dropout_initial'] = cnn_do2_df.dropout\n",
    "    cnn_do2_df['dropout_step'] = 0\n",
    "    cnn_do2_df = cnn_do2_df.drop(['dropout'], axis=1)\n",
    "    cnn_do2_df['bias'] = 1\n",
    "    cnn_do2_df['batch_norm'] = 0\n",
    "    cnn_do2_df['stride'] = 1\n",
    "    # Add flipped flag\n",
    "    cnn_do2_df['flipped'] = 0\n",
    "    cnn_do2_df['fc_layer1'], cnn_do2_df['fc_layer2'] = cnn_do2_df.fc_layer, cnn_do2_df.fc_layer\n",
    "    cnn_do2_df['arch'] = 'alex'\n",
    "    cnn_do2_df['keypoint'] = \"All\"\n",
    "    \n",
    "    # Dropout with changing step dataframe\n",
    "    cnn_do3_df = pd.read_pickle('OutputData/cnn_dropout_step_df.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_do3_df['cum_times'] = cnn_do3_df.groupby(['dropout_initial', 'dropout_step', 'optimizer']).times.cumsum()\n",
    "    # Adjust data for consistency with other dataframes\n",
    "    cnn_do3_df['bias'] = 1\n",
    "    cnn_do3_df['batch_norm'] = 0\n",
    "    cnn_do3_df['stride'] = 1\n",
    "    # Add flipped flag\n",
    "    cnn_do3_df['flipped'] = 0\n",
    "    cnn_do3_df['fc_layer1'], cnn_do3_df['fc_layer2'] = cnn_do3_df.fc_layer, cnn_do3_df.fc_layer\n",
    "    cnn_do3_df['arch'] = 'alex'\n",
    "    cnn_do3_df['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_bn_df = pd.read_pickle('OutputData/cnn_dropout_bn_df.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_bn_df['cum_times'] = cnn_bn_df.groupby(['bias', 'starting_filter', 'dropout_initial', 'dropout_step']).times.cumsum()\n",
    "    cnn_bn_df['stride'] = 1\n",
    "    # Add flipped flag\n",
    "    cnn_bn_df['flipped'] = 0\n",
    "    cnn_bn_df['fc_layer1'], cnn_bn_df['fc_layer2'] = cnn_bn_df.fc_layer, cnn_bn_df.fc_layer\n",
    "    cnn_bn_df['arch'] = 'alex'\n",
    "    cnn_bn_df['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_stride_df = pd.read_pickle('OutputData/cnn_stride_df.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_stride_df['cum_times'] = cnn_stride_df.groupby(['optimizer', 'dropout_initial', 'dropout_step']).times.cumsum()\n",
    "    # Add flipped flag\n",
    "    cnn_stride_df['flipped'] = 0\n",
    "    cnn_stride_df['fc_layer1'], cnn_stride_df['fc_layer2'] = cnn_stride_df.fc_layer, cnn_stride_df.fc_layer\n",
    "    cnn_stride_df['arch'] = 'alex'\n",
    "    cnn_stride_df['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_lr_df = pd.read_pickle('OutputData/cnn_lr_df.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_lr_df['cum_times'] = cnn_lr_df.groupby(['starting_filter', 'dropout_initial', 'dropout_step', 'lrate']).times.cumsum()\n",
    "    # Add flipped flag\n",
    "    cnn_lr_df['flipped'] = 0\n",
    "    cnn_lr_df['arch'] = 'alex'\n",
    "    cnn_lr_df['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_flipped_df = pd.read_pickle('OutputData/cnn_flipped_df.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_flipped_df['cum_times'] = cnn_flipped_df.groupby(['starting_filter', 'dropout_initial', 'dropout_step', 'lrate']).times.cumsum()   \n",
    "    cnn_flipped_df['arch'] = 'alex'\n",
    "    cnn_flipped_df['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_flipped_df2 = pd.read_pickle('OutputData/cnn_flipped_df2.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_flipped_df2['cum_times'] = cnn_flipped_df2.groupby(['starting_filter', 'dropout_initial', 'dropout_step', 'lrate']).times.cumsum()      \n",
    "    cnn_flipped_df2['arch'] = 'alex'\n",
    "    cnn_flipped_df2['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_flipped_df3 = pd.read_pickle('OutputData/cnn_flipped_df3.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_flipped_df3['cum_times'] = cnn_flipped_df3.groupby(['starting_filter', 'dropout_initial', 'dropout_step', 'lrate']).times.cumsum()   \n",
    "    cnn_flipped_df3['arch'] = 'alex'\n",
    "    cnn_flipped_df3['keypoint'] = \"All\"\n",
    "\n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_flipped_df4 = pd.read_pickle('OutputData/cnn_flipped_df4.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_flipped_df4['cum_times'] = cnn_flipped_df4.groupby(['starting_filter', 'dropout_initial', 'dropout_step', 'lrate']).times.cumsum()   \n",
    "    cnn_flipped_df4['arch'] = 'alex'\n",
    "    cnn_flipped_df4['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_flipped_df5 = pd.read_pickle('OutputData/cnn_flipped_df5.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_flipped_df5['cum_times'] = cnn_flipped_df5.groupby(['starting_filter', 'dropout_initial', 'dropout_step', 'lrate']).times.cumsum()   \n",
    "    cnn_flipped_df5['fc_layer2'] = 500\n",
    "    cnn_flipped_df5['arch'] = 'alex'\n",
    "    cnn_flipped_df5['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_flipped_df6 = pd.read_pickle('OutputData/cnn_flipped_df6.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_flipped_df6['cum_times'] = cnn_flipped_df6.groupby(['starting_filter', 'dropout_initial', 'dropout_step', 'lrate']).times.cumsum()   \n",
    "    cnn_flipped_df6['arch'] = 'alex'\n",
    "    cnn_flipped_df6['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_vgg_df = pd.read_pickle('OutputData/cnn_vgg_df.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_vgg_df['cum_times'] = cnn_vgg_df.groupby(['starting_filter', 'dropout_initial', 'dropout_step', 'lrate']).times.cumsum()   \n",
    "    cnn_vgg_df['fc_layer1'] = 500\n",
    "    cnn_vgg_df['fc_layer2'] = 500\n",
    "    cnn_vgg_df['flipped'] = 0\n",
    "    cnn_vgg_df['stride'] = 1\n",
    "    cnn_vgg_df['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_vgg_flipped_df = pd.read_pickle('OutputData/cnn_vgg_flipped_df.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_vgg_flipped_df['cum_times'] = cnn_vgg_flipped_df.groupby(['starting_filter', 'dropout_initial', 'dropout_step', 'lrate']).times.cumsum()   \n",
    "    cnn_vgg_flipped_df['fc_layer1'] = 500\n",
    "    cnn_vgg_flipped_df['fc_layer2'] = 500\n",
    "    cnn_vgg_flipped_df['stride'] = 1\n",
    "    cnn_vgg_flipped_df['flipped'] = 1\n",
    "    cnn_vgg_flipped_df['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_vgg_flipped_df2 = pd.read_pickle('OutputData/cnn_vgg_flipped2_df.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_vgg_flipped_df2['cum_times'] = cnn_vgg_flipped_df2.groupby(['starting_filter', 'dropout_initial', 'dropout_step', 'lrate']).times.cumsum()   \n",
    "    cnn_vgg_flipped_df2['fc_layer1'] = 500\n",
    "    cnn_vgg_flipped_df2['fc_layer2'] = 500\n",
    "    cnn_vgg_flipped_df2['stride'] = 1\n",
    "    cnn_vgg_flipped_df2['flipped'] = 1\n",
    "    cnn_vgg_flipped_df2['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_vgg_flipped_df3 = pd.read_pickle('OutputData/cnn_vgg_flipped3_df.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_vgg_flipped_df3['cum_times'] = cnn_vgg_flipped_df3.groupby(['starting_filter', 'dropout_initial', 'dropout_step', 'lrate']).times.cumsum()   \n",
    "    cnn_vgg_flipped_df3['fc_layer1'] = 500\n",
    "    cnn_vgg_flipped_df3['fc_layer2'] = 500\n",
    "    cnn_vgg_flipped_df3['stride'] = 1\n",
    "    cnn_vgg_flipped_df3['flipped'] = 1\n",
    "    cnn_vgg_flipped_df3['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Batch Normalization Data\n",
    "    cnn_vgg_flipped_df4 = pd.read_pickle('OutputData/cnn_vgg_flipped4_df.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_vgg_flipped_df4['cum_times'] = cnn_vgg_flipped_df4.groupby(['starting_filter', 'dropout_initial', 'dropout_step', 'lrate']).times.cumsum()   \n",
    "    cnn_vgg_flipped_df4['fc_layer1'] = 500\n",
    "    cnn_vgg_flipped_df4['fc_layer2'] = 500\n",
    "    cnn_vgg_flipped_df4['stride'] = 1\n",
    "    cnn_vgg_flipped_df4['flipped'] = 1\n",
    "    cnn_vgg_flipped_df4['keypoint'] = \"All\"\n",
    "    \n",
    "    # Read in Specialist Model Data\n",
    "    cnn_spec01_df = pd.read_pickle('OutputData/spec_01.pkl')\n",
    "    # Add cumulative times to dataframe\n",
    "    cnn_spec01_df['cum_times'] = cnn_spec01_df.groupby(['keypoint']).times.cumsum()   \n",
    "    cnn_spec01_df['arch'] = 'alex'\n",
    "    \n",
    "    cnn_total_df = pd.concat([cnn_spec01_df,\n",
    "                              cnn_vgg_df, cnn_vgg_flipped_df, cnn_vgg_flipped_df2, cnn_vgg_flipped_df3, cnn_vgg_flipped_df4,\n",
    "                              cnn_flipped_df6, cnn_flipped_df5, cnn_flipped_df4, cnn_flipped_df3, cnn_flipped_df2,\n",
    "                              cnn_lr_df, cnn_stride_df, cnn_bn_df, cnn_do_df,\n",
    "                              cnn_do2_df, cnn_do3_df, cnn_filter_df,\n",
    "                              cnn_base_df], sort=True)\n",
    "    cnn_total_df.to_pickle('OutputData/integrated_cnn_df.pkl')\n",
    "    \n",
    "    groups = cnn_total_df.groupby(\n",
    "        ['arch','optimizer', 'dropout_initial', 'dropout_step', 'starting_filter',\n",
    "         'bias', 'batch_norm', 'stride', 'lrate', 'flipped', 'fc_layer1', 'fc_layer2', 'keypoint'])\n",
    "    \n",
    "    # Grab min errors\n",
    "    mins = groups.val_RMSE.min().reset_index() \n",
    "    \n",
    "    # Grab median epoch training time\n",
    "    meds = groups.times.median().reset_index()\n",
    "    \n",
    "    # Insert the median times into the min error dataframe\n",
    "    mins['median_epoch_time'] = meds.times\n",
    "    print(mins)    \n",
    "    \n",
    "    # Write the aggregated dataframe to Output\n",
    "    mins.to_pickle('OutputData/aggregated_cnn_df.pkl')\n",
    "    \n",
    "    return cnn_total_df, mins\n",
    "    \n",
    "cnn_df, min_df = agglomerate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a plotting function for our cnn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plotting function to pass to the interact widget function\n",
    "def plot_cnn_data(df=cnn_df,\n",
    "                  optimizer = cnn_df.optimizer.unique(), \n",
    "                  activation = cnn_df.activation.unique(),\n",
    "                  starting_filter = cnn_df.starting_filter.unique(),\n",
    "                  dropout_initial = cnn_df.dropout_initial.unique(),\n",
    "                  dropout_step = cnn_df.dropout_step.unique(),\n",
    "                  bias = cnn_df.bias.unique(),\n",
    "                  batch_norm = cnn_df.batch_norm.unique(),\n",
    "                  stride= cnn_df.stride.unique(),\n",
    "                  lrate = cnn_df.lrate.unique(),\n",
    "                  fc1 = cnn_df.fc_layer1.unique(),\n",
    "                  fc2 = cnn_df.fc_layer2.unique(),\n",
    "                  flipped = [0,1],\n",
    "#                   arch = cnn_df.arch.unique(),\n",
    "                  keypoint = cnn_df.keypoint.unique(),\n",
    "                  ymax=100,\n",
    "                  timemax=1000):\n",
    "    \n",
    "    sub_df = df[df.activation==activation]\n",
    "    sub_df = df[df.optimizer==optimizer]\n",
    "    sub_df = sub_df[sub_df.starting_filter==starting_filter]\n",
    "    sub_df = sub_df[sub_df.dropout_initial == dropout_initial]\n",
    "    sub_df = sub_df[sub_df.dropout_step == dropout_step]\n",
    "    sub_df = sub_df[sub_df.bias == bias]\n",
    "    sub_df = sub_df[sub_df.batch_norm == batch_norm]\n",
    "    sub_df = sub_df[sub_df.stride == stride]\n",
    "    sub_df = sub_df[sub_df.lrate == lrate]\n",
    "    sub_df = sub_df[sub_df.fc_layer1 == fc1]  \n",
    "    sub_df = sub_df[sub_df.fc_layer2 == fc2] \n",
    "    sub_df = sub_df[sub_df.flipped == flipped]\n",
    "#     sub_df = sub_df[sub_df.arch == arch]\n",
    "    sub_df = sub_df[sub_df.keypoint == keypoint]\n",
    "    \n",
    "    # Group the neural net data by optimizer\n",
    "    groups = sub_df.groupby(['arch'])\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over the grouped data and plot out epoch timing and validation loss data\n",
    "    for name, group in groups:\n",
    "        \n",
    "        # Plot training and validation losses by epoch\n",
    "        axes[0].plot(group.epoch, group.val_RMSE, label=str(name)+' Validation Loss')\n",
    "#         axes[0].scatter(group.epoch, group.RMSE, label=str(name)+' Training Loss')\n",
    "#         axes[0].text(group.epoch.iloc[len(group)//4]-10,group.val_RMSE.iloc[len(group)//4]+0.25,\n",
    "#                      '25% RMSE: {:.2f}'.format(group.val_RMSE.iloc[len(group)//4]), fontsize=12)\n",
    "#         axes[0].text(group.epoch.iloc[2*len(group)//4]-10,group.val_RMSE.iloc[2*len(group)//4]+0.25,\n",
    "#                      '50% RMSE: {:.2f}'.format(group.val_RMSE.iloc[2*len(group)//4]), fontsize=12)        \n",
    "#         axes[0].text(group.epoch.iloc[3*len(group)//4]-10,group.val_RMSE.iloc[3*len(group)//4]+0.25,\n",
    "#                      '75% RMSE: {:.2f}'.format(group.val_RMSE.iloc[3*len(group)//4]), fontsize=12)        \n",
    "#         axes[0].text(group.epoch.iloc[-1]-10,group.val_RMSE.iloc[-1]+0.25,\n",
    "#                      'Final RMSE: {:.2f}'.format(group.val_RMSE.iloc[-1]), fontsize=12)\n",
    "        axes[0].set_ylim([0,ymax])\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Root Mean Square Error')\n",
    "#         axes[0].set_title(\"{} Optimizer and {} Activation\".format(group.optimizer.unique(), group.activation.unique()))\n",
    "        \n",
    "        # Plot train time by epoch\n",
    "        axes[1].scatter(group.epoch, group.times, label=str(name)+' Fit Time')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Fit Time (seconds)')\n",
    "        axes[1].set_ylim([0,timemax/group.epoch.max()])\n",
    "        axes[1].legend()\n",
    "        \n",
    "        # Plot cumulative training time\n",
    "        axes[2].plot(group.epoch, group.cum_times, label=str(name)+' Fit Time (s)', lw=4)\n",
    "        axes[2].set_xlabel('Epoch')\n",
    "        axes[2].set_ylabel('Cumulative Fit Time (seconds)')\n",
    "        axes[2].grid(b=True)\n",
    "        axes[2].set_ylim([0,timemax])\n",
    "        axes[2].legend()\n",
    "        \n",
    "        # Plot cumulative validation loss by cumulative time\n",
    "        axes[3].plot(group.cum_times, group.val_RMSE, label=str(name)+' Validation Loss', lw=3)\n",
    "        axes[3].set_xlabel('Cumulative Fit Time (seconds)')\n",
    "        axes[3].set_ylabel('Validation RMSE')\n",
    "        axes[3].set_xlim([0,timemax])\n",
    "        axes[3].set_ylim([0,ymax])\n",
    "        axes[3].legend()\n",
    "    \n",
    "    # Add line for knr score\n",
    "    axes[0].axhline(2.49, label='kNR Score', lw=5, c='k')\n",
    "    axes[3].axhline(2.49, label='kNR Score', lw=5, c='k')\n",
    "\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Adjust the spacing of the subplots\n",
    "    fig.subplots_adjust(left=0.03, right=0.97, hspace=0.1, wspace=0.15)\n",
    "\n",
    "    # Add an overarching title for these plots\n",
    "    fig.suptitle(\"Performance Comparison for Baseline Convolutional Neural Nets\",\n",
    "                 fontsize=18, y=0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e3f2771325499384b09442b1299bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='optimizer', options=('adam', 'sgd', 'nadam', 'adagrad'), value='ad…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Plot the relu learning rate data\n",
    "interact_manual(plot_cnn_data, df=fixed(cnn_df), \n",
    "                optimizer = cnn_df.optimizer.unique(), \n",
    "                activation = cnn_df.activation.unique(),\n",
    "                starting_filter = cnn_df.starting_filter.unique(),\n",
    "                dropout_initial = cnn_df.dropout_initial.unique(),\n",
    "                dropout_step = cnn_df.dropout_step.unique(),\n",
    "                bias = cnn_df.bias.unique(),\n",
    "                batch_norm = cnn_df.batch_norm.unique(),\n",
    "                stride= cnn_df.stride.unique(),\n",
    "                lrate = cnn_df.lrate.unique(),\n",
    "                fc1 = cnn_df.fc_layer1.unique(),\n",
    "                fc2 = cnn_df.fc_layer2.unique(),\n",
    "#                 arch = cnn_df.arch.unique(),\n",
    "                keypoint = cnn_df.keypoint.unique(),\n",
    "                flipped = [0,1],\n",
    "                ymax=5,\n",
    "                timemax=1000)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment of Initial CNN Results\n",
    "\n",
    "- Base results for our Neural Net achieved a final RMSE of 1.80 using no dropout, and 32 initial filter depth and alternative convolution and pooling layers. The runtime for this model was about 1900 seconds for training.\n",
    "- Results of the filter study indicate that similar or better accuracies can be achieved with a lower initial filter depth while also reducing training time significantly over the base case.  For example the following results were observed:\n",
    "    - Filter Depth of  3, Validation RMSE: 1.74, Training Time: 450 seconds, a little more noise in validation\n",
    "    - Filter Depth of  5, Validation RMSE: 1.71, Training Time: 700 seconds, better behaved validation\n",
    "    - Filter Depth of 12, Validation RMSE: 1.64, Training Time: 950 seconds, more overfit\n",
    "    - Filter Depth of 16, Validation RMSE: 1.57, Training Time: 1100 seconds, most overfit\n",
    "    - Filter Depth of 32, Validation RMSE: 1.80, Training Time: 1900 seconds, Baseline.\n",
    "    We should use a filter depth of 16 for further studies, but to optimize for time the filter depth of 3 or 5 may also be viable\n",
    "- Initial assessments of flat dropout rates of 0.1 and 0.15 do not appear to show favorable behavior. Validation accuracy is very noisy and begins to grow after reaching a minimum. We should look at a more gradual dropout process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"3830\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"3830\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"3830\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '3830' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"3830\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"3830\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"3830\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '3830' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"3830\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook, push_notebook, curdoc\n",
    "from bokeh.models import ColumnDataSource, Select, HoverTool\n",
    "from bokeh.models.widgets import Slider, Select, TextInput\n",
    "from bokeh.layouts import row, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(cnn_df.reset_index())\n",
    "\n",
    "p = figure(tools=\"pan, zoom_in, zoom_out, box_zoom, crosshair, lasso_select, box_select\")\n",
    "p.grid.visible = True\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [\n",
    "    (\"Validation RMSE\", \"@val_RMSE\"),\n",
    "    (\"Learning Rate\", \"@lrate\"),\n",
    "    (\"Dropout Step\", \"@dropout_step\"),\n",
    "    (\"Starting Filter Depth\", \"@starting_filter\")\n",
    "]\n",
    "p.circle(x='index',y='val_RMSE', source=source, size='starting_filter', \n",
    "          fill_color=linear_cmap('lrate', 'Viridis256', 0, max(min_df.lrate.min(), min_df.lrate.max())))\n",
    "p.add_tools(hover)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>activation</th>\n",
       "      <th>arch</th>\n",
       "      <th>batch_norm</th>\n",
       "      <th>bias</th>\n",
       "      <th>cum_times</th>\n",
       "      <th>dropout_initial</th>\n",
       "      <th>dropout_step</th>\n",
       "      <th>epoch</th>\n",
       "      <th>fc_layer</th>\n",
       "      <th>...</th>\n",
       "      <th>lrate</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>pooling</th>\n",
       "      <th>starting_filter</th>\n",
       "      <th>stride</th>\n",
       "      <th>times</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44.880259</td>\n",
       "      <td>relu</td>\n",
       "      <td>vgg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.343745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2014.237671</td>\n",
       "      <td>adam</td>\n",
       "      <td>yes</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>4.343745</td>\n",
       "      <td>44.349447</td>\n",
       "      <td>1966.873270</td>\n",
       "      <td>1966.873413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.735587</td>\n",
       "      <td>relu</td>\n",
       "      <td>vgg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.744449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>944.676331</td>\n",
       "      <td>adam</td>\n",
       "      <td>yes</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1.400704</td>\n",
       "      <td>33.484920</td>\n",
       "      <td>1121.239891</td>\n",
       "      <td>1121.239868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.178155</td>\n",
       "      <td>relu</td>\n",
       "      <td>vgg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.170035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>330.445312</td>\n",
       "      <td>adam</td>\n",
       "      <td>yes</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1.425586</td>\n",
       "      <td>18.197102</td>\n",
       "      <td>331.134516</td>\n",
       "      <td>331.134521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.248165</td>\n",
       "      <td>relu</td>\n",
       "      <td>vgg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.580605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>105.024895</td>\n",
       "      <td>adam</td>\n",
       "      <td>yes</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1.410569</td>\n",
       "      <td>16.980343</td>\n",
       "      <td>288.332089</td>\n",
       "      <td>288.332062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.631339</td>\n",
       "      <td>relu</td>\n",
       "      <td>vgg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.983911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>43.974663</td>\n",
       "      <td>adam</td>\n",
       "      <td>yes</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1.403307</td>\n",
       "      <td>6.111427</td>\n",
       "      <td>37.349535</td>\n",
       "      <td>37.349537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RMSE activation arch  batch_norm  bias  cum_times  dropout_initial  \\\n",
       "0  44.880259       relu  vgg           1     0   4.343745              0.0   \n",
       "1  30.735587       relu  vgg           1     0   5.744449              0.0   \n",
       "2  18.178155       relu  vgg           1     0   7.170035              0.0   \n",
       "3  10.248165       relu  vgg           1     0   8.580605              0.0   \n",
       "4   6.631339       relu  vgg           1     0   9.983911              0.0   \n",
       "\n",
       "   dropout_step  epoch  fc_layer  ...  lrate  mean_squared_error  optimizer  \\\n",
       "0           0.0      0     500.0  ...  0.001         2014.237671       adam   \n",
       "1           0.0      1     500.0  ...  0.001          944.676331       adam   \n",
       "2           0.0      2     500.0  ...  0.001          330.445312       adam   \n",
       "3           0.0      3     500.0  ...  0.001          105.024895       adam   \n",
       "4           0.0      4     500.0  ...  0.001           43.974663       adam   \n",
       "\n",
       "   pooling  starting_filter  stride     times   val_RMSE     val_loss  \\\n",
       "0      yes               16       1  4.343745  44.349447  1966.873270   \n",
       "1      yes               16       1  1.400704  33.484920  1121.239891   \n",
       "2      yes               16       1  1.425586  18.197102   331.134516   \n",
       "3      yes               16       1  1.410569  16.980343   288.332089   \n",
       "4      yes               16       1  1.403307   6.111427    37.349535   \n",
       "\n",
       "   val_mean_squared_error  \n",
       "0             1966.873413  \n",
       "1             1121.239868  \n",
       "2              331.134521  \n",
       "3              288.332062  \n",
       "4               37.349537  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
